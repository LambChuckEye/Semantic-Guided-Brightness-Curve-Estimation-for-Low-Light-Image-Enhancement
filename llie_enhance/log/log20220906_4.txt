Namespace(gpu_id=0, img_path='/home/wsz/workspace/Data/LOLdataset/our485_patch/low/', img_val_path='/home/wsz/workspace/Data/LOLdataset/eval15/low/', batch_size=4, lr=0.0002, weight_decay=0.0004, pretrain_dir='/home/wsz/workspace/Illumination-Adaptive-Transformer/IAT_enhance/workdirs/snapshots_folder_lol_v1_patch_edit_zero_with_bias_220906/best_Epoch.pth', num_epochs=200, display_iter=10, snapshots_folder='workdirs/snapshots_folder_lol_v1_patch_edit_zero_with_bias_220906')
Total examples: 4850
Total examples: 15
the device is: cuda:0
######## Start IAT Training #########
the epoch is: 0
Loss at iteration 10 : 0.07481595873832703
Loss at iteration 20 : 0.17804495990276337
Loss at iteration 30 : 0.06804612278938293
Loss at iteration 40 : 0.13253580033779144
Loss at iteration 50 : 0.09717051684856415
Loss at iteration 60 : 0.10231687873601913
Loss at iteration 70 : 0.06912633776664734
Loss at iteration 80 : 0.08195703476667404
Loss at iteration 90 : 0.1559850573539734
Loss at iteration 100 : 0.11037170141935349
Loss at iteration 110 : 0.08254723250865936
Loss at iteration 120 : 0.09332015365362167
Loss at iteration 130 : 0.11935099959373474
Loss at iteration 140 : 0.09821297973394394
Loss at iteration 150 : 0.08468540012836456
Loss at iteration 160 : 0.1629405915737152
Loss at iteration 170 : 0.08597569912672043
Loss at iteration 180 : 0.0756186693906784
Loss at iteration 190 : 0.10987932980060577
Loss at iteration 200 : 0.09013186395168304
Loss at iteration 210 : 0.11679061502218246
Loss at iteration 220 : 0.09433631598949432
Loss at iteration 230 : 0.09222124516963959
Loss at iteration 240 : 0.14862442016601562
Loss at iteration 250 : 0.08710888028144836
Loss at iteration 260 : 0.05065333843231201
Loss at iteration 270 : 0.11193393170833588
Loss at iteration 280 : 0.11228685826063156
Loss at iteration 290 : 0.0696478933095932
Loss at iteration 300 : 0.07264963537454605
Loss at iteration 310 : 0.17461681365966797
Loss at iteration 320 : 0.11196395009756088
Loss at iteration 330 : 0.06787823140621185
Loss at iteration 340 : 0.08856859803199768
Loss at iteration 350 : 0.07026480883359909
Loss at iteration 360 : 0.09051047265529633
Loss at iteration 370 : 0.07887642830610275
Loss at iteration 380 : 0.11002682149410248
Loss at iteration 390 : 0.06830708682537079
Loss at iteration 400 : 0.103940948843956
Loss at iteration 410 : 0.050994873046875
Loss at iteration 420 : 0.1163972020149231
Loss at iteration 430 : 0.10232498496770859
Loss at iteration 440 : 0.05516299605369568
Loss at iteration 450 : 0.11213994026184082
Loss at iteration 460 : 0.08735798299312592
Loss at iteration 470 : 0.07037487626075745
Loss at iteration 480 : 0.13148748874664307
Loss at iteration 490 : 0.06464891135692596
Loss at iteration 500 : 0.09736105054616928
Loss at iteration 510 : 0.09462838619947433
Loss at iteration 520 : 0.09360016882419586
Loss at iteration 530 : 0.1290942132472992
Loss at iteration 540 : 0.12813496589660645
Loss at iteration 550 : 0.06327114999294281
Loss at iteration 560 : 0.12231680005788803
Loss at iteration 570 : 0.049532514065504074
Loss at iteration 580 : 0.08756758272647858
Loss at iteration 590 : 0.07425280660390854
Loss at iteration 600 : 0.07403566688299179
Loss at iteration 610 : 0.16963613033294678
Loss at iteration 620 : 0.09944288432598114
Loss at iteration 630 : 0.08267930150032043
Loss at iteration 640 : 0.08084353059530258
Loss at iteration 650 : 0.0463314950466156
Loss at iteration 660 : 0.07062875479459763
Loss at iteration 670 : 0.10149773955345154
Loss at iteration 680 : 0.11925104260444641
Loss at iteration 690 : 0.08052200078964233
Loss at iteration 700 : 0.09114386886358261
Loss at iteration 710 : 0.10359382629394531
Loss at iteration 720 : 0.14648744463920593
Loss at iteration 730 : 0.14752720296382904
Loss at iteration 740 : 0.08034327626228333
Loss at iteration 750 : 0.06297965347766876
Loss at iteration 760 : 0.08146330714225769
Loss at iteration 770 : 0.11228781938552856
Loss at iteration 780 : 0.1642419397830963
Loss at iteration 790 : 0.07925376296043396
Loss at iteration 800 : 0.15536633133888245
Loss at iteration 810 : 0.06608371436595917
Loss at iteration 820 : 0.08866213262081146
Loss at iteration 830 : 0.06654870510101318
Loss at iteration 840 : 0.10790780186653137
Loss at iteration 850 : 0.0668732151389122
Loss at iteration 860 : 0.11717711389064789
Loss at iteration 870 : 0.08383878320455551
Loss at iteration 880 : 0.08478944003582001
Loss at iteration 890 : 0.12754201889038086
Loss at iteration 900 : 0.15058103203773499
Loss at iteration 910 : 0.10292769968509674
Loss at iteration 920 : 0.07547882199287415
Loss at iteration 930 : 0.0899503082036972
Loss at iteration 940 : 0.13022702932357788
Loss at iteration 950 : 0.12744063138961792
Loss at iteration 960 : 0.07887327671051025
Loss at iteration 970 : 0.05438981205224991
Loss at iteration 980 : 0.10600628703832626
Loss at iteration 990 : 0.09301536530256271
Loss at iteration 1000 : 0.10312247276306152
Loss at iteration 1010 : 0.06466303765773773
Loss at iteration 1020 : 0.08947907388210297
Loss at iteration 1030 : 0.07504923641681671
Loss at iteration 1040 : 0.09756140410900116
Loss at iteration 1050 : 0.11659395694732666
Loss at iteration 1060 : 0.10315820574760437
Loss at iteration 1070 : 0.0813995823264122
Loss at iteration 1080 : 0.13047699630260468
Loss at iteration 1090 : 0.16089634597301483
Loss at iteration 1100 : 0.06530391424894333
Loss at iteration 1110 : 0.1654990315437317
Loss at iteration 1120 : 0.12185359001159668
Loss at iteration 1130 : 0.11698462814092636
Loss at iteration 1140 : 0.10810017585754395
Loss at iteration 1150 : 0.10528367757797241
Loss at iteration 1160 : 0.10474055260419846
Loss at iteration 1170 : 0.13013526797294617
Loss at iteration 1180 : 0.10357439517974854
Loss at iteration 1190 : 0.11285381019115448
Loss at iteration 1200 : 0.10817794501781464
Loss at iteration 1210 : 0.14261916279792786
The SSIM Value is: 0.6726431846618652
The PSNR Value is: 19.909934425354002
the highest SSIM value is: 19.909934425354002
the epoch is: 1
Loss at iteration 10 : 0.07965333759784698
Loss at iteration 20 : 0.09726953506469727
Loss at iteration 30 : 0.10885745286941528
Loss at iteration 40 : 0.07234343886375427
Loss at iteration 50 : 0.10457690060138702
Loss at iteration 60 : 0.15392160415649414
Loss at iteration 70 : 0.11321746557950974
Loss at iteration 80 : 0.0712767243385315
Loss at iteration 90 : 0.10382639616727829
Loss at iteration 100 : 0.1291864514350891
Loss at iteration 110 : 0.11510654538869858
Loss at iteration 120 : 0.1371687799692154
Loss at iteration 130 : 0.09786395728588104
Loss at iteration 140 : 0.13119935989379883
Loss at iteration 150 : 0.08897833526134491
Loss at iteration 160 : 0.10210675001144409
Loss at iteration 170 : 0.09657037258148193
Loss at iteration 180 : 0.0907256230711937
Loss at iteration 190 : 0.12147568166255951
Loss at iteration 200 : 0.08468436449766159
Loss at iteration 210 : 0.10554127395153046
Loss at iteration 220 : 0.08755771815776825
Loss at iteration 230 : 0.08783363550901413
Loss at iteration 240 : 0.05075230821967125
Loss at iteration 250 : 0.16009992361068726
Loss at iteration 260 : 0.1176164373755455
Loss at iteration 270 : 0.0774606391787529
Loss at iteration 280 : 0.09406055510044098
Loss at iteration 290 : 0.11604785919189453
Loss at iteration 300 : 0.13685795664787292
Loss at iteration 310 : 0.10895127058029175
Loss at iteration 320 : 0.12050514668226242
Loss at iteration 330 : 0.10082107782363892
Loss at iteration 340 : 0.08252894133329391
Loss at iteration 350 : 0.09924235194921494
Loss at iteration 360 : 0.09016724675893784
Loss at iteration 370 : 0.12575861811637878
Loss at iteration 380 : 0.07615555822849274
Loss at iteration 390 : 0.09006349742412567
Loss at iteration 400 : 0.10262636840343475
Loss at iteration 410 : 0.08399218320846558
Loss at iteration 420 : 0.15505895018577576
Loss at iteration 430 : 0.09936293214559555
Loss at iteration 440 : 0.06355157494544983
Loss at iteration 450 : 0.10000914335250854
Loss at iteration 460 : 0.13240517675876617
Loss at iteration 470 : 0.12484099715948105
Loss at iteration 480 : 0.07975396513938904
Loss at iteration 490 : 0.15667012333869934
Loss at iteration 500 : 0.09416641294956207
Loss at iteration 510 : 0.07219210267066956
Loss at iteration 520 : 0.13304808735847473
Loss at iteration 530 : 0.12543247640132904
Loss at iteration 540 : 0.11638393253087997
Loss at iteration 550 : 0.10565515607595444
Loss at iteration 560 : 0.06150391697883606
Loss at iteration 570 : 0.08164193481206894
Loss at iteration 580 : 0.10691063851118088
Loss at iteration 590 : 0.15049681067466736
Loss at iteration 600 : 0.07815294712781906
Loss at iteration 610 : 0.10143440961837769
Loss at iteration 620 : 0.0944870337843895
Loss at iteration 630 : 0.06980062276124954
Loss at iteration 640 : 0.043792061507701874
Loss at iteration 650 : 0.08459979295730591
Loss at iteration 660 : 0.13066430389881134
Loss at iteration 670 : 0.10830700397491455
Loss at iteration 680 : 0.1039266288280487
Loss at iteration 690 : 0.0695241391658783
Loss at iteration 700 : 0.08449511229991913
Loss at iteration 710 : 0.11439062654972076
Loss at iteration 720 : 0.1156301498413086
Loss at iteration 730 : 0.07566652446985245
Loss at iteration 740 : 0.12956801056861877
Loss at iteration 750 : 0.08513538539409637
Loss at iteration 760 : 0.11801185458898544
Loss at iteration 770 : 0.07413173466920853
Loss at iteration 780 : 0.09853284060955048
Loss at iteration 790 : 0.07841221243143082
Loss at iteration 800 : 0.08033011853694916
Loss at iteration 810 : 0.10268201678991318
Loss at iteration 820 : 0.16259609162807465
Loss at iteration 830 : 0.07322942465543747
Loss at iteration 840 : 0.10257505625486374
Loss at iteration 850 : 0.07466543465852737
Loss at iteration 860 : 0.12437286972999573
Loss at iteration 870 : 0.06457071006298065
Loss at iteration 880 : 0.07356920838356018
Loss at iteration 890 : 0.05121616646647453
Loss at iteration 900 : 0.10805229842662811
Loss at iteration 910 : 0.052787840366363525
Loss at iteration 920 : 0.1165166050195694
Loss at iteration 930 : 0.09032406657934189
Loss at iteration 940 : 0.06434562057256699
Loss at iteration 950 : 0.11958017945289612
Loss at iteration 960 : 0.0558265745639801
Loss at iteration 970 : 0.1359892189502716
Loss at iteration 980 : 0.11043435335159302
Loss at iteration 990 : 0.07339230924844742
Loss at iteration 1000 : 0.0840597003698349
Loss at iteration 1010 : 0.07436896860599518
Loss at iteration 1020 : 0.07207038998603821
Loss at iteration 1030 : 0.10053294897079468
Loss at iteration 1040 : 0.07107916474342346
Loss at iteration 1050 : 0.11281764507293701
Loss at iteration 1060 : 0.09749715775251389
Loss at iteration 1070 : 0.07842554152011871
Loss at iteration 1080 : 0.09496638178825378
Loss at iteration 1090 : 0.15436610579490662
Loss at iteration 1100 : 0.09387798607349396
Loss at iteration 1110 : 0.07926023751497269
Loss at iteration 1120 : 0.1644166111946106
Loss at iteration 1130 : 0.09881788492202759
Loss at iteration 1140 : 0.07788260281085968
Loss at iteration 1150 : 0.17413601279258728
Loss at iteration 1160 : 0.06732417643070221
Loss at iteration 1170 : 0.0636114627122879
Loss at iteration 1180 : 0.09466169774532318
Loss at iteration 1190 : 0.07391124218702316
Loss at iteration 1200 : 0.09371089935302734
Loss at iteration 1210 : 0.07263632863759995
The SSIM Value is: 0.6682238618532816
The PSNR Value is: 19.701976839701334
the epoch is: 2
Loss at iteration 10 : 0.1369963437318802
Loss at iteration 20 : 0.0687728226184845
Loss at iteration 30 : 0.1603752225637436
Loss at iteration 40 : 0.060633640736341476
Loss at iteration 50 : 0.12260639667510986
Loss at iteration 60 : 0.07174955308437347
Loss at iteration 70 : 0.0801287442445755
Loss at iteration 80 : 0.10879679769277573
Loss at iteration 90 : 0.11511458456516266
Loss at iteration 100 : 0.07823102921247482
Loss at iteration 110 : 0.1229616031050682
Loss at iteration 120 : 0.1016785129904747
Loss at iteration 130 : 0.1214556097984314
Loss at iteration 140 : 0.10316134989261627
Loss at iteration 150 : 0.10977049916982651
Loss at iteration 160 : 0.14208583533763885
Loss at iteration 170 : 0.08373060822486877
Loss at iteration 180 : 0.10106770694255829
Loss at iteration 190 : 0.07572867721319199
Loss at iteration 200 : 0.12222278118133545
Loss at iteration 210 : 0.09891637414693832
Loss at iteration 220 : 0.10890573263168335
Loss at iteration 230 : 0.07459917664527893
Loss at iteration 240 : 0.06585871428251266
Loss at iteration 250 : 0.09288553893566132
Loss at iteration 260 : 0.11406376212835312
Loss at iteration 270 : 0.18232223391532898
Loss at iteration 280 : 0.10195024311542511
Loss at iteration 290 : 0.1774376481771469
Loss at iteration 300 : 0.05969151854515076
Loss at iteration 310 : 0.0953267365694046
Loss at iteration 320 : 0.16040104627609253
Loss at iteration 330 : 0.09105680882930756
Loss at iteration 340 : 0.06103712320327759
Loss at iteration 350 : 0.0959400087594986
Loss at iteration 360 : 0.0873858854174614
Loss at iteration 370 : 0.1578132063150406
Loss at iteration 380 : 0.07623612880706787
Loss at iteration 390 : 0.10472343862056732
Loss at iteration 400 : 0.0985715240240097
Loss at iteration 410 : 0.06983682513237
Loss at iteration 420 : 0.15499916672706604
Loss at iteration 430 : 0.04710987210273743
Loss at iteration 440 : 0.13448995351791382
Loss at iteration 450 : 0.09902720153331757
Loss at iteration 460 : 0.1483096480369568
Loss at iteration 470 : 0.11037123203277588
Loss at iteration 480 : 0.13942208886146545
Loss at iteration 490 : 0.07383420318365097
Loss at iteration 500 : 0.09118982404470444
Loss at iteration 510 : 0.08289729058742523
Loss at iteration 520 : 0.13846316933631897
Loss at iteration 530 : 0.12212993949651718
Loss at iteration 540 : 0.1257292628288269
Loss at iteration 550 : 0.08421199768781662
Loss at iteration 560 : 0.060373689979314804
Loss at iteration 570 : 0.10145950317382812
Loss at iteration 580 : 0.12782171368598938
Loss at iteration 590 : 0.09171620011329651
Loss at iteration 600 : 0.10833960771560669
Loss at iteration 610 : 0.10591201484203339
Loss at iteration 620 : 0.11630364507436752
Loss at iteration 630 : 0.11090391874313354
Loss at iteration 640 : 0.09612219035625458
Loss at iteration 650 : 0.07519088685512543
Loss at iteration 660 : 0.1131918653845787
Loss at iteration 670 : 0.09486018121242523
Loss at iteration 680 : 0.07481996715068817
Loss at iteration 690 : 0.07008908689022064
Loss at iteration 700 : 0.14973542094230652
Loss at iteration 710 : 0.11557194590568542
Loss at iteration 720 : 0.07337512820959091
Loss at iteration 730 : 0.0906250923871994
Loss at iteration 740 : 0.10591956973075867
Loss at iteration 750 : 0.1138959527015686
Loss at iteration 760 : 0.12662221491336823
Loss at iteration 770 : 0.20851871371269226
Loss at iteration 780 : 0.08029099553823471
Loss at iteration 790 : 0.10958575457334518
Loss at iteration 800 : 0.07670475542545319
Loss at iteration 810 : 0.10870196670293808
Loss at iteration 820 : 0.13394124805927277
Loss at iteration 830 : 0.09814270585775375
Loss at iteration 840 : 0.12502065300941467
Loss at iteration 850 : 0.10728242993354797
Loss at iteration 860 : 0.056175585836172104
Loss at iteration 870 : 0.11745724081993103
Loss at iteration 880 : 0.13349851965904236
Loss at iteration 890 : 0.08962337672710419
Loss at iteration 900 : 0.12374089658260345
Loss at iteration 910 : 0.11752748489379883
Loss at iteration 920 : 0.09546276926994324
Loss at iteration 930 : 0.11468790471553802
Loss at iteration 940 : 0.11177892982959747
Loss at iteration 950 : 0.11714337766170502
Loss at iteration 960 : 0.10158425569534302
Loss at iteration 970 : 0.1345864087343216
Loss at iteration 980 : 0.09292199462652206
Loss at iteration 990 : 0.09074048697948456
Loss at iteration 1000 : 0.08742453902959824
Loss at iteration 1010 : 0.11361982673406601
Loss at iteration 1020 : 0.12687139213085175
Loss at iteration 1030 : 0.04982064291834831
Loss at iteration 1040 : 0.10538177192211151
Loss at iteration 1050 : 0.10558360815048218
Loss at iteration 1060 : 0.14831949770450592
Loss at iteration 1070 : 0.06188957393169403
Loss at iteration 1080 : 0.20366065204143524
Loss at iteration 1090 : 0.12593930959701538
Loss at iteration 1100 : 0.0888393372297287
Loss at iteration 1110 : 0.1487751305103302
Loss at iteration 1120 : 0.0724504292011261
Loss at iteration 1130 : 0.06214125454425812
Loss at iteration 1140 : 0.1327930986881256
Loss at iteration 1150 : 0.07944650948047638
Loss at iteration 1160 : 0.09910424798727036
Loss at iteration 1170 : 0.09083569049835205
Loss at iteration 1180 : 0.12537747621536255
Loss at iteration 1190 : 0.095664381980896
Loss at iteration 1200 : 0.11466382443904877
Loss at iteration 1210 : 0.10383439064025879
The SSIM Value is: 0.6606497208277384
The PSNR Value is: 19.757229487101238
the epoch is: 3
Loss at iteration 10 : 0.14161990582942963
Loss at iteration 20 : 0.13402697443962097
Loss at iteration 30 : 0.07592299580574036
Loss at iteration 40 : 0.10832609981298447
Loss at iteration 50 : 0.09688934683799744
Loss at iteration 60 : 0.11482395976781845
Loss at iteration 70 : 0.08946709334850311
Loss at iteration 80 : 0.10644710063934326
Loss at iteration 90 : 0.06874993443489075
Loss at iteration 100 : 0.07430440932512283
Loss at iteration 110 : 0.11031144112348557
Loss at iteration 120 : 0.07136469334363937
Loss at iteration 130 : 0.0876651257276535
Loss at iteration 140 : 0.12826883792877197
Loss at iteration 150 : 0.0950351357460022
Loss at iteration 160 : 0.09273172914981842
Loss at iteration 170 : 0.06804557889699936
Loss at iteration 180 : 0.05472327396273613
Loss at iteration 190 : 0.05433332175016403
Loss at iteration 200 : 0.05639705806970596
Loss at iteration 210 : 0.1260671317577362
Loss at iteration 220 : 0.06967242062091827
Loss at iteration 230 : 0.08575072139501572
Loss at iteration 240 : 0.08565713465213776
Loss at iteration 250 : 0.13952049612998962
Loss at iteration 260 : 0.13333353400230408
Loss at iteration 270 : 0.12825778126716614
Loss at iteration 280 : 0.10597711056470871
Loss at iteration 290 : 0.1428007185459137
Loss at iteration 300 : 0.09735163301229477
Loss at iteration 310 : 0.08408011496067047
Loss at iteration 320 : 0.11683770269155502
Loss at iteration 330 : 0.0895896777510643
Loss at iteration 340 : 0.10084766149520874
Loss at iteration 350 : 0.0684775710105896
Loss at iteration 360 : 0.06701265275478363
Loss at iteration 370 : 0.07610824704170227
Loss at iteration 380 : 0.11705555021762848
Loss at iteration 390 : 0.11279034614562988
Loss at iteration 400 : 0.06447392702102661
Loss at iteration 410 : 0.08345398306846619
Loss at iteration 420 : 0.09450924396514893
Loss at iteration 430 : 0.05717014521360397
Loss at iteration 440 : 0.12986823916435242
Loss at iteration 450 : 0.09590494632720947
Loss at iteration 460 : 0.12301082909107208
Loss at iteration 470 : 0.11056743562221527
Loss at iteration 480 : 0.06277628988027573
Loss at iteration 490 : 0.08619034290313721
Loss at iteration 500 : 0.10949601978063583
Loss at iteration 510 : 0.10229681432247162
Loss at iteration 520 : 0.11053726077079773
Loss at iteration 530 : 0.05744899809360504
Loss at iteration 540 : 0.08562477678060532
Loss at iteration 550 : 0.09204261749982834
Loss at iteration 560 : 0.10368059575557709
Loss at iteration 570 : 0.08305352181196213
Loss at iteration 580 : 0.06846281886100769
Loss at iteration 590 : 0.08968308568000793
Loss at iteration 600 : 0.09749382734298706
Loss at iteration 610 : 0.11473184823989868
Loss at iteration 620 : 0.09344319999217987
Loss at iteration 630 : 0.2074517011642456
Loss at iteration 640 : 0.08908191323280334
Loss at iteration 650 : 0.07825598120689392
Loss at iteration 660 : 0.09046511352062225
Loss at iteration 670 : 0.10898216813802719
Loss at iteration 680 : 0.18502768874168396
Loss at iteration 690 : 0.08197352290153503
Loss at iteration 700 : 0.0739869698882103
Loss at iteration 710 : 0.10786953568458557
Loss at iteration 720 : 0.12034188210964203
Loss at iteration 730 : 0.0994802936911583
Loss at iteration 740 : 0.11072522401809692
Loss at iteration 750 : 0.1288560926914215
Loss at iteration 760 : 0.11534623801708221
Loss at iteration 770 : 0.15800052881240845
Loss at iteration 780 : 0.06470739841461182
Loss at iteration 790 : 0.0940331444144249
Loss at iteration 800 : 0.08725737035274506
Loss at iteration 810 : 0.09008640050888062
Loss at iteration 820 : 0.07214546948671341
Loss at iteration 830 : 0.10525425523519516
Loss at iteration 840 : 0.08262381702661514
Loss at iteration 850 : 0.09983813762664795
Loss at iteration 860 : 0.1353226602077484
Loss at iteration 870 : 0.08912750333547592
Loss at iteration 880 : 0.10416712611913681
Loss at iteration 890 : 0.1142778992652893
Loss at iteration 900 : 0.08472712337970734
Loss at iteration 910 : 0.10788373649120331
Loss at iteration 920 : 0.09753173589706421
Loss at iteration 930 : 0.06968747824430466
Loss at iteration 940 : 0.11768912523984909
Loss at iteration 950 : 0.07897230237722397
Loss at iteration 960 : 0.10274501889944077
Loss at iteration 970 : 0.08455847203731537
Loss at iteration 980 : 0.12586604058742523
Loss at iteration 990 : 0.11105404794216156
Loss at iteration 1000 : 0.10443957895040512
Loss at iteration 1010 : 0.06825863569974899
Loss at iteration 1020 : 0.11768197268247604
Loss at iteration 1030 : 0.08897708356380463
Loss at iteration 1040 : 0.12553301453590393
Loss at iteration 1050 : 0.09938766807317734
Loss at iteration 1060 : 0.06949575990438461
Loss at iteration 1070 : 0.06587295234203339
Loss at iteration 1080 : 0.16410468518733978
Loss at iteration 1090 : 0.09661423414945602
Loss at iteration 1100 : 0.07391633838415146
Loss at iteration 1110 : 0.0700942873954773
Loss at iteration 1120 : 0.13405567407608032
Loss at iteration 1130 : 0.08942995965480804
Loss at iteration 1140 : 0.0794820487499237
Loss at iteration 1150 : 0.07529104501008987
Loss at iteration 1160 : 0.06262195110321045
Loss at iteration 1170 : 0.07387322187423706
Loss at iteration 1180 : 0.09076999872922897
Loss at iteration 1190 : 0.10115711390972137
Loss at iteration 1200 : 0.12285411357879639
Loss at iteration 1210 : 0.09361784160137177
The SSIM Value is: 0.6581763903299968
The PSNR Value is: 19.180505752563477
the epoch is: 4
Loss at iteration 10 : 0.09298239648342133
Loss at iteration 20 : 0.09395791590213776
Loss at iteration 30 : 0.16021399199962616
Loss at iteration 40 : 0.13405568897724152
Loss at iteration 50 : 0.15394778549671173
Loss at iteration 60 : 0.08879654109477997
Loss at iteration 70 : 0.12127332389354706
Loss at iteration 80 : 0.05557938665151596
Loss at iteration 90 : 0.09803341329097748
Loss at iteration 100 : 0.11589044332504272
Loss at iteration 110 : 0.10609083622694016
Loss at iteration 120 : 0.07303963601589203
Loss at iteration 130 : 0.11048194020986557
Loss at iteration 140 : 0.09223189949989319
Loss at iteration 150 : 0.05336099863052368
Loss at iteration 160 : 0.06658145785331726
Loss at iteration 170 : 0.08557839691638947
Loss at iteration 180 : 0.08341034501791
Loss at iteration 190 : 0.10142660140991211
Loss at iteration 200 : 0.11289891600608826
Loss at iteration 210 : 0.12835019826889038
Loss at iteration 220 : 0.13946107029914856
Loss at iteration 230 : 0.11596928536891937
Loss at iteration 240 : 0.04932340979576111
Loss at iteration 250 : 0.06776291131973267
Loss at iteration 260 : 0.10801371186971664
Loss at iteration 270 : 0.07827936857938766
Loss at iteration 280 : 0.10936897248029709
Loss at iteration 290 : 0.116266630589962
Loss at iteration 300 : 0.11680859327316284
Loss at iteration 310 : 0.08519365638494492
Loss at iteration 320 : 0.14531710743904114
Loss at iteration 330 : 0.06737440079450607
Loss at iteration 340 : 0.11371979117393494
Loss at iteration 350 : 0.07505467534065247
Loss at iteration 360 : 0.09107649326324463
Loss at iteration 370 : 0.07969287782907486
Loss at iteration 380 : 0.08966150134801865
Loss at iteration 390 : 0.1085563525557518
Loss at iteration 400 : 0.07895173132419586
Loss at iteration 410 : 0.10608665645122528
Loss at iteration 420 : 0.11203097552061081
Loss at iteration 430 : 0.06084391474723816
Loss at iteration 440 : 0.1667129248380661
Loss at iteration 450 : 0.06717177480459213
Loss at iteration 460 : 0.06872393190860748
Loss at iteration 470 : 0.10971792787313461
Loss at iteration 480 : 0.10572262853384018
Loss at iteration 490 : 0.07464036345481873
Loss at iteration 500 : 0.0769997090101242
Loss at iteration 510 : 0.04901893064379692
Loss at iteration 520 : 0.09149675071239471
Loss at iteration 530 : 0.07212786376476288
Loss at iteration 540 : 0.08654829859733582
Loss at iteration 550 : 0.08289235830307007
Loss at iteration 560 : 0.09531252831220627
Loss at iteration 570 : 0.07523484528064728
Loss at iteration 580 : 0.06968353688716888
Loss at iteration 590 : 0.11736422777175903
Loss at iteration 600 : 0.11427688598632812
Loss at iteration 610 : 0.08196421712636948
Loss at iteration 620 : 0.10021822154521942
Loss at iteration 630 : 0.08342866599559784
Loss at iteration 640 : 0.1439734548330307
Loss at iteration 650 : 0.1347341537475586
Loss at iteration 660 : 0.08181899040937424
Loss at iteration 670 : 0.11117374151945114
Loss at iteration 680 : 0.14031901955604553
Loss at iteration 690 : 0.08327987790107727
Loss at iteration 700 : 0.07577095925807953
Loss at iteration 710 : 0.18467658758163452
Loss at iteration 720 : 0.10169332474470139
Loss at iteration 730 : 0.1036759614944458
Loss at iteration 740 : 0.14066775143146515
Loss at iteration 750 : 0.07793394476175308
Loss at iteration 760 : 0.12992653250694275
Loss at iteration 770 : 0.09852664172649384
Loss at iteration 780 : 0.10218559205532074
Loss at iteration 790 : 0.1145763248205185
Loss at iteration 800 : 0.08331532776355743
Loss at iteration 810 : 0.0785403698682785
Loss at iteration 820 : 0.1798037886619568
Loss at iteration 830 : 0.11733876913785934
Loss at iteration 840 : 0.0936986654996872
Loss at iteration 850 : 0.08003132045269012
Loss at iteration 860 : 0.11280973255634308
Loss at iteration 870 : 0.10400678962469101
Loss at iteration 880 : 0.10548172891139984
Loss at iteration 890 : 0.11470630764961243
Loss at iteration 900 : 0.08025344461202621
Loss at iteration 910 : 0.08791063725948334
Loss at iteration 920 : 0.062136780470609665
Loss at iteration 930 : 0.13424180448055267
Loss at iteration 940 : 0.10411165654659271
Loss at iteration 950 : 0.14417512714862823
Loss at iteration 960 : 0.11289169639348984
Loss at iteration 970 : 0.08824150264263153
Loss at iteration 980 : 0.05210182070732117
Loss at iteration 990 : 0.09843994677066803
Loss at iteration 1000 : 0.08537431806325912
Loss at iteration 1010 : 0.11929391324520111
Loss at iteration 1020 : 0.13170583546161652
Loss at iteration 1030 : 0.10261939465999603
Loss at iteration 1040 : 0.12794438004493713
Loss at iteration 1050 : 0.1255931556224823
Loss at iteration 1060 : 0.05422753840684891
Loss at iteration 1070 : 0.0640314519405365
Loss at iteration 1080 : 0.06703231483697891
Loss at iteration 1090 : 0.07769985496997833
Loss at iteration 1100 : 0.08512775599956512
Loss at iteration 1110 : 0.13022775948047638
Loss at iteration 1120 : 0.1082133948802948
Loss at iteration 1130 : 0.06822600960731506
Loss at iteration 1140 : 0.08503266423940659
Loss at iteration 1150 : 0.10195282101631165
Loss at iteration 1160 : 0.07221397757530212
Loss at iteration 1170 : 0.10355359315872192
Loss at iteration 1180 : 0.061059556901454926
Loss at iteration 1190 : 0.0642017275094986
Loss at iteration 1200 : 0.1328282654285431
Loss at iteration 1210 : 0.07170802354812622
The SSIM Value is: 0.6673310776551564
The PSNR Value is: 20.05051383972168
the highest SSIM value is: 20.05051383972168
the epoch is: 5
Loss at iteration 10 : 0.09739537537097931
Loss at iteration 20 : 0.0641806349158287
Loss at iteration 30 : 0.08183673024177551
Loss at iteration 40 : 0.07634426653385162
Loss at iteration 50 : 0.13593190908432007
Loss at iteration 60 : 0.10108443349599838
Loss at iteration 70 : 0.1166253313422203
Loss at iteration 80 : 0.08268533647060394
Loss at iteration 90 : 0.04158540815114975
Loss at iteration 100 : 0.07861190289258957
Loss at iteration 110 : 0.08009704947471619
Loss at iteration 120 : 0.09061112999916077
Loss at iteration 130 : 0.12831400334835052
Loss at iteration 140 : 0.07029958069324493
Loss at iteration 150 : 0.07364264130592346
Loss at iteration 160 : 0.13134461641311646
Loss at iteration 170 : 0.12183991074562073
Loss at iteration 180 : 0.07924912869930267
Loss at iteration 190 : 0.07345177233219147
Loss at iteration 200 : 0.15386204421520233
Loss at iteration 210 : 0.1560482680797577
Loss at iteration 220 : 0.06813055276870728
Loss at iteration 230 : 0.08052973449230194
Loss at iteration 240 : 0.09194992482662201
Loss at iteration 250 : 0.07934515178203583
Loss at iteration 260 : 0.0786978155374527
Loss at iteration 270 : 0.10473205149173737
Loss at iteration 280 : 0.103949174284935
Loss at iteration 290 : 0.140309676527977
Loss at iteration 300 : 0.09797309339046478
Loss at iteration 310 : 0.07101352512836456
Loss at iteration 320 : 0.07808159291744232
Loss at iteration 330 : 0.08684247732162476
Loss at iteration 340 : 0.09984986484050751
Loss at iteration 350 : 0.08338000625371933
Loss at iteration 360 : 0.10709899663925171
Loss at iteration 370 : 0.1094188392162323
Loss at iteration 380 : 0.0724647268652916
Loss at iteration 390 : 0.10916906595230103
Loss at iteration 400 : 0.16118143498897552
Loss at iteration 410 : 0.11513079702854156
Loss at iteration 420 : 0.19520026445388794
Loss at iteration 430 : 0.06882491707801819
Loss at iteration 440 : 0.11670030653476715
Loss at iteration 450 : 0.08503656089305878
Loss at iteration 460 : 0.05039408430457115
Loss at iteration 470 : 0.13328157365322113
Loss at iteration 480 : 0.1564032882452011
Loss at iteration 490 : 0.09737969934940338
Loss at iteration 500 : 0.12637633085250854
Loss at iteration 510 : 0.059049248695373535
Loss at iteration 520 : 0.08886538445949554
Loss at iteration 530 : 0.15850721299648285
Loss at iteration 540 : 0.07209501415491104
Loss at iteration 550 : 0.1357942521572113
Loss at iteration 560 : 0.172712504863739
Loss at iteration 570 : 0.15566034615039825
Loss at iteration 580 : 0.0901283323764801
Loss at iteration 590 : 0.15986041724681854
Loss at iteration 600 : 0.04752958565950394
Loss at iteration 610 : 0.088655024766922
Loss at iteration 620 : 0.051457494497299194
Loss at iteration 630 : 0.14763891696929932
Loss at iteration 640 : 0.09467117488384247
Loss at iteration 650 : 0.08428623527288437
Loss at iteration 660 : 0.11859267950057983
Loss at iteration 670 : 0.12331488728523254
Loss at iteration 680 : 0.07642322778701782
Loss at iteration 690 : 0.062176771461963654
Loss at iteration 700 : 0.07921390235424042
Loss at iteration 710 : 0.1029980406165123
Loss at iteration 720 : 0.08499865233898163
Loss at iteration 730 : 0.11391697824001312
Loss at iteration 740 : 0.1308576762676239
Loss at iteration 750 : 0.1208709180355072
Loss at iteration 760 : 0.1103348359465599
Loss at iteration 770 : 0.11467849463224411
Loss at iteration 780 : 0.07637429237365723
Loss at iteration 790 : 0.10957571119070053
Loss at iteration 800 : 0.09181630611419678
Loss at iteration 810 : 0.07924716174602509
Loss at iteration 820 : 0.08676331490278244
Loss at iteration 830 : 0.07345466315746307
Loss at iteration 840 : 0.09439121186733246
Loss at iteration 850 : 0.07824170589447021
Loss at iteration 860 : 0.0851958841085434
Loss at iteration 870 : 0.12259151041507721
Loss at iteration 880 : 0.07311664521694183
Loss at iteration 890 : 0.10602593421936035
Loss at iteration 900 : 0.09724228084087372
Loss at iteration 910 : 0.08930396288633347
Loss at iteration 920 : 0.15038882195949554
Loss at iteration 930 : 0.08287275582551956
Loss at iteration 940 : 0.07399781048297882
Loss at iteration 950 : 0.11075913161039352
Loss at iteration 960 : 0.0828152745962143
Loss at iteration 970 : 0.1089630126953125
Loss at iteration 980 : 0.11775282025337219
Loss at iteration 990 : 0.07602343708276749
Loss at iteration 1000 : 0.09819255769252777
Loss at iteration 1010 : 0.13743272423744202
Loss at iteration 1020 : 0.09298904240131378
Loss at iteration 1030 : 0.12752139568328857
Loss at iteration 1040 : 0.0760742649435997
Loss at iteration 1050 : 0.12234165519475937
Loss at iteration 1060 : 0.07358547300100327
Loss at iteration 1070 : 0.09273067116737366
Loss at iteration 1080 : 0.08172142505645752
Loss at iteration 1090 : 0.13684378564357758
Loss at iteration 1100 : 0.11382909119129181
Loss at iteration 1110 : 0.05102715641260147
Loss at iteration 1120 : 0.08273281902074814
Loss at iteration 1130 : 0.09137376397848129
Loss at iteration 1140 : 0.10744898021221161
Loss at iteration 1150 : 0.10395856946706772
Loss at iteration 1160 : 0.09622947871685028
Loss at iteration 1170 : 0.13820716738700867
Loss at iteration 1180 : 0.10075835883617401
Loss at iteration 1190 : 0.14597925543785095
Loss at iteration 1200 : 0.09135674685239792
Loss at iteration 1210 : 0.13811001181602478
The SSIM Value is: 0.6571450114250184
The PSNR Value is: 19.50830510457357
the epoch is: 6
Loss at iteration 10 : 0.11379798501729965
Loss at iteration 20 : 0.11982356756925583
Loss at iteration 30 : 0.15434019267559052
Loss at iteration 40 : 0.05279681086540222
Loss at iteration 50 : 0.09563426673412323
Loss at iteration 60 : 0.11454560607671738
Loss at iteration 70 : 0.09656243026256561
Loss at iteration 80 : 0.07906074821949005
Loss at iteration 90 : 0.11722346395254135
Loss at iteration 100 : 0.0710107609629631
Loss at iteration 110 : 0.16530562937259674
Loss at iteration 120 : 0.07270616292953491
Loss at iteration 130 : 0.08067874610424042
Loss at iteration 140 : 0.1076040267944336
Loss at iteration 150 : 0.06823962926864624
Loss at iteration 160 : 0.08955132961273193
Loss at iteration 170 : 0.11828933656215668
Loss at iteration 180 : 0.0749206617474556
Loss at iteration 190 : 0.06651582568883896
Loss at iteration 200 : 0.08018115162849426
Loss at iteration 210 : 0.06528358161449432
Loss at iteration 220 : 0.12378726899623871
Loss at iteration 230 : 0.07021816074848175
Loss at iteration 240 : 0.09572316706180573
Loss at iteration 250 : 0.09693606942892075
Loss at iteration 260 : 0.09054349362850189
Loss at iteration 270 : 0.08551301807165146
Loss at iteration 280 : 0.1408536285161972
Loss at iteration 290 : 0.11621755361557007
Loss at iteration 300 : 0.058627765625715256
Loss at iteration 310 : 0.10906904190778732
Loss at iteration 320 : 0.0634581446647644
Loss at iteration 330 : 0.11101367324590683
Loss at iteration 340 : 0.1120457574725151
Loss at iteration 350 : 0.11853967607021332
Loss at iteration 360 : 0.1620822250843048
Loss at iteration 370 : 0.15759554505348206
Loss at iteration 380 : 0.080703966319561
Loss at iteration 390 : 0.1214432343840599
Loss at iteration 400 : 0.07516250014305115
Loss at iteration 410 : 0.08042313903570175
Loss at iteration 420 : 0.0601140558719635
Loss at iteration 430 : 0.12846384942531586
Loss at iteration 440 : 0.07322962582111359
Loss at iteration 450 : 0.08155414462089539
Loss at iteration 460 : 0.09127338975667953
Loss at iteration 470 : 0.10523897409439087
Loss at iteration 480 : 0.08677150309085846
Loss at iteration 490 : 0.09277521073818207
Loss at iteration 500 : 0.12786072492599487
Loss at iteration 510 : 0.06966860592365265
Loss at iteration 520 : 0.09930860251188278
Loss at iteration 530 : 0.18120303750038147
Loss at iteration 540 : 0.15530824661254883
Loss at iteration 550 : 0.05890893191099167
Loss at iteration 560 : 0.057284511625766754
Loss at iteration 570 : 0.10413112491369247
Loss at iteration 580 : 0.08895792067050934
Loss at iteration 590 : 0.08985515683889389
Loss at iteration 600 : 0.08653728663921356
Loss at iteration 610 : 0.12629744410514832
Loss at iteration 620 : 0.1133253276348114
Loss at iteration 630 : 0.07959219813346863
Loss at iteration 640 : 0.05886692553758621
Loss at iteration 650 : 0.12101411819458008
Loss at iteration 660 : 0.061167459934949875
Loss at iteration 670 : 0.145516499876976
Loss at iteration 680 : 0.09697570651769638
Loss at iteration 690 : 0.09223994612693787
Loss at iteration 700 : 0.08026400208473206
Loss at iteration 710 : 0.09829600155353546
Loss at iteration 720 : 0.090609610080719
Loss at iteration 730 : 0.12225914001464844
Loss at iteration 740 : 0.052758507430553436
Loss at iteration 750 : 0.06262024492025375
Loss at iteration 760 : 0.05792638659477234
Loss at iteration 770 : 0.09404973685741425
Loss at iteration 780 : 0.07355302572250366
Loss at iteration 790 : 0.12926453351974487
Loss at iteration 800 : 0.1111142635345459
Loss at iteration 810 : 0.08233289420604706
Loss at iteration 820 : 0.15062791109085083
Loss at iteration 830 : 0.12384115159511566
Loss at iteration 840 : 0.12712743878364563
Loss at iteration 850 : 0.12036453932523727
Loss at iteration 860 : 0.06981600075960159
Loss at iteration 870 : 0.054320286959409714
Loss at iteration 880 : 0.062343887984752655
Loss at iteration 890 : 0.12674608826637268
Loss at iteration 900 : 0.10093130171298981
Loss at iteration 910 : 0.06579115241765976
Loss at iteration 920 : 0.10639850795269012
Loss at iteration 930 : 0.08096205443143845
Loss at iteration 940 : 0.10972897708415985
Loss at iteration 950 : 0.08770498633384705
Loss at iteration 960 : 0.09577864408493042
Loss at iteration 970 : 0.11442464590072632
Loss at iteration 980 : 0.08618348836898804
Loss at iteration 990 : 0.08539493381977081
Loss at iteration 1000 : 0.050284724682569504
Loss at iteration 1010 : 0.09248866140842438
Loss at iteration 1020 : 0.0675143226981163
Loss at iteration 1030 : 0.09730516374111176
Loss at iteration 1040 : 0.06791850924491882
Loss at iteration 1050 : 0.08280456066131592
Loss at iteration 1060 : 0.09200306981801987
Loss at iteration 1070 : 0.09556174278259277
Loss at iteration 1080 : 0.06873704493045807
Loss at iteration 1090 : 0.08523990958929062
Loss at iteration 1100 : 0.10237425565719604
Loss at iteration 1110 : 0.06376801431179047
Loss at iteration 1120 : 0.08034729212522507
Loss at iteration 1130 : 0.10973137617111206
Loss at iteration 1140 : 0.14060911536216736
Loss at iteration 1150 : 0.08588358014822006
Loss at iteration 1160 : 0.0913069099187851
Loss at iteration 1170 : 0.1399814337491989
Loss at iteration 1180 : 0.09804417192935944
Loss at iteration 1190 : 0.0724291056394577
Loss at iteration 1200 : 0.11700042337179184
Loss at iteration 1210 : 0.08976978063583374
The SSIM Value is: 0.6649621506532033
The PSNR Value is: 20.006503295898437
the epoch is: 7
Loss at iteration 10 : 0.08883579075336456
Loss at iteration 20 : 0.12461324036121368
Loss at iteration 30 : 0.09906308352947235
Loss at iteration 40 : 0.08904250711202621
Loss at iteration 50 : 0.15103501081466675
Loss at iteration 60 : 0.11317022144794464
Loss at iteration 70 : 0.13935576379299164
Loss at iteration 80 : 0.0892753154039383
Loss at iteration 90 : 0.14861030876636505
Loss at iteration 100 : 0.05876311659812927
Loss at iteration 110 : 0.06924036890268326
Loss at iteration 120 : 0.10768844932317734
Loss at iteration 130 : 0.08482864499092102
Loss at iteration 140 : 0.05515097826719284
Loss at iteration 150 : 0.11103014647960663
Loss at iteration 160 : 0.1303834617137909
Loss at iteration 170 : 0.08267021924257278
Loss at iteration 180 : 0.17180034518241882
Loss at iteration 190 : 0.07517014443874359
Loss at iteration 200 : 0.11658893525600433
Loss at iteration 210 : 0.06742575764656067
Loss at iteration 220 : 0.14285334944725037
Loss at iteration 230 : 0.08023738116025925
Loss at iteration 240 : 0.06143653392791748
Loss at iteration 250 : 0.07867487519979477
Loss at iteration 260 : 0.10629498958587646
Loss at iteration 270 : 0.14213529229164124
Loss at iteration 280 : 0.08852428197860718
Loss at iteration 290 : 0.09663090109825134
Loss at iteration 300 : 0.11033546924591064
Loss at iteration 310 : 0.11297368258237839
Loss at iteration 320 : 0.06564908474683762
Loss at iteration 330 : 0.11818786710500717
Loss at iteration 340 : 0.10298275202512741
Loss at iteration 350 : 0.10582789778709412
Loss at iteration 360 : 0.1196189746260643
Loss at iteration 370 : 0.07945932447910309
Loss at iteration 380 : 0.050614338368177414
Loss at iteration 390 : 0.06236795708537102
Loss at iteration 400 : 0.10398554801940918
Loss at iteration 410 : 0.07676069438457489
Loss at iteration 420 : 0.10830298066139221
Loss at iteration 430 : 0.06954159587621689
Loss at iteration 440 : 0.11601274460554123
Loss at iteration 450 : 0.10364924371242523
Loss at iteration 460 : 0.06894119083881378
Loss at iteration 470 : 0.0672416090965271
Loss at iteration 480 : 0.14417317509651184
Loss at iteration 490 : 0.08255209773778915
Loss at iteration 500 : 0.05914716422557831
Loss at iteration 510 : 0.13097333908081055
Loss at iteration 520 : 0.12316876649856567
Loss at iteration 530 : 0.09940074384212494
Loss at iteration 540 : 0.08722802251577377
Loss at iteration 550 : 0.15343493223190308
Loss at iteration 560 : 0.11019104719161987
Loss at iteration 570 : 0.06201085448265076
Loss at iteration 580 : 0.11097492277622223
Loss at iteration 590 : 0.0927162617444992
Loss at iteration 600 : 0.12171574681997299
Loss at iteration 610 : 0.08889268338680267
Loss at iteration 620 : 0.07332156598567963
Loss at iteration 630 : 0.10623576492071152
Loss at iteration 640 : 0.12358167767524719
Loss at iteration 650 : 0.1866576373577118
Loss at iteration 660 : 0.11501352488994598
Loss at iteration 670 : 0.10841421782970428
Loss at iteration 680 : 0.07057514786720276
Loss at iteration 690 : 0.07035528123378754
Loss at iteration 700 : 0.15952207148075104
Loss at iteration 710 : 0.09244382381439209
Loss at iteration 720 : 0.09857141226530075
Loss at iteration 730 : 0.08929985761642456
Loss at iteration 740 : 0.0782904252409935
Loss at iteration 750 : 0.13605313003063202
Loss at iteration 760 : 0.10087933391332626
Loss at iteration 770 : 0.07792721688747406
Loss at iteration 780 : 0.047635518014431
Loss at iteration 790 : 0.1076694056391716
Loss at iteration 800 : 0.15977197885513306
Loss at iteration 810 : 0.09139290452003479
Loss at iteration 820 : 0.15032914280891418
Loss at iteration 830 : 0.1375085562467575
Loss at iteration 840 : 0.07427284121513367
Loss at iteration 850 : 0.0803099274635315
Loss at iteration 860 : 0.05807990953326225
Loss at iteration 870 : 0.10734166949987411
Loss at iteration 880 : 0.10277890413999557
Loss at iteration 890 : 0.09721565246582031
Loss at iteration 900 : 0.05306815728545189
Loss at iteration 910 : 0.1206204742193222
Loss at iteration 920 : 0.08067002147436142
Loss at iteration 930 : 0.07862164825201035
Loss at iteration 940 : 0.07621299475431442
Loss at iteration 950 : 0.12851272523403168
Loss at iteration 960 : 0.08153589069843292
Loss at iteration 970 : 0.09846025705337524
Loss at iteration 980 : 0.05857990309596062
Loss at iteration 990 : 0.0867328867316246
Loss at iteration 1000 : 0.102065309882164
Loss at iteration 1010 : 0.11512183398008347
Loss at iteration 1020 : 0.0707239955663681
Loss at iteration 1030 : 0.10063348710536957
Loss at iteration 1040 : 0.08505365252494812
Loss at iteration 1050 : 0.06893058121204376
Loss at iteration 1060 : 0.06598705053329468
Loss at iteration 1070 : 0.10205303132534027
Loss at iteration 1080 : 0.10622186958789825
Loss at iteration 1090 : 0.06717431545257568
Loss at iteration 1100 : 0.08115813136100769
Loss at iteration 1110 : 0.11586755514144897
Loss at iteration 1120 : 0.08515390008687973
Loss at iteration 1130 : 0.09684725105762482
Loss at iteration 1140 : 0.11854615807533264
Loss at iteration 1150 : 0.13152343034744263
Loss at iteration 1160 : 0.10328497737646103
Loss at iteration 1170 : 0.04989154636859894
Loss at iteration 1180 : 0.10695207118988037
Loss at iteration 1190 : 0.10341618955135345
Loss at iteration 1200 : 0.08501210808753967
Loss at iteration 1210 : 0.09960031509399414
The SSIM Value is: 0.6707567234834035
The PSNR Value is: 20.337694549560545
the highest SSIM value is: 20.337694549560545
the epoch is: 8
Loss at iteration 10 : 0.1152515709400177
Loss at iteration 20 : 0.08770722150802612
Loss at iteration 30 : 0.10574708878993988
Loss at iteration 40 : 0.08728205412626266
Loss at iteration 50 : 0.08923988789319992
Loss at iteration 60 : 0.09439778327941895
Loss at iteration 70 : 0.088835708796978
Loss at iteration 80 : 0.08655194938182831
Loss at iteration 90 : 0.10802100598812103
Loss at iteration 100 : 0.11130280792713165
Loss at iteration 110 : 0.1312275528907776
Loss at iteration 120 : 0.13781727850437164
Loss at iteration 130 : 0.11023563146591187
Loss at iteration 140 : 0.08345945179462433
Loss at iteration 150 : 0.1179112046957016
Loss at iteration 160 : 0.0798177719116211
Loss at iteration 170 : 0.1291704773902893
Loss at iteration 180 : 0.07388144731521606
Loss at iteration 190 : 0.08903340995311737
Loss at iteration 200 : 0.08736079931259155
Loss at iteration 210 : 0.1108383983373642
Loss at iteration 220 : 0.15466952323913574
Loss at iteration 230 : 0.11531110852956772
Loss at iteration 240 : 0.10643025487661362
Loss at iteration 250 : 0.07665801048278809
Loss at iteration 260 : 0.08720322698354721
Loss at iteration 270 : 0.09613282978534698
Loss at iteration 280 : 0.07874374836683273
Loss at iteration 290 : 0.10003286600112915
Loss at iteration 300 : 0.10586460679769516
Loss at iteration 310 : 0.1381288468837738
Loss at iteration 320 : 0.09911640733480453
Loss at iteration 330 : 0.1364758163690567
Loss at iteration 340 : 0.09180748462677002
Loss at iteration 350 : 0.07880231738090515
Loss at iteration 360 : 0.07205717265605927
Loss at iteration 370 : 0.09610225260257721
Loss at iteration 380 : 0.10316269099712372
Loss at iteration 390 : 0.07538322359323502
Loss at iteration 400 : 0.07971131056547165
Loss at iteration 410 : 0.11772441864013672
Loss at iteration 420 : 0.09741630405187607
Loss at iteration 430 : 0.08987634629011154
Loss at iteration 440 : 0.08261746168136597
Loss at iteration 450 : 0.07113870978355408
Loss at iteration 460 : 0.08886174857616425
Loss at iteration 470 : 0.09211792796850204
Loss at iteration 480 : 0.11841270327568054
Loss at iteration 490 : 0.09023090451955795
Loss at iteration 500 : 0.06657245755195618
Loss at iteration 510 : 0.06216161698102951
Loss at iteration 520 : 0.12654277682304382
Loss at iteration 530 : 0.10824533551931381
Loss at iteration 540 : 0.08153300732374191
Loss at iteration 550 : 0.10286157578229904
Loss at iteration 560 : 0.09661747515201569
Loss at iteration 570 : 0.10170754045248032
Loss at iteration 580 : 0.052435360848903656
Loss at iteration 590 : 0.10518445074558258
Loss at iteration 600 : 0.10683175176382065
Loss at iteration 610 : 0.0746530145406723
Loss at iteration 620 : 0.08018215000629425
Loss at iteration 630 : 0.05417242646217346
Loss at iteration 640 : 0.09001731872558594
Loss at iteration 650 : 0.05548403784632683
Loss at iteration 660 : 0.13404694199562073
Loss at iteration 670 : 0.09240396320819855
Loss at iteration 680 : 0.0945659726858139
Loss at iteration 690 : 0.05905967205762863
Loss at iteration 700 : 0.08279108256101608
Loss at iteration 710 : 0.11332004517316818
Loss at iteration 720 : 0.07224392890930176
Loss at iteration 730 : 0.13027217984199524
Loss at iteration 740 : 0.10764719545841217
Loss at iteration 750 : 0.0721169114112854
Loss at iteration 760 : 0.09781648963689804
Loss at iteration 770 : 0.12055037915706635
Loss at iteration 780 : 0.131265327334404
Loss at iteration 790 : 0.07560260593891144
Loss at iteration 800 : 0.08909714221954346
Loss at iteration 810 : 0.08482939004898071
Loss at iteration 820 : 0.07818254083395004
Loss at iteration 830 : 0.08068078756332397
Loss at iteration 840 : 0.056418947875499725
Loss at iteration 850 : 0.0805339366197586
Loss at iteration 860 : 0.10750842094421387
Loss at iteration 870 : 0.11186116933822632
Loss at iteration 880 : 0.09842544049024582
Loss at iteration 890 : 0.05624840781092644
Loss at iteration 900 : 0.13427306711673737
Loss at iteration 910 : 0.1261305809020996
Loss at iteration 920 : 0.09492333233356476
Loss at iteration 930 : 0.08809616416692734
Loss at iteration 940 : 0.07889610528945923
Loss at iteration 950 : 0.07400190085172653
Loss at iteration 960 : 0.09470950067043304
Loss at iteration 970 : 0.09166626632213593
Loss at iteration 980 : 0.07263852655887604
Loss at iteration 990 : 0.09745203703641891
Loss at iteration 1000 : 0.08668806403875351
Loss at iteration 1010 : 0.06190083175897598
Loss at iteration 1020 : 0.08090002834796906
Loss at iteration 1030 : 0.10090912878513336
Loss at iteration 1040 : 0.12283341586589813
Loss at iteration 1050 : 0.11217209696769714
Loss at iteration 1060 : 0.08590981364250183
Loss at iteration 1070 : 0.10540538281202316
Loss at iteration 1080 : 0.11031937599182129
Loss at iteration 1090 : 0.058819741010665894
Loss at iteration 1100 : 0.07839547842741013
Loss at iteration 1110 : 0.10101500153541565
Loss at iteration 1120 : 0.09061938524246216
Loss at iteration 1130 : 0.10463651269674301
Loss at iteration 1140 : 0.08765910565853119
Loss at iteration 1150 : 0.12076966464519501
Loss at iteration 1160 : 0.11770179122686386
Loss at iteration 1170 : 0.09533286094665527
Loss at iteration 1180 : 0.10800077021121979
Loss at iteration 1190 : 0.12295912951231003
Loss at iteration 1200 : 0.07217581570148468
Loss at iteration 1210 : 0.06689982116222382
The SSIM Value is: 0.6754504044850668
The PSNR Value is: 20.239670117696125
the epoch is: 9
Loss at iteration 10 : 0.05561219900846481
Loss at iteration 20 : 0.11876313388347626
Loss at iteration 30 : 0.05502627417445183
Loss at iteration 40 : 0.05670773983001709
Loss at iteration 50 : 0.06767384707927704
Loss at iteration 60 : 0.08136768639087677
Loss at iteration 70 : 0.061140500009059906
Loss at iteration 80 : 0.0959891527891159
Loss at iteration 90 : 0.11975960433483124
Loss at iteration 100 : 0.10271098464727402
Loss at iteration 110 : 0.05992864817380905
Loss at iteration 120 : 0.06640947610139847
Loss at iteration 130 : 0.08932213485240936
Loss at iteration 140 : 0.09380511939525604
Loss at iteration 150 : 0.1287035495042801
Loss at iteration 160 : 0.10001051425933838
Loss at iteration 170 : 0.10207046568393707
Loss at iteration 180 : 0.08909516036510468
Loss at iteration 190 : 0.10776935517787933
Loss at iteration 200 : 0.1306568682193756
Loss at iteration 210 : 0.07292936742305756
Loss at iteration 220 : 0.13758300244808197
Loss at iteration 230 : 0.12489383667707443
Loss at iteration 240 : 0.08195538073778152
Loss at iteration 250 : 0.09228526055812836
Loss at iteration 260 : 0.07661141455173492
Loss at iteration 270 : 0.07264161109924316
Loss at iteration 280 : 0.10206028819084167
Loss at iteration 290 : 0.12012408673763275
Loss at iteration 300 : 0.08063656091690063
Loss at iteration 310 : 0.05164973437786102
Loss at iteration 320 : 0.07601770013570786
Loss at iteration 330 : 0.1323264092206955
Loss at iteration 340 : 0.14100590348243713
Loss at iteration 350 : 0.1552514284849167
Loss at iteration 360 : 0.08390922844409943
Loss at iteration 370 : 0.13296791911125183
Loss at iteration 380 : 0.06796188652515411
Loss at iteration 390 : 0.118039071559906
Loss at iteration 400 : 0.12464790791273117
Loss at iteration 410 : 0.11896899342536926
Loss at iteration 420 : 0.1011563166975975
Loss at iteration 430 : 0.13345745205879211
Loss at iteration 440 : 0.10769854485988617
Loss at iteration 450 : 0.09031610190868378
Loss at iteration 460 : 0.11091866344213486
Loss at iteration 470 : 0.10298927873373032
Loss at iteration 480 : 0.07918072491884232
Loss at iteration 490 : 0.07793354988098145
Loss at iteration 500 : 0.10013630986213684
Loss at iteration 510 : 0.10365500301122665
Loss at iteration 520 : 0.08425744622945786
Loss at iteration 530 : 0.053299836814403534
Loss at iteration 540 : 0.06794371455907822
Loss at iteration 550 : 0.10377107560634613
Loss at iteration 560 : 0.12064258754253387
Loss at iteration 570 : 0.07679404318332672
Loss at iteration 580 : 0.08598734438419342
Loss at iteration 590 : 0.12158893793821335
Loss at iteration 600 : 0.07900082319974899
Loss at iteration 610 : 0.0794902890920639
Loss at iteration 620 : 0.11377247422933578
Loss at iteration 630 : 0.10282080620527267
Loss at iteration 640 : 0.08170148730278015
Loss at iteration 650 : 0.08138679713010788
Loss at iteration 660 : 0.14999686181545258
Loss at iteration 670 : 0.06363794952630997
Loss at iteration 680 : 0.08475685119628906
Loss at iteration 690 : 0.09683337062597275
Loss at iteration 700 : 0.06365609914064407
Loss at iteration 710 : 0.11144235730171204
Loss at iteration 720 : 0.06431498378515244
Loss at iteration 730 : 0.10445835441350937
Loss at iteration 740 : 0.09760147333145142
Loss at iteration 750 : 0.16566520929336548
Loss at iteration 760 : 0.10419119894504547
Loss at iteration 770 : 0.1312650740146637
Loss at iteration 780 : 0.11276431381702423
Loss at iteration 790 : 0.0757092759013176
Loss at iteration 800 : 0.11164551973342896
Loss at iteration 810 : 0.14399339258670807
Loss at iteration 820 : 0.11762849986553192
Loss at iteration 830 : 0.11025051772594452
Loss at iteration 840 : 0.1049351617693901
Loss at iteration 850 : 0.09763924777507782
Loss at iteration 860 : 0.11279382556676865
Loss at iteration 870 : 0.09060847014188766
Loss at iteration 880 : 0.09307937324047089
Loss at iteration 890 : 0.15476736426353455
Loss at iteration 900 : 0.08881493657827377
Loss at iteration 910 : 0.044930413365364075
Loss at iteration 920 : 0.10472731292247772
Loss at iteration 930 : 0.09054109454154968
Loss at iteration 940 : 0.08852837979793549
Loss at iteration 950 : 0.11714453995227814
Loss at iteration 960 : 0.07380273193120956
Loss at iteration 970 : 0.11901865899562836
Loss at iteration 980 : 0.09362485259771347
Loss at iteration 990 : 0.12869197130203247
Loss at iteration 1000 : 0.07686032354831696
Loss at iteration 1010 : 0.06685546040534973
Loss at iteration 1020 : 0.10116289556026459
Loss at iteration 1030 : 0.06957023590803146
Loss at iteration 1040 : 0.11608509719371796
Loss at iteration 1050 : 0.1339351236820221
Loss at iteration 1060 : 0.08596765995025635
Loss at iteration 1070 : 0.13567982614040375
Loss at iteration 1080 : 0.06864923238754272
Loss at iteration 1090 : 0.1108405813574791
Loss at iteration 1100 : 0.056716736406087875
Loss at iteration 1110 : 0.13811761140823364
Loss at iteration 1120 : 0.09037666022777557
Loss at iteration 1130 : 0.11495386064052582
Loss at iteration 1140 : 0.11334970593452454
Loss at iteration 1150 : 0.12264025211334229
Loss at iteration 1160 : 0.12053422629833221
Loss at iteration 1170 : 0.08533750474452972
Loss at iteration 1180 : 0.08751630038022995
Loss at iteration 1190 : 0.07578225433826447
Loss at iteration 1200 : 0.16737625002861023
Loss at iteration 1210 : 0.13114190101623535
The SSIM Value is: 0.6680501381556193
The PSNR Value is: 19.835703468322755
the epoch is: 10
Loss at iteration 10 : 0.15609458088874817
Loss at iteration 20 : 0.12420525401830673
Loss at iteration 30 : 0.11623957008123398
Loss at iteration 40 : 0.053174376487731934
Loss at iteration 50 : 0.0808456689119339
Loss at iteration 60 : 0.10405479371547699
Loss at iteration 70 : 0.12800267338752747
Loss at iteration 80 : 0.08709597587585449
Loss at iteration 90 : 0.12329798936843872
Loss at iteration 100 : 0.081624835729599
Loss at iteration 110 : 0.09501665830612183
Loss at iteration 120 : 0.07538172602653503
Loss at iteration 130 : 0.14712896943092346
Loss at iteration 140 : 0.09218509495258331
Loss at iteration 150 : 0.09081944823265076
Loss at iteration 160 : 0.0760800689458847
Loss at iteration 170 : 0.06776345521211624
Loss at iteration 180 : 0.1109476089477539
Loss at iteration 190 : 0.08533038944005966
Loss at iteration 200 : 0.1039513498544693
Loss at iteration 210 : 0.08080781251192093
Loss at iteration 220 : 0.12837892770767212
Loss at iteration 230 : 0.1328411102294922
Loss at iteration 240 : 0.08099744468927383
Loss at iteration 250 : 0.07459473609924316
Loss at iteration 260 : 0.08799787610769272
Loss at iteration 270 : 0.10047559440135956
Loss at iteration 280 : 0.12561583518981934
Loss at iteration 290 : 0.0902467593550682
Loss at iteration 300 : 0.06820123642683029
Loss at iteration 310 : 0.060850463807582855
Loss at iteration 320 : 0.11057732999324799
Loss at iteration 330 : 0.10816147178411484
Loss at iteration 340 : 0.19312283396720886
Loss at iteration 350 : 0.08303568512201309
Loss at iteration 360 : 0.11343558877706528
Loss at iteration 370 : 0.08988073468208313
Loss at iteration 380 : 0.13017818331718445
Loss at iteration 390 : 0.11316357553005219
Loss at iteration 400 : 0.08500823378562927
Loss at iteration 410 : 0.07180226594209671
Loss at iteration 420 : 0.10750331729650497
Loss at iteration 430 : 0.10831063985824585
Loss at iteration 440 : 0.08030806481838226
Loss at iteration 450 : 0.06843366473913193
Loss at iteration 460 : 0.07782213389873505
Loss at iteration 470 : 0.11775322258472443
Loss at iteration 480 : 0.12423863261938095
Loss at iteration 490 : 0.14567703008651733
Loss at iteration 500 : 0.07753697037696838
Loss at iteration 510 : 0.08877214044332504
Loss at iteration 520 : 0.08811867237091064
Loss at iteration 530 : 0.06032919883728027
Loss at iteration 540 : 0.0914258286356926
Loss at iteration 550 : 0.10524922609329224
Loss at iteration 560 : 0.06497307121753693
Loss at iteration 570 : 0.10054843127727509
Loss at iteration 580 : 0.05046889930963516
Loss at iteration 590 : 0.09426052868366241
Loss at iteration 600 : 0.1165226548910141
Loss at iteration 610 : 0.10997039079666138
Loss at iteration 620 : 0.10063683241605759
Loss at iteration 630 : 0.057791054248809814
Loss at iteration 640 : 0.10183708369731903
Loss at iteration 650 : 0.12119114398956299
Loss at iteration 660 : 0.0961516946554184
Loss at iteration 670 : 0.10224102437496185
Loss at iteration 680 : 0.13679049909114838
Loss at iteration 690 : 0.14978131651878357
Loss at iteration 700 : 0.0664156898856163
Loss at iteration 710 : 0.12273657321929932
Loss at iteration 720 : 0.12871384620666504
Loss at iteration 730 : 0.08831602334976196
Loss at iteration 740 : 0.12997731566429138
Loss at iteration 750 : 0.05368877947330475
Loss at iteration 760 : 0.10928463935852051
Loss at iteration 770 : 0.15518084168434143
Loss at iteration 780 : 0.09806092828512192
Loss at iteration 790 : 0.11752652376890182
Loss at iteration 800 : 0.07068021595478058
Loss at iteration 810 : 0.13474991917610168
Loss at iteration 820 : 0.08473943173885345
Loss at iteration 830 : 0.07578281313180923
Loss at iteration 840 : 0.07864966988563538
Loss at iteration 850 : 0.1324518620967865
Loss at iteration 860 : 0.11428454518318176
Loss at iteration 870 : 0.0659588873386383
Loss at iteration 880 : 0.134773850440979
Loss at iteration 890 : 0.08935191482305527
Loss at iteration 900 : 0.07713837921619415
Loss at iteration 910 : 0.0656702071428299
Loss at iteration 920 : 0.09291725605726242
Loss at iteration 930 : 0.09005375951528549
Loss at iteration 940 : 0.08175283670425415
Loss at iteration 950 : 0.10901783406734467
Loss at iteration 960 : 0.13407371938228607
Loss at iteration 970 : 0.09438943862915039
Loss at iteration 980 : 0.05481331795454025
Loss at iteration 990 : 0.06918782740831375
Loss at iteration 1000 : 0.11272381991147995
Loss at iteration 1010 : 0.10486602038145065
Loss at iteration 1020 : 0.11570765823125839
Loss at iteration 1030 : 0.07168881595134735
Loss at iteration 1040 : 0.09145732969045639
Loss at iteration 1050 : 0.10487942397594452
Loss at iteration 1060 : 0.0800335556268692
Loss at iteration 1070 : 0.12349843978881836
Loss at iteration 1080 : 0.08637425303459167
Loss at iteration 1090 : 0.11579208821058273
Loss at iteration 1100 : 0.11335112899541855
Loss at iteration 1110 : 0.14331494271755219
Loss at iteration 1120 : 0.07264770567417145
Loss at iteration 1130 : 0.09713666141033173
Loss at iteration 1140 : 0.10973377525806427
Loss at iteration 1150 : 0.08251160383224487
Loss at iteration 1160 : 0.11452601850032806
Loss at iteration 1170 : 0.16751937568187714
Loss at iteration 1180 : 0.08619274199008942
Loss at iteration 1190 : 0.09546386450529099
Loss at iteration 1200 : 0.0562533363699913
Loss at iteration 1210 : 0.05734562501311302
The SSIM Value is: 0.6647041499614715
The PSNR Value is: 19.900422922770183
the epoch is: 11
Loss at iteration 10 : 0.09604406356811523
Loss at iteration 20 : 0.06179719418287277
Loss at iteration 30 : 0.09531830251216888
Loss at iteration 40 : 0.09208525717258453
Loss at iteration 50 : 0.1293000876903534
Loss at iteration 60 : 0.0943952351808548
Loss at iteration 70 : 0.0887913703918457
Loss at iteration 80 : 0.11588636040687561
Loss at iteration 90 : 0.052894704043865204
Loss at iteration 100 : 0.059077635407447815
Loss at iteration 110 : 0.07588767260313034
Loss at iteration 120 : 0.10709762573242188
Loss at iteration 130 : 0.09900351613759995
Loss at iteration 140 : 0.12977781891822815
Loss at iteration 150 : 0.08158865571022034
Loss at iteration 160 : 0.1044807881116867
Loss at iteration 170 : 0.17303995788097382
Loss at iteration 180 : 0.059315040707588196
Loss at iteration 190 : 0.12545597553253174
Loss at iteration 200 : 0.11916365474462509
Loss at iteration 210 : 0.13806669414043427
Loss at iteration 220 : 0.10526762902736664
Loss at iteration 230 : 0.10570499300956726
Loss at iteration 240 : 0.13112995028495789
Loss at iteration 250 : 0.14157947897911072
Loss at iteration 260 : 0.07003586739301682
Loss at iteration 270 : 0.07746438682079315
Loss at iteration 280 : 0.08619643747806549
Loss at iteration 290 : 0.10413532704114914
Loss at iteration 300 : 0.11514639854431152
Loss at iteration 310 : 0.08810079097747803
Loss at iteration 320 : 0.10843034833669662
Loss at iteration 330 : 0.07519539445638657
Loss at iteration 340 : 0.07159686088562012
Loss at iteration 350 : 0.07486061751842499
Loss at iteration 360 : 0.1327163279056549
Loss at iteration 370 : 0.08181627839803696
Loss at iteration 380 : 0.091624915599823
Loss at iteration 390 : 0.14171457290649414
Loss at iteration 400 : 0.081362783908844
Loss at iteration 410 : 0.06406313180923462
Loss at iteration 420 : 0.10024868696928024
Loss at iteration 430 : 0.05574847757816315
Loss at iteration 440 : 0.06209218502044678
Loss at iteration 450 : 0.0988355204463005
Loss at iteration 460 : 0.07127349078655243
Loss at iteration 470 : 0.06187034398317337
Loss at iteration 480 : 0.11852677911520004
Loss at iteration 490 : 0.07392266392707825
Loss at iteration 500 : 0.1718127578496933
Loss at iteration 510 : 0.07768220454454422
Loss at iteration 520 : 0.0579720139503479
Loss at iteration 530 : 0.11065049469470978
Loss at iteration 540 : 0.08136758953332901
Loss at iteration 550 : 0.07926562428474426
Loss at iteration 560 : 0.09737794101238251
Loss at iteration 570 : 0.06218510866165161
Loss at iteration 580 : 0.06492891907691956
Loss at iteration 590 : 0.045310910791158676
Loss at iteration 600 : 0.10094847530126572
Loss at iteration 610 : 0.09187208116054535
Loss at iteration 620 : 0.10097156465053558
Loss at iteration 630 : 0.087985560297966
Loss at iteration 640 : 0.1061134785413742
Loss at iteration 650 : 0.07014083862304688
Loss at iteration 660 : 0.1362576186656952
Loss at iteration 670 : 0.05994595214724541
Loss at iteration 680 : 0.07960435003042221
Loss at iteration 690 : 0.08351417630910873
Loss at iteration 700 : 0.08707065880298615
Loss at iteration 710 : 0.1378779411315918
Loss at iteration 720 : 0.14375750720500946
Loss at iteration 730 : 0.06759589910507202
Loss at iteration 740 : 0.1507757306098938
Loss at iteration 750 : 0.11250071227550507
Loss at iteration 760 : 0.12999996542930603
Loss at iteration 770 : 0.07357712090015411
Loss at iteration 780 : 0.13705110549926758
Loss at iteration 790 : 0.09357853978872299
Loss at iteration 800 : 0.07369811832904816
Loss at iteration 810 : 0.12625016272068024
Loss at iteration 820 : 0.0804130882024765
Loss at iteration 830 : 0.06893078237771988
Loss at iteration 840 : 0.13526898622512817
Loss at iteration 850 : 0.06580152362585068
Loss at iteration 860 : 0.09229203313589096
Loss at iteration 870 : 0.1479164958000183
Loss at iteration 880 : 0.06737637519836426
Loss at iteration 890 : 0.06289644539356232
Loss at iteration 900 : 0.07243242859840393
Loss at iteration 910 : 0.06322146952152252
Loss at iteration 920 : 0.11245095729827881
Loss at iteration 930 : 0.11767192929983139
Loss at iteration 940 : 0.08949106186628342
Loss at iteration 950 : 0.13308654725551605
Loss at iteration 960 : 0.10666342824697495
Loss at iteration 970 : 0.09550264477729797
Loss at iteration 980 : 0.09191511571407318
Loss at iteration 990 : 0.08011046051979065
Loss at iteration 1000 : 0.07825217396020889
Loss at iteration 1010 : 0.12058393657207489
Loss at iteration 1020 : 0.106906458735466
Loss at iteration 1030 : 0.06955671310424805
Loss at iteration 1040 : 0.10310545563697815
Loss at iteration 1050 : 0.06589809060096741
Loss at iteration 1060 : 0.08886604011058807
Loss at iteration 1070 : 0.11753912270069122
Loss at iteration 1080 : 0.11352451145648956
Loss at iteration 1090 : 0.10791867226362228
Loss at iteration 1100 : 0.10267110913991928
Loss at iteration 1110 : 0.1112142950296402
Loss at iteration 1120 : 0.09416590631008148
Loss at iteration 1130 : 0.06491178274154663
Loss at iteration 1140 : 0.07164466381072998
Loss at iteration 1150 : 0.08193458616733551
Loss at iteration 1160 : 0.06296034902334213
Loss at iteration 1170 : 0.05815404653549194
Loss at iteration 1180 : 0.07423092424869537
Loss at iteration 1190 : 0.06645679473876953
Loss at iteration 1200 : 0.06269259750843048
Loss at iteration 1210 : 0.11544252932071686
The SSIM Value is: 0.6686156332492829
The PSNR Value is: 19.84790064493815
the epoch is: 12
Loss at iteration 10 : 0.06362231075763702
Loss at iteration 20 : 0.06487992405891418
Loss at iteration 30 : 0.04509488493204117
Loss at iteration 40 : 0.11582249402999878
Loss at iteration 50 : 0.11175337433815002
Loss at iteration 60 : 0.0796850174665451
Loss at iteration 70 : 0.11687036603689194
Loss at iteration 80 : 0.06457178294658661
Loss at iteration 90 : 0.11064277589321136
Loss at iteration 100 : 0.10708878934383392
Loss at iteration 110 : 0.0746387168765068
Loss at iteration 120 : 0.05966561287641525
Loss at iteration 130 : 0.06732355058193207
Loss at iteration 140 : 0.07410495728254318
Loss at iteration 150 : 0.08314945548772812
Loss at iteration 160 : 0.05904097482562065
Loss at iteration 170 : 0.10487663000822067
Loss at iteration 180 : 0.10699888318777084
Loss at iteration 190 : 0.11020102351903915
Loss at iteration 200 : 0.0826658234000206
Loss at iteration 210 : 0.0958731397986412
Loss at iteration 220 : 0.08556337654590607
Loss at iteration 230 : 0.07000163197517395
Loss at iteration 240 : 0.07075895369052887
Loss at iteration 250 : 0.11660435050725937
Loss at iteration 260 : 0.08767782151699066
Loss at iteration 270 : 0.17709991335868835
Loss at iteration 280 : 0.07885557413101196
Loss at iteration 290 : 0.10206978023052216
Loss at iteration 300 : 0.08938990533351898
Loss at iteration 310 : 0.1227719783782959
Loss at iteration 320 : 0.06311389803886414
Loss at iteration 330 : 0.12273194640874863
Loss at iteration 340 : 0.12452691048383713
Loss at iteration 350 : 0.08370852470397949
Loss at iteration 360 : 0.05742417275905609
Loss at iteration 370 : 0.08892244100570679
Loss at iteration 380 : 0.12583798170089722
Loss at iteration 390 : 0.10104211419820786
Loss at iteration 400 : 0.10743438452482224
Loss at iteration 410 : 0.10278862714767456
Loss at iteration 420 : 0.09159570932388306
Loss at iteration 430 : 0.09209611266851425
Loss at iteration 440 : 0.0767609179019928
Loss at iteration 450 : 0.13422100245952606
Loss at iteration 460 : 0.07931693643331528
Loss at iteration 470 : 0.13528844714164734
Loss at iteration 480 : 0.11179380118846893
Loss at iteration 490 : 0.11339391767978668
Loss at iteration 500 : 0.09655549377202988
Loss at iteration 510 : 0.04694264382123947
Loss at iteration 520 : 0.1291208267211914
Loss at iteration 530 : 0.09812717139720917
Loss at iteration 540 : 0.08674031496047974
Loss at iteration 550 : 0.11032569408416748
Loss at iteration 560 : 0.05480443686246872
Loss at iteration 570 : 0.047652699053287506
Loss at iteration 580 : 0.11000937223434448
Loss at iteration 590 : 0.08085441589355469
Loss at iteration 600 : 0.0851961076259613
Loss at iteration 610 : 0.10334568470716476
Loss at iteration 620 : 0.08290960639715195
Loss at iteration 630 : 0.09465460479259491
Loss at iteration 640 : 0.13712456822395325
Loss at iteration 650 : 0.04471918195486069
Loss at iteration 660 : 0.10617813467979431
Loss at iteration 670 : 0.14919167757034302
Loss at iteration 680 : 0.07244064658880234
Loss at iteration 690 : 0.13240301609039307
Loss at iteration 700 : 0.1265421211719513
Loss at iteration 710 : 0.11269696801900864
Loss at iteration 720 : 0.07582979649305344
Loss at iteration 730 : 0.11998288333415985
Loss at iteration 740 : 0.08618094772100449
Loss at iteration 750 : 0.06892649829387665
Loss at iteration 760 : 0.05574570223689079
Loss at iteration 770 : 0.1529812067747116
Loss at iteration 780 : 0.09213381260633469
Loss at iteration 790 : 0.08750668168067932
Loss at iteration 800 : 0.07025448977947235
Loss at iteration 810 : 0.11666497588157654
Loss at iteration 820 : 0.1364871859550476
Loss at iteration 830 : 0.08746202290058136
Loss at iteration 840 : 0.09178578853607178
Loss at iteration 850 : 0.07763925194740295
Loss at iteration 860 : 0.078142911195755
Loss at iteration 870 : 0.06150899454951286
Loss at iteration 880 : 0.08465740084648132
Loss at iteration 890 : 0.11686712503433228
Loss at iteration 900 : 0.12286126613616943
Loss at iteration 910 : 0.06994447857141495
Loss at iteration 920 : 0.10262402147054672
Loss at iteration 930 : 0.08026713132858276
Loss at iteration 940 : 0.08176510035991669
Loss at iteration 950 : 0.07139074057340622
Loss at iteration 960 : 0.08030454814434052
Loss at iteration 970 : 0.14079205691814423
Loss at iteration 980 : 0.0915554016828537
Loss at iteration 990 : 0.11097888648509979
Loss at iteration 1000 : 0.05407301336526871
Loss at iteration 1010 : 0.07978279888629913
Loss at iteration 1020 : 0.06066364049911499
Loss at iteration 1030 : 0.10413497686386108
Loss at iteration 1040 : 0.0748753547668457
Loss at iteration 1050 : 0.08603242039680481
Loss at iteration 1060 : 0.10620492696762085
Loss at iteration 1070 : 0.0822419822216034
Loss at iteration 1080 : 0.1338920295238495
Loss at iteration 1090 : 0.10046522319316864
Loss at iteration 1100 : 0.06482294201850891
Loss at iteration 1110 : 0.08898317813873291
Loss at iteration 1120 : 0.11756881326436996
Loss at iteration 1130 : 0.04868912696838379
Loss at iteration 1140 : 0.11924473941326141
Loss at iteration 1150 : 0.0752241462469101
Loss at iteration 1160 : 0.09707522392272949
Loss at iteration 1170 : 0.0851995199918747
Loss at iteration 1180 : 0.14470456540584564
Loss at iteration 1190 : 0.10594382137060165
Loss at iteration 1200 : 0.09196920692920685
Loss at iteration 1210 : 0.08744516968727112
The SSIM Value is: 0.6699483493963877
The PSNR Value is: 20.008883412679037
the epoch is: 13
Loss at iteration 10 : 0.07763467729091644
Loss at iteration 20 : 0.09221911430358887
Loss at iteration 30 : 0.10081788152456284
Loss at iteration 40 : 0.06291604787111282
Loss at iteration 50 : 0.06807157397270203
Loss at iteration 60 : 0.09192737936973572
Loss at iteration 70 : 0.11678960919380188
Loss at iteration 80 : 0.09790623188018799
Loss at iteration 90 : 0.14199718832969666
Loss at iteration 100 : 0.10403554886579514
Loss at iteration 110 : 0.14032092690467834
Loss at iteration 120 : 0.13460741937160492
Loss at iteration 130 : 0.07447513937950134
Loss at iteration 140 : 0.11908425390720367
Loss at iteration 150 : 0.10912841558456421
Loss at iteration 160 : 0.10792910307645798
Loss at iteration 170 : 0.08793296664953232
Loss at iteration 180 : 0.07775856554508209
Loss at iteration 190 : 0.12765435874462128
Loss at iteration 200 : 0.08257139474153519
Loss at iteration 210 : 0.09572380036115646
Loss at iteration 220 : 0.11058153212070465
Loss at iteration 230 : 0.07951328158378601
Loss at iteration 240 : 0.12307499349117279
Loss at iteration 250 : 0.10698111355304718
Loss at iteration 260 : 0.04279930889606476
Loss at iteration 270 : 0.08451911807060242
Loss at iteration 280 : 0.0548073872923851
Loss at iteration 290 : 0.09398870170116425
Loss at iteration 300 : 0.09495657682418823
Loss at iteration 310 : 0.06388092041015625
Loss at iteration 320 : 0.11689230799674988
Loss at iteration 330 : 0.09153656661510468
Loss at iteration 340 : 0.13037973642349243
Loss at iteration 350 : 0.0674462839961052
Loss at iteration 360 : 0.098915696144104
Loss at iteration 370 : 0.1052335649728775
Loss at iteration 380 : 0.07703837752342224
Loss at iteration 390 : 0.07843908667564392
Loss at iteration 400 : 0.11096750944852829
Loss at iteration 410 : 0.07465561479330063
Loss at iteration 420 : 0.15875521302223206
Loss at iteration 430 : 0.06386061757802963
Loss at iteration 440 : 0.06276514381170273
Loss at iteration 450 : 0.1346210241317749
Loss at iteration 460 : 0.08491631597280502
Loss at iteration 470 : 0.07735775411128998
Loss at iteration 480 : 0.09227337688207626
Loss at iteration 490 : 0.13475410640239716
Loss at iteration 500 : 0.05178048461675644
Loss at iteration 510 : 0.09344130009412766
Loss at iteration 520 : 0.08843906223773956
Loss at iteration 530 : 0.12095849961042404
Loss at iteration 540 : 0.11031969636678696
Loss at iteration 550 : 0.10109835863113403
Loss at iteration 560 : 0.07352228462696075
Loss at iteration 570 : 0.05895761772990227
Loss at iteration 580 : 0.058624234050512314
Loss at iteration 590 : 0.15958678722381592
Loss at iteration 600 : 0.10872706025838852
Loss at iteration 610 : 0.08690744638442993
Loss at iteration 620 : 0.06319092214107513
Loss at iteration 630 : 0.07431245595216751
Loss at iteration 640 : 0.13073685765266418
Loss at iteration 650 : 0.07895833253860474
Loss at iteration 660 : 0.08453180640935898
Loss at iteration 670 : 0.11411174386739731
Loss at iteration 680 : 0.08884091675281525
Loss at iteration 690 : 0.07805812358856201
Loss at iteration 700 : 0.10869743674993515
Loss at iteration 710 : 0.09291666746139526
Loss at iteration 720 : 0.09874986112117767
Loss at iteration 730 : 0.09237286448478699
Loss at iteration 740 : 0.06413844227790833
Loss at iteration 750 : 0.11026039719581604
Loss at iteration 760 : 0.06761212646961212
Loss at iteration 770 : 0.07459777593612671
Loss at iteration 780 : 0.072804756462574
Loss at iteration 790 : 0.10474793612957001
Loss at iteration 800 : 0.11727248132228851
Loss at iteration 810 : 0.058765143156051636
Loss at iteration 820 : 0.08363839983940125
Loss at iteration 830 : 0.12595981359481812
Loss at iteration 840 : 0.1017531156539917
Loss at iteration 850 : 0.08525323867797852
Loss at iteration 860 : 0.0606033019721508
Loss at iteration 870 : 0.10745809972286224
Loss at iteration 880 : 0.10067462921142578
Loss at iteration 890 : 0.09648215770721436
Loss at iteration 900 : 0.10274487733840942
Loss at iteration 910 : 0.12824198603630066
Loss at iteration 920 : 0.04308344051241875
Loss at iteration 930 : 0.09951545298099518
Loss at iteration 940 : 0.09074227511882782
Loss at iteration 950 : 0.10935348272323608
Loss at iteration 960 : 0.05119004100561142
Loss at iteration 970 : 0.08820568025112152
Loss at iteration 980 : 0.061573758721351624
Loss at iteration 990 : 0.1487210988998413
Loss at iteration 1000 : 0.11762511730194092
Loss at iteration 1010 : 0.1393510401248932
Loss at iteration 1020 : 0.05955535173416138
Loss at iteration 1030 : 0.08248022943735123
Loss at iteration 1040 : 0.10343426465988159
Loss at iteration 1050 : 0.1030268520116806
Loss at iteration 1060 : 0.07736963778734207
Loss at iteration 1070 : 0.11155909299850464
Loss at iteration 1080 : 0.06895150244235992
Loss at iteration 1090 : 0.0786990374326706
Loss at iteration 1100 : 0.10875507444143295
Loss at iteration 1110 : 0.1441812813282013
Loss at iteration 1120 : 0.07449477910995483
Loss at iteration 1130 : 0.10025963187217712
Loss at iteration 1140 : 0.0532723069190979
Loss at iteration 1150 : 0.08969321846961975
Loss at iteration 1160 : 0.10375414788722992
Loss at iteration 1170 : 0.0868673175573349
Loss at iteration 1180 : 0.08773373067378998
Loss at iteration 1190 : 0.07609334588050842
Loss at iteration 1200 : 0.0761994868516922
Loss at iteration 1210 : 0.09872744977474213
The SSIM Value is: 0.6754768709341685
The PSNR Value is: 20.203329785664877
the epoch is: 14
Loss at iteration 10 : 0.06333670020103455
Loss at iteration 20 : 0.10780471563339233
Loss at iteration 30 : 0.06009230762720108
Loss at iteration 40 : 0.127482607960701
Loss at iteration 50 : 0.0965382307767868
Loss at iteration 60 : 0.11085236072540283
Loss at iteration 70 : 0.10122963041067123
Loss at iteration 80 : 0.08496420085430145
Loss at iteration 90 : 0.044703226536512375
Loss at iteration 100 : 0.12551334500312805
Loss at iteration 110 : 0.10873822867870331
Loss at iteration 120 : 0.0633406937122345
Loss at iteration 130 : 0.07728196680545807
Loss at iteration 140 : 0.11696331948041916
Loss at iteration 150 : 0.06139715760946274
Loss at iteration 160 : 0.08333170413970947
Loss at iteration 170 : 0.13089928030967712
Loss at iteration 180 : 0.088723324239254
Loss at iteration 190 : 0.06794575601816177
Loss at iteration 200 : 0.10463301837444305
Loss at iteration 210 : 0.09195294231176376
Loss at iteration 220 : 0.10602428764104843
Loss at iteration 230 : 0.08570989221334457
Loss at iteration 240 : 0.09269347786903381
Loss at iteration 250 : 0.09939249604940414
Loss at iteration 260 : 0.07729248702526093
Loss at iteration 270 : 0.1300784796476364
Loss at iteration 280 : 0.1591419279575348
Loss at iteration 290 : 0.10034386813640594
Loss at iteration 300 : 0.12588804960250854
Loss at iteration 310 : 0.13809531927108765
Loss at iteration 320 : 0.09153096377849579
Loss at iteration 330 : 0.06097201257944107
Loss at iteration 340 : 0.09924419224262238
Loss at iteration 350 : 0.10779942572116852
Loss at iteration 360 : 0.05496618151664734
Loss at iteration 370 : 0.06612947583198547
Loss at iteration 380 : 0.06659089028835297
Loss at iteration 390 : 0.051275938749313354
Loss at iteration 400 : 0.10228480398654938
Loss at iteration 410 : 0.05248899385333061
Loss at iteration 420 : 0.10378853976726532
Loss at iteration 430 : 0.11973313987255096
Loss at iteration 440 : 0.09288531541824341
Loss at iteration 450 : 0.060092151165008545
Loss at iteration 460 : 0.05713441222906113
Loss at iteration 470 : 0.10860560834407806
Loss at iteration 480 : 0.11808440089225769
Loss at iteration 490 : 0.08385084569454193
Loss at iteration 500 : 0.0601997897028923
Loss at iteration 510 : 0.09529076516628265
Loss at iteration 520 : 0.07263942062854767
Loss at iteration 530 : 0.1429549753665924
Loss at iteration 540 : 0.08148443698883057
Loss at iteration 550 : 0.07651245594024658
Loss at iteration 560 : 0.046031564474105835
Loss at iteration 570 : 0.08093306422233582
Loss at iteration 580 : 0.09920473396778107
Loss at iteration 590 : 0.07543100416660309
Loss at iteration 600 : 0.1519557535648346
Loss at iteration 610 : 0.09678244590759277
Loss at iteration 620 : 0.08550690114498138
Loss at iteration 630 : 0.13524839282035828
Loss at iteration 640 : 0.10999900102615356
Loss at iteration 650 : 0.08693426102399826
Loss at iteration 660 : 0.0950118899345398
Loss at iteration 670 : 0.08987683057785034
Loss at iteration 680 : 0.0789862722158432
Loss at iteration 690 : 0.07241549342870712
Loss at iteration 700 : 0.07843029499053955
Loss at iteration 710 : 0.06589758396148682
Loss at iteration 720 : 0.10829254984855652
Loss at iteration 730 : 0.06960240006446838
Loss at iteration 740 : 0.096730075776577
Loss at iteration 750 : 0.115563303232193
Loss at iteration 760 : 0.1333567202091217
Loss at iteration 770 : 0.11581413447856903
Loss at iteration 780 : 0.10717728734016418
Loss at iteration 790 : 0.07466505467891693
Loss at iteration 800 : 0.09063007682561874
Loss at iteration 810 : 0.08410491794347763
Loss at iteration 820 : 0.09099040925502777
Loss at iteration 830 : 0.09295939654111862
Loss at iteration 840 : 0.07747088372707367
Loss at iteration 850 : 0.10947339236736298
Loss at iteration 860 : 0.06148891896009445
Loss at iteration 870 : 0.1295553296804428
Loss at iteration 880 : 0.06562462449073792
Loss at iteration 890 : 0.10257047414779663
Loss at iteration 900 : 0.08434990048408508
Loss at iteration 910 : 0.12588641047477722
Loss at iteration 920 : 0.11867346614599228
Loss at iteration 930 : 0.05875902995467186
Loss at iteration 940 : 0.08887732028961182
Loss at iteration 950 : 0.1311693787574768
Loss at iteration 960 : 0.0818348079919815
Loss at iteration 970 : 0.10047376155853271
Loss at iteration 980 : 0.08612674474716187
Loss at iteration 990 : 0.08962386846542358
Loss at iteration 1000 : 0.08567949384450912
Loss at iteration 1010 : 0.0838119238615036
Loss at iteration 1020 : 0.08665180206298828
Loss at iteration 1030 : 0.07690577208995819
Loss at iteration 1040 : 0.15555280447006226
Loss at iteration 1050 : 0.08469539880752563
Loss at iteration 1060 : 0.05558650940656662
Loss at iteration 1070 : 0.1107579693198204
Loss at iteration 1080 : 0.08434529602527618
Loss at iteration 1090 : 0.10440399497747421
Loss at iteration 1100 : 0.044649310410022736
Loss at iteration 1110 : 0.06638312339782715
Loss at iteration 1120 : 0.079159215092659
Loss at iteration 1130 : 0.1370513141155243
Loss at iteration 1140 : 0.18157273530960083
Loss at iteration 1150 : 0.07467152178287506
Loss at iteration 1160 : 0.10650873184204102
Loss at iteration 1170 : 0.10314770042896271
Loss at iteration 1180 : 0.10149456560611725
Loss at iteration 1190 : 0.076610267162323
Loss at iteration 1200 : 0.10008998960256577
Loss at iteration 1210 : 0.058152686804533005
The SSIM Value is: 0.6755068520704905
The PSNR Value is: 20.212543996175132
the epoch is: 15
Loss at iteration 10 : 0.07834961265325546
Loss at iteration 20 : 0.10440264642238617
Loss at iteration 30 : 0.08959215879440308
Loss at iteration 40 : 0.09903206676244736
Loss at iteration 50 : 0.08596408367156982
Loss at iteration 60 : 0.13091902434825897
Loss at iteration 70 : 0.07706382125616074
Loss at iteration 80 : 0.07718014717102051
Loss at iteration 90 : 0.09325967729091644
Loss at iteration 100 : 0.1202983409166336
Loss at iteration 110 : 0.07177258282899857
Loss at iteration 120 : 0.08476050943136215
Loss at iteration 130 : 0.06634697318077087
Loss at iteration 140 : 0.07099440693855286
Loss at iteration 150 : 0.0637444257736206
Loss at iteration 160 : 0.09086017310619354
Loss at iteration 170 : 0.16096660494804382
Loss at iteration 180 : 0.10535010695457458
Loss at iteration 190 : 0.09916669130325317
Loss at iteration 200 : 0.10149228572845459
Loss at iteration 210 : 0.10190226882696152
Loss at iteration 220 : 0.11338727176189423
Loss at iteration 230 : 0.08739962428808212
Loss at iteration 240 : 0.14504167437553406
Loss at iteration 250 : 0.13931988179683685
Loss at iteration 260 : 0.11448054760694504
Loss at iteration 270 : 0.09058642387390137
Loss at iteration 280 : 0.05475636571645737
Loss at iteration 290 : 0.11905389279127121
Loss at iteration 300 : 0.07402467727661133
Loss at iteration 310 : 0.09126470983028412
Loss at iteration 320 : 0.15171772241592407
Loss at iteration 330 : 0.07215029001235962
Loss at iteration 340 : 0.060810014605522156
Loss at iteration 350 : 0.1143965944647789
Loss at iteration 360 : 0.061755310744047165
Loss at iteration 370 : 0.10211312770843506
Loss at iteration 380 : 0.13256524503231049
Loss at iteration 390 : 0.06999064981937408
Loss at iteration 400 : 0.13157159090042114
Loss at iteration 410 : 0.07685940712690353
Loss at iteration 420 : 0.1087825819849968
Loss at iteration 430 : 0.09142225235700607
Loss at iteration 440 : 0.11419427394866943
Loss at iteration 450 : 0.06101661175489426
Loss at iteration 460 : 0.07862509042024612
Loss at iteration 470 : 0.07233797013759613
Loss at iteration 480 : 0.05069402977824211
Loss at iteration 490 : 0.0902784913778305
Loss at iteration 500 : 0.14874732494354248
Loss at iteration 510 : 0.0798860415816307
Loss at iteration 520 : 0.07719144225120544
Loss at iteration 530 : 0.08501395583152771
Loss at iteration 540 : 0.05747811123728752
Loss at iteration 550 : 0.0754060447216034
Loss at iteration 560 : 0.09830164164304733
Loss at iteration 570 : 0.08905656635761261
Loss at iteration 580 : 0.11530273407697678
Loss at iteration 590 : 0.14119194447994232
Loss at iteration 600 : 0.07470677047967911
Loss at iteration 610 : 0.08711443841457367
Loss at iteration 620 : 0.09610887616872787
Loss at iteration 630 : 0.08682073652744293
Loss at iteration 640 : 0.08130425214767456
Loss at iteration 650 : 0.08932963013648987
Loss at iteration 660 : 0.11237555742263794
Loss at iteration 670 : 0.11536204814910889
Loss at iteration 680 : 0.09860515594482422
Loss at iteration 690 : 0.09634395688772202
Loss at iteration 700 : 0.07397092878818512
Loss at iteration 710 : 0.09484873712062836
Loss at iteration 720 : 0.09561457484960556
Loss at iteration 730 : 0.0890631377696991
Loss at iteration 740 : 0.08472126722335815
Loss at iteration 750 : 0.1442091166973114
Loss at iteration 760 : 0.11060953140258789
Loss at iteration 770 : 0.09364637732505798
Loss at iteration 780 : 0.11499099433422089
Loss at iteration 790 : 0.07215329259634018
Loss at iteration 800 : 0.09329455345869064
Loss at iteration 810 : 0.08519577980041504
Loss at iteration 820 : 0.08854442834854126
Loss at iteration 830 : 0.13580408692359924
Loss at iteration 840 : 0.1515161097049713
Loss at iteration 850 : 0.09592793881893158
Loss at iteration 860 : 0.09839412569999695
Loss at iteration 870 : 0.07167959213256836
Loss at iteration 880 : 0.09230659902095795
Loss at iteration 890 : 0.0752301961183548
Loss at iteration 900 : 0.065603107213974
Loss at iteration 910 : 0.1085088849067688
Loss at iteration 920 : 0.05758161470293999
Loss at iteration 930 : 0.11614207923412323
Loss at iteration 940 : 0.13618282973766327
Loss at iteration 950 : 0.09339793771505356
Loss at iteration 960 : 0.1285037100315094
Loss at iteration 970 : 0.08396818488836288
Loss at iteration 980 : 0.11831299960613251
Loss at iteration 990 : 0.10289880633354187
Loss at iteration 1000 : 0.13448861241340637
Loss at iteration 1010 : 0.03997288644313812
Loss at iteration 1020 : 0.07574678212404251
Loss at iteration 1030 : 0.11291337013244629
Loss at iteration 1040 : 0.04818296432495117
Loss at iteration 1050 : 0.10854959487915039
Loss at iteration 1060 : 0.0557088777422905
Loss at iteration 1070 : 0.0832132026553154
Loss at iteration 1080 : 0.050216224044561386
Loss at iteration 1090 : 0.07120378315448761
Loss at iteration 1100 : 0.10889130085706711
Loss at iteration 1110 : 0.07931365817785263
Loss at iteration 1120 : 0.07872149348258972
Loss at iteration 1130 : 0.12654362618923187
Loss at iteration 1140 : 0.0936952456831932
Loss at iteration 1150 : 0.09924577176570892
Loss at iteration 1160 : 0.1715947836637497
Loss at iteration 1170 : 0.11570953577756882
Loss at iteration 1180 : 0.06457802653312683
Loss at iteration 1190 : 0.10925427079200745
Loss at iteration 1200 : 0.09722989052534103
Loss at iteration 1210 : 0.07341969758272171
The SSIM Value is: 0.6761868596076965
The PSNR Value is: 20.151082102457682
the epoch is: 16
Loss at iteration 10 : 0.12196914851665497
Loss at iteration 20 : 0.07734965533018112
Loss at iteration 30 : 0.09002711623907089
Loss at iteration 40 : 0.08513781428337097
Loss at iteration 50 : 0.06461575627326965
Loss at iteration 60 : 0.08557376265525818
Loss at iteration 70 : 0.08286105841398239
Loss at iteration 80 : 0.10417576879262924
Loss at iteration 90 : 0.1681065559387207
Loss at iteration 100 : 0.06856081634759903
Loss at iteration 110 : 0.07738970220088959
Loss at iteration 120 : 0.13588744401931763
Loss at iteration 130 : 0.0731704980134964
Loss at iteration 140 : 0.10408654063940048
Loss at iteration 150 : 0.07414638996124268
Loss at iteration 160 : 0.11945786327123642
Loss at iteration 170 : 0.06730299443006516
Loss at iteration 180 : 0.05427916347980499
Loss at iteration 190 : 0.12123921513557434
Loss at iteration 200 : 0.09370895475149155
Loss at iteration 210 : 0.06823481619358063
Loss at iteration 220 : 0.10370878875255585
Loss at iteration 230 : 0.09849654138088226
Loss at iteration 240 : 0.07137510180473328
Loss at iteration 250 : 0.10496979206800461
Loss at iteration 260 : 0.07986167073249817
Loss at iteration 270 : 0.06724949926137924
Loss at iteration 280 : 0.08611920475959778
Loss at iteration 290 : 0.0751720666885376
Loss at iteration 300 : 0.10034094750881195
Loss at iteration 310 : 0.1040952056646347
Loss at iteration 320 : 0.06625531613826752
Loss at iteration 330 : 0.09702956676483154
Loss at iteration 340 : 0.1250256448984146
Loss at iteration 350 : 0.051674120128154755
Loss at iteration 360 : 0.1064683198928833
Loss at iteration 370 : 0.1273825317621231
Loss at iteration 380 : 0.08782574534416199
Loss at iteration 390 : 0.08969172090291977
Loss at iteration 400 : 0.07343896478414536
Loss at iteration 410 : 0.09460684657096863
Loss at iteration 420 : 0.10265900194644928
Loss at iteration 430 : 0.06331675499677658
Loss at iteration 440 : 0.14587253332138062
Loss at iteration 450 : 0.10339885205030441
Loss at iteration 460 : 0.05094210430979729
Loss at iteration 470 : 0.06514851748943329
Loss at iteration 480 : 0.1058356985449791
Loss at iteration 490 : 0.15040472149848938
Loss at iteration 500 : 0.08282078802585602
Loss at iteration 510 : 0.06932736933231354
Loss at iteration 520 : 0.11323335021734238
Loss at iteration 530 : 0.10040714591741562
Loss at iteration 540 : 0.10034924745559692
Loss at iteration 550 : 0.07352400571107864
Loss at iteration 560 : 0.1159774661064148
Loss at iteration 570 : 0.08281323313713074
Loss at iteration 580 : 0.0728917047381401
Loss at iteration 590 : 0.10366790741682053
Loss at iteration 600 : 0.05746588855981827
Loss at iteration 610 : 0.1405813992023468
Loss at iteration 620 : 0.07607408612966537
Loss at iteration 630 : 0.11773458123207092
Loss at iteration 640 : 0.09600561857223511
Loss at iteration 650 : 0.06558556854724884
Loss at iteration 660 : 0.10702966153621674
Loss at iteration 670 : 0.08141197264194489
Loss at iteration 680 : 0.11377577483654022
Loss at iteration 690 : 0.0727662742137909
Loss at iteration 700 : 0.08447512239217758
Loss at iteration 710 : 0.15924598276615143
Loss at iteration 720 : 0.06221257895231247
Loss at iteration 730 : 0.08431869000196457
Loss at iteration 740 : 0.08248455822467804
Loss at iteration 750 : 0.09798756241798401
Loss at iteration 760 : 0.07101244479417801
Loss at iteration 770 : 0.08042192459106445
Loss at iteration 780 : 0.11731599271297455
Loss at iteration 790 : 0.05445943772792816
Loss at iteration 800 : 0.0859784334897995
Loss at iteration 810 : 0.0859699472784996
Loss at iteration 820 : 0.08060871809720993
Loss at iteration 830 : 0.1030036211013794
Loss at iteration 840 : 0.11231061071157455
Loss at iteration 850 : 0.09204451739788055
Loss at iteration 860 : 0.046446867287158966
Loss at iteration 870 : 0.09319275617599487
Loss at iteration 880 : 0.08234955370426178
Loss at iteration 890 : 0.12436848878860474
Loss at iteration 900 : 0.09473635256290436
Loss at iteration 910 : 0.1229715347290039
Loss at iteration 920 : 0.10793659090995789
Loss at iteration 930 : 0.10901887714862823
Loss at iteration 940 : 0.0939130187034607
Loss at iteration 950 : 0.061025094240903854
Loss at iteration 960 : 0.06682609766721725
Loss at iteration 970 : 0.06435370445251465
Loss at iteration 980 : 0.10544856637716293
Loss at iteration 990 : 0.13441097736358643
Loss at iteration 1000 : 0.0862661600112915
Loss at iteration 1010 : 0.11632915586233139
Loss at iteration 1020 : 0.07976556569337845
Loss at iteration 1030 : 0.0986967533826828
Loss at iteration 1040 : 0.1384255737066269
Loss at iteration 1050 : 0.09573899209499359
Loss at iteration 1060 : 0.09989926218986511
Loss at iteration 1070 : 0.0894247516989708
Loss at iteration 1080 : 0.07581872493028641
Loss at iteration 1090 : 0.12289504706859589
Loss at iteration 1100 : 0.05620083212852478
Loss at iteration 1110 : 0.1408216953277588
Loss at iteration 1120 : 0.08753558248281479
Loss at iteration 1130 : 0.09371452033519745
Loss at iteration 1140 : 0.07483666390180588
Loss at iteration 1150 : 0.13978883624076843
Loss at iteration 1160 : 0.1413954496383667
Loss at iteration 1170 : 0.09223273396492004
Loss at iteration 1180 : 0.10362815856933594
Loss at iteration 1190 : 0.14274528622627258
Loss at iteration 1200 : 0.1268300712108612
Loss at iteration 1210 : 0.10437038540840149
The SSIM Value is: 0.6776024083296458
The PSNR Value is: 20.381474240620932
the highest SSIM value is: 20.381474240620932
the epoch is: 17
Loss at iteration 10 : 0.06314344704151154
Loss at iteration 20 : 0.14293813705444336
Loss at iteration 30 : 0.06969073414802551
Loss at iteration 40 : 0.07382595539093018
Loss at iteration 50 : 0.06689536571502686
Loss at iteration 60 : 0.12294387817382812
Loss at iteration 70 : 0.10147049278020859
Loss at iteration 80 : 0.1071619838476181
Loss at iteration 90 : 0.05660579726099968
Loss at iteration 100 : 0.06805827468633652
Loss at iteration 110 : 0.09099610894918442
Loss at iteration 120 : 0.07779579609632492
Loss at iteration 130 : 0.08491110801696777
Loss at iteration 140 : 0.11040915548801422
Loss at iteration 150 : 0.10684224963188171
Loss at iteration 160 : 0.056209027767181396
Loss at iteration 170 : 0.06609565764665604
Loss at iteration 180 : 0.14663870632648468
Loss at iteration 190 : 0.12345980852842331
Loss at iteration 200 : 0.15359200537204742
Loss at iteration 210 : 0.08097243309020996
Loss at iteration 220 : 0.12219536304473877
Loss at iteration 230 : 0.11437636613845825
Loss at iteration 240 : 0.09858900308609009
Loss at iteration 250 : 0.09576408565044403
Loss at iteration 260 : 0.09928856790065765
Loss at iteration 270 : 0.07902006804943085
Loss at iteration 280 : 0.06612729281187057
Loss at iteration 290 : 0.14389359951019287
Loss at iteration 300 : 0.08179058134555817
Loss at iteration 310 : 0.07725699990987778
Loss at iteration 320 : 0.08442362397909164
Loss at iteration 330 : 0.10527580231428146
Loss at iteration 340 : 0.06722422689199448
Loss at iteration 350 : 0.08372330665588379
Loss at iteration 360 : 0.09206768125295639
Loss at iteration 370 : 0.0819569081068039
Loss at iteration 380 : 0.10837380588054657
Loss at iteration 390 : 0.08971107006072998
Loss at iteration 400 : 0.04289428889751434
Loss at iteration 410 : 0.10951944440603256
Loss at iteration 420 : 0.09893330186605453
Loss at iteration 430 : 0.10259264707565308
Loss at iteration 440 : 0.11457161605358124
Loss at iteration 450 : 0.09307634830474854
Loss at iteration 460 : 0.11183507740497589
Loss at iteration 470 : 0.1258678436279297
Loss at iteration 480 : 0.13634005188941956
Loss at iteration 490 : 0.0810365080833435
Loss at iteration 500 : 0.06259124726057053
Loss at iteration 510 : 0.08307624608278275
Loss at iteration 520 : 0.12873297929763794
Loss at iteration 530 : 0.11998528242111206
Loss at iteration 540 : 0.05159839242696762
Loss at iteration 550 : 0.08585136383771896
Loss at iteration 560 : 0.08019945025444031
Loss at iteration 570 : 0.10284116864204407
Loss at iteration 580 : 0.08518242835998535
Loss at iteration 590 : 0.1148720383644104
Loss at iteration 600 : 0.14182670414447784
Loss at iteration 610 : 0.12490956485271454
Loss at iteration 620 : 0.10275005549192429
Loss at iteration 630 : 0.0986737608909607
Loss at iteration 640 : 0.05724913626909256
Loss at iteration 650 : 0.13534292578697205
Loss at iteration 660 : 0.07480180263519287
Loss at iteration 670 : 0.08755989372730255
Loss at iteration 680 : 0.05965320020914078
Loss at iteration 690 : 0.1084672063589096
Loss at iteration 700 : 0.06132793426513672
Loss at iteration 710 : 0.0905635803937912
Loss at iteration 720 : 0.06885001808404922
Loss at iteration 730 : 0.10357936471700668
Loss at iteration 740 : 0.06959778070449829
Loss at iteration 750 : 0.08741926401853561
Loss at iteration 760 : 0.07740900665521622
Loss at iteration 770 : 0.06725608557462692
Loss at iteration 780 : 0.0942189022898674
Loss at iteration 790 : 0.0629202350974083
Loss at iteration 800 : 0.06849803030490875
Loss at iteration 810 : 0.12962429225444794
Loss at iteration 820 : 0.060411207377910614
Loss at iteration 830 : 0.0716177374124527
Loss at iteration 840 : 0.06868235766887665
Loss at iteration 850 : 0.07480259984731674
Loss at iteration 860 : 0.10047724843025208
Loss at iteration 870 : 0.0681360512971878
Loss at iteration 880 : 0.11371973156929016
Loss at iteration 890 : 0.06882380694150925
Loss at iteration 900 : 0.10008305311203003
Loss at iteration 910 : 0.10311019420623779
Loss at iteration 920 : 0.09552287310361862
Loss at iteration 930 : 0.11885794252157211
Loss at iteration 940 : 0.12846139073371887
Loss at iteration 950 : 0.06761042773723602
Loss at iteration 960 : 0.0975116491317749
Loss at iteration 970 : 0.13026659190654755
Loss at iteration 980 : 0.11342287063598633
Loss at iteration 990 : 0.1133461445569992
Loss at iteration 1000 : 0.08891957998275757
Loss at iteration 1010 : 0.07435636222362518
Loss at iteration 1020 : 0.09435676783323288
Loss at iteration 1030 : 0.09098126739263535
Loss at iteration 1040 : 0.12402652949094772
Loss at iteration 1050 : 0.08121281117200851
Loss at iteration 1060 : 0.06250299513339996
Loss at iteration 1070 : 0.07153621315956116
Loss at iteration 1080 : 0.08906356990337372
Loss at iteration 1090 : 0.09021149575710297
Loss at iteration 1100 : 0.07595067471265793
Loss at iteration 1110 : 0.08589710295200348
Loss at iteration 1120 : 0.09823476523160934
Loss at iteration 1130 : 0.0661369264125824
Loss at iteration 1140 : 0.1039355918765068
Loss at iteration 1150 : 0.07913853228092194
Loss at iteration 1160 : 0.09348659217357635
Loss at iteration 1170 : 0.09103326499462128
Loss at iteration 1180 : 0.07860468327999115
Loss at iteration 1190 : 0.0720617026090622
Loss at iteration 1200 : 0.09037061035633087
Loss at iteration 1210 : 0.092010498046875
The SSIM Value is: 0.6781148274739583
The PSNR Value is: 20.293152300516763
the epoch is: 18
Loss at iteration 10 : 0.09281228482723236
Loss at iteration 20 : 0.0837184339761734
Loss at iteration 30 : 0.08961628377437592
Loss at iteration 40 : 0.1184137687087059
Loss at iteration 50 : 0.06839263439178467
Loss at iteration 60 : 0.0924811065196991
Loss at iteration 70 : 0.08315219730138779
Loss at iteration 80 : 0.09304030239582062
Loss at iteration 90 : 0.09793301671743393
Loss at iteration 100 : 0.09277427196502686
Loss at iteration 110 : 0.08030492067337036
Loss at iteration 120 : 0.10824123024940491
Loss at iteration 130 : 0.07589645683765411
Loss at iteration 140 : 0.10733484476804733
Loss at iteration 150 : 0.096959188580513
Loss at iteration 160 : 0.09223712980747223
Loss at iteration 170 : 0.1260737031698227
Loss at iteration 180 : 0.05376764386892319
Loss at iteration 190 : 0.06720294058322906
Loss at iteration 200 : 0.12685595452785492
Loss at iteration 210 : 0.08745857328176498
Loss at iteration 220 : 0.10005414485931396
Loss at iteration 230 : 0.07967911660671234
Loss at iteration 240 : 0.06901457160711288
Loss at iteration 250 : 0.08705727756023407
Loss at iteration 260 : 0.10366491973400116
Loss at iteration 270 : 0.09822505712509155
Loss at iteration 280 : 0.14062157273292542
Loss at iteration 290 : 0.11021535098552704
Loss at iteration 300 : 0.08445678651332855
Loss at iteration 310 : 0.12020846456289291
Loss at iteration 320 : 0.11058540642261505
Loss at iteration 330 : 0.09851086139678955
Loss at iteration 340 : 0.0905369371175766
Loss at iteration 350 : 0.08027435839176178
Loss at iteration 360 : 0.0831291526556015
Loss at iteration 370 : 0.07964737713336945
Loss at iteration 380 : 0.08546429872512817
Loss at iteration 390 : 0.08483951538801193
Loss at iteration 400 : 0.1818009614944458
Loss at iteration 410 : 0.09986592829227448
Loss at iteration 420 : 0.1037168800830841
Loss at iteration 430 : 0.06550641357898712
Loss at iteration 440 : 0.06698354333639145
Loss at iteration 450 : 0.11289986222982407
Loss at iteration 460 : 0.13880109786987305
Loss at iteration 470 : 0.1373865008354187
Loss at iteration 480 : 0.0969453975558281
Loss at iteration 490 : 0.07437720149755478
Loss at iteration 500 : 0.07955263555049896
Loss at iteration 510 : 0.08698134124279022
Loss at iteration 520 : 0.07164239138364792
Loss at iteration 530 : 0.13004940748214722
Loss at iteration 540 : 0.15226110816001892
Loss at iteration 550 : 0.09584009647369385
Loss at iteration 560 : 0.07835667580366135
Loss at iteration 570 : 0.07352319359779358
Loss at iteration 580 : 0.06386392563581467
Loss at iteration 590 : 0.14195704460144043
Loss at iteration 600 : 0.1329766809940338
Loss at iteration 610 : 0.08969134092330933
Loss at iteration 620 : 0.1151660829782486
Loss at iteration 630 : 0.1359163224697113
Loss at iteration 640 : 0.0882236585021019
Loss at iteration 650 : 0.15149426460266113
Loss at iteration 660 : 0.1018064022064209
Loss at iteration 670 : 0.10733930766582489
Loss at iteration 680 : 0.0746326744556427
Loss at iteration 690 : 0.06478486955165863
Loss at iteration 700 : 0.09012272208929062
Loss at iteration 710 : 0.06306640803813934
Loss at iteration 720 : 0.08663403987884521
Loss at iteration 730 : 0.13338187336921692
Loss at iteration 740 : 0.09180396050214767
Loss at iteration 750 : 0.10663839429616928
Loss at iteration 760 : 0.10654684156179428
Loss at iteration 770 : 0.07744354009628296
Loss at iteration 780 : 0.08596434444189072
Loss at iteration 790 : 0.14387837052345276
Loss at iteration 800 : 0.08669304847717285
Loss at iteration 810 : 0.07845565676689148
Loss at iteration 820 : 0.10230287909507751
Loss at iteration 830 : 0.04573787376284599
Loss at iteration 840 : 0.0778806135058403
Loss at iteration 850 : 0.0911233127117157
Loss at iteration 860 : 0.06972788274288177
Loss at iteration 870 : 0.08183366060256958
Loss at iteration 880 : 0.08773598074913025
Loss at iteration 890 : 0.11837606132030487
Loss at iteration 900 : 0.08281607925891876
Loss at iteration 910 : 0.07737759500741959
Loss at iteration 920 : 0.08593323081731796
Loss at iteration 930 : 0.11852961778640747
Loss at iteration 940 : 0.06817030161619186
Loss at iteration 950 : 0.14714469015598297
Loss at iteration 960 : 0.09142628312110901
Loss at iteration 970 : 0.09265430271625519
Loss at iteration 980 : 0.09396424889564514
Loss at iteration 990 : 0.08919303864240646
Loss at iteration 1000 : 0.08436330407857895
Loss at iteration 1010 : 0.13940392434597015
Loss at iteration 1020 : 0.09317668527364731
Loss at iteration 1030 : 0.052305079996585846
Loss at iteration 1040 : 0.09598984569311142
Loss at iteration 1050 : 0.1328558325767517
Loss at iteration 1060 : 0.145059272646904
Loss at iteration 1070 : 0.10329630225896835
Loss at iteration 1080 : 0.14475956559181213
Loss at iteration 1090 : 0.09898417443037033
Loss at iteration 1100 : 0.10401463508605957
Loss at iteration 1110 : 0.11245638132095337
Loss at iteration 1120 : 0.08470454066991806
Loss at iteration 1130 : 0.06702422350645065
Loss at iteration 1140 : 0.1594817340373993
Loss at iteration 1150 : 0.06362934410572052
Loss at iteration 1160 : 0.07797500491142273
Loss at iteration 1170 : 0.0709296241402626
Loss at iteration 1180 : 0.09174724668264389
Loss at iteration 1190 : 0.05963435769081116
Loss at iteration 1200 : 0.09141810983419418
Loss at iteration 1210 : 0.06418834626674652
The SSIM Value is: 0.6722693383693695
The PSNR Value is: 19.83492520650228
the epoch is: 19
Loss at iteration 10 : 0.07005860656499863
Loss at iteration 20 : 0.08206453919410706
Loss at iteration 30 : 0.08238336443901062
Loss at iteration 40 : 0.10752429813146591
Loss at iteration 50 : 0.09777826070785522
Loss at iteration 60 : 0.07030056416988373
Loss at iteration 70 : 0.07675649225711823
Loss at iteration 80 : 0.0689750462770462
Loss at iteration 90 : 0.08897717297077179
Loss at iteration 100 : 0.11291027069091797
Loss at iteration 110 : 0.12780511379241943
Loss at iteration 120 : 0.08819067478179932
Loss at iteration 130 : 0.12463691085577011
Loss at iteration 140 : 0.07742759585380554
Loss at iteration 150 : 0.06398438662290573
Loss at iteration 160 : 0.08124519139528275
Loss at iteration 170 : 0.09522464126348495
Loss at iteration 180 : 0.13559949398040771
Loss at iteration 190 : 0.06849506497383118
Loss at iteration 200 : 0.09712286293506622
Loss at iteration 210 : 0.08784717321395874
Loss at iteration 220 : 0.09899218380451202
Loss at iteration 230 : 0.08357115834951401
Loss at iteration 240 : 0.12213369458913803
Loss at iteration 250 : 0.076692134141922
Loss at iteration 260 : 0.09475623071193695
Loss at iteration 270 : 0.08684743940830231
Loss at iteration 280 : 0.05530440807342529
Loss at iteration 290 : 0.05831430107355118
Loss at iteration 300 : 0.1277952492237091
Loss at iteration 310 : 0.11731420457363129
Loss at iteration 320 : 0.1005006656050682
Loss at iteration 330 : 0.052372515201568604
Loss at iteration 340 : 0.0764005035161972
Loss at iteration 350 : 0.14154671132564545
Loss at iteration 360 : 0.08770765364170074
Loss at iteration 370 : 0.07516182959079742
Loss at iteration 380 : 0.05892711132764816
Loss at iteration 390 : 0.08987760543823242
Loss at iteration 400 : 0.0976112112402916
Loss at iteration 410 : 0.11804169416427612
Loss at iteration 420 : 0.03723324090242386
Loss at iteration 430 : 0.08291793614625931
Loss at iteration 440 : 0.11120401322841644
Loss at iteration 450 : 0.11154274642467499
Loss at iteration 460 : 0.07011894881725311
Loss at iteration 470 : 0.1033635139465332
Loss at iteration 480 : 0.11298635601997375
Loss at iteration 490 : 0.07097436487674713
Loss at iteration 500 : 0.13197384774684906
Loss at iteration 510 : 0.08345890045166016
Loss at iteration 520 : 0.1191224604845047
Loss at iteration 530 : 0.07993742823600769
Loss at iteration 540 : 0.09922900050878525
Loss at iteration 550 : 0.11233524978160858
Loss at iteration 560 : 0.08525532484054565
Loss at iteration 570 : 0.08127357065677643
Loss at iteration 580 : 0.13431328535079956
Loss at iteration 590 : 0.07635679095983505
Loss at iteration 600 : 0.09792034327983856
Loss at iteration 610 : 0.05931824445724487
Loss at iteration 620 : 0.10932718217372894
Loss at iteration 630 : 0.10702549666166306
Loss at iteration 640 : 0.10105685889720917
Loss at iteration 650 : 0.08158417046070099
Loss at iteration 660 : 0.07608814537525177
Loss at iteration 670 : 0.07903506606817245
Loss at iteration 680 : 0.08294187486171722
Loss at iteration 690 : 0.07775042951107025
Loss at iteration 700 : 0.08540105074644089
Loss at iteration 710 : 0.08934739232063293
Loss at iteration 720 : 0.0912262499332428
Loss at iteration 730 : 0.10809338092803955
Loss at iteration 740 : 0.07852642238140106
Loss at iteration 750 : 0.10333961248397827
Loss at iteration 760 : 0.1141619011759758
Loss at iteration 770 : 0.12045149505138397
Loss at iteration 780 : 0.07140296697616577
Loss at iteration 790 : 0.1395721286535263
Loss at iteration 800 : 0.0785171389579773
Loss at iteration 810 : 0.08089699596166611
Loss at iteration 820 : 0.1264869123697281
Loss at iteration 830 : 0.08943664282560349
Loss at iteration 840 : 0.07006639242172241
Loss at iteration 850 : 0.0821337103843689
Loss at iteration 860 : 0.09943392127752304
Loss at iteration 870 : 0.06865545362234116
Loss at iteration 880 : 0.08490802347660065
Loss at iteration 890 : 0.14234988391399384
Loss at iteration 900 : 0.05598657578229904
Loss at iteration 910 : 0.05543381720781326
Loss at iteration 920 : 0.08638530224561691
Loss at iteration 930 : 0.07761097699403763
Loss at iteration 940 : 0.09417171776294708
Loss at iteration 950 : 0.08323007076978683
Loss at iteration 960 : 0.0826813280582428
Loss at iteration 970 : 0.09786214679479599
Loss at iteration 980 : 0.06488028168678284
Loss at iteration 990 : 0.06500902771949768
Loss at iteration 1000 : 0.111883245408535
Loss at iteration 1010 : 0.10572963953018188
Loss at iteration 1020 : 0.12472005188465118
Loss at iteration 1030 : 0.06053972989320755
Loss at iteration 1040 : 0.10058723390102386
Loss at iteration 1050 : 0.0780029147863388
Loss at iteration 1060 : 0.13477061688899994
Loss at iteration 1070 : 0.07951542735099792
Loss at iteration 1080 : 0.06289795786142349
Loss at iteration 1090 : 0.11735902726650238
Loss at iteration 1100 : 0.10702657699584961
Loss at iteration 1110 : 0.08322689682245255
Loss at iteration 1120 : 0.06816995143890381
Loss at iteration 1130 : 0.07748136669397354
Loss at iteration 1140 : 0.08159631490707397
Loss at iteration 1150 : 0.07577110826969147
Loss at iteration 1160 : 0.05574899539351463
Loss at iteration 1170 : 0.10299088060855865
Loss at iteration 1180 : 0.07311546802520752
Loss at iteration 1190 : 0.09466572850942612
Loss at iteration 1200 : 0.0754891037940979
Loss at iteration 1210 : 0.07090167701244354
The SSIM Value is: 0.6793150961399078
The PSNR Value is: 20.25468527475993
the epoch is: 20
Loss at iteration 10 : 0.1331005096435547
Loss at iteration 20 : 0.10703963041305542
Loss at iteration 30 : 0.08384662866592407
Loss at iteration 40 : 0.09563832730054855
Loss at iteration 50 : 0.05989235267043114
Loss at iteration 60 : 0.04861369729042053
Loss at iteration 70 : 0.08685827255249023
Loss at iteration 80 : 0.08032163977622986
Loss at iteration 90 : 0.10061117261648178
Loss at iteration 100 : 0.13127025961875916
Loss at iteration 110 : 0.09261520206928253
Loss at iteration 120 : 0.10952784866094589
Loss at iteration 130 : 0.07202888280153275
Loss at iteration 140 : 0.05374583229422569
Loss at iteration 150 : 0.13241523504257202
Loss at iteration 160 : 0.057185929268598557
Loss at iteration 170 : 0.057284824550151825
Loss at iteration 180 : 0.12679708003997803
Loss at iteration 190 : 0.08077231049537659
Loss at iteration 200 : 0.0845792293548584
Loss at iteration 210 : 0.1045963391661644
Loss at iteration 220 : 0.07748431712388992
Loss at iteration 230 : 0.19226513803005219
Loss at iteration 240 : 0.07879449427127838
Loss at iteration 250 : 0.09045189619064331
Loss at iteration 260 : 0.08600874245166779
Loss at iteration 270 : 0.07186606526374817
Loss at iteration 280 : 0.07789754122495651
Loss at iteration 290 : 0.09666057676076889
Loss at iteration 300 : 0.11504678428173065
Loss at iteration 310 : 0.06569474190473557
Loss at iteration 320 : 0.13710540533065796
Loss at iteration 330 : 0.06168904900550842
Loss at iteration 340 : 0.10637154430150986
Loss at iteration 350 : 0.1220531091094017
Loss at iteration 360 : 0.07317312061786652
Loss at iteration 370 : 0.09032545983791351
Loss at iteration 380 : 0.08440299332141876
Loss at iteration 390 : 0.11047986149787903
Loss at iteration 400 : 0.08396272361278534
Loss at iteration 410 : 0.09969612956047058
Loss at iteration 420 : 0.10644488036632538
Loss at iteration 430 : 0.08127394318580627
Loss at iteration 440 : 0.10093069076538086
Loss at iteration 450 : 0.12257859110832214
Loss at iteration 460 : 0.06031997129321098
Loss at iteration 470 : 0.0798325464129448
Loss at iteration 480 : 0.0931229516863823
Loss at iteration 490 : 0.07524052262306213
Loss at iteration 500 : 0.09218426793813705
Loss at iteration 510 : 0.11922609806060791
Loss at iteration 520 : 0.059406112879514694
Loss at iteration 530 : 0.0842321366071701
Loss at iteration 540 : 0.14665541052818298
Loss at iteration 550 : 0.09372766315937042
Loss at iteration 560 : 0.11781924962997437
Loss at iteration 570 : 0.06290823966264725
Loss at iteration 580 : 0.06202390417456627
Loss at iteration 590 : 0.08523257821798325
Loss at iteration 600 : 0.09465274214744568
Loss at iteration 610 : 0.09004949033260345
Loss at iteration 620 : 0.1349172592163086
Loss at iteration 630 : 0.10203015804290771
Loss at iteration 640 : 0.09006866067647934
Loss at iteration 650 : 0.08691835403442383
Loss at iteration 660 : 0.08767232298851013
Loss at iteration 670 : 0.07440674304962158
Loss at iteration 680 : 0.09625463932752609
Loss at iteration 690 : 0.07478884607553482
Loss at iteration 700 : 0.0878930613398552
Loss at iteration 710 : 0.07171660661697388
Loss at iteration 720 : 0.12870553135871887
Loss at iteration 730 : 0.08211387693881989
Loss at iteration 740 : 0.05912766605615616
Loss at iteration 750 : 0.09630663692951202
Loss at iteration 760 : 0.07961446046829224
Loss at iteration 770 : 0.07246355712413788
Loss at iteration 780 : 0.06416331231594086
Loss at iteration 790 : 0.07044623792171478
Loss at iteration 800 : 0.10543789714574814
Loss at iteration 810 : 0.0918734073638916
Loss at iteration 820 : 0.0690910816192627
Loss at iteration 830 : 0.09985777735710144
Loss at iteration 840 : 0.18706411123275757
Loss at iteration 850 : 0.0657661184668541
Loss at iteration 860 : 0.10465120524168015
Loss at iteration 870 : 0.12752413749694824
Loss at iteration 880 : 0.12819060683250427
Loss at iteration 890 : 0.13084322214126587
Loss at iteration 900 : 0.11881646513938904
Loss at iteration 910 : 0.08664298057556152
Loss at iteration 920 : 0.10487177968025208
Loss at iteration 930 : 0.11587562412023544
Loss at iteration 940 : 0.1110665425658226
Loss at iteration 950 : 0.13591842353343964
Loss at iteration 960 : 0.09938696026802063
Loss at iteration 970 : 0.12205200642347336
Loss at iteration 980 : 0.08569969981908798
Loss at iteration 990 : 0.05200355499982834
Loss at iteration 1000 : 0.10559630393981934
Loss at iteration 1010 : 0.13444295525550842
Loss at iteration 1020 : 0.10581499338150024
Loss at iteration 1030 : 0.07977543771266937
Loss at iteration 1040 : 0.08853863179683685
Loss at iteration 1050 : 0.13347165286540985
Loss at iteration 1060 : 0.13935315608978271
Loss at iteration 1070 : 0.08424388617277145
Loss at iteration 1080 : 0.07384005934000015
Loss at iteration 1090 : 0.09352247416973114
Loss at iteration 1100 : 0.08230891078710556
Loss at iteration 1110 : 0.05187888443470001
Loss at iteration 1120 : 0.08548378944396973
Loss at iteration 1130 : 0.10979360342025757
Loss at iteration 1140 : 0.10879069566726685
Loss at iteration 1150 : 0.07346729189157486
Loss at iteration 1160 : 0.1046600192785263
Loss at iteration 1170 : 0.09863664209842682
Loss at iteration 1180 : 0.08224335312843323
Loss at iteration 1190 : 0.11200231313705444
Loss at iteration 1200 : 0.0451955609023571
Loss at iteration 1210 : 0.07699572294950485
The SSIM Value is: 0.679272464911143
The PSNR Value is: 20.261193784077964
the epoch is: 21
Loss at iteration 10 : 0.06904910504817963
Loss at iteration 20 : 0.13075202703475952
Loss at iteration 30 : 0.06374147534370422
Loss at iteration 40 : 0.11329610645771027
Loss at iteration 50 : 0.08204960823059082
Loss at iteration 60 : 0.13750092685222626
Loss at iteration 70 : 0.08321356773376465
Loss at iteration 80 : 0.0991661548614502
Loss at iteration 90 : 0.10323315858840942
Loss at iteration 100 : 0.076099693775177
Loss at iteration 110 : 0.12381450831890106
Loss at iteration 120 : 0.09825783222913742
Loss at iteration 130 : 0.05006664618849754
Loss at iteration 140 : 0.07659028470516205
Loss at iteration 150 : 0.06792749464511871
Loss at iteration 160 : 0.09074374288320541
Loss at iteration 170 : 0.09131699055433273
Loss at iteration 180 : 0.09272530674934387
Loss at iteration 190 : 0.053557466715574265
Loss at iteration 200 : 0.10088145732879639
Loss at iteration 210 : 0.1341116726398468
Loss at iteration 220 : 0.10507304966449738
Loss at iteration 230 : 0.10104020684957504
Loss at iteration 240 : 0.08907465636730194
Loss at iteration 250 : 0.15687552094459534
Loss at iteration 260 : 0.09793791174888611
Loss at iteration 270 : 0.08074118196964264
Loss at iteration 280 : 0.05170942470431328
Loss at iteration 290 : 0.10978274047374725
Loss at iteration 300 : 0.09897002577781677
Loss at iteration 310 : 0.08926805853843689
Loss at iteration 320 : 0.08649393171072006
Loss at iteration 330 : 0.07411983609199524
Loss at iteration 340 : 0.08567178249359131
Loss at iteration 350 : 0.09638412296772003
Loss at iteration 360 : 0.0646958276629448
Loss at iteration 370 : 0.11745236814022064
Loss at iteration 380 : 0.09117937088012695
Loss at iteration 390 : 0.06963545083999634
Loss at iteration 400 : 0.08758415281772614
Loss at iteration 410 : 0.059048622846603394
Loss at iteration 420 : 0.09493052959442139
Loss at iteration 430 : 0.08771166205406189
Loss at iteration 440 : 0.10806694626808167
Loss at iteration 450 : 0.08598151057958603
Loss at iteration 460 : 0.06817658245563507
Loss at iteration 470 : 0.0856991708278656
Loss at iteration 480 : 0.07986821979284286
Loss at iteration 490 : 0.10378184169530869
Loss at iteration 500 : 0.06114180013537407
Loss at iteration 510 : 0.06794804334640503
Loss at iteration 520 : 0.0957082211971283
Loss at iteration 530 : 0.15172229707241058
Loss at iteration 540 : 0.07153038680553436
Loss at iteration 550 : 0.06901267170906067
Loss at iteration 560 : 0.0462438128888607
Loss at iteration 570 : 0.09288577735424042
Loss at iteration 580 : 0.12496471405029297
Loss at iteration 590 : 0.10871145129203796
Loss at iteration 600 : 0.05821258947253227
Loss at iteration 610 : 0.07456477731466293
Loss at iteration 620 : 0.07561162114143372
Loss at iteration 630 : 0.10152091085910797
Loss at iteration 640 : 0.0984480082988739
Loss at iteration 650 : 0.061623625457286835
Loss at iteration 660 : 0.08793358504772186
Loss at iteration 670 : 0.06826993823051453
Loss at iteration 680 : 0.06431874632835388
Loss at iteration 690 : 0.08404289931058884
Loss at iteration 700 : 0.05755314230918884
Loss at iteration 710 : 0.07665619254112244
Loss at iteration 720 : 0.08705057948827744
Loss at iteration 730 : 0.11356554925441742
Loss at iteration 740 : 0.08121240139007568
Loss at iteration 750 : 0.07399258017539978
Loss at iteration 760 : 0.14222542941570282
Loss at iteration 770 : 0.07358497381210327
Loss at iteration 780 : 0.11740114539861679
Loss at iteration 790 : 0.08987367153167725
Loss at iteration 800 : 0.06608998775482178
Loss at iteration 810 : 0.1226205974817276
Loss at iteration 820 : 0.06955169141292572
Loss at iteration 830 : 0.08067002147436142
Loss at iteration 840 : 0.11465611308813095
Loss at iteration 850 : 0.08265901356935501
Loss at iteration 860 : 0.06252344697713852
Loss at iteration 870 : 0.12506400048732758
Loss at iteration 880 : 0.0768108069896698
Loss at iteration 890 : 0.047706689685583115
Loss at iteration 900 : 0.10073664039373398
Loss at iteration 910 : 0.12608061730861664
Loss at iteration 920 : 0.08401310443878174
Loss at iteration 930 : 0.13006994128227234
Loss at iteration 940 : 0.1385962963104248
Loss at iteration 950 : 0.09116356819868088
Loss at iteration 960 : 0.1294339895248413
Loss at iteration 970 : 0.09205055236816406
Loss at iteration 980 : 0.09084286540746689
Loss at iteration 990 : 0.09519018977880478
Loss at iteration 1000 : 0.1108296737074852
Loss at iteration 1010 : 0.13180534541606903
Loss at iteration 1020 : 0.12237629294395447
Loss at iteration 1030 : 0.07675537467002869
Loss at iteration 1040 : 0.10809864103794098
Loss at iteration 1050 : 0.05035429447889328
Loss at iteration 1060 : 0.06539582461118698
Loss at iteration 1070 : 0.0844358503818512
Loss at iteration 1080 : 0.11554261296987534
Loss at iteration 1090 : 0.04707743227481842
Loss at iteration 1100 : 0.12121188640594482
Loss at iteration 1110 : 0.11299452185630798
Loss at iteration 1120 : 0.05388902872800827
Loss at iteration 1130 : 0.08623966574668884
Loss at iteration 1140 : 0.1192021369934082
Loss at iteration 1150 : 0.07447350770235062
Loss at iteration 1160 : 0.09892696142196655
Loss at iteration 1170 : 0.05900697410106659
Loss at iteration 1180 : 0.09216222167015076
Loss at iteration 1190 : 0.08554933965206146
Loss at iteration 1200 : 0.1123119443655014
Loss at iteration 1210 : 0.0797080546617508
The SSIM Value is: 0.6822323222955068
The PSNR Value is: 20.52945302327474
the highest SSIM value is: 20.52945302327474
the epoch is: 22
Loss at iteration 10 : 0.1381904035806656
Loss at iteration 20 : 0.0749855637550354
Loss at iteration 30 : 0.06415058672428131
Loss at iteration 40 : 0.05845401808619499
Loss at iteration 50 : 0.08631905913352966
Loss at iteration 60 : 0.12501560151576996
Loss at iteration 70 : 0.07743936777114868
Loss at iteration 80 : 0.07791700959205627
Loss at iteration 90 : 0.06919625401496887
Loss at iteration 100 : 0.110767662525177
Loss at iteration 110 : 0.09336283802986145
Loss at iteration 120 : 0.10025223344564438
Loss at iteration 130 : 0.10954507440328598
Loss at iteration 140 : 0.07362578809261322
Loss at iteration 150 : 0.10358220338821411
Loss at iteration 160 : 0.06859739124774933
Loss at iteration 170 : 0.048494040966033936
Loss at iteration 180 : 0.09510576725006104
Loss at iteration 190 : 0.05921408534049988
Loss at iteration 200 : 0.09830056130886078
Loss at iteration 210 : 0.05942597985267639
Loss at iteration 220 : 0.07390151917934418
Loss at iteration 230 : 0.07354030758142471
Loss at iteration 240 : 0.08776862919330597
Loss at iteration 250 : 0.09322947263717651
Loss at iteration 260 : 0.06421888619661331
Loss at iteration 270 : 0.08565747737884521
Loss at iteration 280 : 0.0822494775056839
Loss at iteration 290 : 0.12089642137289047
Loss at iteration 300 : 0.056104667484760284
Loss at iteration 310 : 0.11377045512199402
Loss at iteration 320 : 0.07855626940727234
Loss at iteration 330 : 0.06525938957929611
Loss at iteration 340 : 0.09144359827041626
Loss at iteration 350 : 0.07466672360897064
Loss at iteration 360 : 0.08341774344444275
Loss at iteration 370 : 0.09710681438446045
Loss at iteration 380 : 0.10637947171926498
Loss at iteration 390 : 0.10755942761898041
Loss at iteration 400 : 0.09203429520130157
Loss at iteration 410 : 0.11374510824680328
Loss at iteration 420 : 0.056935589760541916
Loss at iteration 430 : 0.09228110313415527
Loss at iteration 440 : 0.09228527545928955
Loss at iteration 450 : 0.11298418045043945
Loss at iteration 460 : 0.10223333537578583
Loss at iteration 470 : 0.09028966724872589
Loss at iteration 480 : 0.07246147841215134
Loss at iteration 490 : 0.11996200680732727
Loss at iteration 500 : 0.09993039816617966
Loss at iteration 510 : 0.0854644775390625
Loss at iteration 520 : 0.12432362884283066
Loss at iteration 530 : 0.07919634878635406
Loss at iteration 540 : 0.05767495557665825
Loss at iteration 550 : 0.06912277638912201
Loss at iteration 560 : 0.11041577905416489
Loss at iteration 570 : 0.07453691959381104
Loss at iteration 580 : 0.05275776982307434
Loss at iteration 590 : 0.08835365623235703
Loss at iteration 600 : 0.05267217382788658
Loss at iteration 610 : 0.10978411138057709
Loss at iteration 620 : 0.11536537855863571
Loss at iteration 630 : 0.08607091754674911
Loss at iteration 640 : 0.07527220249176025
Loss at iteration 650 : 0.07249774038791656
Loss at iteration 660 : 0.10729404538869858
Loss at iteration 670 : 0.06690047681331635
Loss at iteration 680 : 0.08206569403409958
Loss at iteration 690 : 0.12202658504247665
Loss at iteration 700 : 0.06924189627170563
Loss at iteration 710 : 0.078335702419281
Loss at iteration 720 : 0.08190770447254181
Loss at iteration 730 : 0.08105359971523285
Loss at iteration 740 : 0.11976584792137146
Loss at iteration 750 : 0.08589351177215576
Loss at iteration 760 : 0.0701015293598175
Loss at iteration 770 : 0.09512438625097275
Loss at iteration 780 : 0.06908205151557922
Loss at iteration 790 : 0.09387116134166718
Loss at iteration 800 : 0.12223751842975616
Loss at iteration 810 : 0.10088939964771271
Loss at iteration 820 : 0.057558171451091766
Loss at iteration 830 : 0.08008810877799988
Loss at iteration 840 : 0.054340630769729614
Loss at iteration 850 : 0.05159766972064972
Loss at iteration 860 : 0.07144849747419357
Loss at iteration 870 : 0.07740482687950134
Loss at iteration 880 : 0.09640993922948837
Loss at iteration 890 : 0.06875275075435638
Loss at iteration 900 : 0.12230558693408966
Loss at iteration 910 : 0.08943012356758118
Loss at iteration 920 : 0.08192608505487442
Loss at iteration 930 : 0.06590557098388672
Loss at iteration 940 : 0.13028401136398315
Loss at iteration 950 : 0.11510732024908066
Loss at iteration 960 : 0.1331777274608612
Loss at iteration 970 : 0.061358705163002014
Loss at iteration 980 : 0.09613200277090073
Loss at iteration 990 : 0.12157993018627167
Loss at iteration 1000 : 0.10402794927358627
Loss at iteration 1010 : 0.08827446401119232
Loss at iteration 1020 : 0.16472454369068146
Loss at iteration 1030 : 0.072142593562603
Loss at iteration 1040 : 0.07654246687889099
Loss at iteration 1050 : 0.10338746011257172
Loss at iteration 1060 : 0.09120966494083405
Loss at iteration 1070 : 0.09558656066656113
Loss at iteration 1080 : 0.07998745888471603
Loss at iteration 1090 : 0.08834213018417358
Loss at iteration 1100 : 0.08141811192035675
Loss at iteration 1110 : 0.08367732167243958
Loss at iteration 1120 : 0.09526168555021286
Loss at iteration 1130 : 0.09458165615797043
Loss at iteration 1140 : 0.06058652326464653
Loss at iteration 1150 : 0.11602607369422913
Loss at iteration 1160 : 0.10192875564098358
Loss at iteration 1170 : 0.06847089529037476
Loss at iteration 1180 : 0.07387923449277878
Loss at iteration 1190 : 0.08430304378271103
Loss at iteration 1200 : 0.07322308421134949
Loss at iteration 1210 : 0.13182701170444489
The SSIM Value is: 0.6801097730795542
The PSNR Value is: 19.92694403330485
the epoch is: 23
Loss at iteration 10 : 0.09454929083585739
Loss at iteration 20 : 0.07927219569683075
Loss at iteration 30 : 0.09566693753004074
Loss at iteration 40 : 0.11060076206922531
Loss at iteration 50 : 0.08779799938201904
Loss at iteration 60 : 0.1027810201048851
Loss at iteration 70 : 0.09058316797018051
Loss at iteration 80 : 0.097681924700737
Loss at iteration 90 : 0.10656295716762543
Loss at iteration 100 : 0.07561495155096054
Loss at iteration 110 : 0.06959696114063263
Loss at iteration 120 : 0.05114659667015076
Loss at iteration 130 : 0.08600038290023804
Loss at iteration 140 : 0.07242140173912048
Loss at iteration 150 : 0.10851072520017624
Loss at iteration 160 : 0.08145647495985031
Loss at iteration 170 : 0.07783832401037216
Loss at iteration 180 : 0.10698848962783813
Loss at iteration 190 : 0.07188229262828827
Loss at iteration 200 : 0.12709039449691772
Loss at iteration 210 : 0.07653731107711792
Loss at iteration 220 : 0.11904644966125488
Loss at iteration 230 : 0.08417242765426636
Loss at iteration 240 : 0.10246485471725464
Loss at iteration 250 : 0.06449185311794281
Loss at iteration 260 : 0.0678323358297348
Loss at iteration 270 : 0.08704659342765808
Loss at iteration 280 : 0.10182513296604156
Loss at iteration 290 : 0.07865642011165619
Loss at iteration 300 : 0.07586012780666351
Loss at iteration 310 : 0.1075383871793747
Loss at iteration 320 : 0.08096890151500702
Loss at iteration 330 : 0.13641414046287537
Loss at iteration 340 : 0.0840107649564743
Loss at iteration 350 : 0.08086537569761276
Loss at iteration 360 : 0.07637958973646164
Loss at iteration 370 : 0.08620002120733261
Loss at iteration 380 : 0.07519762217998505
Loss at iteration 390 : 0.09496638178825378
Loss at iteration 400 : 0.08511470258235931
Loss at iteration 410 : 0.05296221375465393
Loss at iteration 420 : 0.07232935726642609
Loss at iteration 430 : 0.08245454728603363
Loss at iteration 440 : 0.17461693286895752
Loss at iteration 450 : 0.11603699624538422
Loss at iteration 460 : 0.06886833161115646
Loss at iteration 470 : 0.09337125718593597
Loss at iteration 480 : 0.11370673030614853
Loss at iteration 490 : 0.06570810824632645
Loss at iteration 500 : 0.11498749256134033
Loss at iteration 510 : 0.12215171754360199
Loss at iteration 520 : 0.10958878695964813
Loss at iteration 530 : 0.13737750053405762
Loss at iteration 540 : 0.09234645217657089
Loss at iteration 550 : 0.10172047466039658
Loss at iteration 560 : 0.06923733651638031
Loss at iteration 570 : 0.13905774056911469
Loss at iteration 580 : 0.09643297642469406
Loss at iteration 590 : 0.10887759923934937
Loss at iteration 600 : 0.09614666551351547
Loss at iteration 610 : 0.08904164284467697
Loss at iteration 620 : 0.09403187036514282
Loss at iteration 630 : 0.08397503942251205
Loss at iteration 640 : 0.11360709369182587
Loss at iteration 650 : 0.10471828281879425
Loss at iteration 660 : 0.11255300045013428
Loss at iteration 670 : 0.056887101382017136
Loss at iteration 680 : 0.09710744023323059
Loss at iteration 690 : 0.127266526222229
Loss at iteration 700 : 0.09825479984283447
Loss at iteration 710 : 0.10407131910324097
Loss at iteration 720 : 0.06768190115690231
Loss at iteration 730 : 0.08412842452526093
Loss at iteration 740 : 0.09048204123973846
Loss at iteration 750 : 0.09847728908061981
Loss at iteration 760 : 0.08726358413696289
Loss at iteration 770 : 0.09201821684837341
Loss at iteration 780 : 0.07436846941709518
Loss at iteration 790 : 0.09140539169311523
Loss at iteration 800 : 0.10492577403783798
Loss at iteration 810 : 0.08219543099403381
Loss at iteration 820 : 0.1361827403306961
Loss at iteration 830 : 0.08083780109882355
Loss at iteration 840 : 0.08842986077070236
Loss at iteration 850 : 0.07676739990711212
Loss at iteration 860 : 0.06679496169090271
Loss at iteration 870 : 0.09475036710500717
Loss at iteration 880 : 0.09114013612270355
Loss at iteration 890 : 0.05841219425201416
Loss at iteration 900 : 0.1328992247581482
Loss at iteration 910 : 0.05962704122066498
Loss at iteration 920 : 0.12134315818548203
Loss at iteration 930 : 0.08907877653837204
Loss at iteration 940 : 0.14972075819969177
Loss at iteration 950 : 0.09058110415935516
Loss at iteration 960 : 0.09033095091581345
Loss at iteration 970 : 0.1463140845298767
Loss at iteration 980 : 0.10403138399124146
Loss at iteration 990 : 0.08783569931983948
Loss at iteration 1000 : 0.08319511264562607
Loss at iteration 1010 : 0.07351045310497284
Loss at iteration 1020 : 0.0811106488108635
Loss at iteration 1030 : 0.08209292590618134
Loss at iteration 1040 : 0.05970500037074089
Loss at iteration 1050 : 0.10380212962627411
Loss at iteration 1060 : 0.131249338388443
Loss at iteration 1070 : 0.05810406059026718
Loss at iteration 1080 : 0.08042236417531967
Loss at iteration 1090 : 0.09351136535406113
Loss at iteration 1100 : 0.10217136144638062
Loss at iteration 1110 : 0.11245744675397873
Loss at iteration 1120 : 0.07421814650297165
Loss at iteration 1130 : 0.049722254276275635
Loss at iteration 1140 : 0.0719795897603035
Loss at iteration 1150 : 0.06872016191482544
Loss at iteration 1160 : 0.07053970545530319
Loss at iteration 1170 : 0.06347253918647766
Loss at iteration 1180 : 0.10011889040470123
Loss at iteration 1190 : 0.13003979623317719
Loss at iteration 1200 : 0.07225742936134338
Loss at iteration 1210 : 0.06791795045137405
The SSIM Value is: 0.680124294757843
The PSNR Value is: 20.48144187927246
the epoch is: 24
Loss at iteration 10 : 0.09634622186422348
Loss at iteration 20 : 0.0839315876364708
Loss at iteration 30 : 0.07440908253192902
Loss at iteration 40 : 0.05089478939771652
Loss at iteration 50 : 0.10286641120910645
Loss at iteration 60 : 0.07477875798940659
Loss at iteration 70 : 0.12805502116680145
Loss at iteration 80 : 0.0686362236738205
Loss at iteration 90 : 0.07644620537757874
Loss at iteration 100 : 0.1279614269733429
Loss at iteration 110 : 0.1112007349729538
Loss at iteration 120 : 0.08224836736917496
Loss at iteration 130 : 0.06180654093623161
Loss at iteration 140 : 0.07894116640090942
Loss at iteration 150 : 0.0894906148314476
Loss at iteration 160 : 0.10381065309047699
Loss at iteration 170 : 0.09173108637332916
Loss at iteration 180 : 0.1575792133808136
Loss at iteration 190 : 0.07989906519651413
Loss at iteration 200 : 0.10237397998571396
Loss at iteration 210 : 0.1072816401720047
Loss at iteration 220 : 0.08489914983510971
Loss at iteration 230 : 0.08226433396339417
Loss at iteration 240 : 0.08413402736186981
Loss at iteration 250 : 0.058970920741558075
Loss at iteration 260 : 0.05696285888552666
Loss at iteration 270 : 0.09284880757331848
Loss at iteration 280 : 0.12141915410757065
Loss at iteration 290 : 0.08479305356740952
Loss at iteration 300 : 0.1032266840338707
Loss at iteration 310 : 0.08147778362035751
Loss at iteration 320 : 0.07774646580219269
Loss at iteration 330 : 0.07070999592542648
Loss at iteration 340 : 0.07575075328350067
Loss at iteration 350 : 0.08571644127368927
Loss at iteration 360 : 0.12201391905546188
Loss at iteration 370 : 0.07110904157161713
Loss at iteration 380 : 0.049994222819805145
Loss at iteration 390 : 0.08869525045156479
Loss at iteration 400 : 0.11025633662939072
Loss at iteration 410 : 0.04001675546169281
Loss at iteration 420 : 0.11700823158025742
Loss at iteration 430 : 0.07290519773960114
Loss at iteration 440 : 0.1522560566663742
Loss at iteration 450 : 0.09834736585617065
Loss at iteration 460 : 0.09619339555501938
Loss at iteration 470 : 0.09116415679454803
Loss at iteration 480 : 0.12016449868679047
Loss at iteration 490 : 0.12029563635587692
Loss at iteration 500 : 0.09111451357603073
Loss at iteration 510 : 0.0812443196773529
Loss at iteration 520 : 0.06480352580547333
Loss at iteration 530 : 0.05226883292198181
Loss at iteration 540 : 0.06955423206090927
Loss at iteration 550 : 0.09087342023849487
Loss at iteration 560 : 0.05290134996175766
Loss at iteration 570 : 0.08388394117355347
Loss at iteration 580 : 0.07113250344991684
Loss at iteration 590 : 0.08834870904684067
Loss at iteration 600 : 0.08813043683767319
Loss at iteration 610 : 0.06353455781936646
Loss at iteration 620 : 0.11717301607131958
Loss at iteration 630 : 0.07547729462385178
Loss at iteration 640 : 0.07117524743080139
Loss at iteration 650 : 0.07293422520160675
Loss at iteration 660 : 0.05654917284846306
Loss at iteration 670 : 0.08241239190101624
Loss at iteration 680 : 0.1344044804573059
Loss at iteration 690 : 0.06827788054943085
Loss at iteration 700 : 0.08152952790260315
Loss at iteration 710 : 0.08940063416957855
Loss at iteration 720 : 0.11428782343864441
Loss at iteration 730 : 0.14930973947048187
Loss at iteration 740 : 0.0749037116765976
Loss at iteration 750 : 0.11445268243551254
Loss at iteration 760 : 0.06114313006401062
Loss at iteration 770 : 0.05869815871119499
Loss at iteration 780 : 0.09351938217878342
Loss at iteration 790 : 0.056290533393621445
Loss at iteration 800 : 0.0832860916852951
Loss at iteration 810 : 0.059510186314582825
Loss at iteration 820 : 0.09556087851524353
Loss at iteration 830 : 0.12051098048686981
Loss at iteration 840 : 0.09482872486114502
Loss at iteration 850 : 0.09064581245183945
Loss at iteration 860 : 0.093883216381073
Loss at iteration 870 : 0.1049841046333313
Loss at iteration 880 : 0.10862500965595245
Loss at iteration 890 : 0.12243756651878357
Loss at iteration 900 : 0.11634021252393723
Loss at iteration 910 : 0.07788994908332825
Loss at iteration 920 : 0.081902876496315
Loss at iteration 930 : 0.08563133329153061
Loss at iteration 940 : 0.06798993051052094
Loss at iteration 950 : 0.11845749616622925
Loss at iteration 960 : 0.09044764935970306
Loss at iteration 970 : 0.10254909843206406
Loss at iteration 980 : 0.11202865093946457
Loss at iteration 990 : 0.05918809771537781
Loss at iteration 1000 : 0.053690336644649506
Loss at iteration 1010 : 0.1295568346977234
Loss at iteration 1020 : 0.0558372363448143
Loss at iteration 1030 : 0.08894889801740646
Loss at iteration 1040 : 0.1065753921866417
Loss at iteration 1050 : 0.08516198396682739
Loss at iteration 1060 : 0.0665283352136612
Loss at iteration 1070 : 0.07893043011426926
Loss at iteration 1080 : 0.11509893834590912
Loss at iteration 1090 : 0.09544478356838226
Loss at iteration 1100 : 0.10899883508682251
Loss at iteration 1110 : 0.09298485517501831
Loss at iteration 1120 : 0.11825563758611679
Loss at iteration 1130 : 0.10367649793624878
Loss at iteration 1140 : 0.08504027128219604
Loss at iteration 1150 : 0.0888623595237732
Loss at iteration 1160 : 0.0800517126917839
Loss at iteration 1170 : 0.08743463456630707
Loss at iteration 1180 : 0.07181069999933243
Loss at iteration 1190 : 0.0907207727432251
Loss at iteration 1200 : 0.13239578902721405
Loss at iteration 1210 : 0.15331076085567474
The SSIM Value is: 0.6876557509104411
The PSNR Value is: 20.803673235575356
the highest SSIM value is: 20.803673235575356
the epoch is: 25
Loss at iteration 10 : 0.07900949567556381
Loss at iteration 20 : 0.11846422404050827
Loss at iteration 30 : 0.0849524438381195
Loss at iteration 40 : 0.1412205696105957
Loss at iteration 50 : 0.07692979276180267
Loss at iteration 60 : 0.09966079890727997
Loss at iteration 70 : 0.12314951419830322
Loss at iteration 80 : 0.08435685932636261
Loss at iteration 90 : 0.045905470848083496
Loss at iteration 100 : 0.13639521598815918
Loss at iteration 110 : 0.11042027175426483
Loss at iteration 120 : 0.08149147033691406
Loss at iteration 130 : 0.10252045094966888
Loss at iteration 140 : 0.0859079584479332
Loss at iteration 150 : 0.10091865062713623
Loss at iteration 160 : 0.10662809014320374
Loss at iteration 170 : 0.049893081188201904
Loss at iteration 180 : 0.10055989027023315
Loss at iteration 190 : 0.15268395841121674
Loss at iteration 200 : 0.07655949145555496
Loss at iteration 210 : 0.07957535982131958
Loss at iteration 220 : 0.06610439717769623
Loss at iteration 230 : 0.07452298700809479
Loss at iteration 240 : 0.08896592259407043
Loss at iteration 250 : 0.08252809941768646
Loss at iteration 260 : 0.13157206773757935
Loss at iteration 270 : 0.09341373294591904
Loss at iteration 280 : 0.1841619610786438
Loss at iteration 290 : 0.10222287476062775
Loss at iteration 300 : 0.06119327247142792
Loss at iteration 310 : 0.11337047815322876
Loss at iteration 320 : 0.09573128074407578
Loss at iteration 330 : 0.1192573830485344
Loss at iteration 340 : 0.07153373211622238
Loss at iteration 350 : 0.08368848264217377
Loss at iteration 360 : 0.08212874829769135
Loss at iteration 370 : 0.0921844094991684
Loss at iteration 380 : 0.13615168631076813
Loss at iteration 390 : 0.10268952697515488
Loss at iteration 400 : 0.092043936252594
Loss at iteration 410 : 0.06187769025564194
Loss at iteration 420 : 0.06752551347017288
Loss at iteration 430 : 0.12475960701704025
Loss at iteration 440 : 0.08091132342815399
Loss at iteration 450 : 0.07111477851867676
Loss at iteration 460 : 0.11025787889957428
Loss at iteration 470 : 0.08682557195425034
Loss at iteration 480 : 0.0836675763130188
Loss at iteration 490 : 0.07676120102405548
Loss at iteration 500 : 0.10029174387454987
Loss at iteration 510 : 0.08845632523298264
Loss at iteration 520 : 0.05838046595454216
Loss at iteration 530 : 0.10099934041500092
Loss at iteration 540 : 0.06734760105609894
Loss at iteration 550 : 0.15348774194717407
Loss at iteration 560 : 0.06079104170203209
Loss at iteration 570 : 0.08917856216430664
Loss at iteration 580 : 0.07718639075756073
Loss at iteration 590 : 0.09446971118450165
Loss at iteration 600 : 0.07253598421812057
Loss at iteration 610 : 0.07819574326276779
Loss at iteration 620 : 0.08611379563808441
Loss at iteration 630 : 0.10176720470190048
Loss at iteration 640 : 0.05705523490905762
Loss at iteration 650 : 0.1099608838558197
Loss at iteration 660 : 0.05317728966474533
Loss at iteration 670 : 0.11729073524475098
Loss at iteration 680 : 0.11786242574453354
Loss at iteration 690 : 0.12733778357505798
Loss at iteration 700 : 0.10773316025733948
Loss at iteration 710 : 0.06789793819189072
Loss at iteration 720 : 0.08478845655918121
Loss at iteration 730 : 0.09492217749357224
Loss at iteration 740 : 0.1251927763223648
Loss at iteration 750 : 0.1140739917755127
Loss at iteration 760 : 0.10056500136852264
Loss at iteration 770 : 0.12805116176605225
Loss at iteration 780 : 0.13615217804908752
Loss at iteration 790 : 0.1104995533823967
Loss at iteration 800 : 0.07152633368968964
Loss at iteration 810 : 0.10326536744832993
Loss at iteration 820 : 0.0662548840045929
Loss at iteration 830 : 0.17485785484313965
Loss at iteration 840 : 0.09186943620443344
Loss at iteration 850 : 0.095448337495327
Loss at iteration 860 : 0.06683424115180969
Loss at iteration 870 : 0.08926734328269958
Loss at iteration 880 : 0.09039922803640366
Loss at iteration 890 : 0.07746326178312302
Loss at iteration 900 : 0.08268453925848007
Loss at iteration 910 : 0.09051273763179779
Loss at iteration 920 : 0.08226008713245392
Loss at iteration 930 : 0.10732241719961166
Loss at iteration 940 : 0.08554291725158691
Loss at iteration 950 : 0.10193079710006714
Loss at iteration 960 : 0.0771353468298912
Loss at iteration 970 : 0.07279765605926514
Loss at iteration 980 : 0.07926975935697556
Loss at iteration 990 : 0.0950019359588623
Loss at iteration 1000 : 0.07911252975463867
Loss at iteration 1010 : 0.046979084610939026
Loss at iteration 1020 : 0.1437702476978302
Loss at iteration 1030 : 0.08578698337078094
Loss at iteration 1040 : 0.11034196615219116
Loss at iteration 1050 : 0.10864882171154022
Loss at iteration 1060 : 0.11576523631811142
Loss at iteration 1070 : 0.04445899650454521
Loss at iteration 1080 : 0.08718998730182648
Loss at iteration 1090 : 0.10165955126285553
Loss at iteration 1100 : 0.07151108980178833
Loss at iteration 1110 : 0.08405828475952148
Loss at iteration 1120 : 0.11770032346248627
Loss at iteration 1130 : 0.09172968566417694
Loss at iteration 1140 : 0.06465868651866913
Loss at iteration 1150 : 0.07844014465808868
Loss at iteration 1160 : 0.07384949922561646
Loss at iteration 1170 : 0.102720245718956
Loss at iteration 1180 : 0.10242174565792084
Loss at iteration 1190 : 0.10570895671844482
Loss at iteration 1200 : 0.0654083639383316
Loss at iteration 1210 : 0.1040717214345932
The SSIM Value is: 0.684075919787089
The PSNR Value is: 20.348990376790365
the epoch is: 26
Loss at iteration 10 : 0.12221312522888184
Loss at iteration 20 : 0.07585635781288147
Loss at iteration 30 : 0.05409672111272812
Loss at iteration 40 : 0.11294736713171005
Loss at iteration 50 : 0.08402311056852341
Loss at iteration 60 : 0.09291898459196091
Loss at iteration 70 : 0.09349179267883301
Loss at iteration 80 : 0.05477478355169296
Loss at iteration 90 : 0.046700239181518555
Loss at iteration 100 : 0.09115201234817505
Loss at iteration 110 : 0.07865479588508606
Loss at iteration 120 : 0.08514505624771118
Loss at iteration 130 : 0.11676712334156036
Loss at iteration 140 : 0.09234657883644104
Loss at iteration 150 : 0.11514608561992645
Loss at iteration 160 : 0.07651609927415848
Loss at iteration 170 : 0.10647248476743698
Loss at iteration 180 : 0.060422953218221664
Loss at iteration 190 : 0.06643851101398468
Loss at iteration 200 : 0.10546889156103134
Loss at iteration 210 : 0.10142143815755844
Loss at iteration 220 : 0.08422408998012543
Loss at iteration 230 : 0.11747114360332489
Loss at iteration 240 : 0.06935742497444153
Loss at iteration 250 : 0.08643728494644165
Loss at iteration 260 : 0.1453588306903839
Loss at iteration 270 : 0.05349045246839523
Loss at iteration 280 : 0.06093839928507805
Loss at iteration 290 : 0.047914594411849976
Loss at iteration 300 : 0.0742931067943573
Loss at iteration 310 : 0.09183277189731598
Loss at iteration 320 : 0.09337727725505829
Loss at iteration 330 : 0.07495531439781189
Loss at iteration 340 : 0.09629800915718079
Loss at iteration 350 : 0.12166264653205872
Loss at iteration 360 : 0.08976998180150986
Loss at iteration 370 : 0.058781903237104416
Loss at iteration 380 : 0.09624382108449936
Loss at iteration 390 : 0.08371621370315552
Loss at iteration 400 : 0.07787629961967468
Loss at iteration 410 : 0.08824970573186874
Loss at iteration 420 : 0.10236294567584991
Loss at iteration 430 : 0.08100874722003937
Loss at iteration 440 : 0.07195024937391281
Loss at iteration 450 : 0.11067426204681396
Loss at iteration 460 : 0.09280087798833847
Loss at iteration 470 : 0.057636916637420654
Loss at iteration 480 : 0.12439660727977753
Loss at iteration 490 : 0.051907382905483246
Loss at iteration 500 : 0.06026952713727951
Loss at iteration 510 : 0.07927757501602173
Loss at iteration 520 : 0.08867315948009491
Loss at iteration 530 : 0.08079585433006287
Loss at iteration 540 : 0.04799267649650574
Loss at iteration 550 : 0.09488698095083237
Loss at iteration 560 : 0.10305103659629822
Loss at iteration 570 : 0.09370706975460052
Loss at iteration 580 : 0.08284871280193329
Loss at iteration 590 : 0.08713953197002411
Loss at iteration 600 : 0.0489632748067379
Loss at iteration 610 : 0.10479198396205902
Loss at iteration 620 : 0.09174861758947372
Loss at iteration 630 : 0.060617923736572266
Loss at iteration 640 : 0.1064683198928833
Loss at iteration 650 : 0.09016691893339157
Loss at iteration 660 : 0.10932385921478271
Loss at iteration 670 : 0.062238436192274094
Loss at iteration 680 : 0.09264130890369415
Loss at iteration 690 : 0.09319063276052475
Loss at iteration 700 : 0.09077504277229309
Loss at iteration 710 : 0.10890845954418182
Loss at iteration 720 : 0.1032070443034172
Loss at iteration 730 : 0.13188888132572174
Loss at iteration 740 : 0.09414003789424896
Loss at iteration 750 : 0.08213967084884644
Loss at iteration 760 : 0.10666104406118393
Loss at iteration 770 : 0.06576229631900787
Loss at iteration 780 : 0.08127367496490479
Loss at iteration 790 : 0.06234255060553551
Loss at iteration 800 : 0.061471253633499146
Loss at iteration 810 : 0.09271740168333054
Loss at iteration 820 : 0.05798619985580444
Loss at iteration 830 : 0.08741619437932968
Loss at iteration 840 : 0.09515639394521713
Loss at iteration 850 : 0.13616037368774414
Loss at iteration 860 : 0.1014571338891983
Loss at iteration 870 : 0.1239403486251831
Loss at iteration 880 : 0.06629490852355957
Loss at iteration 890 : 0.09903104603290558
Loss at iteration 900 : 0.1075170636177063
Loss at iteration 910 : 0.0722995176911354
Loss at iteration 920 : 0.10845248401165009
Loss at iteration 930 : 0.10247302055358887
Loss at iteration 940 : 0.08879208564758301
Loss at iteration 950 : 0.07211098074913025
Loss at iteration 960 : 0.1299414485692978
Loss at iteration 970 : 0.09594333171844482
Loss at iteration 980 : 0.084311842918396
Loss at iteration 990 : 0.06617168337106705
Loss at iteration 1000 : 0.07238605618476868
Loss at iteration 1010 : 0.15901555120944977
Loss at iteration 1020 : 0.12203015387058258
Loss at iteration 1030 : 0.09813877195119858
Loss at iteration 1040 : 0.13305094838142395
Loss at iteration 1050 : 0.130269393324852
Loss at iteration 1060 : 0.06508442759513855
Loss at iteration 1070 : 0.13234636187553406
Loss at iteration 1080 : 0.06563805788755417
Loss at iteration 1090 : 0.09506544470787048
Loss at iteration 1100 : 0.1271859109401703
Loss at iteration 1110 : 0.09405571222305298
Loss at iteration 1120 : 0.07735026627779007
Loss at iteration 1130 : 0.06736651062965393
Loss at iteration 1140 : 0.10249342024326324
Loss at iteration 1150 : 0.09550617635250092
Loss at iteration 1160 : 0.07313141971826553
Loss at iteration 1170 : 0.15522852540016174
Loss at iteration 1180 : 0.07299381494522095
Loss at iteration 1190 : 0.13002419471740723
Loss at iteration 1200 : 0.1326269507408142
Loss at iteration 1210 : 0.10054885596036911
The SSIM Value is: 0.6739256799221038
The PSNR Value is: 19.55004291534424
the epoch is: 27
Loss at iteration 10 : 0.10267376154661179
Loss at iteration 20 : 0.08418023586273193
Loss at iteration 30 : 0.06075858697295189
Loss at iteration 40 : 0.09202703088521957
Loss at iteration 50 : 0.07542248070240021
Loss at iteration 60 : 0.08357258141040802
Loss at iteration 70 : 0.12572512030601501
Loss at iteration 80 : 0.04583511874079704
Loss at iteration 90 : 0.12136179208755493
Loss at iteration 100 : 0.10481619834899902
Loss at iteration 110 : 0.1259557455778122
Loss at iteration 120 : 0.09623579680919647
Loss at iteration 130 : 0.11270749568939209
Loss at iteration 140 : 0.06324928998947144
Loss at iteration 150 : 0.08059003949165344
Loss at iteration 160 : 0.10483350604772568
Loss at iteration 170 : 0.06015794724225998
Loss at iteration 180 : 0.06514998525381088
Loss at iteration 190 : 0.08898621052503586
Loss at iteration 200 : 0.08329135179519653
Loss at iteration 210 : 0.06965318322181702
Loss at iteration 220 : 0.10300805419683456
Loss at iteration 230 : 0.105292908847332
Loss at iteration 240 : 0.07462573796510696
Loss at iteration 250 : 0.0419732928276062
Loss at iteration 260 : 0.061328232288360596
Loss at iteration 270 : 0.08180170506238937
Loss at iteration 280 : 0.08812904357910156
Loss at iteration 290 : 0.07132355868816376
Loss at iteration 300 : 0.10371199250221252
Loss at iteration 310 : 0.08561389148235321
Loss at iteration 320 : 0.11357927322387695
Loss at iteration 330 : 0.09885682910680771
Loss at iteration 340 : 0.05823318287730217
Loss at iteration 350 : 0.09166473150253296
Loss at iteration 360 : 0.06813022494316101
Loss at iteration 370 : 0.06392838060855865
Loss at iteration 380 : 0.06123529002070427
Loss at iteration 390 : 0.11488529294729233
Loss at iteration 400 : 0.09091812372207642
Loss at iteration 410 : 0.04008477181196213
Loss at iteration 420 : 0.10481985658407211
Loss at iteration 430 : 0.10866010189056396
Loss at iteration 440 : 0.0733126774430275
Loss at iteration 450 : 0.11357621848583221
Loss at iteration 460 : 0.11819207668304443
Loss at iteration 470 : 0.06026218831539154
Loss at iteration 480 : 0.071843221783638
Loss at iteration 490 : 0.16382569074630737
Loss at iteration 500 : 0.09631659835577011
Loss at iteration 510 : 0.05201688036322594
Loss at iteration 520 : 0.11891323328018188
Loss at iteration 530 : 0.1159982681274414
Loss at iteration 540 : 0.10985070466995239
Loss at iteration 550 : 0.07306130230426788
Loss at iteration 560 : 0.09102662652730942
Loss at iteration 570 : 0.09030186384916306
Loss at iteration 580 : 0.06303802132606506
Loss at iteration 590 : 0.06527459621429443
Loss at iteration 600 : 0.0774456188082695
Loss at iteration 610 : 0.09596586227416992
Loss at iteration 620 : 0.07401309907436371
Loss at iteration 630 : 0.04723917692899704
Loss at iteration 640 : 0.08215384185314178
Loss at iteration 650 : 0.0932769775390625
Loss at iteration 660 : 0.05388662964105606
Loss at iteration 670 : 0.06907191872596741
Loss at iteration 680 : 0.07490004599094391
Loss at iteration 690 : 0.044059887528419495
Loss at iteration 700 : 0.08989168703556061
Loss at iteration 710 : 0.07228492200374603
Loss at iteration 720 : 0.11569519340991974
Loss at iteration 730 : 0.061855562031269073
Loss at iteration 740 : 0.08714719861745834
Loss at iteration 750 : 0.08607126772403717
Loss at iteration 760 : 0.12014858424663544
Loss at iteration 770 : 0.10255508124828339
Loss at iteration 780 : 0.07767347991466522
Loss at iteration 790 : 0.08390446752309799
Loss at iteration 800 : 0.10768330097198486
Loss at iteration 810 : 0.0774790346622467
Loss at iteration 820 : 0.11355437338352203
Loss at iteration 830 : 0.0764070600271225
Loss at iteration 840 : 0.1002102643251419
Loss at iteration 850 : 0.11680971831083298
Loss at iteration 860 : 0.09058818221092224
Loss at iteration 870 : 0.10210644453763962
Loss at iteration 880 : 0.05244527384638786
Loss at iteration 890 : 0.08412381261587143
Loss at iteration 900 : 0.08128282427787781
Loss at iteration 910 : 0.07745442539453506
Loss at iteration 920 : 0.08552064746618271
Loss at iteration 930 : 0.08275057375431061
Loss at iteration 940 : 0.08644531667232513
Loss at iteration 950 : 0.05918392539024353
Loss at iteration 960 : 0.09487339854240417
Loss at iteration 970 : 0.10900535434484482
Loss at iteration 980 : 0.11478017270565033
Loss at iteration 990 : 0.11191588640213013
Loss at iteration 1000 : 0.07962661981582642
Loss at iteration 1010 : 0.08708944171667099
Loss at iteration 1020 : 0.0979989692568779
Loss at iteration 1030 : 0.07793541252613068
Loss at iteration 1040 : 0.08575773239135742
Loss at iteration 1050 : 0.05292297154664993
Loss at iteration 1060 : 0.07667786628007889
Loss at iteration 1070 : 0.08096130192279816
Loss at iteration 1080 : 0.14742335677146912
Loss at iteration 1090 : 0.10077349841594696
Loss at iteration 1100 : 0.08055295050144196
Loss at iteration 1110 : 0.11005987972021103
Loss at iteration 1120 : 0.09291612356901169
Loss at iteration 1130 : 0.07973548024892807
Loss at iteration 1140 : 0.09289052337408066
Loss at iteration 1150 : 0.08407515287399292
Loss at iteration 1160 : 0.06636486947536469
Loss at iteration 1170 : 0.07226845622062683
Loss at iteration 1180 : 0.060240861028432846
Loss at iteration 1190 : 0.06529249995946884
Loss at iteration 1200 : 0.07461603730916977
Loss at iteration 1210 : 0.06553806364536285
The SSIM Value is: 0.6837876240412394
The PSNR Value is: 20.424226252237954
the epoch is: 28
Loss at iteration 10 : 0.07770562171936035
Loss at iteration 20 : 0.08601506799459457
Loss at iteration 30 : 0.09720686078071594
Loss at iteration 40 : 0.1078932136297226
Loss at iteration 50 : 0.11593779176473618
Loss at iteration 60 : 0.08409683406352997
Loss at iteration 70 : 0.09616778790950775
Loss at iteration 80 : 0.06576681137084961
Loss at iteration 90 : 0.060849420726299286
Loss at iteration 100 : 0.07517597079277039
Loss at iteration 110 : 0.12181234359741211
Loss at iteration 120 : 0.10319851338863373
Loss at iteration 130 : 0.07247206568717957
Loss at iteration 140 : 0.062088917940855026
Loss at iteration 150 : 0.1414015293121338
Loss at iteration 160 : 0.10868054628372192
Loss at iteration 170 : 0.07597824931144714
Loss at iteration 180 : 0.07886441051959991
Loss at iteration 190 : 0.08461794257164001
Loss at iteration 200 : 0.07490969449281693
Loss at iteration 210 : 0.06594076752662659
Loss at iteration 220 : 0.11182969063520432
Loss at iteration 230 : 0.048111628741025925
Loss at iteration 240 : 0.06103341281414032
Loss at iteration 250 : 0.10723519325256348
Loss at iteration 260 : 0.09262239933013916
Loss at iteration 270 : 0.10286351293325424
Loss at iteration 280 : 0.10889985412359238
Loss at iteration 290 : 0.09288327395915985
Loss at iteration 300 : 0.0615202896296978
Loss at iteration 310 : 0.060823749750852585
Loss at iteration 320 : 0.1474701166152954
Loss at iteration 330 : 0.09497987478971481
Loss at iteration 340 : 0.10697174072265625
Loss at iteration 350 : 0.0912565216422081
Loss at iteration 360 : 0.08071523904800415
Loss at iteration 370 : 0.07014921307563782
Loss at iteration 380 : 0.1229964941740036
Loss at iteration 390 : 0.08153755962848663
Loss at iteration 400 : 0.0926349014043808
Loss at iteration 410 : 0.10190461575984955
Loss at iteration 420 : 0.11458715051412582
Loss at iteration 430 : 0.09977191686630249
Loss at iteration 440 : 0.12849965691566467
Loss at iteration 450 : 0.07147961854934692
Loss at iteration 460 : 0.085178904235363
Loss at iteration 470 : 0.07293090969324112
Loss at iteration 480 : 0.06214379519224167
Loss at iteration 490 : 0.07609168440103531
Loss at iteration 500 : 0.10557673871517181
Loss at iteration 510 : 0.07509487867355347
Loss at iteration 520 : 0.047583334147930145
Loss at iteration 530 : 0.13585321605205536
Loss at iteration 540 : 0.09588178992271423
Loss at iteration 550 : 0.14117062091827393
Loss at iteration 560 : 0.12538036704063416
Loss at iteration 570 : 0.08673255145549774
Loss at iteration 580 : 0.07355500012636185
Loss at iteration 590 : 0.10473306477069855
Loss at iteration 600 : 0.10768313705921173
Loss at iteration 610 : 0.10394939035177231
Loss at iteration 620 : 0.12008523941040039
Loss at iteration 630 : 0.048427022993564606
Loss at iteration 640 : 0.07361346483230591
Loss at iteration 650 : 0.1244250163435936
Loss at iteration 660 : 0.1461314558982849
Loss at iteration 670 : 0.13787703216075897
Loss at iteration 680 : 0.06579500436782837
Loss at iteration 690 : 0.05384204909205437
Loss at iteration 700 : 0.06428946554660797
Loss at iteration 710 : 0.07249241322278976
Loss at iteration 720 : 0.08027464151382446
Loss at iteration 730 : 0.061709266155958176
Loss at iteration 740 : 0.05546567961573601
Loss at iteration 750 : 0.13090327382087708
Loss at iteration 760 : 0.06111358851194382
Loss at iteration 770 : 0.05362920090556145
Loss at iteration 780 : 0.06654193997383118
Loss at iteration 790 : 0.09503944963216782
Loss at iteration 800 : 0.10312922298908234
Loss at iteration 810 : 0.14199362695217133
Loss at iteration 820 : 0.09113877266645432
Loss at iteration 830 : 0.05837879702448845
Loss at iteration 840 : 0.061589013785123825
Loss at iteration 850 : 0.09125614166259766
Loss at iteration 860 : 0.06337587535381317
Loss at iteration 870 : 0.08204331248998642
Loss at iteration 880 : 0.08409178256988525
Loss at iteration 890 : 0.09665235131978989
Loss at iteration 900 : 0.0825539082288742
Loss at iteration 910 : 0.14455783367156982
Loss at iteration 920 : 0.05833423137664795
Loss at iteration 930 : 0.10464215278625488
Loss at iteration 940 : 0.1478184163570404
Loss at iteration 950 : 0.09204600751399994
Loss at iteration 960 : 0.07222115993499756
Loss at iteration 970 : 0.12194307893514633
Loss at iteration 980 : 0.0901264175772667
Loss at iteration 990 : 0.10766953229904175
Loss at iteration 1000 : 0.09050779789686203
Loss at iteration 1010 : 0.08231763541698456
Loss at iteration 1020 : 0.09738153219223022
Loss at iteration 1030 : 0.08657288551330566
Loss at iteration 1040 : 0.11000899970531464
Loss at iteration 1050 : 0.08248137682676315
Loss at iteration 1060 : 0.07538824528455734
Loss at iteration 1070 : 0.10166168212890625
Loss at iteration 1080 : 0.11277639865875244
Loss at iteration 1090 : 0.06944359838962555
Loss at iteration 1100 : 0.0518147274851799
Loss at iteration 1110 : 0.0674636960029602
Loss at iteration 1120 : 0.07940130680799484
Loss at iteration 1130 : 0.08255039900541306
Loss at iteration 1140 : 0.08679582923650742
Loss at iteration 1150 : 0.07527618855237961
Loss at iteration 1160 : 0.0660201758146286
Loss at iteration 1170 : 0.12152522802352905
Loss at iteration 1180 : 0.056622665375471115
Loss at iteration 1190 : 0.13610327243804932
Loss at iteration 1200 : 0.06446313858032227
Loss at iteration 1210 : 0.06484273076057434
The SSIM Value is: 0.6768007318178813
The PSNR Value is: 19.669351069132485
the epoch is: 29
Loss at iteration 10 : 0.10441514104604721
Loss at iteration 20 : 0.08031252771615982
Loss at iteration 30 : 0.09960868209600449
Loss at iteration 40 : 0.11456042528152466
Loss at iteration 50 : 0.039201267063617706
Loss at iteration 60 : 0.11651062220335007
Loss at iteration 70 : 0.10510276257991791
Loss at iteration 80 : 0.08538219332695007
Loss at iteration 90 : 0.0942162275314331
Loss at iteration 100 : 0.12217700481414795
Loss at iteration 110 : 0.09788112342357635
Loss at iteration 120 : 0.08777366578578949
Loss at iteration 130 : 0.12896773219108582
Loss at iteration 140 : 0.104554682970047
Loss at iteration 150 : 0.09401549398899078
Loss at iteration 160 : 0.15870383381843567
Loss at iteration 170 : 0.11140616238117218
Loss at iteration 180 : 0.09221208095550537
Loss at iteration 190 : 0.11323027312755585
Loss at iteration 200 : 0.0969180092215538
Loss at iteration 210 : 0.08226853609085083
Loss at iteration 220 : 0.11071356385946274
Loss at iteration 230 : 0.08218564838171005
Loss at iteration 240 : 0.10064925998449326
Loss at iteration 250 : 0.09518207609653473
Loss at iteration 260 : 0.09193431586027145
Loss at iteration 270 : 0.06735751032829285
Loss at iteration 280 : 0.07970479130744934
Loss at iteration 290 : 0.07601349800825119
Loss at iteration 300 : 0.04711148142814636
Loss at iteration 310 : 0.10274124145507812
Loss at iteration 320 : 0.11301138252019882
Loss at iteration 330 : 0.14525547623634338
Loss at iteration 340 : 0.09230633825063705
Loss at iteration 350 : 0.0988469049334526
Loss at iteration 360 : 0.07408471405506134
Loss at iteration 370 : 0.052523039281368256
Loss at iteration 380 : 0.11351668834686279
Loss at iteration 390 : 0.12916438281536102
Loss at iteration 400 : 0.07736866176128387
Loss at iteration 410 : 0.07757674157619476
Loss at iteration 420 : 0.11663073301315308
Loss at iteration 430 : 0.04986318200826645
Loss at iteration 440 : 0.10534825176000595
Loss at iteration 450 : 0.11620506644248962
Loss at iteration 460 : 0.08165822178125381
Loss at iteration 470 : 0.061279378831386566
Loss at iteration 480 : 0.0649179220199585
Loss at iteration 490 : 0.09629170596599579
Loss at iteration 500 : 0.10089140385389328
Loss at iteration 510 : 0.07753794640302658
Loss at iteration 520 : 0.13481445610523224
Loss at iteration 530 : 0.059900324791669846
Loss at iteration 540 : 0.06795936077833176
Loss at iteration 550 : 0.06765307486057281
Loss at iteration 560 : 0.13030701875686646
Loss at iteration 570 : 0.05788389965891838
Loss at iteration 580 : 0.09460952877998352
Loss at iteration 590 : 0.08128267526626587
Loss at iteration 600 : 0.09586966037750244
Loss at iteration 610 : 0.07887843251228333
Loss at iteration 620 : 0.06180976331233978
Loss at iteration 630 : 0.0696941465139389
Loss at iteration 640 : 0.09431630373001099
Loss at iteration 650 : 0.06582242250442505
Loss at iteration 660 : 0.10629008710384369
Loss at iteration 670 : 0.10642606019973755
Loss at iteration 680 : 0.06357880681753159
Loss at iteration 690 : 0.11584987491369247
Loss at iteration 700 : 0.11848564445972443
Loss at iteration 710 : 0.07750216126441956
Loss at iteration 720 : 0.0749543309211731
Loss at iteration 730 : 0.047772474586963654
Loss at iteration 740 : 0.07551537454128265
Loss at iteration 750 : 0.15338000655174255
Loss at iteration 760 : 0.0725722536444664
Loss at iteration 770 : 0.11462815850973129
Loss at iteration 780 : 0.10719567537307739
Loss at iteration 790 : 0.0696115791797638
Loss at iteration 800 : 0.07822482287883759
Loss at iteration 810 : 0.05911733955144882
Loss at iteration 820 : 0.07484781742095947
Loss at iteration 830 : 0.05817074328660965
Loss at iteration 840 : 0.05611766502261162
Loss at iteration 850 : 0.102057084441185
Loss at iteration 860 : 0.09212124347686768
Loss at iteration 870 : 0.12789957225322723
Loss at iteration 880 : 0.07098942995071411
Loss at iteration 890 : 0.05687519907951355
Loss at iteration 900 : 0.07291948050260544
Loss at iteration 910 : 0.09998643398284912
Loss at iteration 920 : 0.08003628253936768
Loss at iteration 930 : 0.08613546937704086
Loss at iteration 940 : 0.09084263443946838
Loss at iteration 950 : 0.08713172376155853
Loss at iteration 960 : 0.06735511869192123
Loss at iteration 970 : 0.08070782572031021
Loss at iteration 980 : 0.0549846813082695
Loss at iteration 990 : 0.06602411717176437
Loss at iteration 1000 : 0.08319507539272308
Loss at iteration 1010 : 0.11419856548309326
Loss at iteration 1020 : 0.09764304012060165
Loss at iteration 1030 : 0.08768558502197266
Loss at iteration 1040 : 0.055605195462703705
Loss at iteration 1050 : 0.09333430230617523
Loss at iteration 1060 : 0.08770178258419037
Loss at iteration 1070 : 0.04332391545176506
Loss at iteration 1080 : 0.0554356686770916
Loss at iteration 1090 : 0.1131831556558609
Loss at iteration 1100 : 0.07624301314353943
Loss at iteration 1110 : 0.11225287616252899
Loss at iteration 1120 : 0.08324878662824631
Loss at iteration 1130 : 0.0566096305847168
Loss at iteration 1140 : 0.05519915372133255
Loss at iteration 1150 : 0.06895264983177185
Loss at iteration 1160 : 0.09050574153661728
Loss at iteration 1170 : 0.05376964062452316
Loss at iteration 1180 : 0.120170459151268
Loss at iteration 1190 : 0.09604679048061371
Loss at iteration 1200 : 0.10593396425247192
Loss at iteration 1210 : 0.10207189619541168
The SSIM Value is: 0.6733277142047882
The PSNR Value is: 19.533305104573568
the epoch is: 30
Loss at iteration 10 : 0.06527964025735855
Loss at iteration 20 : 0.08735184371471405
Loss at iteration 30 : 0.07144958525896072
Loss at iteration 40 : 0.09672525525093079
Loss at iteration 50 : 0.10442548990249634
Loss at iteration 60 : 0.12253101915121078
Loss at iteration 70 : 0.1193210706114769
Loss at iteration 80 : 0.10880748927593231
Loss at iteration 90 : 0.10819374024868011
Loss at iteration 100 : 0.12492568790912628
Loss at iteration 110 : 0.08363422006368637
Loss at iteration 120 : 0.09756802767515182
Loss at iteration 130 : 0.07921602576971054
Loss at iteration 140 : 0.10801559686660767
Loss at iteration 150 : 0.1252909153699875
Loss at iteration 160 : 0.12903377413749695
Loss at iteration 170 : 0.04413938149809837
Loss at iteration 180 : 0.079530730843544
Loss at iteration 190 : 0.0990738794207573
Loss at iteration 200 : 0.10355687141418457
Loss at iteration 210 : 0.10041838139295578
Loss at iteration 220 : 0.08852279186248779
Loss at iteration 230 : 0.09419040381908417
Loss at iteration 240 : 0.11133991926908493
Loss at iteration 250 : 0.1146448403596878
Loss at iteration 260 : 0.07753601670265198
Loss at iteration 270 : 0.07329384237527847
Loss at iteration 280 : 0.10813306272029877
Loss at iteration 290 : 0.11894883960485458
Loss at iteration 300 : 0.10908305644989014
Loss at iteration 310 : 0.10408705472946167
Loss at iteration 320 : 0.08402584493160248
Loss at iteration 330 : 0.07590867578983307
Loss at iteration 340 : 0.11813902854919434
Loss at iteration 350 : 0.09030082821846008
Loss at iteration 360 : 0.07611700892448425
Loss at iteration 370 : 0.07815209031105042
Loss at iteration 380 : 0.07213305681943893
Loss at iteration 390 : 0.11011707037687302
Loss at iteration 400 : 0.10303042829036713
Loss at iteration 410 : 0.05835859104990959
Loss at iteration 420 : 0.08590944856405258
Loss at iteration 430 : 0.057035043835639954
Loss at iteration 440 : 0.07122845202684402
Loss at iteration 450 : 0.07461369037628174
Loss at iteration 460 : 0.08448024839162827
Loss at iteration 470 : 0.14889442920684814
Loss at iteration 480 : 0.06284241378307343
Loss at iteration 490 : 0.10098692029714584
Loss at iteration 500 : 0.09683378785848618
Loss at iteration 510 : 0.06840185821056366
Loss at iteration 520 : 0.10360782593488693
Loss at iteration 530 : 0.09343969821929932
Loss at iteration 540 : 0.11287938058376312
Loss at iteration 550 : 0.08015964925289154
Loss at iteration 560 : 0.07156717777252197
Loss at iteration 570 : 0.06970533728599548
Loss at iteration 580 : 0.13395434617996216
Loss at iteration 590 : 0.09905765950679779
Loss at iteration 600 : 0.08992457389831543
Loss at iteration 610 : 0.11766340583562851
Loss at iteration 620 : 0.07399669289588928
Loss at iteration 630 : 0.0866173729300499
Loss at iteration 640 : 0.10077446699142456
Loss at iteration 650 : 0.08855098485946655
Loss at iteration 660 : 0.10944058001041412
Loss at iteration 670 : 0.068928062915802
Loss at iteration 680 : 0.08017578721046448
Loss at iteration 690 : 0.05213875323534012
Loss at iteration 700 : 0.11578022688627243
Loss at iteration 710 : 0.08291439712047577
Loss at iteration 720 : 0.06971760839223862
Loss at iteration 730 : 0.05494920536875725
Loss at iteration 740 : 0.09597788751125336
Loss at iteration 750 : 0.06519565731287003
Loss at iteration 760 : 0.06765212118625641
Loss at iteration 770 : 0.10911248624324799
Loss at iteration 780 : 0.12136497348546982
Loss at iteration 790 : 0.06593027710914612
Loss at iteration 800 : 0.05562978982925415
Loss at iteration 810 : 0.09632176160812378
Loss at iteration 820 : 0.07124923914670944
Loss at iteration 830 : 0.08210216462612152
Loss at iteration 840 : 0.09517984092235565
Loss at iteration 850 : 0.08204968273639679
Loss at iteration 860 : 0.07739266008138657
Loss at iteration 870 : 0.07075275480747223
Loss at iteration 880 : 0.10353001207113266
Loss at iteration 890 : 0.06831937283277512
Loss at iteration 900 : 0.09745272248983383
Loss at iteration 910 : 0.08112336695194244
Loss at iteration 920 : 0.06588943302631378
Loss at iteration 930 : 0.09142279624938965
Loss at iteration 940 : 0.09850861877202988
Loss at iteration 950 : 0.04440190643072128
Loss at iteration 960 : 0.10063614696264267
Loss at iteration 970 : 0.07456468790769577
Loss at iteration 980 : 0.10856833308935165
Loss at iteration 990 : 0.12199723720550537
Loss at iteration 1000 : 0.07280279695987701
Loss at iteration 1010 : 0.0481283962726593
Loss at iteration 1020 : 0.09020160883665085
Loss at iteration 1030 : 0.06740468740463257
Loss at iteration 1040 : 0.07954314351081848
Loss at iteration 1050 : 0.07975118607282639
Loss at iteration 1060 : 0.14259299635887146
Loss at iteration 1070 : 0.0964878723025322
Loss at iteration 1080 : 0.08416681736707687
Loss at iteration 1090 : 0.05140770971775055
Loss at iteration 1100 : 0.11477302014827728
Loss at iteration 1110 : 0.1086392030119896
Loss at iteration 1120 : 0.09231717884540558
Loss at iteration 1130 : 0.06921228021383286
Loss at iteration 1140 : 0.08775608241558075
Loss at iteration 1150 : 0.0769871324300766
Loss at iteration 1160 : 0.0813143253326416
Loss at iteration 1170 : 0.09648548066616058
Loss at iteration 1180 : 0.1034180149435997
Loss at iteration 1190 : 0.08675854653120041
Loss at iteration 1200 : 0.06102067977190018
Loss at iteration 1210 : 0.10297027230262756
The SSIM Value is: 0.70189728140831
The PSNR Value is: 21.342710494995117
the highest SSIM value is: 21.342710494995117
the epoch is: 31
Loss at iteration 10 : 0.10798753798007965
Loss at iteration 20 : 0.10358136892318726
Loss at iteration 30 : 0.08768557757139206
Loss at iteration 40 : 0.095822274684906
Loss at iteration 50 : 0.09009849280118942
Loss at iteration 60 : 0.06850270926952362
Loss at iteration 70 : 0.07016956061124802
Loss at iteration 80 : 0.07301978766918182
Loss at iteration 90 : 0.10834838449954987
Loss at iteration 100 : 0.09704944491386414
Loss at iteration 110 : 0.10867031663656235
Loss at iteration 120 : 0.07629640400409698
Loss at iteration 130 : 0.07454600185155869
Loss at iteration 140 : 0.07211178541183472
Loss at iteration 150 : 0.11613224446773529
Loss at iteration 160 : 0.07833452522754669
Loss at iteration 170 : 0.07145962119102478
Loss at iteration 180 : 0.10014776885509491
Loss at iteration 190 : 0.12119913101196289
Loss at iteration 200 : 0.09737356752157211
Loss at iteration 210 : 0.06845477968454361
Loss at iteration 220 : 0.11769555509090424
Loss at iteration 230 : 0.06758616864681244
Loss at iteration 240 : 0.06292934715747833
Loss at iteration 250 : 0.07087787240743637
Loss at iteration 260 : 0.09438754618167877
Loss at iteration 270 : 0.09130899608135223
Loss at iteration 280 : 0.07410678267478943
Loss at iteration 290 : 0.0820174440741539
Loss at iteration 300 : 0.11531098932027817
Loss at iteration 310 : 0.07458134740591049
Loss at iteration 320 : 0.07552976161241531
Loss at iteration 330 : 0.06657908111810684
Loss at iteration 340 : 0.05218208581209183
Loss at iteration 350 : 0.09942297637462616
Loss at iteration 360 : 0.13360534608364105
Loss at iteration 370 : 0.07287824898958206
Loss at iteration 380 : 0.12787625193595886
Loss at iteration 390 : 0.07698704302310944
Loss at iteration 400 : 0.06188860163092613
Loss at iteration 410 : 0.04843033105134964
Loss at iteration 420 : 0.07541638612747192
Loss at iteration 430 : 0.09140132367610931
Loss at iteration 440 : 0.0702904462814331
Loss at iteration 450 : 0.07268165051937103
Loss at iteration 460 : 0.1282372623682022
Loss at iteration 470 : 0.0611945204436779
Loss at iteration 480 : 0.09542302787303925
Loss at iteration 490 : 0.06797640770673752
Loss at iteration 500 : 0.05396465212106705
Loss at iteration 510 : 0.11320170015096664
Loss at iteration 520 : 0.15254686772823334
Loss at iteration 530 : 0.13228920102119446
Loss at iteration 540 : 0.07209950685501099
Loss at iteration 550 : 0.0999584048986435
Loss at iteration 560 : 0.08620880544185638
Loss at iteration 570 : 0.07969656586647034
Loss at iteration 580 : 0.1403961181640625
Loss at iteration 590 : 0.0975562259554863
Loss at iteration 600 : 0.10706400871276855
Loss at iteration 610 : 0.09445740282535553
Loss at iteration 620 : 0.10304682701826096
Loss at iteration 630 : 0.10783471912145615
Loss at iteration 640 : 0.07269912958145142
Loss at iteration 650 : 0.13784849643707275
Loss at iteration 660 : 0.08989779651165009
Loss at iteration 670 : 0.05656284838914871
Loss at iteration 680 : 0.10092046111822128
Loss at iteration 690 : 0.07323117554187775
Loss at iteration 700 : 0.05302705988287926
Loss at iteration 710 : 0.0877222940325737
Loss at iteration 720 : 0.07213228195905685
Loss at iteration 730 : 0.12914912402629852
Loss at iteration 740 : 0.09068287909030914
Loss at iteration 750 : 0.06266417354345322
Loss at iteration 760 : 0.05944373458623886
Loss at iteration 770 : 0.09937316179275513
Loss at iteration 780 : 0.13679459691047668
Loss at iteration 790 : 0.10625265538692474
Loss at iteration 800 : 0.12549740076065063
Loss at iteration 810 : 0.06874500215053558
Loss at iteration 820 : 0.11284564435482025
Loss at iteration 830 : 0.08921156823635101
Loss at iteration 840 : 0.09072387218475342
Loss at iteration 850 : 0.09474699199199677
Loss at iteration 860 : 0.10104797780513763
Loss at iteration 870 : 0.10156518220901489
Loss at iteration 880 : 0.10097818076610565
Loss at iteration 890 : 0.1301926225423813
Loss at iteration 900 : 0.0993882566690445
Loss at iteration 910 : 0.10282355546951294
Loss at iteration 920 : 0.06973382085561752
Loss at iteration 930 : 0.08027350157499313
Loss at iteration 940 : 0.1469099223613739
Loss at iteration 950 : 0.14944729208946228
Loss at iteration 960 : 0.122269406914711
Loss at iteration 970 : 0.06254246830940247
Loss at iteration 980 : 0.0808546245098114
Loss at iteration 990 : 0.07435908913612366
Loss at iteration 1000 : 0.10386854410171509
Loss at iteration 1010 : 0.052040353417396545
Loss at iteration 1020 : 0.08107826113700867
Loss at iteration 1030 : 0.08839508891105652
Loss at iteration 1040 : 0.05961351841688156
Loss at iteration 1050 : 0.04948297142982483
Loss at iteration 1060 : 0.10643786191940308
Loss at iteration 1070 : 0.0808718353509903
Loss at iteration 1080 : 0.08096149563789368
Loss at iteration 1090 : 0.0933440625667572
Loss at iteration 1100 : 0.08171404898166656
Loss at iteration 1110 : 0.06270292401313782
Loss at iteration 1120 : 0.06224013864994049
Loss at iteration 1130 : 0.058858804404735565
Loss at iteration 1140 : 0.08478011935949326
Loss at iteration 1150 : 0.08182664215564728
Loss at iteration 1160 : 0.1329474300146103
Loss at iteration 1170 : 0.13329127430915833
Loss at iteration 1180 : 0.10196520388126373
Loss at iteration 1190 : 0.13224820792675018
Loss at iteration 1200 : 0.0800449401140213
Loss at iteration 1210 : 0.08285215497016907
The SSIM Value is: 0.6885745584964752
The PSNR Value is: 20.048229598999022
the epoch is: 32
Loss at iteration 10 : 0.06877794116735458
Loss at iteration 20 : 0.058167897164821625
Loss at iteration 30 : 0.1109791249036789
Loss at iteration 40 : 0.0751589983701706
Loss at iteration 50 : 0.12645694613456726
Loss at iteration 60 : 0.09310408681631088
Loss at iteration 70 : 0.07505907118320465
Loss at iteration 80 : 0.09771864116191864
Loss at iteration 90 : 0.05170488357543945
Loss at iteration 100 : 0.10780711472034454
Loss at iteration 110 : 0.09962614625692368
Loss at iteration 120 : 0.08477625250816345
Loss at iteration 130 : 0.09578099101781845
Loss at iteration 140 : 0.15018431842327118
Loss at iteration 150 : 0.09029078483581543
Loss at iteration 160 : 0.06768032163381577
Loss at iteration 170 : 0.101463183760643
Loss at iteration 180 : 0.10635723173618317
Loss at iteration 190 : 0.0801083892583847
Loss at iteration 200 : 0.11578436940908432
Loss at iteration 210 : 0.048835113644599915
Loss at iteration 220 : 0.09497441351413727
Loss at iteration 230 : 0.06550008058547974
Loss at iteration 240 : 0.10345369577407837
Loss at iteration 250 : 0.09876130521297455
Loss at iteration 260 : 0.06067221611738205
Loss at iteration 270 : 0.10493902862071991
Loss at iteration 280 : 0.0646495372056961
Loss at iteration 290 : 0.07443603128194809
Loss at iteration 300 : 0.08833056688308716
Loss at iteration 310 : 0.08891057968139648
Loss at iteration 320 : 0.12830016016960144
Loss at iteration 330 : 0.05086658522486687
Loss at iteration 340 : 0.11278270184993744
Loss at iteration 350 : 0.10074275732040405
Loss at iteration 360 : 0.06565933674573898
Loss at iteration 370 : 0.06318863481283188
Loss at iteration 380 : 0.089510977268219
Loss at iteration 390 : 0.06469007581472397
Loss at iteration 400 : 0.09321151673793793
Loss at iteration 410 : 0.07296609878540039
Loss at iteration 420 : 0.07807813584804535
Loss at iteration 430 : 0.05863486975431442
Loss at iteration 440 : 0.08546839654445648
Loss at iteration 450 : 0.04789721220731735
Loss at iteration 460 : 0.053900253027677536
Loss at iteration 470 : 0.1328534483909607
Loss at iteration 480 : 0.08311106264591217
Loss at iteration 490 : 0.05693771690130234
Loss at iteration 500 : 0.08651997148990631
Loss at iteration 510 : 0.06435497850179672
Loss at iteration 520 : 0.06520061194896698
Loss at iteration 530 : 0.1140543669462204
Loss at iteration 540 : 0.08212688565254211
Loss at iteration 550 : 0.06813386082649231
Loss at iteration 560 : 0.09962280094623566
Loss at iteration 570 : 0.06424824893474579
Loss at iteration 580 : 0.08124206960201263
Loss at iteration 590 : 0.12090068310499191
Loss at iteration 600 : 0.08735262602567673
Loss at iteration 610 : 0.12045107781887054
Loss at iteration 620 : 0.04663621634244919
Loss at iteration 630 : 0.14650677144527435
Loss at iteration 640 : 0.11212287843227386
Loss at iteration 650 : 0.09154431521892548
Loss at iteration 660 : 0.08046472817659378
Loss at iteration 670 : 0.12056908011436462
Loss at iteration 680 : 0.11748738586902618
Loss at iteration 690 : 0.1000882014632225
Loss at iteration 700 : 0.07859548926353455
Loss at iteration 710 : 0.1288764327764511
Loss at iteration 720 : 0.09906768798828125
Loss at iteration 730 : 0.11825581640005112
Loss at iteration 740 : 0.09830983728170395
Loss at iteration 750 : 0.11723612248897552
Loss at iteration 760 : 0.09146442264318466
Loss at iteration 770 : 0.10550186038017273
Loss at iteration 780 : 0.10747319459915161
Loss at iteration 790 : 0.09564036130905151
Loss at iteration 800 : 0.05975158140063286
Loss at iteration 810 : 0.0429774634540081
Loss at iteration 820 : 0.05955103039741516
Loss at iteration 830 : 0.11536505818367004
Loss at iteration 840 : 0.05047661066055298
Loss at iteration 850 : 0.11755716055631638
Loss at iteration 860 : 0.09919673204421997
Loss at iteration 870 : 0.11364880204200745
Loss at iteration 880 : 0.09485931694507599
Loss at iteration 890 : 0.06796108931303024
Loss at iteration 900 : 0.1241958886384964
Loss at iteration 910 : 0.14764121174812317
Loss at iteration 920 : 0.08921100944280624
Loss at iteration 930 : 0.0938258171081543
Loss at iteration 940 : 0.07221104204654694
Loss at iteration 950 : 0.09985880553722382
Loss at iteration 960 : 0.08256831020116806
Loss at iteration 970 : 0.05682447552680969
Loss at iteration 980 : 0.09266160428524017
Loss at iteration 990 : 0.08803950995206833
Loss at iteration 1000 : 0.07616761326789856
Loss at iteration 1010 : 0.10393142700195312
Loss at iteration 1020 : 0.08550316095352173
Loss at iteration 1030 : 0.11516885459423065
Loss at iteration 1040 : 0.11275206506252289
Loss at iteration 1050 : 0.08295585215091705
Loss at iteration 1060 : 0.07904936373233795
Loss at iteration 1070 : 0.07622583210468292
Loss at iteration 1080 : 0.09916432201862335
Loss at iteration 1090 : 0.07824306190013885
Loss at iteration 1100 : 0.05143659934401512
Loss at iteration 1110 : 0.0564192533493042
Loss at iteration 1120 : 0.17471250891685486
Loss at iteration 1130 : 0.07555416971445084
Loss at iteration 1140 : 0.10998447239398956
Loss at iteration 1150 : 0.1273380070924759
Loss at iteration 1160 : 0.05374875292181969
Loss at iteration 1170 : 0.06008155643939972
Loss at iteration 1180 : 0.10601307451725006
Loss at iteration 1190 : 0.10017292201519012
Loss at iteration 1200 : 0.07326606661081314
Loss at iteration 1210 : 0.0761486142873764
The SSIM Value is: 0.6918685038884481
The PSNR Value is: 20.535360908508302
the epoch is: 33
Loss at iteration 10 : 0.06622379273176193
Loss at iteration 20 : 0.12722504138946533
Loss at iteration 30 : 0.05139490216970444
Loss at iteration 40 : 0.0739162489771843
Loss at iteration 50 : 0.08620186150074005
Loss at iteration 60 : 0.09618926048278809
Loss at iteration 70 : 0.13267312943935394
Loss at iteration 80 : 0.10839065909385681
Loss at iteration 90 : 0.1075754463672638
Loss at iteration 100 : 0.05930682644248009
Loss at iteration 110 : 0.07366675138473511
Loss at iteration 120 : 0.06348452717065811
Loss at iteration 130 : 0.0952233299612999
Loss at iteration 140 : 0.06960012018680573
Loss at iteration 150 : 0.10764497518539429
Loss at iteration 160 : 0.08230727910995483
Loss at iteration 170 : 0.09588869661092758
Loss at iteration 180 : 0.07945605367422104
Loss at iteration 190 : 0.09298932552337646
Loss at iteration 200 : 0.049797289073467255
Loss at iteration 210 : 0.06552951782941818
Loss at iteration 220 : 0.08874078094959259
Loss at iteration 230 : 0.11139222234487534
Loss at iteration 240 : 0.09743037819862366
Loss at iteration 250 : 0.11649110168218613
Loss at iteration 260 : 0.05066385865211487
Loss at iteration 270 : 0.057562343776226044
Loss at iteration 280 : 0.10306438058614731
Loss at iteration 290 : 0.061304204165935516
Loss at iteration 300 : 0.0926649272441864
Loss at iteration 310 : 0.09584176540374756
Loss at iteration 320 : 0.07701609283685684
Loss at iteration 330 : 0.14462511241436005
Loss at iteration 340 : 0.10693924129009247
Loss at iteration 350 : 0.1351276934146881
Loss at iteration 360 : 0.09280742704868317
Loss at iteration 370 : 0.06721480935811996
Loss at iteration 380 : 0.10215020179748535
Loss at iteration 390 : 0.10129173845052719
Loss at iteration 400 : 0.10812762379646301
Loss at iteration 410 : 0.0878225564956665
Loss at iteration 420 : 0.05484820529818535
Loss at iteration 430 : 0.13676941394805908
Loss at iteration 440 : 0.15534622967243195
Loss at iteration 450 : 0.08546336740255356
Loss at iteration 460 : 0.066175177693367
Loss at iteration 470 : 0.06923677772283554
Loss at iteration 480 : 0.10254039615392685
Loss at iteration 490 : 0.08074477314949036
Loss at iteration 500 : 0.08274095505475998
Loss at iteration 510 : 0.09586143493652344
Loss at iteration 520 : 0.09826670587062836
Loss at iteration 530 : 0.05498802661895752
Loss at iteration 540 : 0.06322687864303589
Loss at iteration 550 : 0.06496821343898773
Loss at iteration 560 : 0.07163053750991821
Loss at iteration 570 : 0.11144739389419556
Loss at iteration 580 : 0.09718026965856552
Loss at iteration 590 : 0.05167562514543533
Loss at iteration 600 : 0.08403738588094711
Loss at iteration 610 : 0.10931824892759323
Loss at iteration 620 : 0.07703426480293274
Loss at iteration 630 : 0.10027608275413513
Loss at iteration 640 : 0.09580172598361969
Loss at iteration 650 : 0.06292997300624847
Loss at iteration 660 : 0.10014890134334564
Loss at iteration 670 : 0.09046123921871185
Loss at iteration 680 : 0.10686236619949341
Loss at iteration 690 : 0.06857076287269592
Loss at iteration 700 : 0.1602066457271576
Loss at iteration 710 : 0.04960111528635025
Loss at iteration 720 : 0.10211832821369171
Loss at iteration 730 : 0.07116814702749252
Loss at iteration 740 : 0.04121657460927963
Loss at iteration 750 : 0.058308254927396774
Loss at iteration 760 : 0.11835670471191406
Loss at iteration 770 : 0.12724988162517548
Loss at iteration 780 : 0.07213407009840012
Loss at iteration 790 : 0.0692206472158432
Loss at iteration 800 : 0.11748452484607697
Loss at iteration 810 : 0.12453874200582504
Loss at iteration 820 : 0.09292827546596527
Loss at iteration 830 : 0.11024411022663116
Loss at iteration 840 : 0.1315036118030548
Loss at iteration 850 : 0.055349212139844894
Loss at iteration 860 : 0.0964052602648735
Loss at iteration 870 : 0.0673758015036583
Loss at iteration 880 : 0.10872982442378998
Loss at iteration 890 : 0.1267571896314621
Loss at iteration 900 : 0.1079448014497757
Loss at iteration 910 : 0.039336707442998886
Loss at iteration 920 : 0.06875881552696228
Loss at iteration 930 : 0.07706151902675629
Loss at iteration 940 : 0.10111840814352036
Loss at iteration 950 : 0.05318416282534599
Loss at iteration 960 : 0.11321955919265747
Loss at iteration 970 : 0.06660597026348114
Loss at iteration 980 : 0.07743041962385178
Loss at iteration 990 : 0.057472579181194305
Loss at iteration 1000 : 0.1204504668712616
Loss at iteration 1010 : 0.10697250068187714
Loss at iteration 1020 : 0.07837943732738495
Loss at iteration 1030 : 0.06920486688613892
Loss at iteration 1040 : 0.13666564226150513
Loss at iteration 1050 : 0.06583648175001144
Loss at iteration 1060 : 0.06829486787319183
Loss at iteration 1070 : 0.04338839650154114
Loss at iteration 1080 : 0.08571376651525497
Loss at iteration 1090 : 0.07847695052623749
Loss at iteration 1100 : 0.08803467452526093
Loss at iteration 1110 : 0.07474154233932495
Loss at iteration 1120 : 0.06409632414579391
Loss at iteration 1130 : 0.07841392606496811
Loss at iteration 1140 : 0.09995211660861969
Loss at iteration 1150 : 0.09820099920034409
Loss at iteration 1160 : 0.07235772907733917
Loss at iteration 1170 : 0.09121659398078918
Loss at iteration 1180 : 0.08386053889989853
Loss at iteration 1190 : 0.0803530365228653
Loss at iteration 1200 : 0.10230394452810287
Loss at iteration 1210 : 0.07540640234947205
The SSIM Value is: 0.6926356454690298
The PSNR Value is: 20.92220573425293
the epoch is: 34
Loss at iteration 10 : 0.1169363483786583
Loss at iteration 20 : 0.08070094883441925
Loss at iteration 30 : 0.12530073523521423
Loss at iteration 40 : 0.0812697634100914
Loss at iteration 50 : 0.06941370666027069
Loss at iteration 60 : 0.10213230550289154
Loss at iteration 70 : 0.06597837060689926
Loss at iteration 80 : 0.08377306908369064
Loss at iteration 90 : 0.09465690702199936
Loss at iteration 100 : 0.07887303829193115
Loss at iteration 110 : 0.13058000802993774
Loss at iteration 120 : 0.1408347338438034
Loss at iteration 130 : 0.1100686639547348
Loss at iteration 140 : 0.06453489512205124
Loss at iteration 150 : 0.07983354479074478
Loss at iteration 160 : 0.07871245592832565
Loss at iteration 170 : 0.08569832146167755
Loss at iteration 180 : 0.09896644204854965
Loss at iteration 190 : 0.08557113260030746
Loss at iteration 200 : 0.10585445165634155
Loss at iteration 210 : 0.06089939922094345
Loss at iteration 220 : 0.09447072446346283
Loss at iteration 230 : 0.055114518851041794
Loss at iteration 240 : 0.07895227521657944
Loss at iteration 250 : 0.06697174906730652
Loss at iteration 260 : 0.09660041332244873
Loss at iteration 270 : 0.09372203052043915
Loss at iteration 280 : 0.07440739125013351
Loss at iteration 290 : 0.11556240171194077
Loss at iteration 300 : 0.059407975524663925
Loss at iteration 310 : 0.10595957934856415
Loss at iteration 320 : 0.11307048797607422
Loss at iteration 330 : 0.08663757890462875
Loss at iteration 340 : 0.07926326245069504
Loss at iteration 350 : 0.1312813013792038
Loss at iteration 360 : 0.07632260769605637
Loss at iteration 370 : 0.12043123692274094
Loss at iteration 380 : 0.09918002784252167
Loss at iteration 390 : 0.08752362430095673
Loss at iteration 400 : 0.10500621795654297
Loss at iteration 410 : 0.09171596169471741
Loss at iteration 420 : 0.09583248943090439
Loss at iteration 430 : 0.09192761033773422
Loss at iteration 440 : 0.11151020228862762
Loss at iteration 450 : 0.09858594834804535
Loss at iteration 460 : 0.10277915000915527
Loss at iteration 470 : 0.06886323541402817
Loss at iteration 480 : 0.06737345457077026
Loss at iteration 490 : 0.08383916318416595
Loss at iteration 500 : 0.13716143369674683
Loss at iteration 510 : 0.10973143577575684
Loss at iteration 520 : 0.055529143661260605
Loss at iteration 530 : 0.07730752974748611
Loss at iteration 540 : 0.09752705693244934
Loss at iteration 550 : 0.13084499537944794
Loss at iteration 560 : 0.07019826769828796
Loss at iteration 570 : 0.12467934936285019
Loss at iteration 580 : 0.08332888782024384
Loss at iteration 590 : 0.1000833734869957
Loss at iteration 600 : 0.07255523651838303
Loss at iteration 610 : 0.07390859723091125
Loss at iteration 620 : 0.055081576108932495
Loss at iteration 630 : 0.0845860168337822
Loss at iteration 640 : 0.06390048563480377
Loss at iteration 650 : 0.07233240455389023
Loss at iteration 660 : 0.13750505447387695
Loss at iteration 670 : 0.09164632856845856
Loss at iteration 680 : 0.12028195708990097
Loss at iteration 690 : 0.13720208406448364
Loss at iteration 700 : 0.1421261727809906
Loss at iteration 710 : 0.09161578118801117
Loss at iteration 720 : 0.0712069422006607
Loss at iteration 730 : 0.11238359659910202
Loss at iteration 740 : 0.07078466564416885
Loss at iteration 750 : 0.09698428958654404
Loss at iteration 760 : 0.08709787577390671
Loss at iteration 770 : 0.053959112614393234
Loss at iteration 780 : 0.058719977736473083
Loss at iteration 790 : 0.05452398583292961
Loss at iteration 800 : 0.09468382596969604
Loss at iteration 810 : 0.10313844680786133
Loss at iteration 820 : 0.08077391982078552
Loss at iteration 830 : 0.11046090722084045
Loss at iteration 840 : 0.07776312530040741
Loss at iteration 850 : 0.14599564671516418
Loss at iteration 860 : 0.09485270828008652
Loss at iteration 870 : 0.12871849536895752
Loss at iteration 880 : 0.0846552699804306
Loss at iteration 890 : 0.07643106579780579
Loss at iteration 900 : 0.08882536739110947
Loss at iteration 910 : 0.11065196990966797
Loss at iteration 920 : 0.1421566605567932
Loss at iteration 930 : 0.12053115665912628
Loss at iteration 940 : 0.07025553286075592
Loss at iteration 950 : 0.10268773138523102
Loss at iteration 960 : 0.0938180685043335
Loss at iteration 970 : 0.0716090202331543
Loss at iteration 980 : 0.07279916852712631
Loss at iteration 990 : 0.12055971473455429
Loss at iteration 1000 : 0.10403472185134888
Loss at iteration 1010 : 0.0625058263540268
Loss at iteration 1020 : 0.11838128417730331
Loss at iteration 1030 : 0.09434722363948822
Loss at iteration 1040 : 0.05771273002028465
Loss at iteration 1050 : 0.07231159508228302
Loss at iteration 1060 : 0.08766617625951767
Loss at iteration 1070 : 0.055406004190444946
Loss at iteration 1080 : 0.12135283648967743
Loss at iteration 1090 : 0.09247633069753647
Loss at iteration 1100 : 0.05809197574853897
Loss at iteration 1110 : 0.13915228843688965
Loss at iteration 1120 : 0.11302584409713745
Loss at iteration 1130 : 0.07027652859687805
Loss at iteration 1140 : 0.0837285965681076
Loss at iteration 1150 : 0.09449635446071625
Loss at iteration 1160 : 0.10022177547216415
Loss at iteration 1170 : 0.06791189312934875
Loss at iteration 1180 : 0.08186434954404831
Loss at iteration 1190 : 0.10970952361822128
Loss at iteration 1200 : 0.060220491141080856
Loss at iteration 1210 : 0.07714039832353592
The SSIM Value is: 0.6963336348533631
The PSNR Value is: 20.735318883260092
the epoch is: 35
Loss at iteration 10 : 0.0867377519607544
Loss at iteration 20 : 0.06329967826604843
Loss at iteration 30 : 0.09739938378334045
Loss at iteration 40 : 0.08485367149114609
Loss at iteration 50 : 0.06425505876541138
Loss at iteration 60 : 0.06557774543762207
Loss at iteration 70 : 0.10525240749120712
Loss at iteration 80 : 0.09815645217895508
Loss at iteration 90 : 0.06983395665884018
Loss at iteration 100 : 0.05970493704080582
Loss at iteration 110 : 0.10186457633972168
Loss at iteration 120 : 0.13000982999801636
Loss at iteration 130 : 0.05412432178854942
Loss at iteration 140 : 0.0875992625951767
Loss at iteration 150 : 0.08264702558517456
Loss at iteration 160 : 0.10068929195404053
Loss at iteration 170 : 0.1109299287199974
Loss at iteration 180 : 0.07121964544057846
Loss at iteration 190 : 0.08260379731655121
Loss at iteration 200 : 0.14127616584300995
Loss at iteration 210 : 0.0631842315196991
Loss at iteration 220 : 0.06117909774184227
Loss at iteration 230 : 0.1024896651506424
Loss at iteration 240 : 0.06975658237934113
Loss at iteration 250 : 0.04677987098693848
Loss at iteration 260 : 0.09773578494787216
Loss at iteration 270 : 0.08125719428062439
Loss at iteration 280 : 0.07646133005619049
Loss at iteration 290 : 0.09186187386512756
Loss at iteration 300 : 0.09025369584560394
Loss at iteration 310 : 0.07997632026672363
Loss at iteration 320 : 0.12824301421642303
Loss at iteration 330 : 0.07213422656059265
Loss at iteration 340 : 0.07625213265419006
Loss at iteration 350 : 0.10401195287704468
Loss at iteration 360 : 0.10076241195201874
Loss at iteration 370 : 0.10504688322544098
Loss at iteration 380 : 0.08965460956096649
Loss at iteration 390 : 0.11482258886098862
Loss at iteration 400 : 0.05762157961726189
Loss at iteration 410 : 0.079659603536129
Loss at iteration 420 : 0.08289701491594315
Loss at iteration 430 : 0.06903009116649628
Loss at iteration 440 : 0.10886958241462708
Loss at iteration 450 : 0.08930015563964844
Loss at iteration 460 : 0.06830283254384995
Loss at iteration 470 : 0.11317696422338486
Loss at iteration 480 : 0.05440782755613327
Loss at iteration 490 : 0.07450514286756516
Loss at iteration 500 : 0.057244330644607544
Loss at iteration 510 : 0.12234148383140564
Loss at iteration 520 : 0.12192653864622116
Loss at iteration 530 : 0.06863363087177277
Loss at iteration 540 : 0.1091146469116211
Loss at iteration 550 : 0.07807290554046631
Loss at iteration 560 : 0.06958922743797302
Loss at iteration 570 : 0.09674568474292755
Loss at iteration 580 : 0.07927125692367554
Loss at iteration 590 : 0.06619127094745636
Loss at iteration 600 : 0.10403530299663544
Loss at iteration 610 : 0.09729161113500595
Loss at iteration 620 : 0.12129129469394684
Loss at iteration 630 : 0.07810061424970627
Loss at iteration 640 : 0.06365177035331726
Loss at iteration 650 : 0.06509504467248917
Loss at iteration 660 : 0.09299217164516449
Loss at iteration 670 : 0.09032939374446869
Loss at iteration 680 : 0.11421835422515869
Loss at iteration 690 : 0.11330616474151611
Loss at iteration 700 : 0.06545344740152359
Loss at iteration 710 : 0.06454281508922577
Loss at iteration 720 : 0.06983087956905365
Loss at iteration 730 : 0.10641031712293625
Loss at iteration 740 : 0.08855386078357697
Loss at iteration 750 : 0.07309743016958237
Loss at iteration 760 : 0.10161326825618744
Loss at iteration 770 : 0.09815804660320282
Loss at iteration 780 : 0.06358426809310913
Loss at iteration 790 : 0.05099054425954819
Loss at iteration 800 : 0.08080586045980453
Loss at iteration 810 : 0.09351667016744614
Loss at iteration 820 : 0.06738309562206268
Loss at iteration 830 : 0.06702514737844467
Loss at iteration 840 : 0.0687151700258255
Loss at iteration 850 : 0.08074663579463959
Loss at iteration 860 : 0.07161926478147507
Loss at iteration 870 : 0.09096655249595642
Loss at iteration 880 : 0.09551487863063812
Loss at iteration 890 : 0.11968719959259033
Loss at iteration 900 : 0.0738755315542221
Loss at iteration 910 : 0.05672367662191391
Loss at iteration 920 : 0.08466751873493195
Loss at iteration 930 : 0.14023660123348236
Loss at iteration 940 : 0.06986160576343536
Loss at iteration 950 : 0.08637735247612
Loss at iteration 960 : 0.13040250539779663
Loss at iteration 970 : 0.06416302919387817
Loss at iteration 980 : 0.10743077099323273
Loss at iteration 990 : 0.10493453592061996
Loss at iteration 1000 : 0.08823952078819275
Loss at iteration 1010 : 0.16434749960899353
Loss at iteration 1020 : 0.046819716691970825
Loss at iteration 1030 : 0.09569938480854034
Loss at iteration 1040 : 0.07848995178937912
Loss at iteration 1050 : 0.057106297463178635
Loss at iteration 1060 : 0.15006530284881592
Loss at iteration 1070 : 0.1309271901845932
Loss at iteration 1080 : 0.06511238217353821
Loss at iteration 1090 : 0.11003522574901581
Loss at iteration 1100 : 0.08535987138748169
Loss at iteration 1110 : 0.07645592838525772
Loss at iteration 1120 : 0.0741707980632782
Loss at iteration 1130 : 0.10350599139928818
Loss at iteration 1140 : 0.11527667939662933
Loss at iteration 1150 : 0.09168756008148193
Loss at iteration 1160 : 0.12472306191921234
Loss at iteration 1170 : 0.1420721709728241
Loss at iteration 1180 : 0.122392438352108
Loss at iteration 1190 : 0.07255448400974274
Loss at iteration 1200 : 0.10563719272613525
Loss at iteration 1210 : 0.15779510140419006
The SSIM Value is: 0.7024351278940837
The PSNR Value is: 20.89713083902995
the epoch is: 36
Loss at iteration 10 : 0.08940388262271881
Loss at iteration 20 : 0.07426120340824127
Loss at iteration 30 : 0.08784084022045135
Loss at iteration 40 : 0.0846637487411499
Loss at iteration 50 : 0.11039434373378754
Loss at iteration 60 : 0.08649839460849762
Loss at iteration 70 : 0.08045467734336853
Loss at iteration 80 : 0.06999491900205612
Loss at iteration 90 : 0.08735209703445435
Loss at iteration 100 : 0.06765016168355942
Loss at iteration 110 : 0.0903581827878952
Loss at iteration 120 : 0.09164512157440186
Loss at iteration 130 : 0.11236844956874847
Loss at iteration 140 : 0.04928857088088989
Loss at iteration 150 : 0.059506915509700775
Loss at iteration 160 : 0.08433251827955246
Loss at iteration 170 : 0.07748489081859589
Loss at iteration 180 : 0.07037936896085739
Loss at iteration 190 : 0.08130337297916412
Loss at iteration 200 : 0.15311554074287415
Loss at iteration 210 : 0.055417872965335846
Loss at iteration 220 : 0.09676187485456467
Loss at iteration 230 : 0.13208644092082977
Loss at iteration 240 : 0.08532237261533737
Loss at iteration 250 : 0.10850720852613449
Loss at iteration 260 : 0.09238497912883759
Loss at iteration 270 : 0.07528330385684967
Loss at iteration 280 : 0.1136428639292717
Loss at iteration 290 : 0.08887937664985657
Loss at iteration 300 : 0.08058472722768784
Loss at iteration 310 : 0.1136787161231041
Loss at iteration 320 : 0.05884157866239548
Loss at iteration 330 : 0.12839683890342712
Loss at iteration 340 : 0.0998602956533432
Loss at iteration 350 : 0.09446047246456146
Loss at iteration 360 : 0.099137082695961
Loss at iteration 370 : 0.08172210305929184
Loss at iteration 380 : 0.10224682092666626
Loss at iteration 390 : 0.06781569123268127
Loss at iteration 400 : 0.09374000877141953
Loss at iteration 410 : 0.11433308571577072
Loss at iteration 420 : 0.08553171157836914
Loss at iteration 430 : 0.06948281824588776
Loss at iteration 440 : 0.09444700181484222
Loss at iteration 450 : 0.13805219531059265
Loss at iteration 460 : 0.10210176557302475
Loss at iteration 470 : 0.0684501975774765
Loss at iteration 480 : 0.1035735160112381
Loss at iteration 490 : 0.08358685672283173
Loss at iteration 500 : 0.10597793757915497
Loss at iteration 510 : 0.06693397462368011
Loss at iteration 520 : 0.07491575181484222
Loss at iteration 530 : 0.07312653958797455
Loss at iteration 540 : 0.13283321261405945
Loss at iteration 550 : 0.07390464097261429
Loss at iteration 560 : 0.11582905054092407
Loss at iteration 570 : 0.1419038027524948
Loss at iteration 580 : 0.08236588537693024
Loss at iteration 590 : 0.10249143093824387
Loss at iteration 600 : 0.10391586273908615
Loss at iteration 610 : 0.08768655359745026
Loss at iteration 620 : 0.10245826095342636
Loss at iteration 630 : 0.06273951381444931
Loss at iteration 640 : 0.08594220131635666
Loss at iteration 650 : 0.07378876209259033
Loss at iteration 660 : 0.060237228870391846
Loss at iteration 670 : 0.09264765679836273
Loss at iteration 680 : 0.1350553035736084
Loss at iteration 690 : 0.04845251888036728
Loss at iteration 700 : 0.060736529529094696
Loss at iteration 710 : 0.12810997664928436
Loss at iteration 720 : 0.09871326386928558
Loss at iteration 730 : 0.060372885316610336
Loss at iteration 740 : 0.1019018217921257
Loss at iteration 750 : 0.08501791954040527
Loss at iteration 760 : 0.10875251144170761
Loss at iteration 770 : 0.0836011990904808
Loss at iteration 780 : 0.06384174525737762
Loss at iteration 790 : 0.10224257409572601
Loss at iteration 800 : 0.08332259953022003
Loss at iteration 810 : 0.08499734103679657
Loss at iteration 820 : 0.059089940041303635
Loss at iteration 830 : 0.07639669626951218
Loss at iteration 840 : 0.1479242742061615
Loss at iteration 850 : 0.06744451820850372
Loss at iteration 860 : 0.13034695386886597
Loss at iteration 870 : 0.08912388980388641
Loss at iteration 880 : 0.07791181653738022
Loss at iteration 890 : 0.10621362924575806
Loss at iteration 900 : 0.1431281566619873
Loss at iteration 910 : 0.0748695656657219
Loss at iteration 920 : 0.09281516075134277
Loss at iteration 930 : 0.06617421656847
Loss at iteration 940 : 0.08219227194786072
Loss at iteration 950 : 0.06789202988147736
Loss at iteration 960 : 0.08712013810873032
Loss at iteration 970 : 0.11293544620275497
Loss at iteration 980 : 0.12060820311307907
Loss at iteration 990 : 0.0922093391418457
Loss at iteration 1000 : 0.06864094734191895
Loss at iteration 1010 : 0.11979822814464569
Loss at iteration 1020 : 0.08716484159231186
Loss at iteration 1030 : 0.08593189716339111
Loss at iteration 1040 : 0.048301517963409424
Loss at iteration 1050 : 0.09698151051998138
Loss at iteration 1060 : 0.06512309610843658
Loss at iteration 1070 : 0.046170737594366074
Loss at iteration 1080 : 0.0835624560713768
Loss at iteration 1090 : 0.0903051495552063
Loss at iteration 1100 : 0.0862896665930748
Loss at iteration 1110 : 0.06550556421279907
Loss at iteration 1120 : 0.07571819424629211
Loss at iteration 1130 : 0.10032161325216293
Loss at iteration 1140 : 0.11772595345973969
Loss at iteration 1150 : 0.08164222538471222
Loss at iteration 1160 : 0.09838893264532089
Loss at iteration 1170 : 0.08434396982192993
Loss at iteration 1180 : 0.09005630016326904
Loss at iteration 1190 : 0.0855044350028038
Loss at iteration 1200 : 0.1032799705862999
Loss at iteration 1210 : 0.06994353979825974
The SSIM Value is: 0.6936675826708476
The PSNR Value is: 20.764703432718914
the epoch is: 37
Loss at iteration 10 : 0.11955222487449646
Loss at iteration 20 : 0.09995304048061371
Loss at iteration 30 : 0.12816612422466278
Loss at iteration 40 : 0.09979119151830673
Loss at iteration 50 : 0.09387331455945969
Loss at iteration 60 : 0.12233448028564453
Loss at iteration 70 : 0.06588270515203476
Loss at iteration 80 : 0.14858175814151764
Loss at iteration 90 : 0.08071555942296982
Loss at iteration 100 : 0.09072767198085785
Loss at iteration 110 : 0.04568663239479065
Loss at iteration 120 : 0.08438276499509811
Loss at iteration 130 : 0.12413853406906128
Loss at iteration 140 : 0.07048530876636505
Loss at iteration 150 : 0.0417427122592926
Loss at iteration 160 : 0.07702174037694931
Loss at iteration 170 : 0.11919483542442322
Loss at iteration 180 : 0.10891111940145493
Loss at iteration 190 : 0.0956183522939682
Loss at iteration 200 : 0.10430490970611572
Loss at iteration 210 : 0.09946846961975098
Loss at iteration 220 : 0.08576741069555283
Loss at iteration 230 : 0.09505943953990936
Loss at iteration 240 : 0.1005871519446373
Loss at iteration 250 : 0.09364871680736542
Loss at iteration 260 : 0.04612748324871063
Loss at iteration 270 : 0.15591204166412354
Loss at iteration 280 : 0.0849342942237854
Loss at iteration 290 : 0.0941372662782669
Loss at iteration 300 : 0.09890519082546234
Loss at iteration 310 : 0.10516530275344849
Loss at iteration 320 : 0.09159618616104126
Loss at iteration 330 : 0.14931921660900116
Loss at iteration 340 : 0.08147305995225906
Loss at iteration 350 : 0.12718161940574646
Loss at iteration 360 : 0.10052633285522461
Loss at iteration 370 : 0.06705744564533234
Loss at iteration 380 : 0.1879919469356537
Loss at iteration 390 : 0.14026477932929993
Loss at iteration 400 : 0.10078069567680359
Loss at iteration 410 : 0.10112039744853973
Loss at iteration 420 : 0.08508642762899399
Loss at iteration 430 : 0.07734962552785873
Loss at iteration 440 : 0.056010257452726364
Loss at iteration 450 : 0.056519702076911926
Loss at iteration 460 : 0.06978443264961243
Loss at iteration 470 : 0.07101108878850937
Loss at iteration 480 : 0.055019281804561615
Loss at iteration 490 : 0.05914802476763725
Loss at iteration 500 : 0.07016013562679291
Loss at iteration 510 : 0.06282582879066467
Loss at iteration 520 : 0.1098220944404602
Loss at iteration 530 : 0.18030187487602234
Loss at iteration 540 : 0.09528996050357819
Loss at iteration 550 : 0.12542006373405457
Loss at iteration 560 : 0.07868771255016327
Loss at iteration 570 : 0.06568124890327454
Loss at iteration 580 : 0.06847763061523438
Loss at iteration 590 : 0.08275985717773438
Loss at iteration 600 : 0.11383619159460068
Loss at iteration 610 : 0.07269077003002167
Loss at iteration 620 : 0.10394882410764694
Loss at iteration 630 : 0.08860886096954346
Loss at iteration 640 : 0.08675607293844223
Loss at iteration 650 : 0.08104270696640015
Loss at iteration 660 : 0.0512969009578228
Loss at iteration 670 : 0.10347484052181244
Loss at iteration 680 : 0.05159824341535568
Loss at iteration 690 : 0.11029746383428574
Loss at iteration 700 : 0.08614259958267212
Loss at iteration 710 : 0.12056480348110199
Loss at iteration 720 : 0.08134155720472336
Loss at iteration 730 : 0.09973190724849701
Loss at iteration 740 : 0.05702534317970276
Loss at iteration 750 : 0.10874184966087341
Loss at iteration 760 : 0.10918757319450378
Loss at iteration 770 : 0.13989019393920898
Loss at iteration 780 : 0.1011732667684555
Loss at iteration 790 : 0.12085859477519989
Loss at iteration 800 : 0.09786617010831833
Loss at iteration 810 : 0.0808824747800827
Loss at iteration 820 : 0.09789778292179108
Loss at iteration 830 : 0.07036763429641724
Loss at iteration 840 : 0.06273359060287476
Loss at iteration 850 : 0.07331228256225586
Loss at iteration 860 : 0.1016407236456871
Loss at iteration 870 : 0.13121581077575684
Loss at iteration 880 : 0.05834108591079712
Loss at iteration 890 : 0.09992283582687378
Loss at iteration 900 : 0.11139197647571564
Loss at iteration 910 : 0.0900409072637558
Loss at iteration 920 : 0.08435624092817307
Loss at iteration 930 : 0.06128542870283127
Loss at iteration 940 : 0.09988518804311752
Loss at iteration 950 : 0.08609732240438461
Loss at iteration 960 : 0.11058604717254639
Loss at iteration 970 : 0.08261851966381073
Loss at iteration 980 : 0.07588540762662888
Loss at iteration 990 : 0.11902491748332977
Loss at iteration 1000 : 0.10526470839977264
Loss at iteration 1010 : 0.06700962036848068
Loss at iteration 1020 : 0.09811793267726898
Loss at iteration 1030 : 0.05394759401679039
Loss at iteration 1040 : 0.10509943962097168
Loss at iteration 1050 : 0.09644248336553574
Loss at iteration 1060 : 0.06318590044975281
Loss at iteration 1070 : 0.06456948816776276
Loss at iteration 1080 : 0.08535440266132355
Loss at iteration 1090 : 0.06269398331642151
Loss at iteration 1100 : 0.09235681593418121
Loss at iteration 1110 : 0.060524728149175644
Loss at iteration 1120 : 0.06387978792190552
Loss at iteration 1130 : 0.0824240893125534
Loss at iteration 1140 : 0.13909725844860077
Loss at iteration 1150 : 0.15782372653484344
Loss at iteration 1160 : 0.09654240310192108
Loss at iteration 1170 : 0.1007382720708847
Loss at iteration 1180 : 0.10689058154821396
Loss at iteration 1190 : 0.07029696553945541
Loss at iteration 1200 : 0.08033373951911926
Loss at iteration 1210 : 0.11386853456497192
The SSIM Value is: 0.6920014023780823
The PSNR Value is: 20.39281260172526
the epoch is: 38
Loss at iteration 10 : 0.06597781181335449
Loss at iteration 20 : 0.10271243751049042
Loss at iteration 30 : 0.05409476161003113
Loss at iteration 40 : 0.1298762559890747
Loss at iteration 50 : 0.07030978053808212
Loss at iteration 60 : 0.05837596580386162
Loss at iteration 70 : 0.14474795758724213
Loss at iteration 80 : 0.0635608658194542
Loss at iteration 90 : 0.1564824879169464
Loss at iteration 100 : 0.07243728637695312
Loss at iteration 110 : 0.13563674688339233
Loss at iteration 120 : 0.09039968252182007
Loss at iteration 130 : 0.11267359554767609
Loss at iteration 140 : 0.1603071242570877
Loss at iteration 150 : 0.05662877857685089
Loss at iteration 160 : 0.05094987154006958
Loss at iteration 170 : 0.08369117975234985
Loss at iteration 180 : 0.08653256297111511
Loss at iteration 190 : 0.06652390211820602
Loss at iteration 200 : 0.10168209671974182
Loss at iteration 210 : 0.07532735168933868
Loss at iteration 220 : 0.09885694831609726
Loss at iteration 230 : 0.06573251634836197
Loss at iteration 240 : 0.14082345366477966
Loss at iteration 250 : 0.11523890495300293
Loss at iteration 260 : 0.1068890318274498
Loss at iteration 270 : 0.06627144664525986
Loss at iteration 280 : 0.12907445430755615
Loss at iteration 290 : 0.14134174585342407
Loss at iteration 300 : 0.08309345692396164
Loss at iteration 310 : 0.07056964188814163
Loss at iteration 320 : 0.1332346796989441
Loss at iteration 330 : 0.06565818190574646
Loss at iteration 340 : 0.07111256569623947
Loss at iteration 350 : 0.12888821959495544
Loss at iteration 360 : 0.07657507061958313
Loss at iteration 370 : 0.054086219519376755
Loss at iteration 380 : 0.07498296350240707
Loss at iteration 390 : 0.10109099000692368
Loss at iteration 400 : 0.06357070803642273
Loss at iteration 410 : 0.10443183034658432
Loss at iteration 420 : 0.07735542207956314
Loss at iteration 430 : 0.07228125631809235
Loss at iteration 440 : 0.09814624488353729
Loss at iteration 450 : 0.0943957194685936
Loss at iteration 460 : 0.10978418588638306
Loss at iteration 470 : 0.09623847901821136
Loss at iteration 480 : 0.08923550695180893
Loss at iteration 490 : 0.08351346105337143
Loss at iteration 500 : 0.059594929218292236
Loss at iteration 510 : 0.09587859362363815
Loss at iteration 520 : 0.04890689253807068
Loss at iteration 530 : 0.09219178557395935
Loss at iteration 540 : 0.08512558788061142
Loss at iteration 550 : 0.14420679211616516
Loss at iteration 560 : 0.06748257577419281
Loss at iteration 570 : 0.05460847541689873
Loss at iteration 580 : 0.06575305014848709
Loss at iteration 590 : 0.09527063369750977
Loss at iteration 600 : 0.05391252040863037
Loss at iteration 610 : 0.11528855562210083
Loss at iteration 620 : 0.08131955564022064
Loss at iteration 630 : 0.0690874457359314
Loss at iteration 640 : 0.08697905391454697
Loss at iteration 650 : 0.11750360578298569
Loss at iteration 660 : 0.05587959289550781
Loss at iteration 670 : 0.05996627360582352
Loss at iteration 680 : 0.10061170905828476
Loss at iteration 690 : 0.04761488363146782
Loss at iteration 700 : 0.060111355036497116
Loss at iteration 710 : 0.07161157578229904
Loss at iteration 720 : 0.08256875723600388
Loss at iteration 730 : 0.09186528623104095
Loss at iteration 740 : 0.16963869333267212
Loss at iteration 750 : 0.10365036129951477
Loss at iteration 760 : 0.09926220029592514
Loss at iteration 770 : 0.08018182963132858
Loss at iteration 780 : 0.08107920736074448
Loss at iteration 790 : 0.07022464275360107
Loss at iteration 800 : 0.06728705018758774
Loss at iteration 810 : 0.0746336579322815
Loss at iteration 820 : 0.07134795188903809
Loss at iteration 830 : 0.10891565680503845
Loss at iteration 840 : 0.059497930109500885
Loss at iteration 850 : 0.08782770484685898
Loss at iteration 860 : 0.08689606189727783
Loss at iteration 870 : 0.07675366848707199
Loss at iteration 880 : 0.07918697595596313
Loss at iteration 890 : 0.1276722252368927
Loss at iteration 900 : 0.07274997979402542
Loss at iteration 910 : 0.08335153758525848
Loss at iteration 920 : 0.0776892751455307
Loss at iteration 930 : 0.09925978630781174
Loss at iteration 940 : 0.07903553545475006
Loss at iteration 950 : 0.10684967041015625
Loss at iteration 960 : 0.09955354779958725
Loss at iteration 970 : 0.10715454816818237
Loss at iteration 980 : 0.0987885594367981
Loss at iteration 990 : 0.04546881839632988
Loss at iteration 1000 : 0.12762930989265442
Loss at iteration 1010 : 0.10687558352947235
Loss at iteration 1020 : 0.07313346117734909
Loss at iteration 1030 : 0.0839897096157074
Loss at iteration 1040 : 0.12066887319087982
Loss at iteration 1050 : 0.06361415982246399
Loss at iteration 1060 : 0.06192747876048088
Loss at iteration 1070 : 0.08880441635847092
Loss at iteration 1080 : 0.14183056354522705
Loss at iteration 1090 : 0.10843738168478012
Loss at iteration 1100 : 0.07686521857976913
Loss at iteration 1110 : 0.08765149116516113
Loss at iteration 1120 : 0.16254618763923645
Loss at iteration 1130 : 0.09885925054550171
Loss at iteration 1140 : 0.12684546411037445
Loss at iteration 1150 : 0.1482815146446228
Loss at iteration 1160 : 0.13564494252204895
Loss at iteration 1170 : 0.066103994846344
Loss at iteration 1180 : 0.07999452948570251
Loss at iteration 1190 : 0.07850709557533264
Loss at iteration 1200 : 0.07259415090084076
Loss at iteration 1210 : 0.1051095724105835
The SSIM Value is: 0.7019759356975556
The PSNR Value is: 21.368839518229166
the highest SSIM value is: 21.368839518229166
the epoch is: 39
Loss at iteration 10 : 0.05264260992407799
Loss at iteration 20 : 0.05004426836967468
Loss at iteration 30 : 0.042747821658849716
Loss at iteration 40 : 0.09840834140777588
Loss at iteration 50 : 0.11870068311691284
Loss at iteration 60 : 0.10095621645450592
Loss at iteration 70 : 0.11900769174098969
Loss at iteration 80 : 0.10511766374111176
Loss at iteration 90 : 0.08631622791290283
Loss at iteration 100 : 0.06650875508785248
Loss at iteration 110 : 0.12106873840093613
Loss at iteration 120 : 0.07714837044477463
Loss at iteration 130 : 0.07344531267881393
Loss at iteration 140 : 0.07873053848743439
Loss at iteration 150 : 0.09719257056713104
Loss at iteration 160 : 0.08222135901451111
Loss at iteration 170 : 0.05847117304801941
Loss at iteration 180 : 0.09873214364051819
Loss at iteration 190 : 0.06251277029514313
Loss at iteration 200 : 0.0951509177684784
Loss at iteration 210 : 0.1208374947309494
Loss at iteration 220 : 0.09908568859100342
Loss at iteration 230 : 0.06895165145397186
Loss at iteration 240 : 0.0792975127696991
Loss at iteration 250 : 0.10875435918569565
Loss at iteration 260 : 0.052077554166316986
Loss at iteration 270 : 0.0710032507777214
Loss at iteration 280 : 0.06397063285112381
Loss at iteration 290 : 0.09315218776464462
Loss at iteration 300 : 0.095955990254879
Loss at iteration 310 : 0.06244489178061485
Loss at iteration 320 : 0.0891052708029747
Loss at iteration 330 : 0.11156891286373138
Loss at iteration 340 : 0.09935196489095688
Loss at iteration 350 : 0.08908471465110779
Loss at iteration 360 : 0.09155870974063873
Loss at iteration 370 : 0.07449106127023697
Loss at iteration 380 : 0.094654880464077
Loss at iteration 390 : 0.12812112271785736
Loss at iteration 400 : 0.12579356133937836
Loss at iteration 410 : 0.08513709157705307
Loss at iteration 420 : 0.1116880476474762
Loss at iteration 430 : 0.08935018628835678
Loss at iteration 440 : 0.11934567987918854
Loss at iteration 450 : 0.08518760651350021
Loss at iteration 460 : 0.09078719466924667
Loss at iteration 470 : 0.07119445502758026
Loss at iteration 480 : 0.09701219201087952
Loss at iteration 490 : 0.06517557799816132
Loss at iteration 500 : 0.09158943593502045
Loss at iteration 510 : 0.07240124046802521
Loss at iteration 520 : 0.08586889505386353
Loss at iteration 530 : 0.06278053671121597
Loss at iteration 540 : 0.07743175327777863
Loss at iteration 550 : 0.07731705904006958
Loss at iteration 560 : 0.10837213695049286
Loss at iteration 570 : 0.08367155492305756
Loss at iteration 580 : 0.08156271278858185
Loss at iteration 590 : 0.08499473333358765
Loss at iteration 600 : 0.07002972066402435
Loss at iteration 610 : 0.09734886139631271
Loss at iteration 620 : 0.09077666699886322
Loss at iteration 630 : 0.06798554956912994
Loss at iteration 640 : 0.08934943377971649
Loss at iteration 650 : 0.05170252174139023
Loss at iteration 660 : 0.07067577540874481
Loss at iteration 670 : 0.05911664292216301
Loss at iteration 680 : 0.07140882313251495
Loss at iteration 690 : 0.06821207702159882
Loss at iteration 700 : 0.049329884350299835
Loss at iteration 710 : 0.13676154613494873
Loss at iteration 720 : 0.08616743981838226
Loss at iteration 730 : 0.08438383042812347
Loss at iteration 740 : 0.06216811388731003
Loss at iteration 750 : 0.13888171315193176
Loss at iteration 760 : 0.10956963151693344
Loss at iteration 770 : 0.08160649240016937
Loss at iteration 780 : 0.0551365427672863
Loss at iteration 790 : 0.060231443494558334
Loss at iteration 800 : 0.064754918217659
Loss at iteration 810 : 0.09726129472255707
Loss at iteration 820 : 0.07312363386154175
Loss at iteration 830 : 0.07219689339399338
Loss at iteration 840 : 0.07014946639537811
Loss at iteration 850 : 0.116095170378685
Loss at iteration 860 : 0.09851226210594177
Loss at iteration 870 : 0.08720346540212631
Loss at iteration 880 : 0.06711436808109283
Loss at iteration 890 : 0.08202096819877625
Loss at iteration 900 : 0.06397625803947449
Loss at iteration 910 : 0.05694844573736191
Loss at iteration 920 : 0.06723614782094955
Loss at iteration 930 : 0.0647987425327301
Loss at iteration 940 : 0.12204789370298386
Loss at iteration 950 : 0.06522190570831299
Loss at iteration 960 : 0.07596122473478317
Loss at iteration 970 : 0.057947561144828796
Loss at iteration 980 : 0.09737227857112885
Loss at iteration 990 : 0.15385061502456665
Loss at iteration 1000 : 0.04550672695040703
Loss at iteration 1010 : 0.11926855891942978
Loss at iteration 1020 : 0.10817037522792816
Loss at iteration 1030 : 0.08131936192512512
Loss at iteration 1040 : 0.0619698166847229
Loss at iteration 1050 : 0.08943252265453339
Loss at iteration 1060 : 0.13346515595912933
Loss at iteration 1070 : 0.11489035934209824
Loss at iteration 1080 : 0.07197172939777374
Loss at iteration 1090 : 0.1515316665172577
Loss at iteration 1100 : 0.05310159921646118
Loss at iteration 1110 : 0.0981215089559555
Loss at iteration 1120 : 0.0939502939581871
Loss at iteration 1130 : 0.10091471672058105
Loss at iteration 1140 : 0.10239066183567047
Loss at iteration 1150 : 0.07757945358753204
Loss at iteration 1160 : 0.1599281132221222
Loss at iteration 1170 : 0.11926428973674774
Loss at iteration 1180 : 0.09048503637313843
Loss at iteration 1190 : 0.05714654549956322
Loss at iteration 1200 : 0.07479076087474823
Loss at iteration 1210 : 0.09167265892028809
The SSIM Value is: 0.6955812791983287
The PSNR Value is: 20.270036951700845
the epoch is: 40
Loss at iteration 10 : 0.11629877984523773
Loss at iteration 20 : 0.0979558452963829
Loss at iteration 30 : 0.11838577687740326
Loss at iteration 40 : 0.06077055260539055
Loss at iteration 50 : 0.0750025287270546
Loss at iteration 60 : 0.07434205710887909
Loss at iteration 70 : 0.091377854347229
Loss at iteration 80 : 0.09890598058700562
Loss at iteration 90 : 0.06230149418115616
Loss at iteration 100 : 0.1073702797293663
Loss at iteration 110 : 0.06214001774787903
Loss at iteration 120 : 0.09532158076763153
Loss at iteration 130 : 0.10527056455612183
Loss at iteration 140 : 0.11838855594396591
Loss at iteration 150 : 0.09855832904577255
Loss at iteration 160 : 0.07348624616861343
Loss at iteration 170 : 0.06626720726490021
Loss at iteration 180 : 0.08632022142410278
Loss at iteration 190 : 0.07714444398880005
Loss at iteration 200 : 0.10064409673213959
Loss at iteration 210 : 0.05900805816054344
Loss at iteration 220 : 0.14337937533855438
Loss at iteration 230 : 0.07138367742300034
Loss at iteration 240 : 0.0940382108092308
Loss at iteration 250 : 0.10776285827159882
Loss at iteration 260 : 0.11898215860128403
Loss at iteration 270 : 0.060137711465358734
Loss at iteration 280 : 0.11749657243490219
Loss at iteration 290 : 0.11206112802028656
Loss at iteration 300 : 0.0985901802778244
Loss at iteration 310 : 0.08468767255544662
Loss at iteration 320 : 0.07803530991077423
Loss at iteration 330 : 0.11936573684215546
Loss at iteration 340 : 0.10653700679540634
Loss at iteration 350 : 0.09907174110412598
Loss at iteration 360 : 0.13574248552322388
Loss at iteration 370 : 0.09683360904455185
Loss at iteration 380 : 0.08007235825061798
Loss at iteration 390 : 0.10481732338666916
Loss at iteration 400 : 0.07695674151182175
Loss at iteration 410 : 0.061244409531354904
Loss at iteration 420 : 0.10460474342107773
Loss at iteration 430 : 0.09046722948551178
Loss at iteration 440 : 0.059813242405653
Loss at iteration 450 : 0.08365768194198608
Loss at iteration 460 : 0.10404239594936371
Loss at iteration 470 : 0.08139367401599884
Loss at iteration 480 : 0.10033665597438812
Loss at iteration 490 : 0.11928173154592514
Loss at iteration 500 : 0.058896809816360474
Loss at iteration 510 : 0.07068851590156555
Loss at iteration 520 : 0.08815102279186249
Loss at iteration 530 : 0.0551244392991066
Loss at iteration 540 : 0.09808081388473511
Loss at iteration 550 : 0.11281315982341766
Loss at iteration 560 : 0.06234027445316315
Loss at iteration 570 : 0.08547452092170715
Loss at iteration 580 : 0.05386349558830261
Loss at iteration 590 : 0.09270068258047104
Loss at iteration 600 : 0.08567037433385849
Loss at iteration 610 : 0.08471092581748962
Loss at iteration 620 : 0.0830807089805603
Loss at iteration 630 : 0.05740594118833542
Loss at iteration 640 : 0.10884543508291245
Loss at iteration 650 : 0.0848773941397667
Loss at iteration 660 : 0.10413499176502228
Loss at iteration 670 : 0.08093094825744629
Loss at iteration 680 : 0.0863790512084961
Loss at iteration 690 : 0.12563756108283997
Loss at iteration 700 : 0.12589393556118011
Loss at iteration 710 : 0.0710020661354065
Loss at iteration 720 : 0.0882878303527832
Loss at iteration 730 : 0.15195533633232117
Loss at iteration 740 : 0.13041336834430695
Loss at iteration 750 : 0.06294935941696167
Loss at iteration 760 : 0.07445644587278366
Loss at iteration 770 : 0.06836744397878647
Loss at iteration 780 : 0.08551913499832153
Loss at iteration 790 : 0.10864925384521484
Loss at iteration 800 : 0.08394785970449448
Loss at iteration 810 : 0.042618073523044586
Loss at iteration 820 : 0.06309245526790619
Loss at iteration 830 : 0.09311056137084961
Loss at iteration 840 : 0.09254848957061768
Loss at iteration 850 : 0.0861617848277092
Loss at iteration 860 : 0.12738385796546936
Loss at iteration 870 : 0.06502373516559601
Loss at iteration 880 : 0.08458404242992401
Loss at iteration 890 : 0.05894403159618378
Loss at iteration 900 : 0.09634091705083847
Loss at iteration 910 : 0.04472321271896362
Loss at iteration 920 : 0.08820577710866928
Loss at iteration 930 : 0.09467076510190964
Loss at iteration 940 : 0.07759484648704529
Loss at iteration 950 : 0.06041836366057396
Loss at iteration 960 : 0.08364363759756088
Loss at iteration 970 : 0.1391218900680542
Loss at iteration 980 : 0.05926620587706566
Loss at iteration 990 : 0.07338075339794159
Loss at iteration 1000 : 0.11990690231323242
Loss at iteration 1010 : 0.09506254643201828
Loss at iteration 1020 : 0.09016144275665283
Loss at iteration 1030 : 0.1130421981215477
Loss at iteration 1040 : 0.060358576476573944
Loss at iteration 1050 : 0.0812639445066452
Loss at iteration 1060 : 0.05557842180132866
Loss at iteration 1070 : 0.06519081443548203
Loss at iteration 1080 : 0.10819157212972641
Loss at iteration 1090 : 0.0986681580543518
Loss at iteration 1100 : 0.06978325545787811
Loss at iteration 1110 : 0.08018353581428528
Loss at iteration 1120 : 0.11110812425613403
Loss at iteration 1130 : 0.08403132855892181
Loss at iteration 1140 : 0.037353914231061935
Loss at iteration 1150 : 0.07833205908536911
Loss at iteration 1160 : 0.06396578997373581
Loss at iteration 1170 : 0.0854918509721756
Loss at iteration 1180 : 0.11002349853515625
Loss at iteration 1190 : 0.06928151100873947
Loss at iteration 1200 : 0.09520494937896729
Loss at iteration 1210 : 0.12457003444433212
The SSIM Value is: 0.6961744129657745
The PSNR Value is: 20.92406717936198
the epoch is: 41
Loss at iteration 10 : 0.12999111413955688
Loss at iteration 20 : 0.11031512916088104
Loss at iteration 30 : 0.10904773324728012
Loss at iteration 40 : 0.0892680287361145
Loss at iteration 50 : 0.06994311511516571
Loss at iteration 60 : 0.07479493319988251
Loss at iteration 70 : 0.06270687282085419
Loss at iteration 80 : 0.05398046225309372
Loss at iteration 90 : 0.08115282654762268
Loss at iteration 100 : 0.1371515989303589
Loss at iteration 110 : 0.06895899027585983
Loss at iteration 120 : 0.0957605242729187
Loss at iteration 130 : 0.09483145922422409
Loss at iteration 140 : 0.07671894878149033
Loss at iteration 150 : 0.07942530512809753
Loss at iteration 160 : 0.08925881236791611
Loss at iteration 170 : 0.07803212851285934
Loss at iteration 180 : 0.1074313223361969
Loss at iteration 190 : 0.07779814302921295
Loss at iteration 200 : 0.08461736142635345
Loss at iteration 210 : 0.12013129889965057
Loss at iteration 220 : 0.07292704284191132
Loss at iteration 230 : 0.07791914790868759
Loss at iteration 240 : 0.1081581637263298
Loss at iteration 250 : 0.10497981309890747
Loss at iteration 260 : 0.07708637416362762
Loss at iteration 270 : 0.09003329277038574
Loss at iteration 280 : 0.0680091455578804
Loss at iteration 290 : 0.06744178384542465
Loss at iteration 300 : 0.09016191959381104
Loss at iteration 310 : 0.12951979041099548
Loss at iteration 320 : 0.10661562532186508
Loss at iteration 330 : 0.09234873950481415
Loss at iteration 340 : 0.1202172189950943
Loss at iteration 350 : 0.06687451153993607
Loss at iteration 360 : 0.08558756113052368
Loss at iteration 370 : 0.1708562821149826
Loss at iteration 380 : 0.12257470935583115
Loss at iteration 390 : 0.04459664225578308
Loss at iteration 400 : 0.05674011632800102
Loss at iteration 410 : 0.09118294715881348
Loss at iteration 420 : 0.12345105409622192
Loss at iteration 430 : 0.06001539155840874
Loss at iteration 440 : 0.049195777624845505
Loss at iteration 450 : 0.1243124008178711
Loss at iteration 460 : 0.08543367683887482
Loss at iteration 470 : 0.08577647060155869
Loss at iteration 480 : 0.08612208813428879
Loss at iteration 490 : 0.12072344869375229
Loss at iteration 500 : 0.07252651453018188
Loss at iteration 510 : 0.07658891379833221
Loss at iteration 520 : 0.10779889672994614
Loss at iteration 530 : 0.09250569343566895
Loss at iteration 540 : 0.04944177716970444
Loss at iteration 550 : 0.06088300049304962
Loss at iteration 560 : 0.09370734542608261
Loss at iteration 570 : 0.11011006683111191
Loss at iteration 580 : 0.10010837018489838
Loss at iteration 590 : 0.09503841400146484
Loss at iteration 600 : 0.07326050847768784
Loss at iteration 610 : 0.10205380618572235
Loss at iteration 620 : 0.061578355729579926
Loss at iteration 630 : 0.0706879049539566
Loss at iteration 640 : 0.06385496258735657
Loss at iteration 650 : 0.08022764325141907
Loss at iteration 660 : 0.1011345311999321
Loss at iteration 670 : 0.08537209033966064
Loss at iteration 680 : 0.09503649175167084
Loss at iteration 690 : 0.07390483468770981
Loss at iteration 700 : 0.07302922010421753
Loss at iteration 710 : 0.13689687848091125
Loss at iteration 720 : 0.1127355620265007
Loss at iteration 730 : 0.09592785686254501
Loss at iteration 740 : 0.1017022356390953
Loss at iteration 750 : 0.10117386281490326
Loss at iteration 760 : 0.04894034191966057
Loss at iteration 770 : 0.0938873142004013
Loss at iteration 780 : 0.08770089596509933
Loss at iteration 790 : 0.06700053811073303
Loss at iteration 800 : 0.08927643299102783
Loss at iteration 810 : 0.06250231713056564
Loss at iteration 820 : 0.07070344686508179
Loss at iteration 830 : 0.09880077838897705
Loss at iteration 840 : 0.11333956569433212
Loss at iteration 850 : 0.1208004355430603
Loss at iteration 860 : 0.06246979162096977
Loss at iteration 870 : 0.09457743167877197
Loss at iteration 880 : 0.069464772939682
Loss at iteration 890 : 0.0912318080663681
Loss at iteration 900 : 0.05590489134192467
Loss at iteration 910 : 0.08386843651533127
Loss at iteration 920 : 0.08159555494785309
Loss at iteration 930 : 0.05919908732175827
Loss at iteration 940 : 0.115517757833004
Loss at iteration 950 : 0.10189974308013916
Loss at iteration 960 : 0.07983112335205078
Loss at iteration 970 : 0.08558081090450287
Loss at iteration 980 : 0.05769045650959015
Loss at iteration 990 : 0.10636027902364731
Loss at iteration 1000 : 0.08112674951553345
Loss at iteration 1010 : 0.0945764109492302
Loss at iteration 1020 : 0.09550806134939194
Loss at iteration 1030 : 0.10202314704656601
Loss at iteration 1040 : 0.16710802912712097
Loss at iteration 1050 : 0.12972590327262878
Loss at iteration 1060 : 0.10564853996038437
Loss at iteration 1070 : 0.14445266127586365
Loss at iteration 1080 : 0.0725405365228653
Loss at iteration 1090 : 0.10848315805196762
Loss at iteration 1100 : 0.08172592520713806
Loss at iteration 1110 : 0.08504629135131836
Loss at iteration 1120 : 0.06702524423599243
Loss at iteration 1130 : 0.09491725265979767
Loss at iteration 1140 : 0.1342453807592392
Loss at iteration 1150 : 0.13455970585346222
Loss at iteration 1160 : 0.12622223794460297
Loss at iteration 1170 : 0.0982845276594162
Loss at iteration 1180 : 0.07167159020900726
Loss at iteration 1190 : 0.06974377483129501
Loss at iteration 1200 : 0.07478699088096619
Loss at iteration 1210 : 0.13106440007686615
The SSIM Value is: 0.6981807907422384
The PSNR Value is: 20.64896125793457
the epoch is: 42
Loss at iteration 10 : 0.09785573184490204
Loss at iteration 20 : 0.09667877852916718
Loss at iteration 30 : 0.07086515426635742
Loss at iteration 40 : 0.08148298412561417
Loss at iteration 50 : 0.08354004472494125
Loss at iteration 60 : 0.10011737048625946
Loss at iteration 70 : 0.08373772352933884
Loss at iteration 80 : 0.09403957426548004
Loss at iteration 90 : 0.09326057136058807
Loss at iteration 100 : 0.0776674896478653
Loss at iteration 110 : 0.10474371910095215
Loss at iteration 120 : 0.10377468168735504
Loss at iteration 130 : 0.09432270377874374
Loss at iteration 140 : 0.0736360177397728
Loss at iteration 150 : 0.11345168948173523
Loss at iteration 160 : 0.100432388484478
Loss at iteration 170 : 0.10873351991176605
Loss at iteration 180 : 0.14941339194774628
Loss at iteration 190 : 0.10469849407672882
Loss at iteration 200 : 0.054998405277729034
Loss at iteration 210 : 0.0702681839466095
Loss at iteration 220 : 0.06820745766162872
Loss at iteration 230 : 0.05720243602991104
Loss at iteration 240 : 0.11475559324026108
Loss at iteration 250 : 0.11577119678258896
Loss at iteration 260 : 0.07308484613895416
Loss at iteration 270 : 0.08954848349094391
Loss at iteration 280 : 0.057763345539569855
Loss at iteration 290 : 0.09193059802055359
Loss at iteration 300 : 0.06625872850418091
Loss at iteration 310 : 0.08949005603790283
Loss at iteration 320 : 0.045772336423397064
Loss at iteration 330 : 0.11135312169790268
Loss at iteration 340 : 0.08258896321058273
Loss at iteration 350 : 0.08061480522155762
Loss at iteration 360 : 0.10943038761615753
Loss at iteration 370 : 0.09030991047620773
Loss at iteration 380 : 0.09484729170799255
Loss at iteration 390 : 0.07655487954616547
Loss at iteration 400 : 0.07254187762737274
Loss at iteration 410 : 0.05643613263964653
Loss at iteration 420 : 0.08912964910268784
Loss at iteration 430 : 0.06823898106813431
Loss at iteration 440 : 0.04954955726861954
Loss at iteration 450 : 0.10629574954509735
Loss at iteration 460 : 0.09308505058288574
Loss at iteration 470 : 0.0983085185289383
Loss at iteration 480 : 0.0687122792005539
Loss at iteration 490 : 0.10419969260692596
Loss at iteration 500 : 0.1174246072769165
Loss at iteration 510 : 0.10469618439674377
Loss at iteration 520 : 0.09085243940353394
Loss at iteration 530 : 0.12004236876964569
Loss at iteration 540 : 0.058129023760557175
Loss at iteration 550 : 0.06677389144897461
Loss at iteration 560 : 0.10714306682348251
Loss at iteration 570 : 0.08395751565694809
Loss at iteration 580 : 0.08212640881538391
Loss at iteration 590 : 0.09445752948522568
Loss at iteration 600 : 0.06712266057729721
Loss at iteration 610 : 0.07325100898742676
Loss at iteration 620 : 0.10603436082601547
Loss at iteration 630 : 0.12417586892843246
Loss at iteration 640 : 0.09997674077749252
Loss at iteration 650 : 0.0972251296043396
Loss at iteration 660 : 0.04532074183225632
Loss at iteration 670 : 0.07289430499076843
Loss at iteration 680 : 0.08880090713500977
Loss at iteration 690 : 0.11105098575353622
Loss at iteration 700 : 0.05684119090437889
Loss at iteration 710 : 0.0949111208319664
Loss at iteration 720 : 0.07900255918502808
Loss at iteration 730 : 0.11934474110603333
Loss at iteration 740 : 0.0963367372751236
Loss at iteration 750 : 0.07702751457691193
Loss at iteration 760 : 0.08972533792257309
Loss at iteration 770 : 0.11187230795621872
Loss at iteration 780 : 0.10344576835632324
Loss at iteration 790 : 0.10897544026374817
Loss at iteration 800 : 0.08144678175449371
Loss at iteration 810 : 0.0664384514093399
Loss at iteration 820 : 0.07643380016088486
Loss at iteration 830 : 0.051268320530653
Loss at iteration 840 : 0.06312819570302963
Loss at iteration 850 : 0.13095784187316895
Loss at iteration 860 : 0.08670290559530258
Loss at iteration 870 : 0.07108888030052185
Loss at iteration 880 : 0.07767220586538315
Loss at iteration 890 : 0.0867275819182396
Loss at iteration 900 : 0.08759687840938568
Loss at iteration 910 : 0.10425293445587158
Loss at iteration 920 : 0.10236571729183197
Loss at iteration 930 : 0.10998891294002533
Loss at iteration 940 : 0.06834682077169418
Loss at iteration 950 : 0.0865807831287384
Loss at iteration 960 : 0.09091272950172424
Loss at iteration 970 : 0.06400562822818756
Loss at iteration 980 : 0.16273599863052368
Loss at iteration 990 : 0.07814100384712219
Loss at iteration 1000 : 0.12446564435958862
Loss at iteration 1010 : 0.06441465020179749
Loss at iteration 1020 : 0.060541074723005295
Loss at iteration 1030 : 0.12889187037944794
Loss at iteration 1040 : 0.0632098913192749
Loss at iteration 1050 : 0.159440279006958
Loss at iteration 1060 : 0.09324942529201508
Loss at iteration 1070 : 0.0871170163154602
Loss at iteration 1080 : 0.07143768668174744
Loss at iteration 1090 : 0.1403191089630127
Loss at iteration 1100 : 0.08818118274211884
Loss at iteration 1110 : 0.08213329315185547
Loss at iteration 1120 : 0.06625427305698395
Loss at iteration 1130 : 0.11212754249572754
Loss at iteration 1140 : 0.11460310220718384
Loss at iteration 1150 : 0.08476676791906357
Loss at iteration 1160 : 0.06497269868850708
Loss at iteration 1170 : 0.08814167976379395
Loss at iteration 1180 : 0.1256592571735382
Loss at iteration 1190 : 0.06282857805490494
Loss at iteration 1200 : 0.11339858174324036
Loss at iteration 1210 : 0.09623776376247406
The SSIM Value is: 0.6994202772776286
The PSNR Value is: 21.012199783325194
the epoch is: 43
Loss at iteration 10 : 0.1364365816116333
Loss at iteration 20 : 0.0709461122751236
Loss at iteration 30 : 0.048100512474775314
Loss at iteration 40 : 0.07591982930898666
Loss at iteration 50 : 0.08115839213132858
Loss at iteration 60 : 0.06258663535118103
Loss at iteration 70 : 0.0892799124121666
Loss at iteration 80 : 0.15202894806861877
Loss at iteration 90 : 0.09103574603796005
Loss at iteration 100 : 0.053385332226753235
Loss at iteration 110 : 0.09365932643413544
Loss at iteration 120 : 0.06845032423734665
Loss at iteration 130 : 0.11061792075634003
Loss at iteration 140 : 0.11646392196416855
Loss at iteration 150 : 0.10669063031673431
Loss at iteration 160 : 0.10019584000110626
Loss at iteration 170 : 0.06788919866085052
Loss at iteration 180 : 0.0754232108592987
Loss at iteration 190 : 0.10028784722089767
Loss at iteration 200 : 0.09386410564184189
Loss at iteration 210 : 0.06124202907085419
Loss at iteration 220 : 0.09019868075847626
Loss at iteration 230 : 0.1013563871383667
Loss at iteration 240 : 0.1018046960234642
Loss at iteration 250 : 0.05983762443065643
Loss at iteration 260 : 0.10061203688383102
Loss at iteration 270 : 0.10414481163024902
Loss at iteration 280 : 0.10856738686561584
Loss at iteration 290 : 0.07743321359157562
Loss at iteration 300 : 0.12481727451086044
Loss at iteration 310 : 0.10307910293340683
Loss at iteration 320 : 0.11195886135101318
Loss at iteration 330 : 0.1012059897184372
Loss at iteration 340 : 0.07406505942344666
Loss at iteration 350 : 0.055592309683561325
Loss at iteration 360 : 0.09759677946567535
Loss at iteration 370 : 0.08771885931491852
Loss at iteration 380 : 0.12256942689418793
Loss at iteration 390 : 0.099443219602108
Loss at iteration 400 : 0.08214536309242249
Loss at iteration 410 : 0.0721886083483696
Loss at iteration 420 : 0.0755777582526207
Loss at iteration 430 : 0.10917268693447113
Loss at iteration 440 : 0.11779525130987167
Loss at iteration 450 : 0.09717150032520294
Loss at iteration 460 : 0.10798276960849762
Loss at iteration 470 : 0.11047685146331787
Loss at iteration 480 : 0.06360375881195068
Loss at iteration 490 : 0.0728505328297615
Loss at iteration 500 : 0.10829125344753265
Loss at iteration 510 : 0.08765226602554321
Loss at iteration 520 : 0.06268078088760376
Loss at iteration 530 : 0.09004440158605576
Loss at iteration 540 : 0.10415838658809662
Loss at iteration 550 : 0.06477054953575134
Loss at iteration 560 : 0.08638744801282883
Loss at iteration 570 : 0.10567784309387207
Loss at iteration 580 : 0.11103782057762146
Loss at iteration 590 : 0.050371646881103516
Loss at iteration 600 : 0.100991390645504
Loss at iteration 610 : 0.07662604004144669
Loss at iteration 620 : 0.1104545146226883
Loss at iteration 630 : 0.04568402096629143
Loss at iteration 640 : 0.07565726339817047
Loss at iteration 650 : 0.09325709939002991
Loss at iteration 660 : 0.10178162902593613
Loss at iteration 670 : 0.07527730613946915
Loss at iteration 680 : 0.15308189392089844
Loss at iteration 690 : 0.05334446579217911
Loss at iteration 700 : 0.11183976382017136
Loss at iteration 710 : 0.10308226197957993
Loss at iteration 720 : 0.07581678777933121
Loss at iteration 730 : 0.05071781203150749
Loss at iteration 740 : 0.09133367240428925
Loss at iteration 750 : 0.11406110227108002
Loss at iteration 760 : 0.0625246912240982
Loss at iteration 770 : 0.08133591711521149
Loss at iteration 780 : 0.07926525920629501
Loss at iteration 790 : 0.1366567462682724
Loss at iteration 800 : 0.07730884850025177
Loss at iteration 810 : 0.0825318694114685
Loss at iteration 820 : 0.07285124063491821
Loss at iteration 830 : 0.0893017053604126
Loss at iteration 840 : 0.0896969884634018
Loss at iteration 850 : 0.09448810666799545
Loss at iteration 860 : 0.07189550995826721
Loss at iteration 870 : 0.09623797237873077
Loss at iteration 880 : 0.07599255442619324
Loss at iteration 890 : 0.09392575919628143
Loss at iteration 900 : 0.07502610236406326
Loss at iteration 910 : 0.09160387516021729
Loss at iteration 920 : 0.08388292044401169
Loss at iteration 930 : 0.08493082970380783
Loss at iteration 940 : 0.06355679035186768
Loss at iteration 950 : 0.08149339258670807
Loss at iteration 960 : 0.054681938141584396
Loss at iteration 970 : 0.07185038924217224
Loss at iteration 980 : 0.061615895479917526
Loss at iteration 990 : 0.14610113203525543
Loss at iteration 1000 : 0.1486867517232895
Loss at iteration 1010 : 0.07899956405162811
Loss at iteration 1020 : 0.14451566338539124
Loss at iteration 1030 : 0.050368450582027435
Loss at iteration 1040 : 0.08018267154693604
Loss at iteration 1050 : 0.06276963651180267
Loss at iteration 1060 : 0.06068915128707886
Loss at iteration 1070 : 0.10217322409152985
Loss at iteration 1080 : 0.10135285556316376
Loss at iteration 1090 : 0.10131081193685532
Loss at iteration 1100 : 0.05755367875099182
Loss at iteration 1110 : 0.06299979984760284
Loss at iteration 1120 : 0.08950551599264145
Loss at iteration 1130 : 0.13332779705524445
Loss at iteration 1140 : 0.09349267929792404
Loss at iteration 1150 : 0.1583186537027359
Loss at iteration 1160 : 0.09872855991125107
Loss at iteration 1170 : 0.09728910773992538
Loss at iteration 1180 : 0.11669386923313141
Loss at iteration 1190 : 0.08034934848546982
Loss at iteration 1200 : 0.057790786027908325
Loss at iteration 1210 : 0.056793443858623505
The SSIM Value is: 0.6963300188382466
The PSNR Value is: 20.910175959269207
the epoch is: 44
Loss at iteration 10 : 0.10593660175800323
Loss at iteration 20 : 0.08817770332098007
Loss at iteration 30 : 0.10504619777202606
Loss at iteration 40 : 0.06618932634592056
Loss at iteration 50 : 0.1483127474784851
Loss at iteration 60 : 0.13470447063446045
Loss at iteration 70 : 0.11598696559667587
Loss at iteration 80 : 0.07413792610168457
Loss at iteration 90 : 0.07980746030807495
Loss at iteration 100 : 0.0902208536863327
Loss at iteration 110 : 0.05631360411643982
Loss at iteration 120 : 0.05219227820634842
Loss at iteration 130 : 0.0999583750963211
Loss at iteration 140 : 0.06150538846850395
Loss at iteration 150 : 0.10961484909057617
Loss at iteration 160 : 0.08578026294708252
Loss at iteration 170 : 0.10078700631856918
Loss at iteration 180 : 0.07883375883102417
Loss at iteration 190 : 0.09454263746738434
Loss at iteration 200 : 0.11740582436323166
Loss at iteration 210 : 0.0656333863735199
Loss at iteration 220 : 0.0927717536687851
Loss at iteration 230 : 0.07782953232526779
Loss at iteration 240 : 0.09723614156246185
Loss at iteration 250 : 0.09939040243625641
Loss at iteration 260 : 0.0812201276421547
Loss at iteration 270 : 0.11212334036827087
Loss at iteration 280 : 0.09853555262088776
Loss at iteration 290 : 0.06880378723144531
Loss at iteration 300 : 0.0837104544043541
Loss at iteration 310 : 0.07716106623411179
Loss at iteration 320 : 0.08249253034591675
Loss at iteration 330 : 0.05711495131254196
Loss at iteration 340 : 0.10212370753288269
Loss at iteration 350 : 0.07853582501411438
Loss at iteration 360 : 0.11130128800868988
Loss at iteration 370 : 0.10534930229187012
Loss at iteration 380 : 0.07273387908935547
Loss at iteration 390 : 0.05675343796610832
Loss at iteration 400 : 0.11173008382320404
Loss at iteration 410 : 0.08786024898290634
Loss at iteration 420 : 0.06127878278493881
Loss at iteration 430 : 0.0766993910074234
Loss at iteration 440 : 0.12691915035247803
Loss at iteration 450 : 0.08732743561267853
Loss at iteration 460 : 0.08614405244588852
Loss at iteration 470 : 0.1014493852853775
Loss at iteration 480 : 0.08668442070484161
Loss at iteration 490 : 0.14968150854110718
Loss at iteration 500 : 0.07795209437608719
Loss at iteration 510 : 0.1081540584564209
Loss at iteration 520 : 0.0780867263674736
Loss at iteration 530 : 0.1382438838481903
Loss at iteration 540 : 0.1465047299861908
Loss at iteration 550 : 0.09284719824790955
Loss at iteration 560 : 0.07312247157096863
Loss at iteration 570 : 0.1400197595357895
Loss at iteration 580 : 0.10284732282161713
Loss at iteration 590 : 0.10795494168996811
Loss at iteration 600 : 0.07962164282798767
Loss at iteration 610 : 0.10937356948852539
Loss at iteration 620 : 0.09529262036085129
Loss at iteration 630 : 0.1388036012649536
Loss at iteration 640 : 0.11461976170539856
Loss at iteration 650 : 0.10270034521818161
Loss at iteration 660 : 0.0718567818403244
Loss at iteration 670 : 0.08305274695158005
Loss at iteration 680 : 0.15952818095684052
Loss at iteration 690 : 0.08760812133550644
Loss at iteration 700 : 0.10486042499542236
Loss at iteration 710 : 0.06714458763599396
Loss at iteration 720 : 0.09693580865859985
Loss at iteration 730 : 0.0891495868563652
Loss at iteration 740 : 0.08324350416660309
Loss at iteration 750 : 0.11635544896125793
Loss at iteration 760 : 0.09380035102367401
Loss at iteration 770 : 0.0801452025771141
Loss at iteration 780 : 0.1463654637336731
Loss at iteration 790 : 0.14560607075691223
Loss at iteration 800 : 0.11283193528652191
Loss at iteration 810 : 0.09160393476486206
Loss at iteration 820 : 0.10063539445400238
Loss at iteration 830 : 0.07868510484695435
Loss at iteration 840 : 0.08482331037521362
Loss at iteration 850 : 0.11709734052419662
Loss at iteration 860 : 0.09663785994052887
Loss at iteration 870 : 0.10557763278484344
Loss at iteration 880 : 0.06391304731369019
Loss at iteration 890 : 0.06548663973808289
Loss at iteration 900 : 0.0788663774728775
Loss at iteration 910 : 0.07222942262887955
Loss at iteration 920 : 0.06330050528049469
Loss at iteration 930 : 0.11305028200149536
Loss at iteration 940 : 0.07298558950424194
Loss at iteration 950 : 0.10621649026870728
Loss at iteration 960 : 0.11017851531505585
Loss at iteration 970 : 0.08148396015167236
Loss at iteration 980 : 0.10554998368024826
Loss at iteration 990 : 0.11596320569515228
Loss at iteration 1000 : 0.08637778460979462
Loss at iteration 1010 : 0.08310091495513916
Loss at iteration 1020 : 0.09483541548252106
Loss at iteration 1030 : 0.11852353066205978
Loss at iteration 1040 : 0.06632000207901001
Loss at iteration 1050 : 0.06276750564575195
Loss at iteration 1060 : 0.1109151542186737
Loss at iteration 1070 : 0.09325065463781357
Loss at iteration 1080 : 0.07528726756572723
Loss at iteration 1090 : 0.16378939151763916
Loss at iteration 1100 : 0.13213591277599335
Loss at iteration 1110 : 0.0882331132888794
Loss at iteration 1120 : 0.08089867234230042
Loss at iteration 1130 : 0.10034533590078354
Loss at iteration 1140 : 0.07830706238746643
Loss at iteration 1150 : 0.0975419282913208
Loss at iteration 1160 : 0.0898045003414154
Loss at iteration 1170 : 0.09970509260892868
Loss at iteration 1180 : 0.11340135335922241
Loss at iteration 1190 : 0.10170741379261017
Loss at iteration 1200 : 0.1090371161699295
Loss at iteration 1210 : 0.040123529732227325
The SSIM Value is: 0.6986434698104859
The PSNR Value is: 20.789680989583335
the epoch is: 45
Loss at iteration 10 : 0.1113065704703331
Loss at iteration 20 : 0.11457841098308563
Loss at iteration 30 : 0.0779375359416008
Loss at iteration 40 : 0.058501556515693665
Loss at iteration 50 : 0.16141141951084137
Loss at iteration 60 : 0.1090761348605156
Loss at iteration 70 : 0.05702167749404907
Loss at iteration 80 : 0.08179298043251038
Loss at iteration 90 : 0.09938334673643112
Loss at iteration 100 : 0.05012843757867813
Loss at iteration 110 : 0.06337550282478333
Loss at iteration 120 : 0.12188223004341125
Loss at iteration 130 : 0.08101370185613632
Loss at iteration 140 : 0.05981399863958359
Loss at iteration 150 : 0.09907814115285873
Loss at iteration 160 : 0.05324023216962814
Loss at iteration 170 : 0.05003124475479126
Loss at iteration 180 : 0.08758854120969772
Loss at iteration 190 : 0.08831887692213058
Loss at iteration 200 : 0.11463936418294907
Loss at iteration 210 : 0.12739112973213196
Loss at iteration 220 : 0.07030712068080902
Loss at iteration 230 : 0.12564361095428467
Loss at iteration 240 : 0.1030803769826889
Loss at iteration 250 : 0.08890542387962341
Loss at iteration 260 : 0.086484894156456
Loss at iteration 270 : 0.08906853199005127
Loss at iteration 280 : 0.05448590964078903
Loss at iteration 290 : 0.09827417135238647
Loss at iteration 300 : 0.07237447798252106
Loss at iteration 310 : 0.10297702997922897
Loss at iteration 320 : 0.09538137912750244
Loss at iteration 330 : 0.1507617086172104
Loss at iteration 340 : 0.1318991780281067
Loss at iteration 350 : 0.10989776253700256
Loss at iteration 360 : 0.12365379184484482
Loss at iteration 370 : 0.07112877070903778
Loss at iteration 380 : 0.062068961560726166
Loss at iteration 390 : 0.09193064272403717
Loss at iteration 400 : 0.11990055441856384
Loss at iteration 410 : 0.06557678431272507
Loss at iteration 420 : 0.10277126729488373
Loss at iteration 430 : 0.11617010086774826
Loss at iteration 440 : 0.08665463328361511
Loss at iteration 450 : 0.06727469712495804
Loss at iteration 460 : 0.07401017844676971
Loss at iteration 470 : 0.07702259719371796
Loss at iteration 480 : 0.12929664552211761
Loss at iteration 490 : 0.08390109241008759
Loss at iteration 500 : 0.06802861392498016
Loss at iteration 510 : 0.10858534276485443
Loss at iteration 520 : 0.0957721471786499
Loss at iteration 530 : 0.08196620643138885
Loss at iteration 540 : 0.06499464064836502
Loss at iteration 550 : 0.13369397819042206
Loss at iteration 560 : 0.07015039771795273
Loss at iteration 570 : 0.11067305505275726
Loss at iteration 580 : 0.0849141925573349
Loss at iteration 590 : 0.08859794586896896
Loss at iteration 600 : 0.21184736490249634
Loss at iteration 610 : 0.05155963823199272
Loss at iteration 620 : 0.0643506720662117
Loss at iteration 630 : 0.07023096829652786
Loss at iteration 640 : 0.09715469181537628
Loss at iteration 650 : 0.09915976226329803
Loss at iteration 660 : 0.1051098108291626
Loss at iteration 670 : 0.07819244265556335
Loss at iteration 680 : 0.08392234891653061
Loss at iteration 690 : 0.05630214512348175
Loss at iteration 700 : 0.09556497633457184
Loss at iteration 710 : 0.08786258101463318
Loss at iteration 720 : 0.06044011563062668
Loss at iteration 730 : 0.14322838187217712
Loss at iteration 740 : 0.10907523334026337
Loss at iteration 750 : 0.06573919951915741
Loss at iteration 760 : 0.11388401687145233
Loss at iteration 770 : 0.08252823352813721
Loss at iteration 780 : 0.06754767894744873
Loss at iteration 790 : 0.06139890104532242
Loss at iteration 800 : 0.09956909716129303
Loss at iteration 810 : 0.09215293824672699
Loss at iteration 820 : 0.13655847311019897
Loss at iteration 830 : 0.07305926084518433
Loss at iteration 840 : 0.08063212037086487
Loss at iteration 850 : 0.12867332994937897
Loss at iteration 860 : 0.07021565735340118
Loss at iteration 870 : 0.09744789451360703
Loss at iteration 880 : 0.09387942403554916
Loss at iteration 890 : 0.09162960946559906
Loss at iteration 900 : 0.0889277309179306
Loss at iteration 910 : 0.12214508652687073
Loss at iteration 920 : 0.06655611097812653
Loss at iteration 930 : 0.1460787057876587
Loss at iteration 940 : 0.09722130000591278
Loss at iteration 950 : 0.10221528261899948
Loss at iteration 960 : 0.07217158377170563
Loss at iteration 970 : 0.09751951694488525
Loss at iteration 980 : 0.06880927085876465
Loss at iteration 990 : 0.06819121539592743
Loss at iteration 1000 : 0.08491674065589905
Loss at iteration 1010 : 0.12540613114833832
Loss at iteration 1020 : 0.09862252324819565
Loss at iteration 1030 : 0.0976312980055809
Loss at iteration 1040 : 0.10575800389051437
Loss at iteration 1050 : 0.10481061786413193
Loss at iteration 1060 : 0.06420719623565674
Loss at iteration 1070 : 0.08544477075338364
Loss at iteration 1080 : 0.07291057705879211
Loss at iteration 1090 : 0.08840229362249374
Loss at iteration 1100 : 0.08480970561504364
Loss at iteration 1110 : 0.1532725989818573
Loss at iteration 1120 : 0.09239622205495834
Loss at iteration 1130 : 0.0662660002708435
Loss at iteration 1140 : 0.06051076576113701
Loss at iteration 1150 : 0.09352316707372665
Loss at iteration 1160 : 0.047152336686849594
Loss at iteration 1170 : 0.06572927534580231
Loss at iteration 1180 : 0.0950557217001915
Loss at iteration 1190 : 0.05701449513435364
Loss at iteration 1200 : 0.08540491759777069
Loss at iteration 1210 : 0.06433488428592682
The SSIM Value is: 0.6934895912806193
The PSNR Value is: 20.442973454793293
the epoch is: 46
Loss at iteration 10 : 0.059783320873975754
Loss at iteration 20 : 0.0973156988620758
Loss at iteration 30 : 0.09521430730819702
Loss at iteration 40 : 0.0854293555021286
Loss at iteration 50 : 0.06277863681316376
Loss at iteration 60 : 0.11356489360332489
Loss at iteration 70 : 0.08222630620002747
Loss at iteration 80 : 0.07211750745773315
Loss at iteration 90 : 0.0499892383813858
Loss at iteration 100 : 0.12535583972930908
Loss at iteration 110 : 0.15694527328014374
Loss at iteration 120 : 0.09549650549888611
Loss at iteration 130 : 0.08489716053009033
Loss at iteration 140 : 0.07134202122688293
Loss at iteration 150 : 0.12245350331068039
Loss at iteration 160 : 0.07772888243198395
Loss at iteration 170 : 0.06549285352230072
Loss at iteration 180 : 0.11835287511348724
Loss at iteration 190 : 0.07508570700883865
Loss at iteration 200 : 0.07489080727100372
Loss at iteration 210 : 0.11881866306066513
Loss at iteration 220 : 0.07855649292469025
Loss at iteration 230 : 0.09372350573539734
Loss at iteration 240 : 0.1007625162601471
Loss at iteration 250 : 0.10395272076129913
Loss at iteration 260 : 0.08617803454399109
Loss at iteration 270 : 0.07947516441345215
Loss at iteration 280 : 0.09599195420742035
Loss at iteration 290 : 0.16699665784835815
Loss at iteration 300 : 0.08809013664722443
Loss at iteration 310 : 0.0576886348426342
Loss at iteration 320 : 0.10882716625928879
Loss at iteration 330 : 0.08540509641170502
Loss at iteration 340 : 0.054824888706207275
Loss at iteration 350 : 0.0734604001045227
Loss at iteration 360 : 0.11041688919067383
Loss at iteration 370 : 0.09953050315380096
Loss at iteration 380 : 0.07312111556529999
Loss at iteration 390 : 0.10027801245450974
Loss at iteration 400 : 0.0581272691488266
Loss at iteration 410 : 0.07376047968864441
Loss at iteration 420 : 0.08851724863052368
Loss at iteration 430 : 0.1277938187122345
Loss at iteration 440 : 0.082701176404953
Loss at iteration 450 : 0.0875762552022934
Loss at iteration 460 : 0.0602731816470623
Loss at iteration 470 : 0.12924258410930634
Loss at iteration 480 : 0.0954594835639
Loss at iteration 490 : 0.10034924745559692
Loss at iteration 500 : 0.1550026535987854
Loss at iteration 510 : 0.07752003520727158
Loss at iteration 520 : 0.09845566749572754
Loss at iteration 530 : 0.08265969157218933
Loss at iteration 540 : 0.0890791043639183
Loss at iteration 550 : 0.09015416353940964
Loss at iteration 560 : 0.06217386573553085
Loss at iteration 570 : 0.10861208289861679
Loss at iteration 580 : 0.056233346462249756
Loss at iteration 590 : 0.0785147175192833
Loss at iteration 600 : 0.11629271507263184
Loss at iteration 610 : 0.08110074698925018
Loss at iteration 620 : 0.07904630154371262
Loss at iteration 630 : 0.13450026512145996
Loss at iteration 640 : 0.09071360528469086
Loss at iteration 650 : 0.09611012786626816
Loss at iteration 660 : 0.0612955205142498
Loss at iteration 670 : 0.06890662759542465
Loss at iteration 680 : 0.13850751519203186
Loss at iteration 690 : 0.04504749923944473
Loss at iteration 700 : 0.09154771268367767
Loss at iteration 710 : 0.09690730273723602
Loss at iteration 720 : 0.07621045410633087
Loss at iteration 730 : 0.07447296380996704
Loss at iteration 740 : 0.08561306446790695
Loss at iteration 750 : 0.06373712420463562
Loss at iteration 760 : 0.11663191020488739
Loss at iteration 770 : 0.07388973236083984
Loss at iteration 780 : 0.13874393701553345
Loss at iteration 790 : 0.07978236675262451
Loss at iteration 800 : 0.07730885595083237
Loss at iteration 810 : 0.08633981645107269
Loss at iteration 820 : 0.07818491756916046
Loss at iteration 830 : 0.05031120777130127
Loss at iteration 840 : 0.07102455943822861
Loss at iteration 850 : 0.1154327392578125
Loss at iteration 860 : 0.08564194291830063
Loss at iteration 870 : 0.08063648641109467
Loss at iteration 880 : 0.06310661137104034
Loss at iteration 890 : 0.0999048575758934
Loss at iteration 900 : 0.0793408453464508
Loss at iteration 910 : 0.11719121783971786
Loss at iteration 920 : 0.06616926193237305
Loss at iteration 930 : 0.08627121895551682
Loss at iteration 940 : 0.06001976132392883
Loss at iteration 950 : 0.04993569850921631
Loss at iteration 960 : 0.0852629542350769
Loss at iteration 970 : 0.08342605829238892
Loss at iteration 980 : 0.0802702084183693
Loss at iteration 990 : 0.08878888934850693
Loss at iteration 1000 : 0.0840381607413292
Loss at iteration 1010 : 0.10353448987007141
Loss at iteration 1020 : 0.06666343659162521
Loss at iteration 1030 : 0.08857536315917969
Loss at iteration 1040 : 0.053231146186590195
Loss at iteration 1050 : 0.09431776404380798
Loss at iteration 1060 : 0.1406804323196411
Loss at iteration 1070 : 0.0885222777724266
Loss at iteration 1080 : 0.07055993378162384
Loss at iteration 1090 : 0.11838006973266602
Loss at iteration 1100 : 0.06147228926420212
Loss at iteration 1110 : 0.09783222526311874
Loss at iteration 1120 : 0.08229925483465195
Loss at iteration 1130 : 0.11118976771831512
Loss at iteration 1140 : 0.07462935894727707
Loss at iteration 1150 : 0.08606972545385361
Loss at iteration 1160 : 0.04670831561088562
Loss at iteration 1170 : 0.05387865751981735
Loss at iteration 1180 : 0.07830506563186646
Loss at iteration 1190 : 0.10016332566738129
Loss at iteration 1200 : 0.08478929102420807
Loss at iteration 1210 : 0.05814842879772186
The SSIM Value is: 0.6984654903411865
The PSNR Value is: 20.666505940755208
the epoch is: 47
Loss at iteration 10 : 0.10766234248876572
Loss at iteration 20 : 0.07847058773040771
Loss at iteration 30 : 0.07822220772504807
Loss at iteration 40 : 0.06467049568891525
Loss at iteration 50 : 0.09572543203830719
Loss at iteration 60 : 0.07437491416931152
Loss at iteration 70 : 0.07007201015949249
Loss at iteration 80 : 0.09636496007442474
Loss at iteration 90 : 0.1238311380147934
Loss at iteration 100 : 0.07749821990728378
Loss at iteration 110 : 0.10083671659231186
Loss at iteration 120 : 0.09002353996038437
Loss at iteration 130 : 0.08026234805583954
Loss at iteration 140 : 0.09390836954116821
Loss at iteration 150 : 0.10634492337703705
Loss at iteration 160 : 0.09147807955741882
Loss at iteration 170 : 0.06868874281644821
Loss at iteration 180 : 0.12510234117507935
Loss at iteration 190 : 0.09641977399587631
Loss at iteration 200 : 0.1122332215309143
Loss at iteration 210 : 0.057356081902980804
Loss at iteration 220 : 0.062346767634153366
Loss at iteration 230 : 0.09894385188817978
Loss at iteration 240 : 0.07369496673345566
Loss at iteration 250 : 0.05731464922428131
Loss at iteration 260 : 0.07660868018865585
Loss at iteration 270 : 0.08695860207080841
Loss at iteration 280 : 0.08306840062141418
Loss at iteration 290 : 0.06397795677185059
Loss at iteration 300 : 0.049449484795331955
Loss at iteration 310 : 0.08690308034420013
Loss at iteration 320 : 0.09607166051864624
Loss at iteration 330 : 0.06023237109184265
Loss at iteration 340 : 0.057792212814092636
Loss at iteration 350 : 0.07430737465620041
Loss at iteration 360 : 0.07042135298252106
Loss at iteration 370 : 0.12604328989982605
Loss at iteration 380 : 0.06255875527858734
Loss at iteration 390 : 0.13072198629379272
Loss at iteration 400 : 0.09191947430372238
Loss at iteration 410 : 0.06654778867959976
Loss at iteration 420 : 0.0843011662364006
Loss at iteration 430 : 0.08344852924346924
Loss at iteration 440 : 0.05626669153571129
Loss at iteration 450 : 0.1311701089143753
Loss at iteration 460 : 0.07630689442157745
Loss at iteration 470 : 0.0560385100543499
Loss at iteration 480 : 0.12738412618637085
Loss at iteration 490 : 0.08982104063034058
Loss at iteration 500 : 0.06671909987926483
Loss at iteration 510 : 0.10686638951301575
Loss at iteration 520 : 0.08749306201934814
Loss at iteration 530 : 0.08300835639238358
Loss at iteration 540 : 0.09432175755500793
Loss at iteration 550 : 0.1202910989522934
Loss at iteration 560 : 0.07821875065565109
Loss at iteration 570 : 0.15443280339241028
Loss at iteration 580 : 0.06505230814218521
Loss at iteration 590 : 0.0953335165977478
Loss at iteration 600 : 0.09015804529190063
Loss at iteration 610 : 0.08093159645795822
Loss at iteration 620 : 0.06895211338996887
Loss at iteration 630 : 0.1086815819144249
Loss at iteration 640 : 0.08663758635520935
Loss at iteration 650 : 0.06612318009138107
Loss at iteration 660 : 0.0544949471950531
Loss at iteration 670 : 0.0768372192978859
Loss at iteration 680 : 0.06450308859348297
Loss at iteration 690 : 0.06368838995695114
Loss at iteration 700 : 0.10044457763433456
Loss at iteration 710 : 0.10710734128952026
Loss at iteration 720 : 0.09325633943080902
Loss at iteration 730 : 0.08394279330968857
Loss at iteration 740 : 0.07553651928901672
Loss at iteration 750 : 0.14193403720855713
Loss at iteration 760 : 0.08260035514831543
Loss at iteration 770 : 0.07061252743005753
Loss at iteration 780 : 0.10381566733121872
Loss at iteration 790 : 0.08629576861858368
Loss at iteration 800 : 0.10113020986318588
Loss at iteration 810 : 0.08353831619024277
Loss at iteration 820 : 0.09256783127784729
Loss at iteration 830 : 0.057329386472702026
Loss at iteration 840 : 0.1016855537891388
Loss at iteration 850 : 0.04685274884104729
Loss at iteration 860 : 0.1356019675731659
Loss at iteration 870 : 0.05555254966020584
Loss at iteration 880 : 0.07022762298583984
Loss at iteration 890 : 0.09779085218906403
Loss at iteration 900 : 0.10047125816345215
Loss at iteration 910 : 0.08365371823310852
Loss at iteration 920 : 0.07645633816719055
Loss at iteration 930 : 0.06372281908988953
Loss at iteration 940 : 0.12427771091461182
Loss at iteration 950 : 0.0888742208480835
Loss at iteration 960 : 0.08222131431102753
Loss at iteration 970 : 0.0850091427564621
Loss at iteration 980 : 0.09053629636764526
Loss at iteration 990 : 0.050321441143751144
Loss at iteration 1000 : 0.08060863614082336
Loss at iteration 1010 : 0.1108647808432579
Loss at iteration 1020 : 0.07241867482662201
Loss at iteration 1030 : 0.08407913148403168
Loss at iteration 1040 : 0.07810553908348083
Loss at iteration 1050 : 0.1594146490097046
Loss at iteration 1060 : 0.09941302239894867
Loss at iteration 1070 : 0.07333807647228241
Loss at iteration 1080 : 0.11918147653341293
Loss at iteration 1090 : 0.15024983882904053
Loss at iteration 1100 : 0.10214149951934814
Loss at iteration 1110 : 0.1417933851480484
Loss at iteration 1120 : 0.061717450618743896
Loss at iteration 1130 : 0.10038559883832932
Loss at iteration 1140 : 0.1319238692522049
Loss at iteration 1150 : 0.09454196691513062
Loss at iteration 1160 : 0.14573588967323303
Loss at iteration 1170 : 0.055493663996458054
Loss at iteration 1180 : 0.037764742970466614
Loss at iteration 1190 : 0.1436469554901123
Loss at iteration 1200 : 0.05617281422019005
Loss at iteration 1210 : 0.06850920617580414
The SSIM Value is: 0.6990141888459523
The PSNR Value is: 20.77999636332194
the epoch is: 48
Loss at iteration 10 : 0.08721604943275452
Loss at iteration 20 : 0.06663643568754196
Loss at iteration 30 : 0.11329028010368347
Loss at iteration 40 : 0.08018660545349121
Loss at iteration 50 : 0.0921500027179718
Loss at iteration 60 : 0.13046643137931824
Loss at iteration 70 : 0.08928324282169342
Loss at iteration 80 : 0.08748876303434372
Loss at iteration 90 : 0.04867761582136154
Loss at iteration 100 : 0.12788626551628113
Loss at iteration 110 : 0.10754495859146118
Loss at iteration 120 : 0.14345970749855042
Loss at iteration 130 : 0.11405204981565475
Loss at iteration 140 : 0.07319583743810654
Loss at iteration 150 : 0.06958938390016556
Loss at iteration 160 : 0.07321595400571823
Loss at iteration 170 : 0.1006147563457489
Loss at iteration 180 : 0.1368328332901001
Loss at iteration 190 : 0.0974561795592308
Loss at iteration 200 : 0.07699215412139893
Loss at iteration 210 : 0.11389227211475372
Loss at iteration 220 : 0.10607286542654037
Loss at iteration 230 : 0.06737818568944931
Loss at iteration 240 : 0.0516870841383934
Loss at iteration 250 : 0.089507095515728
Loss at iteration 260 : 0.06716758012771606
Loss at iteration 270 : 0.08440013229846954
Loss at iteration 280 : 0.058804821223020554
Loss at iteration 290 : 0.12045415490865707
Loss at iteration 300 : 0.07007424533367157
Loss at iteration 310 : 0.06455840915441513
Loss at iteration 320 : 0.09731046855449677
Loss at iteration 330 : 0.07861097902059555
Loss at iteration 340 : 0.0955478847026825
Loss at iteration 350 : 0.0686495453119278
Loss at iteration 360 : 0.09303891658782959
Loss at iteration 370 : 0.11127456277608871
Loss at iteration 380 : 0.0622863844037056
Loss at iteration 390 : 0.11901333183050156
Loss at iteration 400 : 0.1317909061908722
Loss at iteration 410 : 0.08129337430000305
Loss at iteration 420 : 0.06359149515628815
Loss at iteration 430 : 0.07369416952133179
Loss at iteration 440 : 0.05799608677625656
Loss at iteration 450 : 0.07488279789686203
Loss at iteration 460 : 0.06719999015331268
Loss at iteration 470 : 0.059421978890895844
Loss at iteration 480 : 0.08517806231975555
Loss at iteration 490 : 0.12404059618711472
Loss at iteration 500 : 0.09609739482402802
Loss at iteration 510 : 0.08000119030475616
Loss at iteration 520 : 0.09635375440120697
Loss at iteration 530 : 0.09165440499782562
Loss at iteration 540 : 0.07266601920127869
Loss at iteration 550 : 0.08150725811719894
Loss at iteration 560 : 0.07836355268955231
Loss at iteration 570 : 0.09125622361898422
Loss at iteration 580 : 0.10909891128540039
Loss at iteration 590 : 0.09408621490001678
Loss at iteration 600 : 0.06579847633838654
Loss at iteration 610 : 0.07008880376815796
Loss at iteration 620 : 0.10306088626384735
Loss at iteration 630 : 0.07757848501205444
Loss at iteration 640 : 0.045931991189718246
Loss at iteration 650 : 0.08724071085453033
Loss at iteration 660 : 0.059838633984327316
Loss at iteration 670 : 0.08286374062299728
Loss at iteration 680 : 0.09800209105014801
Loss at iteration 690 : 0.12985754013061523
Loss at iteration 700 : 0.1191335916519165
Loss at iteration 710 : 0.0915352925658226
Loss at iteration 720 : 0.09380321949720383
Loss at iteration 730 : 0.06206827610731125
Loss at iteration 740 : 0.08388309925794601
Loss at iteration 750 : 0.06535907089710236
Loss at iteration 760 : 0.06007201597094536
Loss at iteration 770 : 0.09677824378013611
Loss at iteration 780 : 0.07117877900600433
Loss at iteration 790 : 0.09226995706558228
Loss at iteration 800 : 0.07268759608268738
Loss at iteration 810 : 0.06147433817386627
Loss at iteration 820 : 0.06586649268865585
Loss at iteration 830 : 0.07894977182149887
Loss at iteration 840 : 0.05159153789281845
Loss at iteration 850 : 0.12247881293296814
Loss at iteration 860 : 0.07551632821559906
Loss at iteration 870 : 0.07277791947126389
Loss at iteration 880 : 0.09695383161306381
Loss at iteration 890 : 0.07513799518346786
Loss at iteration 900 : 0.08868571370840073
Loss at iteration 910 : 0.054744601249694824
Loss at iteration 920 : 0.056497037410736084
Loss at iteration 930 : 0.11293822526931763
Loss at iteration 940 : 0.0900009498000145
Loss at iteration 950 : 0.08178187906742096
Loss at iteration 960 : 0.059450309723615646
Loss at iteration 970 : 0.1368197798728943
Loss at iteration 980 : 0.10436111688613892
Loss at iteration 990 : 0.130809485912323
Loss at iteration 1000 : 0.11356308311223984
Loss at iteration 1010 : 0.07757047563791275
Loss at iteration 1020 : 0.06462308764457703
Loss at iteration 1030 : 0.08261515945196152
Loss at iteration 1040 : 0.12811841070652008
Loss at iteration 1050 : 0.08559596538543701
Loss at iteration 1060 : 0.04759949445724487
Loss at iteration 1070 : 0.1001317948102951
Loss at iteration 1080 : 0.07349709421396255
Loss at iteration 1090 : 0.05958877131342888
Loss at iteration 1100 : 0.12386578321456909
Loss at iteration 1110 : 0.052053164690732956
Loss at iteration 1120 : 0.06416517496109009
Loss at iteration 1130 : 0.06428349018096924
Loss at iteration 1140 : 0.07201959192752838
Loss at iteration 1150 : 0.0294092558324337
Loss at iteration 1160 : 0.0884292870759964
Loss at iteration 1170 : 0.1106259822845459
Loss at iteration 1180 : 0.08138927072286606
Loss at iteration 1190 : 0.06827059388160706
Loss at iteration 1200 : 0.13305984437465668
Loss at iteration 1210 : 0.0833158940076828
The SSIM Value is: 0.6988476514816284
The PSNR Value is: 20.88199939727783
the epoch is: 49
Loss at iteration 10 : 0.06752742826938629
Loss at iteration 20 : 0.10807474702596664
Loss at iteration 30 : 0.07953774929046631
Loss at iteration 40 : 0.06563867628574371
Loss at iteration 50 : 0.04922445863485336
Loss at iteration 60 : 0.09121161699295044
Loss at iteration 70 : 0.10518825054168701
Loss at iteration 80 : 0.11465108394622803
Loss at iteration 90 : 0.10157233476638794
Loss at iteration 100 : 0.0613405704498291
Loss at iteration 110 : 0.09173665940761566
Loss at iteration 120 : 0.07825291901826859
Loss at iteration 130 : 0.08944442123174667
Loss at iteration 140 : 0.09473630040884018
Loss at iteration 150 : 0.08045026659965515
Loss at iteration 160 : 0.09486663341522217
Loss at iteration 170 : 0.14877790212631226
Loss at iteration 180 : 0.10620769113302231
Loss at iteration 190 : 0.09164482355117798
Loss at iteration 200 : 0.05484118312597275
Loss at iteration 210 : 0.062044691294431686
Loss at iteration 220 : 0.06813645362854004
Loss at iteration 230 : 0.06383585929870605
Loss at iteration 240 : 0.04512106627225876
Loss at iteration 250 : 0.06944330036640167
Loss at iteration 260 : 0.05683905631303787
Loss at iteration 270 : 0.0897342711687088
Loss at iteration 280 : 0.08463438600301743
Loss at iteration 290 : 0.06599913537502289
Loss at iteration 300 : 0.06345396488904953
Loss at iteration 310 : 0.07793094217777252
Loss at iteration 320 : 0.06549860537052155
Loss at iteration 330 : 0.10488595068454742
Loss at iteration 340 : 0.09192007035017014
Loss at iteration 350 : 0.07326186448335648
Loss at iteration 360 : 0.08666236698627472
Loss at iteration 370 : 0.09447750449180603
Loss at iteration 380 : 0.10580627620220184
Loss at iteration 390 : 0.05096103250980377
Loss at iteration 400 : 0.057242970913648605
Loss at iteration 410 : 0.10528500378131866
Loss at iteration 420 : 0.06411908566951752
Loss at iteration 430 : 0.0855821818113327
Loss at iteration 440 : 0.1069440096616745
Loss at iteration 450 : 0.08321070671081543
Loss at iteration 460 : 0.06941598653793335
Loss at iteration 470 : 0.06711126863956451
Loss at iteration 480 : 0.08703352510929108
Loss at iteration 490 : 0.12109820544719696
Loss at iteration 500 : 0.09501749277114868
Loss at iteration 510 : 0.09082548320293427
Loss at iteration 520 : 0.09287931770086288
Loss at iteration 530 : 0.09262051433324814
Loss at iteration 540 : 0.07007943093776703
Loss at iteration 550 : 0.11992733180522919
Loss at iteration 560 : 0.08037620782852173
Loss at iteration 570 : 0.08669402450323105
Loss at iteration 580 : 0.06709174066781998
Loss at iteration 590 : 0.1243024617433548
Loss at iteration 600 : 0.09467808902263641
Loss at iteration 610 : 0.05280574411153793
Loss at iteration 620 : 0.08538507670164108
Loss at iteration 630 : 0.06738780438899994
Loss at iteration 640 : 0.08109307289123535
Loss at iteration 650 : 0.10278960317373276
Loss at iteration 660 : 0.09705328941345215
Loss at iteration 670 : 0.0977465808391571
Loss at iteration 680 : 0.1317329704761505
Loss at iteration 690 : 0.09044626355171204
Loss at iteration 700 : 0.10351943969726562
Loss at iteration 710 : 0.05819503590464592
Loss at iteration 720 : 0.10675148665904999
Loss at iteration 730 : 0.07065783441066742
Loss at iteration 740 : 0.06327011436223984
Loss at iteration 750 : 0.07533052563667297
Loss at iteration 760 : 0.07199884951114655
Loss at iteration 770 : 0.0673459991812706
Loss at iteration 780 : 0.07370179891586304
Loss at iteration 790 : 0.04922778531908989
Loss at iteration 800 : 0.05378362536430359
Loss at iteration 810 : 0.11091947555541992
Loss at iteration 820 : 0.08759354054927826
Loss at iteration 830 : 0.0795375406742096
Loss at iteration 840 : 0.06903483718633652
Loss at iteration 850 : 0.07358644157648087
Loss at iteration 860 : 0.0810500830411911
Loss at iteration 870 : 0.0960770845413208
Loss at iteration 880 : 0.08407120406627655
Loss at iteration 890 : 0.060080017894506454
Loss at iteration 900 : 0.08231698721647263
Loss at iteration 910 : 0.09106656908988953
Loss at iteration 920 : 0.07517413794994354
Loss at iteration 930 : 0.07390858232975006
Loss at iteration 940 : 0.1018129289150238
Loss at iteration 950 : 0.055576711893081665
Loss at iteration 960 : 0.07968118041753769
Loss at iteration 970 : 0.15544381737709045
Loss at iteration 980 : 0.06493674218654633
Loss at iteration 990 : 0.06472986936569214
Loss at iteration 1000 : 0.08083557337522507
Loss at iteration 1010 : 0.14778761565685272
Loss at iteration 1020 : 0.08611063659191132
Loss at iteration 1030 : 0.08905502408742905
Loss at iteration 1040 : 0.07081213593482971
Loss at iteration 1050 : 0.14638951420783997
Loss at iteration 1060 : 0.14275285601615906
Loss at iteration 1070 : 0.08514677733182907
Loss at iteration 1080 : 0.06649847328662872
Loss at iteration 1090 : 0.10089600086212158
Loss at iteration 1100 : 0.06469744443893433
Loss at iteration 1110 : 0.07974857091903687
Loss at iteration 1120 : 0.06774523109197617
Loss at iteration 1130 : 0.1546165645122528
Loss at iteration 1140 : 0.07941722124814987
Loss at iteration 1150 : 0.07028064131736755
Loss at iteration 1160 : 0.08735871315002441
Loss at iteration 1170 : 0.052745234221220016
Loss at iteration 1180 : 0.09396105259656906
Loss at iteration 1190 : 0.12970343232154846
Loss at iteration 1200 : 0.07709559798240662
Loss at iteration 1210 : 0.0615316778421402
The SSIM Value is: 0.7000930031140645
The PSNR Value is: 20.70385799407959
the epoch is: 50
Loss at iteration 10 : 0.09286446869373322
Loss at iteration 20 : 0.07554073631763458
Loss at iteration 30 : 0.077440045773983
Loss at iteration 40 : 0.10570545494556427
Loss at iteration 50 : 0.11927205324172974
Loss at iteration 60 : 0.046956878155469894
Loss at iteration 70 : 0.06587843596935272
Loss at iteration 80 : 0.09878680109977722
Loss at iteration 90 : 0.06892560422420502
Loss at iteration 100 : 0.11526595056056976
Loss at iteration 110 : 0.05902717262506485
Loss at iteration 120 : 0.06539922952651978
Loss at iteration 130 : 0.13073980808258057
Loss at iteration 140 : 0.1204095110297203
Loss at iteration 150 : 0.06601589918136597
Loss at iteration 160 : 0.05779765173792839
Loss at iteration 170 : 0.09827807545661926
Loss at iteration 180 : 0.06328386068344116
Loss at iteration 190 : 0.1101391613483429
Loss at iteration 200 : 0.09013746678829193
Loss at iteration 210 : 0.08057072013616562
Loss at iteration 220 : 0.07326381653547287
Loss at iteration 230 : 0.08594265580177307
Loss at iteration 240 : 0.041622333228588104
Loss at iteration 250 : 0.05311378091573715
Loss at iteration 260 : 0.10443662106990814
Loss at iteration 270 : 0.10253022611141205
Loss at iteration 280 : 0.10469095408916473
Loss at iteration 290 : 0.1308220475912094
Loss at iteration 300 : 0.07408586144447327
Loss at iteration 310 : 0.06247049197554588
Loss at iteration 320 : 0.09278281033039093
Loss at iteration 330 : 0.12147407978773117
Loss at iteration 340 : 0.06196523457765579
Loss at iteration 350 : 0.0563754141330719
Loss at iteration 360 : 0.052524250000715256
Loss at iteration 370 : 0.05517290160059929
Loss at iteration 380 : 0.10544277727603912
Loss at iteration 390 : 0.08910945057868958
Loss at iteration 400 : 0.06692097336053848
Loss at iteration 410 : 0.05551706254482269
Loss at iteration 420 : 0.06919512152671814
Loss at iteration 430 : 0.1326099932193756
Loss at iteration 440 : 0.12216510623693466
Loss at iteration 450 : 0.08554279804229736
Loss at iteration 460 : 0.06881191581487656
Loss at iteration 470 : 0.10968190431594849
Loss at iteration 480 : 0.06942691653966904
Loss at iteration 490 : 0.062060605734586716
Loss at iteration 500 : 0.12541387975215912
Loss at iteration 510 : 0.10334231704473495
Loss at iteration 520 : 0.08432962745428085
Loss at iteration 530 : 0.07704943418502808
Loss at iteration 540 : 0.12565812468528748
Loss at iteration 550 : 0.11596445739269257
Loss at iteration 560 : 0.08390501141548157
Loss at iteration 570 : 0.056726984679698944
Loss at iteration 580 : 0.08401787281036377
Loss at iteration 590 : 0.04446682333946228
Loss at iteration 600 : 0.09058421850204468
Loss at iteration 610 : 0.12854114174842834
Loss at iteration 620 : 0.04882579296827316
Loss at iteration 630 : 0.07323861122131348
Loss at iteration 640 : 0.06188161298632622
Loss at iteration 650 : 0.08656346797943115
Loss at iteration 660 : 0.06888629496097565
Loss at iteration 670 : 0.08245849609375
Loss at iteration 680 : 0.09266255795955658
Loss at iteration 690 : 0.07297612726688385
Loss at iteration 700 : 0.06412665545940399
Loss at iteration 710 : 0.06145681068301201
Loss at iteration 720 : 0.06577279418706894
Loss at iteration 730 : 0.051261913031339645
Loss at iteration 740 : 0.08833406865596771
Loss at iteration 750 : 0.12700599431991577
Loss at iteration 760 : 0.05534787103533745
Loss at iteration 770 : 0.09325753152370453
Loss at iteration 780 : 0.06870727241039276
Loss at iteration 790 : 0.06242869049310684
Loss at iteration 800 : 0.11536256968975067
Loss at iteration 810 : 0.0635099932551384
Loss at iteration 820 : 0.09081461280584335
Loss at iteration 830 : 0.11366116255521774
Loss at iteration 840 : 0.06976303458213806
Loss at iteration 850 : 0.10731947422027588
Loss at iteration 860 : 0.08745580911636353
Loss at iteration 870 : 0.12263843417167664
Loss at iteration 880 : 0.07583889365196228
Loss at iteration 890 : 0.11491595208644867
Loss at iteration 900 : 0.08978338539600372
Loss at iteration 910 : 0.09342601150274277
Loss at iteration 920 : 0.09924568235874176
Loss at iteration 930 : 0.12456753849983215
Loss at iteration 940 : 0.09715454280376434
Loss at iteration 950 : 0.12746894359588623
Loss at iteration 960 : 0.06881295889616013
Loss at iteration 970 : 0.08225647360086441
Loss at iteration 980 : 0.11059308052062988
Loss at iteration 990 : 0.07690927386283875
Loss at iteration 1000 : 0.08413577079772949
Loss at iteration 1010 : 0.11548569053411484
Loss at iteration 1020 : 0.08768551796674728
Loss at iteration 1030 : 0.10956530272960663
Loss at iteration 1040 : 0.1191810816526413
Loss at iteration 1050 : 0.05233880504965782
Loss at iteration 1060 : 0.08346815407276154
Loss at iteration 1070 : 0.0954575389623642
Loss at iteration 1080 : 0.15351413190364838
Loss at iteration 1090 : 0.08332397788763046
Loss at iteration 1100 : 0.06344649195671082
Loss at iteration 1110 : 0.08964967727661133
Loss at iteration 1120 : 0.0647815614938736
Loss at iteration 1130 : 0.052052538841962814
Loss at iteration 1140 : 0.05028027296066284
Loss at iteration 1150 : 0.11111699044704437
Loss at iteration 1160 : 0.07665517926216125
Loss at iteration 1170 : 0.05919253081083298
Loss at iteration 1180 : 0.07221119105815887
Loss at iteration 1190 : 0.08859699964523315
Loss at iteration 1200 : 0.14393988251686096
Loss at iteration 1210 : 0.07284897565841675
The SSIM Value is: 0.6998959958553315
The PSNR Value is: 20.845765813191733
the epoch is: 51
Loss at iteration 10 : 0.08265399187803268
Loss at iteration 20 : 0.11456315964460373
Loss at iteration 30 : 0.0568557009100914
Loss at iteration 40 : 0.12888136506080627
Loss at iteration 50 : 0.06880151480436325
Loss at iteration 60 : 0.15075847506523132
Loss at iteration 70 : 0.05576006695628166
Loss at iteration 80 : 0.08932187408208847
Loss at iteration 90 : 0.05214507877826691
Loss at iteration 100 : 0.10825240612030029
Loss at iteration 110 : 0.12272137403488159
Loss at iteration 120 : 0.03146154433488846
Loss at iteration 130 : 0.11231034994125366
Loss at iteration 140 : 0.06469198316335678
Loss at iteration 150 : 0.13617351651191711
Loss at iteration 160 : 0.09191988408565521
Loss at iteration 170 : 0.11361350119113922
Loss at iteration 180 : 0.08297717571258545
Loss at iteration 190 : 0.07932373881340027
Loss at iteration 200 : 0.12036007642745972
Loss at iteration 210 : 0.08336296677589417
Loss at iteration 220 : 0.05661289021372795
Loss at iteration 230 : 0.08426382392644882
Loss at iteration 240 : 0.086904376745224
Loss at iteration 250 : 0.07182685285806656
Loss at iteration 260 : 0.07394536584615707
Loss at iteration 270 : 0.10578157007694244
Loss at iteration 280 : 0.13938644528388977
Loss at iteration 290 : 0.05423160642385483
Loss at iteration 300 : 0.08137207478284836
Loss at iteration 310 : 0.12762193381786346
Loss at iteration 320 : 0.08968068659305573
Loss at iteration 330 : 0.08262917399406433
Loss at iteration 340 : 0.06030741333961487
Loss at iteration 350 : 0.08835379779338837
Loss at iteration 360 : 0.09123417735099792
Loss at iteration 370 : 0.11225520074367523
Loss at iteration 380 : 0.11724909394979477
Loss at iteration 390 : 0.09384821355342865
Loss at iteration 400 : 0.05189839005470276
Loss at iteration 410 : 0.11980310082435608
Loss at iteration 420 : 0.06865774095058441
Loss at iteration 430 : 0.04015694558620453
Loss at iteration 440 : 0.06689821183681488
Loss at iteration 450 : 0.08053766936063766
Loss at iteration 460 : 0.06629052758216858
Loss at iteration 470 : 0.1362573653459549
Loss at iteration 480 : 0.10462501645088196
Loss at iteration 490 : 0.09794893115758896
Loss at iteration 500 : 0.07899533957242966
Loss at iteration 510 : 0.07159274816513062
Loss at iteration 520 : 0.08417116105556488
Loss at iteration 530 : 0.0770161971449852
Loss at iteration 540 : 0.047993384301662445
Loss at iteration 550 : 0.08466392010450363
Loss at iteration 560 : 0.06210814416408539
Loss at iteration 570 : 0.13530775904655457
Loss at iteration 580 : 0.09181833267211914
Loss at iteration 590 : 0.08941062539815903
Loss at iteration 600 : 0.10498204082250595
Loss at iteration 610 : 0.06075494736433029
Loss at iteration 620 : 0.07677575200796127
Loss at iteration 630 : 0.14411814510822296
Loss at iteration 640 : 0.056146081537008286
Loss at iteration 650 : 0.09146138280630112
Loss at iteration 660 : 0.09868066757917404
Loss at iteration 670 : 0.06749212741851807
Loss at iteration 680 : 0.07088509947061539
Loss at iteration 690 : 0.10461555421352386
Loss at iteration 700 : 0.11320623010396957
Loss at iteration 710 : 0.07050983607769012
Loss at iteration 720 : 0.08648288249969482
Loss at iteration 730 : 0.054646171629428864
Loss at iteration 740 : 0.06407757848501205
Loss at iteration 750 : 0.10852664709091187
Loss at iteration 760 : 0.061589401215314865
Loss at iteration 770 : 0.08191308379173279
Loss at iteration 780 : 0.07133303582668304
Loss at iteration 790 : 0.08481918275356293
Loss at iteration 800 : 0.10646887868642807
Loss at iteration 810 : 0.06118839234113693
Loss at iteration 820 : 0.07938574254512787
Loss at iteration 830 : 0.06771610677242279
Loss at iteration 840 : 0.0706268846988678
Loss at iteration 850 : 0.11430282890796661
Loss at iteration 860 : 0.07514815032482147
Loss at iteration 870 : 0.08815251290798187
Loss at iteration 880 : 0.06374317407608032
Loss at iteration 890 : 0.05582365021109581
Loss at iteration 900 : 0.10242417454719543
Loss at iteration 910 : 0.08159426599740982
Loss at iteration 920 : 0.10178225487470627
Loss at iteration 930 : 0.1068587526679039
Loss at iteration 940 : 0.08020168542861938
Loss at iteration 950 : 0.08081133663654327
Loss at iteration 960 : 0.11793770641088486
Loss at iteration 970 : 0.0786643996834755
Loss at iteration 980 : 0.1345519870519638
Loss at iteration 990 : 0.10910411924123764
Loss at iteration 1000 : 0.07336677610874176
Loss at iteration 1010 : 0.1006244644522667
Loss at iteration 1020 : 0.049880724400281906
Loss at iteration 1030 : 0.1305333971977234
Loss at iteration 1040 : 0.07826115190982819
Loss at iteration 1050 : 0.068984255194664
Loss at iteration 1060 : 0.10939399898052216
Loss at iteration 1070 : 0.09472068399190903
Loss at iteration 1080 : 0.13676270842552185
Loss at iteration 1090 : 0.058069225400686264
Loss at iteration 1100 : 0.09909136593341827
Loss at iteration 1110 : 0.07414158433675766
Loss at iteration 1120 : 0.1309961974620819
Loss at iteration 1130 : 0.0861680656671524
Loss at iteration 1140 : 0.09747517108917236
Loss at iteration 1150 : 0.08437775820493698
Loss at iteration 1160 : 0.04461684823036194
Loss at iteration 1170 : 0.07874259352684021
Loss at iteration 1180 : 0.12416990846395493
Loss at iteration 1190 : 0.07717177271842957
Loss at iteration 1200 : 0.04729365557432175
Loss at iteration 1210 : 0.08815144002437592
The SSIM Value is: 0.6994654655456543
The PSNR Value is: 20.725579071044923
the epoch is: 52
Loss at iteration 10 : 0.07980577647686005
Loss at iteration 20 : 0.08470527827739716
Loss at iteration 30 : 0.09913855791091919
Loss at iteration 40 : 0.08954092115163803
Loss at iteration 50 : 0.054785072803497314
Loss at iteration 60 : 0.09251367300748825
Loss at iteration 70 : 0.09551914781332016
Loss at iteration 80 : 0.07080500572919846
Loss at iteration 90 : 0.06353151798248291
Loss at iteration 100 : 0.09908746927976608
Loss at iteration 110 : 0.08345161378383636
Loss at iteration 120 : 0.07195469737052917
Loss at iteration 130 : 0.09566628932952881
Loss at iteration 140 : 0.11377537250518799
Loss at iteration 150 : 0.11779600381851196
Loss at iteration 160 : 0.06082025170326233
Loss at iteration 170 : 0.10125168412923813
Loss at iteration 180 : 0.13182693719863892
Loss at iteration 190 : 0.11901895701885223
Loss at iteration 200 : 0.08188699930906296
Loss at iteration 210 : 0.09270946681499481
Loss at iteration 220 : 0.08182965219020844
Loss at iteration 230 : 0.08829476684331894
Loss at iteration 240 : 0.06386980414390564
Loss at iteration 250 : 0.07483810186386108
Loss at iteration 260 : 0.06998543441295624
Loss at iteration 270 : 0.1503351926803589
Loss at iteration 280 : 0.06857314705848694
Loss at iteration 290 : 0.08047240972518921
Loss at iteration 300 : 0.07644864916801453
Loss at iteration 310 : 0.05555219575762749
Loss at iteration 320 : 0.05964633822441101
Loss at iteration 330 : 0.0919887125492096
Loss at iteration 340 : 0.1080755889415741
Loss at iteration 350 : 0.07886497676372528
Loss at iteration 360 : 0.11050301790237427
Loss at iteration 370 : 0.07180880755186081
Loss at iteration 380 : 0.07571932673454285
Loss at iteration 390 : 0.08667778968811035
Loss at iteration 400 : 0.11859928071498871
Loss at iteration 410 : 0.1313888281583786
Loss at iteration 420 : 0.08616015315055847
Loss at iteration 430 : 0.14481058716773987
Loss at iteration 440 : 0.1153801903128624
Loss at iteration 450 : 0.07454760372638702
Loss at iteration 460 : 0.05205025523900986
Loss at iteration 470 : 0.06422296911478043
Loss at iteration 480 : 0.09847216308116913
Loss at iteration 490 : 0.08116544783115387
Loss at iteration 500 : 0.09027732908725739
Loss at iteration 510 : 0.06778712570667267
Loss at iteration 520 : 0.10027110576629639
Loss at iteration 530 : 0.0692574679851532
Loss at iteration 540 : 0.0956769809126854
Loss at iteration 550 : 0.06403224915266037
Loss at iteration 560 : 0.08759377896785736
Loss at iteration 570 : 0.11606016010046005
Loss at iteration 580 : 0.07182790338993073
Loss at iteration 590 : 0.07656396925449371
Loss at iteration 600 : 0.0845031589269638
Loss at iteration 610 : 0.11797235906124115
Loss at iteration 620 : 0.096372589468956
Loss at iteration 630 : 0.06289758533239365
Loss at iteration 640 : 0.05718234181404114
Loss at iteration 650 : 0.07861003279685974
Loss at iteration 660 : 0.12494176626205444
Loss at iteration 670 : 0.07389338314533234
Loss at iteration 680 : 0.11081337928771973
Loss at iteration 690 : 0.07593122124671936
Loss at iteration 700 : 0.0861043855547905
Loss at iteration 710 : 0.07392828911542892
Loss at iteration 720 : 0.06826847791671753
Loss at iteration 730 : 0.09805025160312653
Loss at iteration 740 : 0.051985591650009155
Loss at iteration 750 : 0.07097332179546356
Loss at iteration 760 : 0.09575706720352173
Loss at iteration 770 : 0.06503674387931824
Loss at iteration 780 : 0.04776296019554138
Loss at iteration 790 : 0.10879538953304291
Loss at iteration 800 : 0.11419139802455902
Loss at iteration 810 : 0.07137942314147949
Loss at iteration 820 : 0.07162962853908539
Loss at iteration 830 : 0.08340711891651154
Loss at iteration 840 : 0.06721397489309311
Loss at iteration 850 : 0.14498305320739746
Loss at iteration 860 : 0.08808388561010361
Loss at iteration 870 : 0.12580329179763794
Loss at iteration 880 : 0.09229924529790878
Loss at iteration 890 : 0.07788342237472534
Loss at iteration 900 : 0.1436491310596466
Loss at iteration 910 : 0.06414413452148438
Loss at iteration 920 : 0.09047858417034149
Loss at iteration 930 : 0.06788162887096405
Loss at iteration 940 : 0.07125085592269897
Loss at iteration 950 : 0.12308716773986816
Loss at iteration 960 : 0.10573627799749374
Loss at iteration 970 : 0.0886363610625267
Loss at iteration 980 : 0.10167990624904633
Loss at iteration 990 : 0.07950355112552643
Loss at iteration 1000 : 0.13278143107891083
Loss at iteration 1010 : 0.07196447253227234
Loss at iteration 1020 : 0.04613028094172478
Loss at iteration 1030 : 0.09360665082931519
Loss at iteration 1040 : 0.051868077367544174
Loss at iteration 1050 : 0.11563523858785629
Loss at iteration 1060 : 0.07451317459344864
Loss at iteration 1070 : 0.09436285495758057
Loss at iteration 1080 : 0.0386112779378891
Loss at iteration 1090 : 0.07062903046607971
Loss at iteration 1100 : 0.05152612179517746
Loss at iteration 1110 : 0.12040267884731293
Loss at iteration 1120 : 0.06440936774015427
Loss at iteration 1130 : 0.09323480725288391
Loss at iteration 1140 : 0.09442144632339478
Loss at iteration 1150 : 0.09620174020528793
Loss at iteration 1160 : 0.12332382798194885
Loss at iteration 1170 : 0.06676287204027176
Loss at iteration 1180 : 0.06298164278268814
Loss at iteration 1190 : 0.08276838064193726
Loss at iteration 1200 : 0.11099236458539963
Loss at iteration 1210 : 0.06259960681200027
The SSIM Value is: 0.6962211569150288
The PSNR Value is: 20.7669953028361
the epoch is: 53
Loss at iteration 10 : 0.08486229181289673
Loss at iteration 20 : 0.09608471393585205
Loss at iteration 30 : 0.10451730340719223
Loss at iteration 40 : 0.08627726137638092
Loss at iteration 50 : 0.03644264489412308
Loss at iteration 60 : 0.09236910194158554
Loss at iteration 70 : 0.0970313772559166
Loss at iteration 80 : 0.10967224836349487
Loss at iteration 90 : 0.07334195822477341
Loss at iteration 100 : 0.10685235261917114
Loss at iteration 110 : 0.09858918190002441
Loss at iteration 120 : 0.058856017887592316
Loss at iteration 130 : 0.09338464587926865
Loss at iteration 140 : 0.0902397483587265
Loss at iteration 150 : 0.07956191897392273
Loss at iteration 160 : 0.09289941191673279
Loss at iteration 170 : 0.08688764274120331
Loss at iteration 180 : 0.16371889412403107
Loss at iteration 190 : 0.10319985449314117
Loss at iteration 200 : 0.06964420527219772
Loss at iteration 210 : 0.05939934775233269
Loss at iteration 220 : 0.07491766661405563
Loss at iteration 230 : 0.07586369663476944
Loss at iteration 240 : 0.1147921234369278
Loss at iteration 250 : 0.1389072835445404
Loss at iteration 260 : 0.06404997408390045
Loss at iteration 270 : 0.08675451576709747
Loss at iteration 280 : 0.1139340028166771
Loss at iteration 290 : 0.06023354083299637
Loss at iteration 300 : 0.09163208305835724
Loss at iteration 310 : 0.09906014055013657
Loss at iteration 320 : 0.0664559155702591
Loss at iteration 330 : 0.055254410952329636
Loss at iteration 340 : 0.10831612348556519
Loss at iteration 350 : 0.08813493698835373
Loss at iteration 360 : 0.09514804929494858
Loss at iteration 370 : 0.05946783348917961
Loss at iteration 380 : 0.0899600237607956
Loss at iteration 390 : 0.07939337193965912
Loss at iteration 400 : 0.04201052710413933
Loss at iteration 410 : 0.0941551923751831
Loss at iteration 420 : 0.06860946118831635
Loss at iteration 430 : 0.051186561584472656
Loss at iteration 440 : 0.06098484992980957
Loss at iteration 450 : 0.0709753930568695
Loss at iteration 460 : 0.09997403621673584
Loss at iteration 470 : 0.0944192111492157
Loss at iteration 480 : 0.11665453761816025
Loss at iteration 490 : 0.12213009595870972
Loss at iteration 500 : 0.11794164031744003
Loss at iteration 510 : 0.07621577382087708
Loss at iteration 520 : 0.1149940937757492
Loss at iteration 530 : 0.08894871920347214
Loss at iteration 540 : 0.08932505548000336
Loss at iteration 550 : 0.06231778860092163
Loss at iteration 560 : 0.08727706968784332
Loss at iteration 570 : 0.0562259815633297
Loss at iteration 580 : 0.13921886682510376
Loss at iteration 590 : 0.09438644349575043
Loss at iteration 600 : 0.06960593163967133
Loss at iteration 610 : 0.07247523963451385
Loss at iteration 620 : 0.11658032983541489
Loss at iteration 630 : 0.07545854896306992
Loss at iteration 640 : 0.04469874128699303
Loss at iteration 650 : 0.058662496507167816
Loss at iteration 660 : 0.08116313815116882
Loss at iteration 670 : 0.090665802359581
Loss at iteration 680 : 0.07816662639379501
Loss at iteration 690 : 0.09150807559490204
Loss at iteration 700 : 0.1136319488286972
Loss at iteration 710 : 0.08303005993366241
Loss at iteration 720 : 0.08735550940036774
Loss at iteration 730 : 0.05995449051260948
Loss at iteration 740 : 0.10290392488241196
Loss at iteration 750 : 0.10199861228466034
Loss at iteration 760 : 0.0824279859662056
Loss at iteration 770 : 0.053028639405965805
Loss at iteration 780 : 0.06546737998723984
Loss at iteration 790 : 0.0950838178396225
Loss at iteration 800 : 0.1122962087392807
Loss at iteration 810 : 0.07616712152957916
Loss at iteration 820 : 0.10235389322042465
Loss at iteration 830 : 0.08573033660650253
Loss at iteration 840 : 0.11004573106765747
Loss at iteration 850 : 0.07588951289653778
Loss at iteration 860 : 0.11215144395828247
Loss at iteration 870 : 0.07853695005178452
Loss at iteration 880 : 0.0740656852722168
Loss at iteration 890 : 0.10658144950866699
Loss at iteration 900 : 0.10889303684234619
Loss at iteration 910 : 0.1241145133972168
Loss at iteration 920 : 0.05414023995399475
Loss at iteration 930 : 0.12339316308498383
Loss at iteration 940 : 0.09515164792537689
Loss at iteration 950 : 0.1035800501704216
Loss at iteration 960 : 0.11221124231815338
Loss at iteration 970 : 0.071672722697258
Loss at iteration 980 : 0.06254838407039642
Loss at iteration 990 : 0.07967625558376312
Loss at iteration 1000 : 0.07301092147827148
Loss at iteration 1010 : 0.11042505502700806
Loss at iteration 1020 : 0.09100663661956787
Loss at iteration 1030 : 0.07183082401752472
Loss at iteration 1040 : 0.06764614582061768
Loss at iteration 1050 : 0.08261226117610931
Loss at iteration 1060 : 0.09433968365192413
Loss at iteration 1070 : 0.07817744463682175
Loss at iteration 1080 : 0.08656683564186096
Loss at iteration 1090 : 0.09337341785430908
Loss at iteration 1100 : 0.04378927871584892
Loss at iteration 1110 : 0.07586400955915451
Loss at iteration 1120 : 0.10416441410779953
Loss at iteration 1130 : 0.14023636281490326
Loss at iteration 1140 : 0.06068561598658562
Loss at iteration 1150 : 0.06416793167591095
Loss at iteration 1160 : 0.06424744427204132
Loss at iteration 1170 : 0.05132408067584038
Loss at iteration 1180 : 0.07475891709327698
Loss at iteration 1190 : 0.1030198335647583
Loss at iteration 1200 : 0.04690272733569145
Loss at iteration 1210 : 0.09714655578136444
The SSIM Value is: 0.6978025436401367
The PSNR Value is: 20.744206364949545
the epoch is: 54
Loss at iteration 10 : 0.07235822081565857
Loss at iteration 20 : 0.07953207939863205
Loss at iteration 30 : 0.08597221225500107
Loss at iteration 40 : 0.1283355951309204
Loss at iteration 50 : 0.1027410626411438
Loss at iteration 60 : 0.09220816940069199
Loss at iteration 70 : 0.10275731980800629
Loss at iteration 80 : 0.04189562797546387
Loss at iteration 90 : 0.10373888909816742
Loss at iteration 100 : 0.0868062973022461
Loss at iteration 110 : 0.07112841308116913
Loss at iteration 120 : 0.17646244168281555
Loss at iteration 130 : 0.11240915954113007
Loss at iteration 140 : 0.061833374202251434
Loss at iteration 150 : 0.06631210446357727
Loss at iteration 160 : 0.1273539960384369
Loss at iteration 170 : 0.07548181712627411
Loss at iteration 180 : 0.12983623147010803
Loss at iteration 190 : 0.12398388236761093
Loss at iteration 200 : 0.08865487575531006
Loss at iteration 210 : 0.11526206135749817
Loss at iteration 220 : 0.09363742172718048
Loss at iteration 230 : 0.07910358905792236
Loss at iteration 240 : 0.08001010864973068
Loss at iteration 250 : 0.09955596923828125
Loss at iteration 260 : 0.08471056818962097
Loss at iteration 270 : 0.0633745938539505
Loss at iteration 280 : 0.11062806844711304
Loss at iteration 290 : 0.13448861241340637
Loss at iteration 300 : 0.11142885684967041
Loss at iteration 310 : 0.049902379512786865
Loss at iteration 320 : 0.08308380842208862
Loss at iteration 330 : 0.09063661098480225
Loss at iteration 340 : 0.060640495270490646
Loss at iteration 350 : 0.07419219613075256
Loss at iteration 360 : 0.08304835110902786
Loss at iteration 370 : 0.09723763167858124
Loss at iteration 380 : 0.12067832797765732
Loss at iteration 390 : 0.09500938653945923
Loss at iteration 400 : 0.06759117543697357
Loss at iteration 410 : 0.07225232571363449
Loss at iteration 420 : 0.07476154714822769
Loss at iteration 430 : 0.13523617386817932
Loss at iteration 440 : 0.10683763027191162
Loss at iteration 450 : 0.07651063799858093
Loss at iteration 460 : 0.1738867610692978
Loss at iteration 470 : 0.08047923445701599
Loss at iteration 480 : 0.10863500833511353
Loss at iteration 490 : 0.05126332491636276
Loss at iteration 500 : 0.056502994149923325
Loss at iteration 510 : 0.11660590022802353
Loss at iteration 520 : 0.07150149345397949
Loss at iteration 530 : 0.09155955910682678
Loss at iteration 540 : 0.05984506011009216
Loss at iteration 550 : 0.10813512653112411
Loss at iteration 560 : 0.0758543312549591
Loss at iteration 570 : 0.07911669462919235
Loss at iteration 580 : 0.06840864568948746
Loss at iteration 590 : 0.08225784450769424
Loss at iteration 600 : 0.14670000970363617
Loss at iteration 610 : 0.08962813019752502
Loss at iteration 620 : 0.10960213840007782
Loss at iteration 630 : 0.11989711225032806
Loss at iteration 640 : 0.11895205080509186
Loss at iteration 650 : 0.1134490817785263
Loss at iteration 660 : 0.08332912623882294
Loss at iteration 670 : 0.07620785385370255
Loss at iteration 680 : 0.11532646417617798
Loss at iteration 690 : 0.07391608506441116
Loss at iteration 700 : 0.07001098990440369
Loss at iteration 710 : 0.09877942502498627
Loss at iteration 720 : 0.07557457685470581
Loss at iteration 730 : 0.10114197432994843
Loss at iteration 740 : 0.06375911086797714
Loss at iteration 750 : 0.09297727793455124
Loss at iteration 760 : 0.110679492354393
Loss at iteration 770 : 0.05403914302587509
Loss at iteration 780 : 0.13565421104431152
Loss at iteration 790 : 0.07489211857318878
Loss at iteration 800 : 0.0713207870721817
Loss at iteration 810 : 0.10493594408035278
Loss at iteration 820 : 0.08167648315429688
Loss at iteration 830 : 0.05174238607287407
Loss at iteration 840 : 0.0903543159365654
Loss at iteration 850 : 0.11138889193534851
Loss at iteration 860 : 0.05665503442287445
Loss at iteration 870 : 0.11474578827619553
Loss at iteration 880 : 0.04352278262376785
Loss at iteration 890 : 0.12287649512290955
Loss at iteration 900 : 0.08350996673107147
Loss at iteration 910 : 0.10552903264760971
Loss at iteration 920 : 0.12238709628582001
Loss at iteration 930 : 0.08010651171207428
Loss at iteration 940 : 0.0972461849451065
Loss at iteration 950 : 0.0502014085650444
Loss at iteration 960 : 0.06931447237730026
Loss at iteration 970 : 0.0687226876616478
Loss at iteration 980 : 0.07720355689525604
Loss at iteration 990 : 0.056402526795864105
Loss at iteration 1000 : 0.10963545739650726
Loss at iteration 1010 : 0.09536638855934143
Loss at iteration 1020 : 0.07014807313680649
Loss at iteration 1030 : 0.11542224884033203
Loss at iteration 1040 : 0.1061827540397644
Loss at iteration 1050 : 0.058802198618650436
Loss at iteration 1060 : 0.10158254951238632
Loss at iteration 1070 : 0.03969044238328934
Loss at iteration 1080 : 0.09771328419446945
Loss at iteration 1090 : 0.10225661098957062
Loss at iteration 1100 : 0.07765249162912369
Loss at iteration 1110 : 0.099403977394104
Loss at iteration 1120 : 0.09151913970708847
Loss at iteration 1130 : 0.06601747870445251
Loss at iteration 1140 : 0.09748321771621704
Loss at iteration 1150 : 0.057922422885894775
Loss at iteration 1160 : 0.05339353531599045
Loss at iteration 1170 : 0.09492921084165573
Loss at iteration 1180 : 0.07242228090763092
Loss at iteration 1190 : 0.09876459836959839
Loss at iteration 1200 : 0.06796157360076904
Loss at iteration 1210 : 0.08681933581829071
The SSIM Value is: 0.6978204925855
The PSNR Value is: 20.597031275431316
the epoch is: 55
Loss at iteration 10 : 0.11246031522750854
Loss at iteration 20 : 0.10742368549108505
Loss at iteration 30 : 0.10869999974966049
Loss at iteration 40 : 0.08615609258413315
Loss at iteration 50 : 0.11206122487783432
Loss at iteration 60 : 0.08869527280330658
Loss at iteration 70 : 0.0757150948047638
Loss at iteration 80 : 0.08203300088644028
Loss at iteration 90 : 0.10154217481613159
Loss at iteration 100 : 0.07771805673837662
Loss at iteration 110 : 0.07181128859519958
Loss at iteration 120 : 0.07441720366477966
Loss at iteration 130 : 0.0969037339091301
Loss at iteration 140 : 0.11579246819019318
Loss at iteration 150 : 0.0608847551047802
Loss at iteration 160 : 0.10962124913930893
Loss at iteration 170 : 0.08962351083755493
Loss at iteration 180 : 0.09145817160606384
Loss at iteration 190 : 0.07940101623535156
Loss at iteration 200 : 0.07597898691892624
Loss at iteration 210 : 0.06871556490659714
Loss at iteration 220 : 0.06744349002838135
Loss at iteration 230 : 0.09282408654689789
Loss at iteration 240 : 0.09787418693304062
Loss at iteration 250 : 0.09626437723636627
Loss at iteration 260 : 0.09390362352132797
Loss at iteration 270 : 0.0923762172460556
Loss at iteration 280 : 0.13293075561523438
Loss at iteration 290 : 0.07260870188474655
Loss at iteration 300 : 0.09623291343450546
Loss at iteration 310 : 0.08332502841949463
Loss at iteration 320 : 0.08529665321111679
Loss at iteration 330 : 0.12617510557174683
Loss at iteration 340 : 0.042520131915807724
Loss at iteration 350 : 0.12290601432323456
Loss at iteration 360 : 0.08029543608427048
Loss at iteration 370 : 0.06970464438199997
Loss at iteration 380 : 0.08698111772537231
Loss at iteration 390 : 0.09581812471151352
Loss at iteration 400 : 0.06292645633220673
Loss at iteration 410 : 0.11048375070095062
Loss at iteration 420 : 0.09416317194700241
Loss at iteration 430 : 0.07457861304283142
Loss at iteration 440 : 0.09832847118377686
Loss at iteration 450 : 0.06129433959722519
Loss at iteration 460 : 0.12320967763662338
Loss at iteration 470 : 0.09077869355678558
Loss at iteration 480 : 0.07985269278287888
Loss at iteration 490 : 0.08494099229574203
Loss at iteration 500 : 0.12321710586547852
Loss at iteration 510 : 0.08960972726345062
Loss at iteration 520 : 0.06378611922264099
Loss at iteration 530 : 0.12783437967300415
Loss at iteration 540 : 0.08375135064125061
Loss at iteration 550 : 0.06455308198928833
Loss at iteration 560 : 0.06568163633346558
Loss at iteration 570 : 0.055887527763843536
Loss at iteration 580 : 0.15445636212825775
Loss at iteration 590 : 0.11807435750961304
Loss at iteration 600 : 0.11085532605648041
Loss at iteration 610 : 0.09875234961509705
Loss at iteration 620 : 0.10796640813350677
Loss at iteration 630 : 0.062114160507917404
Loss at iteration 640 : 0.11816702783107758
Loss at iteration 650 : 0.045029424130916595
Loss at iteration 660 : 0.07758575677871704
Loss at iteration 670 : 0.08163411915302277
Loss at iteration 680 : 0.06910541653633118
Loss at iteration 690 : 0.057990677654743195
Loss at iteration 700 : 0.10332755744457245
Loss at iteration 710 : 0.041262801736593246
Loss at iteration 720 : 0.14936479926109314
Loss at iteration 730 : 0.07769721746444702
Loss at iteration 740 : 0.08841335773468018
Loss at iteration 750 : 0.05913718789815903
Loss at iteration 760 : 0.08362000435590744
Loss at iteration 770 : 0.07695107161998749
Loss at iteration 780 : 0.07182270288467407
Loss at iteration 790 : 0.07981400936841965
Loss at iteration 800 : 0.07465733587741852
Loss at iteration 810 : 0.08116013556718826
Loss at iteration 820 : 0.06624223291873932
Loss at iteration 830 : 0.06538230180740356
Loss at iteration 840 : 0.10594169795513153
Loss at iteration 850 : 0.05360236391425133
Loss at iteration 860 : 0.09446735680103302
Loss at iteration 870 : 0.06846106052398682
Loss at iteration 880 : 0.15113936364650726
Loss at iteration 890 : 0.09181839227676392
Loss at iteration 900 : 0.11550977826118469
Loss at iteration 910 : 0.20041309297084808
Loss at iteration 920 : 0.09258201718330383
Loss at iteration 930 : 0.08117431402206421
Loss at iteration 940 : 0.11129137128591537
Loss at iteration 950 : 0.08608353137969971
Loss at iteration 960 : 0.06792100518941879
Loss at iteration 970 : 0.08650746941566467
Loss at iteration 980 : 0.08538627624511719
Loss at iteration 990 : 0.0854308232665062
Loss at iteration 1000 : 0.08211464434862137
Loss at iteration 1010 : 0.13141880929470062
Loss at iteration 1020 : 0.09986548125743866
Loss at iteration 1030 : 0.08896320313215256
Loss at iteration 1040 : 0.07845999300479889
Loss at iteration 1050 : 0.1442846953868866
Loss at iteration 1060 : 0.11157022416591644
Loss at iteration 1070 : 0.07686355710029602
Loss at iteration 1080 : 0.11293232440948486
Loss at iteration 1090 : 0.09212030470371246
Loss at iteration 1100 : 0.05171738192439079
Loss at iteration 1110 : 0.06548279523849487
Loss at iteration 1120 : 0.08446373790502548
Loss at iteration 1130 : 0.11399804055690765
Loss at iteration 1140 : 0.09244251251220703
Loss at iteration 1150 : 0.11105839908123016
Loss at iteration 1160 : 0.06616068631410599
Loss at iteration 1170 : 0.1074453741312027
Loss at iteration 1180 : 0.09736986458301544
Loss at iteration 1190 : 0.1071203202009201
Loss at iteration 1200 : 0.09780800342559814
Loss at iteration 1210 : 0.07186158001422882
The SSIM Value is: 0.6980422178904215
The PSNR Value is: 20.80241476694743
the epoch is: 56
Loss at iteration 10 : 0.08415646851062775
Loss at iteration 20 : 0.11991614103317261
Loss at iteration 30 : 0.0832737535238266
Loss at iteration 40 : 0.12745244801044464
Loss at iteration 50 : 0.10096171498298645
Loss at iteration 60 : 0.09810614585876465
Loss at iteration 70 : 0.10890604555606842
Loss at iteration 80 : 0.08442725986242294
Loss at iteration 90 : 0.07181303948163986
Loss at iteration 100 : 0.11179113388061523
Loss at iteration 110 : 0.09826511144638062
Loss at iteration 120 : 0.10621599853038788
Loss at iteration 130 : 0.10728675127029419
Loss at iteration 140 : 0.09534710645675659
Loss at iteration 150 : 0.07443951070308685
Loss at iteration 160 : 0.04836930334568024
Loss at iteration 170 : 0.0890258178114891
Loss at iteration 180 : 0.09191223233938217
Loss at iteration 190 : 0.10187431424856186
Loss at iteration 200 : 0.05314067751169205
Loss at iteration 210 : 0.11661531031131744
Loss at iteration 220 : 0.05910097807645798
Loss at iteration 230 : 0.07702672481536865
Loss at iteration 240 : 0.10713882744312286
Loss at iteration 250 : 0.04349883645772934
Loss at iteration 260 : 0.092501699924469
Loss at iteration 270 : 0.04803849384188652
Loss at iteration 280 : 0.06321357190608978
Loss at iteration 290 : 0.09183483570814133
Loss at iteration 300 : 0.115410715341568
Loss at iteration 310 : 0.08123882859945297
Loss at iteration 320 : 0.07646935433149338
Loss at iteration 330 : 0.06926552206277847
Loss at iteration 340 : 0.047697946429252625
Loss at iteration 350 : 0.1118057370185852
Loss at iteration 360 : 0.09443910419940948
Loss at iteration 370 : 0.050281643867492676
Loss at iteration 380 : 0.0764351487159729
Loss at iteration 390 : 0.11108744144439697
Loss at iteration 400 : 0.09822820872068405
Loss at iteration 410 : 0.08726155757904053
Loss at iteration 420 : 0.1334374099969864
Loss at iteration 430 : 0.07504371553659439
Loss at iteration 440 : 0.061216630041599274
Loss at iteration 450 : 0.10200846195220947
Loss at iteration 460 : 0.10034690797328949
Loss at iteration 470 : 0.10605137795209885
Loss at iteration 480 : 0.12468075752258301
Loss at iteration 490 : 0.08672074973583221
Loss at iteration 500 : 0.10952836275100708
Loss at iteration 510 : 0.1071079671382904
Loss at iteration 520 : 0.07733365893363953
Loss at iteration 530 : 0.08219891786575317
Loss at iteration 540 : 0.06535052508115768
Loss at iteration 550 : 0.0834774598479271
Loss at iteration 560 : 0.06180925294756889
Loss at iteration 570 : 0.09498970210552216
Loss at iteration 580 : 0.07749193906784058
Loss at iteration 590 : 0.16029080748558044
Loss at iteration 600 : 0.05661578103899956
Loss at iteration 610 : 0.1175181120634079
Loss at iteration 620 : 0.06703387200832367
Loss at iteration 630 : 0.11066874861717224
Loss at iteration 640 : 0.13097451627254486
Loss at iteration 650 : 0.11530330032110214
Loss at iteration 660 : 0.0950256809592247
Loss at iteration 670 : 0.09943528473377228
Loss at iteration 680 : 0.09711600840091705
Loss at iteration 690 : 0.055387064814567566
Loss at iteration 700 : 0.15288648009300232
Loss at iteration 710 : 0.05749737471342087
Loss at iteration 720 : 0.08484813570976257
Loss at iteration 730 : 0.0957176685333252
Loss at iteration 740 : 0.0734127014875412
Loss at iteration 750 : 0.10271257162094116
Loss at iteration 760 : 0.1089055985212326
Loss at iteration 770 : 0.13881446421146393
Loss at iteration 780 : 0.0710354596376419
Loss at iteration 790 : 0.085519939661026
Loss at iteration 800 : 0.06333031505346298
Loss at iteration 810 : 0.131341814994812
Loss at iteration 820 : 0.07310160249471664
Loss at iteration 830 : 0.11022293567657471
Loss at iteration 840 : 0.06765329092741013
Loss at iteration 850 : 0.09980800002813339
Loss at iteration 860 : 0.09122620522975922
Loss at iteration 870 : 0.09098919481039047
Loss at iteration 880 : 0.10580141842365265
Loss at iteration 890 : 0.05727101489901543
Loss at iteration 900 : 0.09669025242328644
Loss at iteration 910 : 0.1494549810886383
Loss at iteration 920 : 0.08759307861328125
Loss at iteration 930 : 0.05035295709967613
Loss at iteration 940 : 0.1193373054265976
Loss at iteration 950 : 0.09898023307323456
Loss at iteration 960 : 0.1047542542219162
Loss at iteration 970 : 0.07003673166036606
Loss at iteration 980 : 0.10492300242185593
Loss at iteration 990 : 0.07505298405885696
Loss at iteration 1000 : 0.0730966180562973
Loss at iteration 1010 : 0.09258852899074554
Loss at iteration 1020 : 0.10034064948558807
Loss at iteration 1030 : 0.09786537289619446
Loss at iteration 1040 : 0.0935080349445343
Loss at iteration 1050 : 0.06331653892993927
Loss at iteration 1060 : 0.10029798746109009
Loss at iteration 1070 : 0.07130949199199677
Loss at iteration 1080 : 0.12187676131725311
Loss at iteration 1090 : 0.06813347339630127
Loss at iteration 1100 : 0.0763079971075058
Loss at iteration 1110 : 0.06666681915521622
Loss at iteration 1120 : 0.09397279471158981
Loss at iteration 1130 : 0.09854815900325775
Loss at iteration 1140 : 0.07376724481582642
Loss at iteration 1150 : 0.07088084518909454
Loss at iteration 1160 : 0.0647805780172348
Loss at iteration 1170 : 0.1007392406463623
Loss at iteration 1180 : 0.07644842565059662
Loss at iteration 1190 : 0.09708293527364731
Loss at iteration 1200 : 0.09556889533996582
Loss at iteration 1210 : 0.10242629051208496
The SSIM Value is: 0.708680780728658
The PSNR Value is: 20.944986979166668
the epoch is: 57
Loss at iteration 10 : 0.05657724663615227
Loss at iteration 20 : 0.09326134622097015
Loss at iteration 30 : 0.13966165482997894
Loss at iteration 40 : 0.0788777768611908
Loss at iteration 50 : 0.10846659541130066
Loss at iteration 60 : 0.08171933144330978
Loss at iteration 70 : 0.07977917790412903
Loss at iteration 80 : 0.091181680560112
Loss at iteration 90 : 0.12285465002059937
Loss at iteration 100 : 0.03803527355194092
Loss at iteration 110 : 0.11199461668729782
Loss at iteration 120 : 0.08678770065307617
Loss at iteration 130 : 0.09225746989250183
Loss at iteration 140 : 0.06297175586223602
Loss at iteration 150 : 0.08010995388031006
Loss at iteration 160 : 0.06165313720703125
Loss at iteration 170 : 0.10793697834014893
Loss at iteration 180 : 0.07768826186656952
Loss at iteration 190 : 0.06926040351390839
Loss at iteration 200 : 0.09460075944662094
Loss at iteration 210 : 0.08590084314346313
Loss at iteration 220 : 0.06896388530731201
Loss at iteration 230 : 0.15109364688396454
Loss at iteration 240 : 0.1324136108160019
Loss at iteration 250 : 0.06012313812971115
Loss at iteration 260 : 0.09614098072052002
Loss at iteration 270 : 0.08186683058738708
Loss at iteration 280 : 0.086497001349926
Loss at iteration 290 : 0.06988810002803802
Loss at iteration 300 : 0.11104383319616318
Loss at iteration 310 : 0.06448091566562653
Loss at iteration 320 : 0.09966246783733368
Loss at iteration 330 : 0.09975077956914902
Loss at iteration 340 : 0.09264036267995834
Loss at iteration 350 : 0.08269545435905457
Loss at iteration 360 : 0.11010009050369263
Loss at iteration 370 : 0.06883911043405533
Loss at iteration 380 : 0.05362650007009506
Loss at iteration 390 : 0.0641990602016449
Loss at iteration 400 : 0.0793110653758049
Loss at iteration 410 : 0.08509421348571777
Loss at iteration 420 : 0.10437579452991486
Loss at iteration 430 : 0.08064910769462585
Loss at iteration 440 : 0.11019125580787659
Loss at iteration 450 : 0.09775684773921967
Loss at iteration 460 : 0.09016776084899902
Loss at iteration 470 : 0.07122816145420074
Loss at iteration 480 : 0.07780438661575317
Loss at iteration 490 : 0.10402386635541916
Loss at iteration 500 : 0.06466645002365112
Loss at iteration 510 : 0.11854337900876999
Loss at iteration 520 : 0.12087979167699814
Loss at iteration 530 : 0.053410761058330536
Loss at iteration 540 : 0.08221182227134705
Loss at iteration 550 : 0.09681491553783417
Loss at iteration 560 : 0.12008796632289886
Loss at iteration 570 : 0.1071183905005455
Loss at iteration 580 : 0.0843525156378746
Loss at iteration 590 : 0.1139879822731018
Loss at iteration 600 : 0.12721110880374908
Loss at iteration 610 : 0.08487635105848312
Loss at iteration 620 : 0.08920513093471527
Loss at iteration 630 : 0.06114707514643669
Loss at iteration 640 : 0.10036329925060272
Loss at iteration 650 : 0.10007040947675705
Loss at iteration 660 : 0.08520656079053879
Loss at iteration 670 : 0.1062718853354454
Loss at iteration 680 : 0.08263856172561646
Loss at iteration 690 : 0.11208270490169525
Loss at iteration 700 : 0.07146775722503662
Loss at iteration 710 : 0.053493231534957886
Loss at iteration 720 : 0.12935656309127808
Loss at iteration 730 : 0.05993837118148804
Loss at iteration 740 : 0.06562833487987518
Loss at iteration 750 : 0.09042581915855408
Loss at iteration 760 : 0.10766448825597763
Loss at iteration 770 : 0.07870110124349594
Loss at iteration 780 : 0.0630359873175621
Loss at iteration 790 : 0.09098853915929794
Loss at iteration 800 : 0.09985063970088959
Loss at iteration 810 : 0.07357262074947357
Loss at iteration 820 : 0.11400710791349411
Loss at iteration 830 : 0.08603086322546005
Loss at iteration 840 : 0.063414566218853
Loss at iteration 850 : 0.09105770289897919
Loss at iteration 860 : 0.08116891980171204
Loss at iteration 870 : 0.07398498058319092
Loss at iteration 880 : 0.07861151546239853
Loss at iteration 890 : 0.10344468057155609
Loss at iteration 900 : 0.0715673640370369
Loss at iteration 910 : 0.11029120534658432
Loss at iteration 920 : 0.09592825174331665
Loss at iteration 930 : 0.1030430719256401
Loss at iteration 940 : 0.09291218221187592
Loss at iteration 950 : 0.088163822889328
Loss at iteration 960 : 0.09081226587295532
Loss at iteration 970 : 0.09533758461475372
Loss at iteration 980 : 0.05426909774541855
Loss at iteration 990 : 0.09323035180568695
Loss at iteration 1000 : 0.08053994178771973
Loss at iteration 1010 : 0.09608978033065796
Loss at iteration 1020 : 0.0824376568198204
Loss at iteration 1030 : 0.09687639772891998
Loss at iteration 1040 : 0.08864937722682953
Loss at iteration 1050 : 0.08833371102809906
Loss at iteration 1060 : 0.16340266168117523
Loss at iteration 1070 : 0.04430844634771347
Loss at iteration 1080 : 0.06915734708309174
Loss at iteration 1090 : 0.05974309891462326
Loss at iteration 1100 : 0.1186174750328064
Loss at iteration 1110 : 0.1325734257698059
Loss at iteration 1120 : 0.08376600593328476
Loss at iteration 1130 : 0.07001513242721558
Loss at iteration 1140 : 0.06516745686531067
Loss at iteration 1150 : 0.10791520774364471
Loss at iteration 1160 : 0.05266082286834717
Loss at iteration 1170 : 0.08248736709356308
Loss at iteration 1180 : 0.0572882704436779
Loss at iteration 1190 : 0.1387222707271576
Loss at iteration 1200 : 0.09198920428752899
Loss at iteration 1210 : 0.0816720724105835
The SSIM Value is: 0.7050987323125203
The PSNR Value is: 21.013731066385905
the epoch is: 58
Loss at iteration 10 : 0.08716561645269394
Loss at iteration 20 : 0.06935729831457138
Loss at iteration 30 : 0.10970701277256012
Loss at iteration 40 : 0.06776446104049683
Loss at iteration 50 : 0.04900594800710678
Loss at iteration 60 : 0.10423281043767929
Loss at iteration 70 : 0.1442432999610901
Loss at iteration 80 : 0.08432857692241669
Loss at iteration 90 : 0.09486362338066101
Loss at iteration 100 : 0.09232725948095322
Loss at iteration 110 : 0.09188475459814072
Loss at iteration 120 : 0.05788668245077133
Loss at iteration 130 : 0.07706913352012634
Loss at iteration 140 : 0.11351586878299713
Loss at iteration 150 : 0.09087137877941132
Loss at iteration 160 : 0.06729774177074432
Loss at iteration 170 : 0.06115715950727463
Loss at iteration 180 : 0.0836571455001831
Loss at iteration 190 : 0.08699069917201996
Loss at iteration 200 : 0.1035611629486084
Loss at iteration 210 : 0.08908283710479736
Loss at iteration 220 : 0.08639635890722275
Loss at iteration 230 : 0.08532817661762238
Loss at iteration 240 : 0.06567927449941635
Loss at iteration 250 : 0.11587601900100708
Loss at iteration 260 : 0.09771870821714401
Loss at iteration 270 : 0.09346326440572739
Loss at iteration 280 : 0.09233178943395615
Loss at iteration 290 : 0.0867757499217987
Loss at iteration 300 : 0.13375163078308105
Loss at iteration 310 : 0.09624242782592773
Loss at iteration 320 : 0.05800534039735794
Loss at iteration 330 : 0.072987399995327
Loss at iteration 340 : 0.0663924366235733
Loss at iteration 350 : 0.08181077986955643
Loss at iteration 360 : 0.06316356360912323
Loss at iteration 370 : 0.06062436103820801
Loss at iteration 380 : 0.08240444958209991
Loss at iteration 390 : 0.08959795534610748
Loss at iteration 400 : 0.11930632591247559
Loss at iteration 410 : 0.14612244069576263
Loss at iteration 420 : 0.1016930490732193
Loss at iteration 430 : 0.07764384150505066
Loss at iteration 440 : 0.11428111046552658
Loss at iteration 450 : 0.09502600133419037
Loss at iteration 460 : 0.08455215394496918
Loss at iteration 470 : 0.09058547019958496
Loss at iteration 480 : 0.08952467888593674
Loss at iteration 490 : 0.08704688400030136
Loss at iteration 500 : 0.10559912025928497
Loss at iteration 510 : 0.037926606833934784
Loss at iteration 520 : 0.10822035372257233
Loss at iteration 530 : 0.07609431445598602
Loss at iteration 540 : 0.06322509050369263
Loss at iteration 550 : 0.07507804036140442
Loss at iteration 560 : 0.12575796246528625
Loss at iteration 570 : 0.07640688866376877
Loss at iteration 580 : 0.07207459211349487
Loss at iteration 590 : 0.0573437437415123
Loss at iteration 600 : 0.09750641882419586
Loss at iteration 610 : 0.0802592933177948
Loss at iteration 620 : 0.05200762674212456
Loss at iteration 630 : 0.10493451356887817
Loss at iteration 640 : 0.05735859274864197
Loss at iteration 650 : 0.06988842785358429
Loss at iteration 660 : 0.06712625175714493
Loss at iteration 670 : 0.08809386193752289
Loss at iteration 680 : 0.10038986057043076
Loss at iteration 690 : 0.1266430765390396
Loss at iteration 700 : 0.06821504235267639
Loss at iteration 710 : 0.12975090742111206
Loss at iteration 720 : 0.07417534291744232
Loss at iteration 730 : 0.07069139927625656
Loss at iteration 740 : 0.09030742943286896
Loss at iteration 750 : 0.0931510254740715
Loss at iteration 760 : 0.07239929586648941
Loss at iteration 770 : 0.07401742041110992
Loss at iteration 780 : 0.10462398827075958
Loss at iteration 790 : 0.04878563806414604
Loss at iteration 800 : 0.0978744626045227
Loss at iteration 810 : 0.06047594174742699
Loss at iteration 820 : 0.091794952750206
Loss at iteration 830 : 0.09689014405012131
Loss at iteration 840 : 0.07347220927476883
Loss at iteration 850 : 0.07992719113826752
Loss at iteration 860 : 0.08947231620550156
Loss at iteration 870 : 0.07148060202598572
Loss at iteration 880 : 0.05725795030593872
Loss at iteration 890 : 0.11595448851585388
Loss at iteration 900 : 0.11129170656204224
Loss at iteration 910 : 0.08342073857784271
Loss at iteration 920 : 0.1262504607439041
Loss at iteration 930 : 0.08309803158044815
Loss at iteration 940 : 0.06078721582889557
Loss at iteration 950 : 0.14352929592132568
Loss at iteration 960 : 0.08846994489431381
Loss at iteration 970 : 0.05010577663779259
Loss at iteration 980 : 0.1227731704711914
Loss at iteration 990 : 0.09759347140789032
Loss at iteration 1000 : 0.11085343360900879
Loss at iteration 1010 : 0.07604287564754486
Loss at iteration 1020 : 0.10530402511358261
Loss at iteration 1030 : 0.05934751033782959
Loss at iteration 1040 : 0.08109486848115921
Loss at iteration 1050 : 0.11658476293087006
Loss at iteration 1060 : 0.09135545790195465
Loss at iteration 1070 : 0.0631885677576065
Loss at iteration 1080 : 0.06857211887836456
Loss at iteration 1090 : 0.07499631494283676
Loss at iteration 1100 : 0.08373047411441803
Loss at iteration 1110 : 0.08266960829496384
Loss at iteration 1120 : 0.11361414194107056
Loss at iteration 1130 : 0.10426782071590424
Loss at iteration 1140 : 0.06659804284572601
Loss at iteration 1150 : 0.056820571422576904
Loss at iteration 1160 : 0.07655303925275803
Loss at iteration 1170 : 0.11921592801809311
Loss at iteration 1180 : 0.05056142061948776
Loss at iteration 1190 : 0.06651826202869415
Loss at iteration 1200 : 0.059771161526441574
Loss at iteration 1210 : 0.08008953183889389
The SSIM Value is: 0.706345780690511
The PSNR Value is: 21.06314888000488
the epoch is: 59
Loss at iteration 10 : 0.07294663786888123
Loss at iteration 20 : 0.11449524015188217
Loss at iteration 30 : 0.11725892871618271
Loss at iteration 40 : 0.1247166246175766
Loss at iteration 50 : 0.09672726690769196
Loss at iteration 60 : 0.09894704818725586
Loss at iteration 70 : 0.0943392738699913
Loss at iteration 80 : 0.06937065720558167
Loss at iteration 90 : 0.08457234501838684
Loss at iteration 100 : 0.08593082427978516
Loss at iteration 110 : 0.06414163112640381
Loss at iteration 120 : 0.1375339925289154
Loss at iteration 130 : 0.1494453400373459
Loss at iteration 140 : 0.07091590762138367
Loss at iteration 150 : 0.11195108294487
Loss at iteration 160 : 0.07676061987876892
Loss at iteration 170 : 0.07882076501846313
Loss at iteration 180 : 0.07438437640666962
Loss at iteration 190 : 0.05691775679588318
Loss at iteration 200 : 0.10140229761600494
Loss at iteration 210 : 0.11409168690443039
Loss at iteration 220 : 0.08481401205062866
Loss at iteration 230 : 0.08294334262609482
Loss at iteration 240 : 0.16414839029312134
Loss at iteration 250 : 0.0669197291135788
Loss at iteration 260 : 0.07834228128194809
Loss at iteration 270 : 0.0681392177939415
Loss at iteration 280 : 0.10381655395030975
Loss at iteration 290 : 0.09093776345252991
Loss at iteration 300 : 0.04405643790960312
Loss at iteration 310 : 0.06746405363082886
Loss at iteration 320 : 0.09247997403144836
Loss at iteration 330 : 0.09532463550567627
Loss at iteration 340 : 0.04802138730883598
Loss at iteration 350 : 0.07257957756519318
Loss at iteration 360 : 0.0737004429101944
Loss at iteration 370 : 0.10617510974407196
Loss at iteration 380 : 0.08515795320272446
Loss at iteration 390 : 0.13172680139541626
Loss at iteration 400 : 0.06926874816417694
Loss at iteration 410 : 0.07040010392665863
Loss at iteration 420 : 0.1255820244550705
Loss at iteration 430 : 0.06721343100070953
Loss at iteration 440 : 0.09748122096061707
Loss at iteration 450 : 0.06675748527050018
Loss at iteration 460 : 0.1148005947470665
Loss at iteration 470 : 0.0794161856174469
Loss at iteration 480 : 0.11890174448490143
Loss at iteration 490 : 0.055374544113874435
Loss at iteration 500 : 0.1190568208694458
Loss at iteration 510 : 0.059440530836582184
Loss at iteration 520 : 0.1085360050201416
Loss at iteration 530 : 0.08831655979156494
Loss at iteration 540 : 0.0625031590461731
Loss at iteration 550 : 0.0812005028128624
Loss at iteration 560 : 0.08343568444252014
Loss at iteration 570 : 0.09662061929702759
Loss at iteration 580 : 0.07531648129224777
Loss at iteration 590 : 0.08234479278326035
Loss at iteration 600 : 0.13122284412384033
Loss at iteration 610 : 0.058169446885585785
Loss at iteration 620 : 0.05253526568412781
Loss at iteration 630 : 0.12858039140701294
Loss at iteration 640 : 0.08214762806892395
Loss at iteration 650 : 0.12262566387653351
Loss at iteration 660 : 0.07175806164741516
Loss at iteration 670 : 0.09822449088096619
Loss at iteration 680 : 0.08864898979663849
Loss at iteration 690 : 0.10292734205722809
Loss at iteration 700 : 0.1233253926038742
Loss at iteration 710 : 0.0868852362036705
Loss at iteration 720 : 0.05543091148138046
Loss at iteration 730 : 0.0889112800359726
Loss at iteration 740 : 0.06239849328994751
Loss at iteration 750 : 0.10426396131515503
Loss at iteration 760 : 0.08361829817295074
Loss at iteration 770 : 0.0712955966591835
Loss at iteration 780 : 0.08999743312597275
Loss at iteration 790 : 0.07290840893983841
Loss at iteration 800 : 0.06748471409082413
Loss at iteration 810 : 0.09327243268489838
Loss at iteration 820 : 0.1097801923751831
Loss at iteration 830 : 0.08549675345420837
Loss at iteration 840 : 0.06755506247282028
Loss at iteration 850 : 0.07725393772125244
Loss at iteration 860 : 0.12963859736919403
Loss at iteration 870 : 0.13065962493419647
Loss at iteration 880 : 0.1317555159330368
Loss at iteration 890 : 0.07808555662631989
Loss at iteration 900 : 0.0793241560459137
Loss at iteration 910 : 0.0731804296374321
Loss at iteration 920 : 0.06793563812971115
Loss at iteration 930 : 0.05642181262373924
Loss at iteration 940 : 0.07824935019016266
Loss at iteration 950 : 0.08661051094532013
Loss at iteration 960 : 0.12501856684684753
Loss at iteration 970 : 0.08734571188688278
Loss at iteration 980 : 0.06345195323228836
Loss at iteration 990 : 0.12161672115325928
Loss at iteration 1000 : 0.06138873100280762
Loss at iteration 1010 : 0.12690944969654083
Loss at iteration 1020 : 0.11144751310348511
Loss at iteration 1030 : 0.08287592232227325
Loss at iteration 1040 : 0.07311366498470306
Loss at iteration 1050 : 0.08251708745956421
Loss at iteration 1060 : 0.07472646236419678
Loss at iteration 1070 : 0.060797594487667084
Loss at iteration 1080 : 0.052468471229076385
Loss at iteration 1090 : 0.06552230566740036
Loss at iteration 1100 : 0.06468307971954346
Loss at iteration 1110 : 0.14119097590446472
Loss at iteration 1120 : 0.06463335454463959
Loss at iteration 1130 : 0.07551053166389465
Loss at iteration 1140 : 0.07724478095769882
Loss at iteration 1150 : 0.06690573692321777
Loss at iteration 1160 : 0.09204467386007309
Loss at iteration 1170 : 0.08306945115327835
Loss at iteration 1180 : 0.056112438440322876
Loss at iteration 1190 : 0.09631117433309555
Loss at iteration 1200 : 0.0960676521062851
Loss at iteration 1210 : 0.11723680794239044
The SSIM Value is: 0.6992023507754008
The PSNR Value is: 20.491984621683756
the epoch is: 60
Loss at iteration 10 : 0.07058984786272049
Loss at iteration 20 : 0.07477179169654846
Loss at iteration 30 : 0.09708432853221893
Loss at iteration 40 : 0.11343277990818024
Loss at iteration 50 : 0.058033496141433716
Loss at iteration 60 : 0.09563872218132019
Loss at iteration 70 : 0.05471450090408325
Loss at iteration 80 : 0.05093462020158768
Loss at iteration 90 : 0.08972581475973129
Loss at iteration 100 : 0.11748059839010239
Loss at iteration 110 : 0.07239121198654175
Loss at iteration 120 : 0.06177791208028793
Loss at iteration 130 : 0.08350922167301178
Loss at iteration 140 : 0.06804575771093369
Loss at iteration 150 : 0.08033385127782822
Loss at iteration 160 : 0.06544534116983414
Loss at iteration 170 : 0.07193482667207718
Loss at iteration 180 : 0.0805567130446434
Loss at iteration 190 : 0.07143373042345047
Loss at iteration 200 : 0.09851115942001343
Loss at iteration 210 : 0.08377958834171295
Loss at iteration 220 : 0.05917242169380188
Loss at iteration 230 : 0.09152808040380478
Loss at iteration 240 : 0.07126635313034058
Loss at iteration 250 : 0.09100960195064545
Loss at iteration 260 : 0.06047306954860687
Loss at iteration 270 : 0.06765958666801453
Loss at iteration 280 : 0.11242544651031494
Loss at iteration 290 : 0.05456920713186264
Loss at iteration 300 : 0.09391017258167267
Loss at iteration 310 : 0.126158207654953
Loss at iteration 320 : 0.11278797686100006
Loss at iteration 330 : 0.0837826132774353
Loss at iteration 340 : 0.12746389210224152
Loss at iteration 350 : 0.12992405891418457
Loss at iteration 360 : 0.10888075828552246
Loss at iteration 370 : 0.038760047405958176
Loss at iteration 380 : 0.11638578027486801
Loss at iteration 390 : 0.07482637465000153
Loss at iteration 400 : 0.06807642430067062
Loss at iteration 410 : 0.09397044032812119
Loss at iteration 420 : 0.1294453740119934
Loss at iteration 430 : 0.12744861841201782
Loss at iteration 440 : 0.10096117109060287
Loss at iteration 450 : 0.07925201952457428
Loss at iteration 460 : 0.09840520471334457
Loss at iteration 470 : 0.10177984088659286
Loss at iteration 480 : 0.06331229209899902
Loss at iteration 490 : 0.06447769701480865
Loss at iteration 500 : 0.05423470959067345
Loss at iteration 510 : 0.16169671714305878
Loss at iteration 520 : 0.07939654588699341
Loss at iteration 530 : 0.11230121552944183
Loss at iteration 540 : 0.049574628472328186
Loss at iteration 550 : 0.09269951283931732
Loss at iteration 560 : 0.08088268339633942
Loss at iteration 570 : 0.08791019767522812
Loss at iteration 580 : 0.10601918399333954
Loss at iteration 590 : 0.07893732190132141
Loss at iteration 600 : 0.060396142303943634
Loss at iteration 610 : 0.10350136458873749
Loss at iteration 620 : 0.10702013969421387
Loss at iteration 630 : 0.08721252530813217
Loss at iteration 640 : 0.07832380384206772
Loss at iteration 650 : 0.06181233748793602
Loss at iteration 660 : 0.09083738923072815
Loss at iteration 670 : 0.05161967873573303
Loss at iteration 680 : 0.06507732719182968
Loss at iteration 690 : 0.11053942143917084
Loss at iteration 700 : 0.07402288913726807
Loss at iteration 710 : 0.09613659977912903
Loss at iteration 720 : 0.05827220529317856
Loss at iteration 730 : 0.14589986205101013
Loss at iteration 740 : 0.07175379246473312
Loss at iteration 750 : 0.10060755908489227
Loss at iteration 760 : 0.10408204048871994
Loss at iteration 770 : 0.08723898977041245
Loss at iteration 780 : 0.05692517012357712
Loss at iteration 790 : 0.07264520972967148
Loss at iteration 800 : 0.13780830800533295
Loss at iteration 810 : 0.06077267974615097
Loss at iteration 820 : 0.11229151487350464
Loss at iteration 830 : 0.1375676989555359
Loss at iteration 840 : 0.08717065304517746
Loss at iteration 850 : 0.07646417617797852
Loss at iteration 860 : 0.07791638374328613
Loss at iteration 870 : 0.0654572993516922
Loss at iteration 880 : 0.12155541032552719
Loss at iteration 890 : 0.12466558814048767
Loss at iteration 900 : 0.04493297263979912
Loss at iteration 910 : 0.08117999136447906
Loss at iteration 920 : 0.051359936594963074
Loss at iteration 930 : 0.09684343636035919
Loss at iteration 940 : 0.10192447900772095
Loss at iteration 950 : 0.07087593525648117
Loss at iteration 960 : 0.0884641632437706
Loss at iteration 970 : 0.08984916657209396
Loss at iteration 980 : 0.11276566982269287
Loss at iteration 990 : 0.06210513412952423
Loss at iteration 1000 : 0.06068877875804901
Loss at iteration 1010 : 0.06767294555902481
Loss at iteration 1020 : 0.0662248358130455
Loss at iteration 1030 : 0.07686485350131989
Loss at iteration 1040 : 0.06302490085363388
Loss at iteration 1050 : 0.0789317712187767
Loss at iteration 1060 : 0.10278467833995819
Loss at iteration 1070 : 0.11540227383375168
Loss at iteration 1080 : 0.07443844527006149
Loss at iteration 1090 : 0.1253538876771927
Loss at iteration 1100 : 0.13272300362586975
Loss at iteration 1110 : 0.11920380592346191
Loss at iteration 1120 : 0.07392776012420654
Loss at iteration 1130 : 0.0807623416185379
Loss at iteration 1140 : 0.11047576367855072
Loss at iteration 1150 : 0.06956134736537933
Loss at iteration 1160 : 0.12090514600276947
Loss at iteration 1170 : 0.10923616588115692
Loss at iteration 1180 : 0.07396092265844345
Loss at iteration 1190 : 0.09148293733596802
Loss at iteration 1200 : 0.15400227904319763
Loss at iteration 1210 : 0.07015833258628845
The SSIM Value is: 0.6932774722576142
The PSNR Value is: 20.133906745910643
the epoch is: 61
Loss at iteration 10 : 0.06454791128635406
Loss at iteration 20 : 0.10090610384941101
Loss at iteration 30 : 0.04834551736712456
Loss at iteration 40 : 0.09153443574905396
Loss at iteration 50 : 0.06302336603403091
Loss at iteration 60 : 0.07540583610534668
Loss at iteration 70 : 0.11096645891666412
Loss at iteration 80 : 0.11128678172826767
Loss at iteration 90 : 0.09759604930877686
Loss at iteration 100 : 0.11969920992851257
Loss at iteration 110 : 0.0696832463145256
Loss at iteration 120 : 0.07039695233106613
Loss at iteration 130 : 0.09452887624502182
Loss at iteration 140 : 0.10572431981563568
Loss at iteration 150 : 0.0757104903459549
Loss at iteration 160 : 0.050168510526418686
Loss at iteration 170 : 0.11199179291725159
Loss at iteration 180 : 0.09182210266590118
Loss at iteration 190 : 0.08339621126651764
Loss at iteration 200 : 0.06534135341644287
Loss at iteration 210 : 0.0730186402797699
Loss at iteration 220 : 0.0676535964012146
Loss at iteration 230 : 0.10857941210269928
Loss at iteration 240 : 0.1085895299911499
Loss at iteration 250 : 0.1038568764925003
Loss at iteration 260 : 0.08949961513280869
Loss at iteration 270 : 0.05379445478320122
Loss at iteration 280 : 0.109452985227108
Loss at iteration 290 : 0.04872555658221245
Loss at iteration 300 : 0.09263454377651215
Loss at iteration 310 : 0.11183655261993408
Loss at iteration 320 : 0.09767387807369232
Loss at iteration 330 : 0.058171700686216354
Loss at iteration 340 : 0.07326927036046982
Loss at iteration 350 : 0.08852744102478027
Loss at iteration 360 : 0.08484874665737152
Loss at iteration 370 : 0.09951014816761017
Loss at iteration 380 : 0.08322140574455261
Loss at iteration 390 : 0.09893858432769775
Loss at iteration 400 : 0.07445009797811508
Loss at iteration 410 : 0.10915610194206238
Loss at iteration 420 : 0.10791707038879395
Loss at iteration 430 : 0.1454145312309265
Loss at iteration 440 : 0.05948803946375847
Loss at iteration 450 : 0.09696944802999496
Loss at iteration 460 : 0.05514075607061386
Loss at iteration 470 : 0.10799075663089752
Loss at iteration 480 : 0.14224079251289368
Loss at iteration 490 : 0.06601789593696594
Loss at iteration 500 : 0.09394089877605438
Loss at iteration 510 : 0.08071667701005936
Loss at iteration 520 : 0.10629019141197205
Loss at iteration 530 : 0.10626514256000519
Loss at iteration 540 : 0.09318914264440536
Loss at iteration 550 : 0.12341353297233582
Loss at iteration 560 : 0.09268318116664886
Loss at iteration 570 : 0.07904201745986938
Loss at iteration 580 : 0.06821727752685547
Loss at iteration 590 : 0.0823027715086937
Loss at iteration 600 : 0.0693507194519043
Loss at iteration 610 : 0.121613048017025
Loss at iteration 620 : 0.061732254922389984
Loss at iteration 630 : 0.08117946982383728
Loss at iteration 640 : 0.09172388166189194
Loss at iteration 650 : 0.08091622591018677
Loss at iteration 660 : 0.11446838080883026
Loss at iteration 670 : 0.13773784041404724
Loss at iteration 680 : 0.11599044501781464
Loss at iteration 690 : 0.09303834289312363
Loss at iteration 700 : 0.10137443244457245
Loss at iteration 710 : 0.09280771017074585
Loss at iteration 720 : 0.04636943340301514
Loss at iteration 730 : 0.05042020231485367
Loss at iteration 740 : 0.07440860569477081
Loss at iteration 750 : 0.049174241721630096
Loss at iteration 760 : 0.07325875759124756
Loss at iteration 770 : 0.1055700033903122
Loss at iteration 780 : 0.07318750768899918
Loss at iteration 790 : 0.06192328780889511
Loss at iteration 800 : 0.10161169618368149
Loss at iteration 810 : 0.11086156219244003
Loss at iteration 820 : 0.0508207231760025
Loss at iteration 830 : 0.09390890598297119
Loss at iteration 840 : 0.07140617072582245
Loss at iteration 850 : 0.06314035505056381
Loss at iteration 860 : 0.09488257765769958
Loss at iteration 870 : 0.06334337592124939
Loss at iteration 880 : 0.10780863463878632
Loss at iteration 890 : 0.063385508954525
Loss at iteration 900 : 0.09323972463607788
Loss at iteration 910 : 0.07431210577487946
Loss at iteration 920 : 0.052007101476192474
Loss at iteration 930 : 0.1027943342924118
Loss at iteration 940 : 0.08751191943883896
Loss at iteration 950 : 0.08931972086429596
Loss at iteration 960 : 0.06075410172343254
Loss at iteration 970 : 0.07758170366287231
Loss at iteration 980 : 0.09692013263702393
Loss at iteration 990 : 0.09440594911575317
Loss at iteration 1000 : 0.09160139411687851
Loss at iteration 1010 : 0.05213560909032822
Loss at iteration 1020 : 0.0986413061618805
Loss at iteration 1030 : 0.12102504819631577
Loss at iteration 1040 : 0.06149975582957268
Loss at iteration 1050 : 0.08878211677074432
Loss at iteration 1060 : 0.07762481272220612
Loss at iteration 1070 : 0.08455173671245575
Loss at iteration 1080 : 0.11314009130001068
Loss at iteration 1090 : 0.13203592598438263
Loss at iteration 1100 : 0.05813615024089813
Loss at iteration 1110 : 0.1428343951702118
Loss at iteration 1120 : 0.060160309076309204
Loss at iteration 1130 : 0.08555871248245239
Loss at iteration 1140 : 0.0753491148352623
Loss at iteration 1150 : 0.05412572622299194
Loss at iteration 1160 : 0.11225046217441559
Loss at iteration 1170 : 0.07517215609550476
Loss at iteration 1180 : 0.09022655338048935
Loss at iteration 1190 : 0.08567048609256744
Loss at iteration 1200 : 0.06088967248797417
Loss at iteration 1210 : 0.09416930377483368
The SSIM Value is: 0.7026194413503011
The PSNR Value is: 20.109342575073242
the epoch is: 62
Loss at iteration 10 : 0.07310017943382263
Loss at iteration 20 : 0.11668507754802704
Loss at iteration 30 : 0.13094639778137207
Loss at iteration 40 : 0.1306544840335846
Loss at iteration 50 : 0.08113467693328857
Loss at iteration 60 : 0.14075779914855957
Loss at iteration 70 : 0.08146822452545166
Loss at iteration 80 : 0.09095825254917145
Loss at iteration 90 : 0.05221501737833023
Loss at iteration 100 : 0.0662386417388916
Loss at iteration 110 : 0.06866928189992905
Loss at iteration 120 : 0.08781864494085312
Loss at iteration 130 : 0.07584759593009949
Loss at iteration 140 : 0.140084370970726
Loss at iteration 150 : 0.051552653312683105
Loss at iteration 160 : 0.058452215045690536
Loss at iteration 170 : 0.08432873338460922
Loss at iteration 180 : 0.09664338827133179
Loss at iteration 190 : 0.051525309681892395
Loss at iteration 200 : 0.06265752017498016
Loss at iteration 210 : 0.09906654059886932
Loss at iteration 220 : 0.06430547684431076
Loss at iteration 230 : 0.04146768897771835
Loss at iteration 240 : 0.07633543014526367
Loss at iteration 250 : 0.09046323597431183
Loss at iteration 260 : 0.07374057918787003
Loss at iteration 270 : 0.09086094796657562
Loss at iteration 280 : 0.11064101755619049
Loss at iteration 290 : 0.09115701168775558
Loss at iteration 300 : 0.06039376184344292
Loss at iteration 310 : 0.09174525737762451
Loss at iteration 320 : 0.06520658731460571
Loss at iteration 330 : 0.0768306702375412
Loss at iteration 340 : 0.13586203753948212
Loss at iteration 350 : 0.129481241106987
Loss at iteration 360 : 0.0882430225610733
Loss at iteration 370 : 0.09039755910634995
Loss at iteration 380 : 0.06210654601454735
Loss at iteration 390 : 0.10905741155147552
Loss at iteration 400 : 0.06926341354846954
Loss at iteration 410 : 0.10133825242519379
Loss at iteration 420 : 0.07606979459524155
Loss at iteration 430 : 0.11592277139425278
Loss at iteration 440 : 0.1420789361000061
Loss at iteration 450 : 0.09609602391719818
Loss at iteration 460 : 0.05864414572715759
Loss at iteration 470 : 0.08457046747207642
Loss at iteration 480 : 0.07123816013336182
Loss at iteration 490 : 0.08356624841690063
Loss at iteration 500 : 0.07961394637823105
Loss at iteration 510 : 0.057428449392318726
Loss at iteration 520 : 0.08120556175708771
Loss at iteration 530 : 0.10463838279247284
Loss at iteration 540 : 0.131544828414917
Loss at iteration 550 : 0.09699179977178574
Loss at iteration 560 : 0.07108601182699203
Loss at iteration 570 : 0.056773267686367035
Loss at iteration 580 : 0.1370929479598999
Loss at iteration 590 : 0.09980051219463348
Loss at iteration 600 : 0.09755115956068039
Loss at iteration 610 : 0.06284352391958237
Loss at iteration 620 : 0.07817482203245163
Loss at iteration 630 : 0.06500136852264404
Loss at iteration 640 : 0.08504065126180649
Loss at iteration 650 : 0.15185710787773132
Loss at iteration 660 : 0.07374493032693863
Loss at iteration 670 : 0.07945489883422852
Loss at iteration 680 : 0.11390649527311325
Loss at iteration 690 : 0.052738942205905914
Loss at iteration 700 : 0.05794884264469147
Loss at iteration 710 : 0.07773943990468979
Loss at iteration 720 : 0.06744560599327087
Loss at iteration 730 : 0.08287065476179123
Loss at iteration 740 : 0.0867093950510025
Loss at iteration 750 : 0.08712601661682129
Loss at iteration 760 : 0.09214102476835251
Loss at iteration 770 : 0.05711689218878746
Loss at iteration 780 : 0.08125671744346619
Loss at iteration 790 : 0.060616448521614075
Loss at iteration 800 : 0.07149428129196167
Loss at iteration 810 : 0.046673864126205444
Loss at iteration 820 : 0.0956573337316513
Loss at iteration 830 : 0.09386235475540161
Loss at iteration 840 : 0.08286133408546448
Loss at iteration 850 : 0.08305688202381134
Loss at iteration 860 : 0.10007455199956894
Loss at iteration 870 : 0.08944071829319
Loss at iteration 880 : 0.09252244234085083
Loss at iteration 890 : 0.154845729470253
Loss at iteration 900 : 0.09512403607368469
Loss at iteration 910 : 0.0742451399564743
Loss at iteration 920 : 0.07152317464351654
Loss at iteration 930 : 0.0754796490073204
Loss at iteration 940 : 0.07191704213619232
Loss at iteration 950 : 0.11491402238607407
Loss at iteration 960 : 0.0597592294216156
Loss at iteration 970 : 0.06924218684434891
Loss at iteration 980 : 0.06022576615214348
Loss at iteration 990 : 0.08150293678045273
Loss at iteration 1000 : 0.145193949341774
Loss at iteration 1010 : 0.092588409781456
Loss at iteration 1020 : 0.1344377100467682
Loss at iteration 1030 : 0.11084570735692978
Loss at iteration 1040 : 0.06839712709188461
Loss at iteration 1050 : 0.10764233767986298
Loss at iteration 1060 : 0.07540176808834076
Loss at iteration 1070 : 0.08731353282928467
Loss at iteration 1080 : 0.07397740334272385
Loss at iteration 1090 : 0.12090247124433517
Loss at iteration 1100 : 0.12045611441135406
Loss at iteration 1110 : 0.13284139335155487
Loss at iteration 1120 : 0.09166505187749863
Loss at iteration 1130 : 0.08335959166288376
Loss at iteration 1140 : 0.07297268509864807
Loss at iteration 1150 : 0.06269606202840805
Loss at iteration 1160 : 0.09494888037443161
Loss at iteration 1170 : 0.09780693054199219
Loss at iteration 1180 : 0.08931785821914673
Loss at iteration 1190 : 0.1459224969148636
Loss at iteration 1200 : 0.08313900232315063
Loss at iteration 1210 : 0.07561268657445908
The SSIM Value is: 0.7086044033368428
The PSNR Value is: 20.50065040588379
the epoch is: 63
Loss at iteration 10 : 0.06141453981399536
Loss at iteration 20 : 0.0746169462800026
Loss at iteration 30 : 0.049542270600795746
Loss at iteration 40 : 0.08198164403438568
Loss at iteration 50 : 0.09374655783176422
Loss at iteration 60 : 0.09759088605642319
Loss at iteration 70 : 0.10593070834875107
Loss at iteration 80 : 0.1021014079451561
Loss at iteration 90 : 0.15517497062683105
Loss at iteration 100 : 0.046876512467861176
Loss at iteration 110 : 0.08864524960517883
Loss at iteration 120 : 0.08476275205612183
Loss at iteration 130 : 0.05927030369639397
Loss at iteration 140 : 0.06238211318850517
Loss at iteration 150 : 0.06972671300172806
Loss at iteration 160 : 0.0870228260755539
Loss at iteration 170 : 0.08728550374507904
Loss at iteration 180 : 0.07477661967277527
Loss at iteration 190 : 0.04118593782186508
Loss at iteration 200 : 0.09286316484212875
Loss at iteration 210 : 0.11496041715145111
Loss at iteration 220 : 0.09606727212667465
Loss at iteration 230 : 0.09867008030414581
Loss at iteration 240 : 0.10013848543167114
Loss at iteration 250 : 0.06968024373054504
Loss at iteration 260 : 0.0867205411195755
Loss at iteration 270 : 0.10599184036254883
Loss at iteration 280 : 0.06817786395549774
Loss at iteration 290 : 0.11977507174015045
Loss at iteration 300 : 0.07385438680648804
Loss at iteration 310 : 0.06040497124195099
Loss at iteration 320 : 0.0997738391160965
Loss at iteration 330 : 0.05574607849121094
Loss at iteration 340 : 0.07086746394634247
Loss at iteration 350 : 0.0879993662238121
Loss at iteration 360 : 0.09961848706007004
Loss at iteration 370 : 0.06205257028341293
Loss at iteration 380 : 0.0841822475194931
Loss at iteration 390 : 0.05982109159231186
Loss at iteration 400 : 0.11889086663722992
Loss at iteration 410 : 0.05770627781748772
Loss at iteration 420 : 0.09126749634742737
Loss at iteration 430 : 0.12223982810974121
Loss at iteration 440 : 0.09129529446363449
Loss at iteration 450 : 0.07617788016796112
Loss at iteration 460 : 0.08020293712615967
Loss at iteration 470 : 0.06092914193868637
Loss at iteration 480 : 0.11694195121526718
Loss at iteration 490 : 0.06603872030973434
Loss at iteration 500 : 0.06555882841348648
Loss at iteration 510 : 0.0762745812535286
Loss at iteration 520 : 0.09309640526771545
Loss at iteration 530 : 0.06372606754302979
Loss at iteration 540 : 0.11073600500822067
Loss at iteration 550 : 0.08856180310249329
Loss at iteration 560 : 0.09924814105033875
Loss at iteration 570 : 0.10114473104476929
Loss at iteration 580 : 0.11876808851957321
Loss at iteration 590 : 0.07040341198444366
Loss at iteration 600 : 0.09794051945209503
Loss at iteration 610 : 0.06472533941268921
Loss at iteration 620 : 0.06691408902406693
Loss at iteration 630 : 0.08374850451946259
Loss at iteration 640 : 0.10875537991523743
Loss at iteration 650 : 0.0903499573469162
Loss at iteration 660 : 0.07487043738365173
Loss at iteration 670 : 0.05466338247060776
Loss at iteration 680 : 0.0938647985458374
Loss at iteration 690 : 0.07689379155635834
Loss at iteration 700 : 0.07484069466590881
Loss at iteration 710 : 0.09071626514196396
Loss at iteration 720 : 0.07493206113576889
Loss at iteration 730 : 0.10699425637722015
Loss at iteration 740 : 0.058394379913806915
Loss at iteration 750 : 0.09863843023777008
Loss at iteration 760 : 0.08997029066085815
Loss at iteration 770 : 0.12340449541807175
Loss at iteration 780 : 0.06392364203929901
Loss at iteration 790 : 0.10415095090866089
Loss at iteration 800 : 0.07361776381731033
Loss at iteration 810 : 0.09338730573654175
Loss at iteration 820 : 0.11155907809734344
Loss at iteration 830 : 0.09720419347286224
Loss at iteration 840 : 0.052758969366550446
Loss at iteration 850 : 0.09143239259719849
Loss at iteration 860 : 0.1162024661898613
Loss at iteration 870 : 0.06875739991664886
Loss at iteration 880 : 0.07079623639583588
Loss at iteration 890 : 0.04820405691862106
Loss at iteration 900 : 0.07639855146408081
Loss at iteration 910 : 0.08126945793628693
Loss at iteration 920 : 0.10001236200332642
Loss at iteration 930 : 0.09146945178508759
Loss at iteration 940 : 0.11110567301511765
Loss at iteration 950 : 0.05131866782903671
Loss at iteration 960 : 0.0936165601015091
Loss at iteration 970 : 0.0665358304977417
Loss at iteration 980 : 0.10919368267059326
Loss at iteration 990 : 0.08337889611721039
Loss at iteration 1000 : 0.08270210772752762
Loss at iteration 1010 : 0.0943070501089096
Loss at iteration 1020 : 0.06090478599071503
Loss at iteration 1030 : 0.0732102319598198
Loss at iteration 1040 : 0.13502095639705658
Loss at iteration 1050 : 0.08689922094345093
Loss at iteration 1060 : 0.10660839080810547
Loss at iteration 1070 : 0.08434473723173141
Loss at iteration 1080 : 0.07569416612386703
Loss at iteration 1090 : 0.07842124998569489
Loss at iteration 1100 : 0.07073737680912018
Loss at iteration 1110 : 0.11900198459625244
Loss at iteration 1120 : 0.11701049655675888
Loss at iteration 1130 : 0.09038208425045013
Loss at iteration 1140 : 0.09742239117622375
Loss at iteration 1150 : 0.09085023403167725
Loss at iteration 1160 : 0.07568856328725815
Loss at iteration 1170 : 0.06803645193576813
Loss at iteration 1180 : 0.061932485550642014
Loss at iteration 1190 : 0.07847529649734497
Loss at iteration 1200 : 0.07694251835346222
Loss at iteration 1210 : 0.07201270759105682
The SSIM Value is: 0.7096693634986877
The PSNR Value is: 20.29278736114502
the epoch is: 64
Loss at iteration 10 : 0.08218397200107574
Loss at iteration 20 : 0.07849684357643127
Loss at iteration 30 : 0.09202031791210175
Loss at iteration 40 : 0.09091859310865402
Loss at iteration 50 : 0.09507423639297485
Loss at iteration 60 : 0.04757542535662651
Loss at iteration 70 : 0.11052515357732773
Loss at iteration 80 : 0.13291999697685242
Loss at iteration 90 : 0.08126488327980042
Loss at iteration 100 : 0.1321084052324295
Loss at iteration 110 : 0.0696173906326294
Loss at iteration 120 : 0.09626942873001099
Loss at iteration 130 : 0.15767952799797058
Loss at iteration 140 : 0.07861030101776123
Loss at iteration 150 : 0.10762175172567368
Loss at iteration 160 : 0.11494521796703339
Loss at iteration 170 : 0.05222531408071518
Loss at iteration 180 : 0.09557513892650604
Loss at iteration 190 : 0.10772712528705597
Loss at iteration 200 : 0.11889027059078217
Loss at iteration 210 : 0.07001185417175293
Loss at iteration 220 : 0.05907076224684715
Loss at iteration 230 : 0.161505326628685
Loss at iteration 240 : 0.08641552925109863
Loss at iteration 250 : 0.08115684241056442
Loss at iteration 260 : 0.11859799921512604
Loss at iteration 270 : 0.07545676827430725
Loss at iteration 280 : 0.08416926115751266
Loss at iteration 290 : 0.10077082365751266
Loss at iteration 300 : 0.07065925002098083
Loss at iteration 310 : 0.06452260911464691
Loss at iteration 320 : 0.09020644426345825
Loss at iteration 330 : 0.13302068412303925
Loss at iteration 340 : 0.08712732791900635
Loss at iteration 350 : 0.1259467899799347
Loss at iteration 360 : 0.09557120501995087
Loss at iteration 370 : 0.10023559629917145
Loss at iteration 380 : 0.09381881356239319
Loss at iteration 390 : 0.08075213432312012
Loss at iteration 400 : 0.04409182071685791
Loss at iteration 410 : 0.055296823382377625
Loss at iteration 420 : 0.05922389775514603
Loss at iteration 430 : 0.09630714356899261
Loss at iteration 440 : 0.06909188628196716
Loss at iteration 450 : 0.06871507316827774
Loss at iteration 460 : 0.07900397479534149
Loss at iteration 470 : 0.07449682056903839
Loss at iteration 480 : 0.06294404715299606
Loss at iteration 490 : 0.06051580235362053
Loss at iteration 500 : 0.07731624692678452
Loss at iteration 510 : 0.09232230484485626
Loss at iteration 520 : 0.08568134158849716
Loss at iteration 530 : 0.11663534492254257
Loss at iteration 540 : 0.08997863531112671
Loss at iteration 550 : 0.1023126095533371
Loss at iteration 560 : 0.0872005820274353
Loss at iteration 570 : 0.09713419526815414
Loss at iteration 580 : 0.1061011403799057
Loss at iteration 590 : 0.0768694281578064
Loss at iteration 600 : 0.09823010861873627
Loss at iteration 610 : 0.11623227596282959
Loss at iteration 620 : 0.1110871285200119
Loss at iteration 630 : 0.09682014584541321
Loss at iteration 640 : 0.08301795274019241
Loss at iteration 650 : 0.09551575034856796
Loss at iteration 660 : 0.05272291228175163
Loss at iteration 670 : 0.07601481676101685
Loss at iteration 680 : 0.07081823796033859
Loss at iteration 690 : 0.1231006383895874
Loss at iteration 700 : 0.13004013895988464
Loss at iteration 710 : 0.09136895835399628
Loss at iteration 720 : 0.08750686049461365
Loss at iteration 730 : 0.06450953334569931
Loss at iteration 740 : 0.06400025635957718
Loss at iteration 750 : 0.05242633819580078
Loss at iteration 760 : 0.0869918093085289
Loss at iteration 770 : 0.13605058193206787
Loss at iteration 780 : 0.04876062273979187
Loss at iteration 790 : 0.10469555854797363
Loss at iteration 800 : 0.05515645071864128
Loss at iteration 810 : 0.09185292571783066
Loss at iteration 820 : 0.051226790994405746
Loss at iteration 830 : 0.09334582090377808
Loss at iteration 840 : 0.07573992758989334
Loss at iteration 850 : 0.11202117055654526
Loss at iteration 860 : 0.0483708456158638
Loss at iteration 870 : 0.15326425433158875
Loss at iteration 880 : 0.08373008668422699
Loss at iteration 890 : 0.10448255389928818
Loss at iteration 900 : 0.07543326914310455
Loss at iteration 910 : 0.06568052619695663
Loss at iteration 920 : 0.08193457126617432
Loss at iteration 930 : 0.06941550225019455
Loss at iteration 940 : 0.08275976032018661
Loss at iteration 950 : 0.12613826990127563
Loss at iteration 960 : 0.08098536729812622
Loss at iteration 970 : 0.07409819215536118
Loss at iteration 980 : 0.07162760943174362
Loss at iteration 990 : 0.08780202269554138
Loss at iteration 1000 : 0.059932731091976166
Loss at iteration 1010 : 0.05981411039829254
Loss at iteration 1020 : 0.1568334847688675
Loss at iteration 1030 : 0.10168308019638062
Loss at iteration 1040 : 0.0696771889925003
Loss at iteration 1050 : 0.08837354183197021
Loss at iteration 1060 : 0.09669025987386703
Loss at iteration 1070 : 0.10178343951702118
Loss at iteration 1080 : 0.1128348857164383
Loss at iteration 1090 : 0.12491375207901001
Loss at iteration 1100 : 0.08046367764472961
Loss at iteration 1110 : 0.07456182688474655
Loss at iteration 1120 : 0.09291380643844604
Loss at iteration 1130 : 0.07873858511447906
Loss at iteration 1140 : 0.08544594049453735
Loss at iteration 1150 : 0.11659596860408783
Loss at iteration 1160 : 0.05931815132498741
Loss at iteration 1170 : 0.11952774226665497
Loss at iteration 1180 : 0.0752861425280571
Loss at iteration 1190 : 0.07777804136276245
Loss at iteration 1200 : 0.054358720779418945
Loss at iteration 1210 : 0.11306655406951904
The SSIM Value is: 0.7072515428066254
The PSNR Value is: 21.14362996419271
the epoch is: 65
Loss at iteration 10 : 0.08741666376590729
Loss at iteration 20 : 0.09113290905952454
Loss at iteration 30 : 0.10059700906276703
Loss at iteration 40 : 0.08271025121212006
Loss at iteration 50 : 0.08607702702283859
Loss at iteration 60 : 0.10231094807386398
Loss at iteration 70 : 0.07727409154176712
Loss at iteration 80 : 0.07166625559329987
Loss at iteration 90 : 0.046084433794021606
Loss at iteration 100 : 0.05176050588488579
Loss at iteration 110 : 0.06978023052215576
Loss at iteration 120 : 0.09443444013595581
Loss at iteration 130 : 0.0763852596282959
Loss at iteration 140 : 0.09072723239660263
Loss at iteration 150 : 0.11993582546710968
Loss at iteration 160 : 0.057824209332466125
Loss at iteration 170 : 0.08286140859127045
Loss at iteration 180 : 0.06700760126113892
Loss at iteration 190 : 0.08763589709997177
Loss at iteration 200 : 0.07871498167514801
Loss at iteration 210 : 0.0578860267996788
Loss at iteration 220 : 0.06867566704750061
Loss at iteration 230 : 0.05473613366484642
Loss at iteration 240 : 0.11372831463813782
Loss at iteration 250 : 0.0873955711722374
Loss at iteration 260 : 0.1167537122964859
Loss at iteration 270 : 0.08416406810283661
Loss at iteration 280 : 0.12722045183181763
Loss at iteration 290 : 0.07428928464651108
Loss at iteration 300 : 0.05763828009366989
Loss at iteration 310 : 0.0889565572142601
Loss at iteration 320 : 0.11571450531482697
Loss at iteration 330 : 0.1163480132818222
Loss at iteration 340 : 0.08577387034893036
Loss at iteration 350 : 0.06210390850901604
Loss at iteration 360 : 0.1265956610441208
Loss at iteration 370 : 0.08140785992145538
Loss at iteration 380 : 0.12700699269771576
Loss at iteration 390 : 0.05713800713419914
Loss at iteration 400 : 0.0673951730132103
Loss at iteration 410 : 0.07626275718212128
Loss at iteration 420 : 0.09399142116308212
Loss at iteration 430 : 0.12826219201087952
Loss at iteration 440 : 0.07025642693042755
Loss at iteration 450 : 0.08023481070995331
Loss at iteration 460 : 0.0808362066745758
Loss at iteration 470 : 0.07132365554571152
Loss at iteration 480 : 0.10684200376272202
Loss at iteration 490 : 0.09828802198171616
Loss at iteration 500 : 0.06868574768304825
Loss at iteration 510 : 0.06521874666213989
Loss at iteration 520 : 0.09585752338171005
Loss at iteration 530 : 0.07693196088075638
Loss at iteration 540 : 0.06071355938911438
Loss at iteration 550 : 0.06076386570930481
Loss at iteration 560 : 0.11027448624372482
Loss at iteration 570 : 0.04770302772521973
Loss at iteration 580 : 0.06495169550180435
Loss at iteration 590 : 0.08459194749593735
Loss at iteration 600 : 0.07614918053150177
Loss at iteration 610 : 0.06729879975318909
Loss at iteration 620 : 0.11341822147369385
Loss at iteration 630 : 0.08214600384235382
Loss at iteration 640 : 0.041583865880966187
Loss at iteration 650 : 0.08136259764432907
Loss at iteration 660 : 0.10527051985263824
Loss at iteration 670 : 0.08114495128393173
Loss at iteration 680 : 0.06111753731966019
Loss at iteration 690 : 0.12780067324638367
Loss at iteration 700 : 0.13061311841011047
Loss at iteration 710 : 0.050422750413417816
Loss at iteration 720 : 0.06745006144046783
Loss at iteration 730 : 0.10319069027900696
Loss at iteration 740 : 0.10598086565732956
Loss at iteration 750 : 0.06245796009898186
Loss at iteration 760 : 0.06131768226623535
Loss at iteration 770 : 0.07749144732952118
Loss at iteration 780 : 0.10158833116292953
Loss at iteration 790 : 0.11583852022886276
Loss at iteration 800 : 0.1244722232222557
Loss at iteration 810 : 0.06745477765798569
Loss at iteration 820 : 0.13981494307518005
Loss at iteration 830 : 0.08489257097244263
Loss at iteration 840 : 0.08095960319042206
Loss at iteration 850 : 0.10363748669624329
Loss at iteration 860 : 0.10460267961025238
Loss at iteration 870 : 0.10900606960058212
Loss at iteration 880 : 0.10936950892210007
Loss at iteration 890 : 0.06717175245285034
Loss at iteration 900 : 0.07758048176765442
Loss at iteration 910 : 0.061979811638593674
Loss at iteration 920 : 0.139646515250206
Loss at iteration 930 : 0.05571804195642471
Loss at iteration 940 : 0.12803050875663757
Loss at iteration 950 : 0.06462697684764862
Loss at iteration 960 : 0.05338039994239807
Loss at iteration 970 : 0.0903014987707138
Loss at iteration 980 : 0.08745305985212326
Loss at iteration 990 : 0.11715032160282135
Loss at iteration 1000 : 0.05311793088912964
Loss at iteration 1010 : 0.0948205217719078
Loss at iteration 1020 : 0.04846339672803879
Loss at iteration 1030 : 0.07797840237617493
Loss at iteration 1040 : 0.10480791330337524
Loss at iteration 1050 : 0.09961757063865662
Loss at iteration 1060 : 0.0674196407198906
Loss at iteration 1070 : 0.06200595200061798
Loss at iteration 1080 : 0.061673134565353394
Loss at iteration 1090 : 0.0930887907743454
Loss at iteration 1100 : 0.11662672460079193
Loss at iteration 1110 : 0.0999506339430809
Loss at iteration 1120 : 0.0697040781378746
Loss at iteration 1130 : 0.07927075028419495
Loss at iteration 1140 : 0.11275916546583176
Loss at iteration 1150 : 0.09983545541763306
Loss at iteration 1160 : 0.07292719930410385
Loss at iteration 1170 : 0.08386071771383286
Loss at iteration 1180 : 0.11232372373342514
Loss at iteration 1190 : 0.07878974080085754
Loss at iteration 1200 : 0.05709116905927658
Loss at iteration 1210 : 0.05988835543394089
The SSIM Value is: 0.7066553473472595
The PSNR Value is: 21.012515767415366
the epoch is: 66
Loss at iteration 10 : 0.09027902781963348
Loss at iteration 20 : 0.06671112030744553
Loss at iteration 30 : 0.06112564727663994
Loss at iteration 40 : 0.07873567938804626
Loss at iteration 50 : 0.09377404302358627
Loss at iteration 60 : 0.07207928597927094
Loss at iteration 70 : 0.1328895539045334
Loss at iteration 80 : 0.10308466851711273
Loss at iteration 90 : 0.08369004726409912
Loss at iteration 100 : 0.10600235313177109
Loss at iteration 110 : 0.08955379575490952
Loss at iteration 120 : 0.0982549786567688
Loss at iteration 130 : 0.09180407226085663
Loss at iteration 140 : 0.09661011397838593
Loss at iteration 150 : 0.07304442673921585
Loss at iteration 160 : 0.05653766915202141
Loss at iteration 170 : 0.05080479383468628
Loss at iteration 180 : 0.08008736371994019
Loss at iteration 190 : 0.0688154399394989
Loss at iteration 200 : 0.07085613906383514
Loss at iteration 210 : 0.051973067224025726
Loss at iteration 220 : 0.10005110502243042
Loss at iteration 230 : 0.09540777653455734
Loss at iteration 240 : 0.06462462991476059
Loss at iteration 250 : 0.09966711699962616
Loss at iteration 260 : 0.11564081907272339
Loss at iteration 270 : 0.14604035019874573
Loss at iteration 280 : 0.10488244891166687
Loss at iteration 290 : 0.1459755152463913
Loss at iteration 300 : 0.046583663672208786
Loss at iteration 310 : 0.08213299512863159
Loss at iteration 320 : 0.09292499721050262
Loss at iteration 330 : 0.08893810212612152
Loss at iteration 340 : 0.09075485169887543
Loss at iteration 350 : 0.046324603259563446
Loss at iteration 360 : 0.08716966211795807
Loss at iteration 370 : 0.05602843686938286
Loss at iteration 380 : 0.054730337113142014
Loss at iteration 390 : 0.06769517064094543
Loss at iteration 400 : 0.06551900506019592
Loss at iteration 410 : 0.0583537332713604
Loss at iteration 420 : 0.10246296972036362
Loss at iteration 430 : 0.10320968925952911
Loss at iteration 440 : 0.0877922922372818
Loss at iteration 450 : 0.07599754631519318
Loss at iteration 460 : 0.12311597913503647
Loss at iteration 470 : 0.11542033404111862
Loss at iteration 480 : 0.08842828124761581
Loss at iteration 490 : 0.11886506527662277
Loss at iteration 500 : 0.04446318745613098
Loss at iteration 510 : 0.05985833704471588
Loss at iteration 520 : 0.06812798976898193
Loss at iteration 530 : 0.09638023376464844
Loss at iteration 540 : 0.0693642869591713
Loss at iteration 550 : 0.07847793400287628
Loss at iteration 560 : 0.09775625914335251
Loss at iteration 570 : 0.0538141205906868
Loss at iteration 580 : 0.07972986996173859
Loss at iteration 590 : 0.0897773951292038
Loss at iteration 600 : 0.12258051335811615
Loss at iteration 610 : 0.08268900215625763
Loss at iteration 620 : 0.08153845369815826
Loss at iteration 630 : 0.10464228689670563
Loss at iteration 640 : 0.08370722830295563
Loss at iteration 650 : 0.07329697906970978
Loss at iteration 660 : 0.12982892990112305
Loss at iteration 670 : 0.12349169701337814
Loss at iteration 680 : 0.10923685133457184
Loss at iteration 690 : 0.08112053573131561
Loss at iteration 700 : 0.08210469037294388
Loss at iteration 710 : 0.0904003381729126
Loss at iteration 720 : 0.12360171973705292
Loss at iteration 730 : 0.06890398263931274
Loss at iteration 740 : 0.07859029620885849
Loss at iteration 750 : 0.09093636274337769
Loss at iteration 760 : 0.055610887706279755
Loss at iteration 770 : 0.07234673947095871
Loss at iteration 780 : 0.09816204011440277
Loss at iteration 790 : 0.0950668677687645
Loss at iteration 800 : 0.14843732118606567
Loss at iteration 810 : 0.06413780152797699
Loss at iteration 820 : 0.10037239640951157
Loss at iteration 830 : 0.09001823514699936
Loss at iteration 840 : 0.07554331421852112
Loss at iteration 850 : 0.10366707295179367
Loss at iteration 860 : 0.12311050295829773
Loss at iteration 870 : 0.09463578462600708
Loss at iteration 880 : 0.08355826139450073
Loss at iteration 890 : 0.12468668073415756
Loss at iteration 900 : 0.06493428349494934
Loss at iteration 910 : 0.07769720256328583
Loss at iteration 920 : 0.049040596932172775
Loss at iteration 930 : 0.13418470323085785
Loss at iteration 940 : 0.059264324605464935
Loss at iteration 950 : 0.08129358291625977
Loss at iteration 960 : 0.07377858459949493
Loss at iteration 970 : 0.06602959334850311
Loss at iteration 980 : 0.07780440896749496
Loss at iteration 990 : 0.06786439567804337
Loss at iteration 1000 : 0.1263696551322937
Loss at iteration 1010 : 0.059228699654340744
Loss at iteration 1020 : 0.08158205449581146
Loss at iteration 1030 : 0.05368037149310112
Loss at iteration 1040 : 0.08277733623981476
Loss at iteration 1050 : 0.1179676279425621
Loss at iteration 1060 : 0.13499686121940613
Loss at iteration 1070 : 0.08728283643722534
Loss at iteration 1080 : 0.09546991437673569
Loss at iteration 1090 : 0.07127445936203003
Loss at iteration 1100 : 0.053345225751399994
Loss at iteration 1110 : 0.12013270705938339
Loss at iteration 1120 : 0.08024197816848755
Loss at iteration 1130 : 0.14659276604652405
Loss at iteration 1140 : 0.05650220438838005
Loss at iteration 1150 : 0.08000195026397705
Loss at iteration 1160 : 0.1202760636806488
Loss at iteration 1170 : 0.09596525877714157
Loss at iteration 1180 : 0.0789002925157547
Loss at iteration 1190 : 0.13283516466617584
Loss at iteration 1200 : 0.08646150678396225
Loss at iteration 1210 : 0.10262726992368698
The SSIM Value is: 0.705052540699641
The PSNR Value is: 20.875698852539063
the epoch is: 67
Loss at iteration 10 : 0.07388665527105331
Loss at iteration 20 : 0.09960335493087769
Loss at iteration 30 : 0.14992009103298187
Loss at iteration 40 : 0.051201120018959045
Loss at iteration 50 : 0.08686511218547821
Loss at iteration 60 : 0.04501524567604065
Loss at iteration 70 : 0.16396334767341614
Loss at iteration 80 : 0.10996010899543762
Loss at iteration 90 : 0.09237106889486313
Loss at iteration 100 : 0.11782663315534592
Loss at iteration 110 : 0.07921423017978668
Loss at iteration 120 : 0.07114483416080475
Loss at iteration 130 : 0.06996327638626099
Loss at iteration 140 : 0.10406912863254547
Loss at iteration 150 : 0.10356482118368149
Loss at iteration 160 : 0.04303113371133804
Loss at iteration 170 : 0.07884492725133896
Loss at iteration 180 : 0.06738132238388062
Loss at iteration 190 : 0.06435390561819077
Loss at iteration 200 : 0.09929987043142319
Loss at iteration 210 : 0.05399824306368828
Loss at iteration 220 : 0.06745420396327972
Loss at iteration 230 : 0.08163544535636902
Loss at iteration 240 : 0.1279262751340866
Loss at iteration 250 : 0.11706621944904327
Loss at iteration 260 : 0.09632017463445663
Loss at iteration 270 : 0.08412899821996689
Loss at iteration 280 : 0.05037299916148186
Loss at iteration 290 : 0.05764295160770416
Loss at iteration 300 : 0.07957901805639267
Loss at iteration 310 : 0.07995182275772095
Loss at iteration 320 : 0.08106037229299545
Loss at iteration 330 : 0.10080182552337646
Loss at iteration 340 : 0.09120750427246094
Loss at iteration 350 : 0.06938289105892181
Loss at iteration 360 : 0.05486501753330231
Loss at iteration 370 : 0.09538637101650238
Loss at iteration 380 : 0.10173702239990234
Loss at iteration 390 : 0.06969388574361801
Loss at iteration 400 : 0.12025543302297592
Loss at iteration 410 : 0.10162149369716644
Loss at iteration 420 : 0.048925530165433884
Loss at iteration 430 : 0.0723896324634552
Loss at iteration 440 : 0.08728723227977753
Loss at iteration 450 : 0.07975256443023682
Loss at iteration 460 : 0.10651153326034546
Loss at iteration 470 : 0.09605789184570312
Loss at iteration 480 : 0.07559452205896378
Loss at iteration 490 : 0.09588789939880371
Loss at iteration 500 : 0.06284348666667938
Loss at iteration 510 : 0.088920459151268
Loss at iteration 520 : 0.07755454629659653
Loss at iteration 530 : 0.07165880501270294
Loss at iteration 540 : 0.05747407674789429
Loss at iteration 550 : 0.05658024549484253
Loss at iteration 560 : 0.11012111604213715
Loss at iteration 570 : 0.07584810256958008
Loss at iteration 580 : 0.07660343497991562
Loss at iteration 590 : 0.04404306411743164
Loss at iteration 600 : 0.11433904618024826
Loss at iteration 610 : 0.08887743949890137
Loss at iteration 620 : 0.06500363349914551
Loss at iteration 630 : 0.12141045182943344
Loss at iteration 640 : 0.05693121254444122
Loss at iteration 650 : 0.07670393586158752
Loss at iteration 660 : 0.09646628797054291
Loss at iteration 670 : 0.06583347916603088
Loss at iteration 680 : 0.08778281509876251
Loss at iteration 690 : 0.08500637859106064
Loss at iteration 700 : 0.06812436133623123
Loss at iteration 710 : 0.059103500097990036
Loss at iteration 720 : 0.06052249297499657
Loss at iteration 730 : 0.10676340758800507
Loss at iteration 740 : 0.0857464075088501
Loss at iteration 750 : 0.09825341403484344
Loss at iteration 760 : 0.045950084924697876
Loss at iteration 770 : 0.10739310830831528
Loss at iteration 780 : 0.16598966717720032
Loss at iteration 790 : 0.13319121301174164
Loss at iteration 800 : 0.10062567889690399
Loss at iteration 810 : 0.11284645646810532
Loss at iteration 820 : 0.05486907809972763
Loss at iteration 830 : 0.061319105327129364
Loss at iteration 840 : 0.05701134726405144
Loss at iteration 850 : 0.04682031273841858
Loss at iteration 860 : 0.1049756109714508
Loss at iteration 870 : 0.10242493450641632
Loss at iteration 880 : 0.058193400502204895
Loss at iteration 890 : 0.08024003356695175
Loss at iteration 900 : 0.11855171620845795
Loss at iteration 910 : 0.0746009349822998
Loss at iteration 920 : 0.08441276848316193
Loss at iteration 930 : 0.08542867749929428
Loss at iteration 940 : 0.06271594762802124
Loss at iteration 950 : 0.09386976063251495
Loss at iteration 960 : 0.08589045703411102
Loss at iteration 970 : 0.12694409489631653
Loss at iteration 980 : 0.0946982353925705
Loss at iteration 990 : 0.15996360778808594
Loss at iteration 1000 : 0.12770286202430725
Loss at iteration 1010 : 0.05915869027376175
Loss at iteration 1020 : 0.10167749971151352
Loss at iteration 1030 : 0.07409357279539108
Loss at iteration 1040 : 0.06650863587856293
Loss at iteration 1050 : 0.06284143775701523
Loss at iteration 1060 : 0.09057927131652832
Loss at iteration 1070 : 0.08787417411804199
Loss at iteration 1080 : 0.13980542123317719
Loss at iteration 1090 : 0.0806560143828392
Loss at iteration 1100 : 0.0692259669303894
Loss at iteration 1110 : 0.08196386694908142
Loss at iteration 1120 : 0.07970687001943588
Loss at iteration 1130 : 0.09007876366376877
Loss at iteration 1140 : 0.12906703352928162
Loss at iteration 1150 : 0.08733456581830978
Loss at iteration 1160 : 0.07873303443193436
Loss at iteration 1170 : 0.05764421075582504
Loss at iteration 1180 : 0.0852721557021141
Loss at iteration 1190 : 0.07170404493808746
Loss at iteration 1200 : 0.07936929166316986
Loss at iteration 1210 : 0.06070674583315849
The SSIM Value is: 0.6994074563185374
The PSNR Value is: 20.55628490447998
the epoch is: 68
Loss at iteration 10 : 0.07172314822673798
Loss at iteration 20 : 0.07946830242872238
Loss at iteration 30 : 0.16805794835090637
Loss at iteration 40 : 0.10364049673080444
Loss at iteration 50 : 0.08692290633916855
Loss at iteration 60 : 0.08512739837169647
Loss at iteration 70 : 0.06324806809425354
Loss at iteration 80 : 0.07377412170171738
Loss at iteration 90 : 0.07630040496587753
Loss at iteration 100 : 0.13171613216400146
Loss at iteration 110 : 0.05033787339925766
Loss at iteration 120 : 0.07766801863908768
Loss at iteration 130 : 0.08667875826358795
Loss at iteration 140 : 0.08523102849721909
Loss at iteration 150 : 0.09676079452037811
Loss at iteration 160 : 0.0882604718208313
Loss at iteration 170 : 0.09546506404876709
Loss at iteration 180 : 0.08694817125797272
Loss at iteration 190 : 0.14091730117797852
Loss at iteration 200 : 0.08307227492332458
Loss at iteration 210 : 0.09680230915546417
Loss at iteration 220 : 0.07725527882575989
Loss at iteration 230 : 0.08261948823928833
Loss at iteration 240 : 0.13263432681560516
Loss at iteration 250 : 0.06008836627006531
Loss at iteration 260 : 0.13560429215431213
Loss at iteration 270 : 0.06769545376300812
Loss at iteration 280 : 0.07056587189435959
Loss at iteration 290 : 0.12213395535945892
Loss at iteration 300 : 0.09165500849485397
Loss at iteration 310 : 0.10149645060300827
Loss at iteration 320 : 0.15951070189476013
Loss at iteration 330 : 0.06488034129142761
Loss at iteration 340 : 0.09083741903305054
Loss at iteration 350 : 0.04821210354566574
Loss at iteration 360 : 0.14914856851100922
Loss at iteration 370 : 0.0961119532585144
Loss at iteration 380 : 0.12211217731237411
Loss at iteration 390 : 0.08761578798294067
Loss at iteration 400 : 0.11586549133062363
Loss at iteration 410 : 0.09762003272771835
Loss at iteration 420 : 0.08627908676862717
Loss at iteration 430 : 0.07342454791069031
Loss at iteration 440 : 0.07469835877418518
Loss at iteration 450 : 0.09744051098823547
Loss at iteration 460 : 0.09044758975505829
Loss at iteration 470 : 0.11129164695739746
Loss at iteration 480 : 0.08158287405967712
Loss at iteration 490 : 0.0882493108510971
Loss at iteration 500 : 0.057741548866033554
Loss at iteration 510 : 0.0694165974855423
Loss at iteration 520 : 0.10573503375053406
Loss at iteration 530 : 0.12037431448698044
Loss at iteration 540 : 0.05638348311185837
Loss at iteration 550 : 0.06851322948932648
Loss at iteration 560 : 0.08085699379444122
Loss at iteration 570 : 0.15317027270793915
Loss at iteration 580 : 0.09848561882972717
Loss at iteration 590 : 0.06769365072250366
Loss at iteration 600 : 0.07793574035167694
Loss at iteration 610 : 0.10132886469364166
Loss at iteration 620 : 0.11661659181118011
Loss at iteration 630 : 0.07721157371997833
Loss at iteration 640 : 0.09500643610954285
Loss at iteration 650 : 0.0799003541469574
Loss at iteration 660 : 0.0968613475561142
Loss at iteration 670 : 0.11389549821615219
Loss at iteration 680 : 0.09948006272315979
Loss at iteration 690 : 0.12996530532836914
Loss at iteration 700 : 0.09616412967443466
Loss at iteration 710 : 0.07568791508674622
Loss at iteration 720 : 0.10051435232162476
Loss at iteration 730 : 0.08647941797971725
Loss at iteration 740 : 0.09588444232940674
Loss at iteration 750 : 0.1253737211227417
Loss at iteration 760 : 0.05348861217498779
Loss at iteration 770 : 0.0709758996963501
Loss at iteration 780 : 0.09575927257537842
Loss at iteration 790 : 0.07955464720726013
Loss at iteration 800 : 0.0767923891544342
Loss at iteration 810 : 0.07822142541408539
Loss at iteration 820 : 0.07872486114501953
Loss at iteration 830 : 0.10436247289180756
Loss at iteration 840 : 0.08029328286647797
Loss at iteration 850 : 0.0786646232008934
Loss at iteration 860 : 0.0480438694357872
Loss at iteration 870 : 0.09127559512853622
Loss at iteration 880 : 0.06006282567977905
Loss at iteration 890 : 0.05577383562922478
Loss at iteration 900 : 0.08628636598587036
Loss at iteration 910 : 0.10196416079998016
Loss at iteration 920 : 0.11008348315954208
Loss at iteration 930 : 0.11797650903463364
Loss at iteration 940 : 0.07731153815984726
Loss at iteration 950 : 0.05165731906890869
Loss at iteration 960 : 0.07107245922088623
Loss at iteration 970 : 0.08516806364059448
Loss at iteration 980 : 0.05290623754262924
Loss at iteration 990 : 0.11340048164129257
Loss at iteration 1000 : 0.10955393314361572
Loss at iteration 1010 : 0.0634612962603569
Loss at iteration 1020 : 0.08374842256307602
Loss at iteration 1030 : 0.1332768201828003
Loss at iteration 1040 : 0.09385018050670624
Loss at iteration 1050 : 0.08613552898168564
Loss at iteration 1060 : 0.06199389696121216
Loss at iteration 1070 : 0.10596399009227753
Loss at iteration 1080 : 0.09050017595291138
Loss at iteration 1090 : 0.0711238831281662
Loss at iteration 1100 : 0.07849383354187012
Loss at iteration 1110 : 0.06626974791288376
Loss at iteration 1120 : 0.13023313879966736
Loss at iteration 1130 : 0.10718514770269394
Loss at iteration 1140 : 0.047456949949264526
Loss at iteration 1150 : 0.10737203061580658
Loss at iteration 1160 : 0.07799603790044785
Loss at iteration 1170 : 0.11165542900562286
Loss at iteration 1180 : 0.11687737703323364
Loss at iteration 1190 : 0.1130046471953392
Loss at iteration 1200 : 0.07120304554700851
Loss at iteration 1210 : 0.06448999047279358
The SSIM Value is: 0.7066215674082438
The PSNR Value is: 21.036345354715984
the epoch is: 69
Loss at iteration 10 : 0.05724882334470749
Loss at iteration 20 : 0.08305217325687408
Loss at iteration 30 : 0.082518570125103
Loss at iteration 40 : 0.08946816623210907
Loss at iteration 50 : 0.08228848874568939
Loss at iteration 60 : 0.05571939796209335
Loss at iteration 70 : 0.08639483153820038
Loss at iteration 80 : 0.09468959271907806
Loss at iteration 90 : 0.05767781659960747
Loss at iteration 100 : 0.11251336336135864
Loss at iteration 110 : 0.08117881417274475
Loss at iteration 120 : 0.09961114823818207
Loss at iteration 130 : 0.185327410697937
Loss at iteration 140 : 0.0943598747253418
Loss at iteration 150 : 0.07850868999958038
Loss at iteration 160 : 0.09892144054174423
Loss at iteration 170 : 0.06911513209342957
Loss at iteration 180 : 0.09444501250982285
Loss at iteration 190 : 0.09501974284648895
Loss at iteration 200 : 0.08146677166223526
Loss at iteration 210 : 0.051028646528720856
Loss at iteration 220 : 0.07897017896175385
Loss at iteration 230 : 0.07952840626239777
Loss at iteration 240 : 0.09952817857265472
Loss at iteration 250 : 0.12328755855560303
Loss at iteration 260 : 0.10940217971801758
Loss at iteration 270 : 0.09228631854057312
Loss at iteration 280 : 0.09758186340332031
Loss at iteration 290 : 0.0759214386343956
Loss at iteration 300 : 0.0702979564666748
Loss at iteration 310 : 0.09604986757040024
Loss at iteration 320 : 0.08623719215393066
Loss at iteration 330 : 0.1006510853767395
Loss at iteration 340 : 0.07788883149623871
Loss at iteration 350 : 0.12165923416614532
Loss at iteration 360 : 0.07371735572814941
Loss at iteration 370 : 0.11877288669347763
Loss at iteration 380 : 0.10760601609945297
Loss at iteration 390 : 0.05624987185001373
Loss at iteration 400 : 0.04671376943588257
Loss at iteration 410 : 0.09894826263189316
Loss at iteration 420 : 0.09501767158508301
Loss at iteration 430 : 0.10093268007040024
Loss at iteration 440 : 0.11040651798248291
Loss at iteration 450 : 0.0833081603050232
Loss at iteration 460 : 0.07465062290430069
Loss at iteration 470 : 0.0941600352525711
Loss at iteration 480 : 0.08073732256889343
Loss at iteration 490 : 0.07825010269880295
Loss at iteration 500 : 0.05087197944521904
Loss at iteration 510 : 0.06906372308731079
Loss at iteration 520 : 0.13955870270729065
Loss at iteration 530 : 0.0839337706565857
Loss at iteration 540 : 0.06040634959936142
Loss at iteration 550 : 0.06010563299059868
Loss at iteration 560 : 0.07064486294984818
Loss at iteration 570 : 0.08656705915927887
Loss at iteration 580 : 0.060429614037275314
Loss at iteration 590 : 0.06083604320883751
Loss at iteration 600 : 0.0699576586484909
Loss at iteration 610 : 0.11177830398082733
Loss at iteration 620 : 0.07077471911907196
Loss at iteration 630 : 0.07990211248397827
Loss at iteration 640 : 0.05470619723200798
Loss at iteration 650 : 0.07610960304737091
Loss at iteration 660 : 0.06506465375423431
Loss at iteration 670 : 0.08859827369451523
Loss at iteration 680 : 0.08213036507368088
Loss at iteration 690 : 0.05565088987350464
Loss at iteration 700 : 0.0953972190618515
Loss at iteration 710 : 0.09590872377157211
Loss at iteration 720 : 0.13108474016189575
Loss at iteration 730 : 0.06082547456026077
Loss at iteration 740 : 0.04459291696548462
Loss at iteration 750 : 0.12507078051567078
Loss at iteration 760 : 0.062275536358356476
Loss at iteration 770 : 0.06483617424964905
Loss at iteration 780 : 0.09565936028957367
Loss at iteration 790 : 0.13880616426467896
Loss at iteration 800 : 0.061802737414836884
Loss at iteration 810 : 0.05373634025454521
Loss at iteration 820 : 0.06464456021785736
Loss at iteration 830 : 0.1023189052939415
Loss at iteration 840 : 0.18068575859069824
Loss at iteration 850 : 0.13770970702171326
Loss at iteration 860 : 0.08310996741056442
Loss at iteration 870 : 0.11931166052818298
Loss at iteration 880 : 0.09903176128864288
Loss at iteration 890 : 0.1051715612411499
Loss at iteration 900 : 0.12300946563482285
Loss at iteration 910 : 0.12000688165426254
Loss at iteration 920 : 0.09747343510389328
Loss at iteration 930 : 0.0576944462954998
Loss at iteration 940 : 0.0854090079665184
Loss at iteration 950 : 0.08010609447956085
Loss at iteration 960 : 0.1269933581352234
Loss at iteration 970 : 0.07620038092136383
Loss at iteration 980 : 0.08419564366340637
Loss at iteration 990 : 0.07130413502454758
Loss at iteration 1000 : 0.07346896082162857
Loss at iteration 1010 : 0.0940365195274353
Loss at iteration 1020 : 0.07347904145717621
Loss at iteration 1030 : 0.08783533424139023
Loss at iteration 1040 : 0.08966322988271713
Loss at iteration 1050 : 0.09652726352214813
Loss at iteration 1060 : 0.06853749603033066
Loss at iteration 1070 : 0.11334235966205597
Loss at iteration 1080 : 0.061116039752960205
Loss at iteration 1090 : 0.10082405805587769
Loss at iteration 1100 : 0.08081544935703278
Loss at iteration 1110 : 0.14028897881507874
Loss at iteration 1120 : 0.09380917251110077
Loss at iteration 1130 : 0.12936162948608398
Loss at iteration 1140 : 0.09657618403434753
Loss at iteration 1150 : 0.052945107221603394
Loss at iteration 1160 : 0.0639706701040268
Loss at iteration 1170 : 0.0964159220457077
Loss at iteration 1180 : 0.08364495635032654
Loss at iteration 1190 : 0.07117261737585068
Loss at iteration 1200 : 0.09856671094894409
Loss at iteration 1210 : 0.05979660153388977
The SSIM Value is: 0.7009419838587443
The PSNR Value is: 20.83395137786865
the epoch is: 70
Loss at iteration 10 : 0.08715701103210449
Loss at iteration 20 : 0.06950413435697556
Loss at iteration 30 : 0.09131477773189545
Loss at iteration 40 : 0.09035639464855194
Loss at iteration 50 : 0.1306746006011963
Loss at iteration 60 : 0.0743841677904129
Loss at iteration 70 : 0.07355853170156479
Loss at iteration 80 : 0.10695930570363998
Loss at iteration 90 : 0.07664468884468079
Loss at iteration 100 : 0.07828166335821152
Loss at iteration 110 : 0.11221805214881897
Loss at iteration 120 : 0.08299945294857025
Loss at iteration 130 : 0.04231284186244011
Loss at iteration 140 : 0.05916112661361694
Loss at iteration 150 : 0.08087760210037231
Loss at iteration 160 : 0.10929085314273834
Loss at iteration 170 : 0.0683249831199646
Loss at iteration 180 : 0.08262641727924347
Loss at iteration 190 : 0.08173377811908722
Loss at iteration 200 : 0.0948554128408432
Loss at iteration 210 : 0.06121907755732536
Loss at iteration 220 : 0.06949629634618759
Loss at iteration 230 : 0.09572663903236389
Loss at iteration 240 : 0.07125110924243927
Loss at iteration 250 : 0.12860901653766632
Loss at iteration 260 : 0.07801942527294159
Loss at iteration 270 : 0.051747433841228485
Loss at iteration 280 : 0.07112696766853333
Loss at iteration 290 : 0.10843422263860703
Loss at iteration 300 : 0.08218025416135788
Loss at iteration 310 : 0.07296621054410934
Loss at iteration 320 : 0.090629443526268
Loss at iteration 330 : 0.06794010102748871
Loss at iteration 340 : 0.05612083524465561
Loss at iteration 350 : 0.05607522651553154
Loss at iteration 360 : 0.08619754761457443
Loss at iteration 370 : 0.07754577696323395
Loss at iteration 380 : 0.08332543075084686
Loss at iteration 390 : 0.07634121179580688
Loss at iteration 400 : 0.08314947783946991
Loss at iteration 410 : 0.05855869501829147
Loss at iteration 420 : 0.07905429601669312
Loss at iteration 430 : 0.1577228605747223
Loss at iteration 440 : 0.09715692698955536
Loss at iteration 450 : 0.08477633446455002
Loss at iteration 460 : 0.11896593868732452
Loss at iteration 470 : 0.13811033964157104
Loss at iteration 480 : 0.07869350910186768
Loss at iteration 490 : 0.07843382656574249
Loss at iteration 500 : 0.09258928149938583
Loss at iteration 510 : 0.10298272967338562
Loss at iteration 520 : 0.14815247058868408
Loss at iteration 530 : 0.11246831715106964
Loss at iteration 540 : 0.11386588215827942
Loss at iteration 550 : 0.057391636073589325
Loss at iteration 560 : 0.07340461760759354
Loss at iteration 570 : 0.12192414700984955
Loss at iteration 580 : 0.11898710578680038
Loss at iteration 590 : 0.08333414793014526
Loss at iteration 600 : 0.1338443011045456
Loss at iteration 610 : 0.10200297832489014
Loss at iteration 620 : 0.08548200130462646
Loss at iteration 630 : 0.11375218629837036
Loss at iteration 640 : 0.07114192843437195
Loss at iteration 650 : 0.08307252824306488
Loss at iteration 660 : 0.08586537837982178
Loss at iteration 670 : 0.03500649705529213
Loss at iteration 680 : 0.12237854301929474
Loss at iteration 690 : 0.07012017071247101
Loss at iteration 700 : 0.08271288871765137
Loss at iteration 710 : 0.06935956329107285
Loss at iteration 720 : 0.08275732398033142
Loss at iteration 730 : 0.1307644546031952
Loss at iteration 740 : 0.10153158754110336
Loss at iteration 750 : 0.058806411921978
Loss at iteration 760 : 0.12865100800991058
Loss at iteration 770 : 0.06936682760715485
Loss at iteration 780 : 0.0670190304517746
Loss at iteration 790 : 0.09689971059560776
Loss at iteration 800 : 0.03688567876815796
Loss at iteration 810 : 0.047000568360090256
Loss at iteration 820 : 0.08909726142883301
Loss at iteration 830 : 0.11605598777532578
Loss at iteration 840 : 0.06905816495418549
Loss at iteration 850 : 0.06000632047653198
Loss at iteration 860 : 0.08988809585571289
Loss at iteration 870 : 0.08901059627532959
Loss at iteration 880 : 0.0835135281085968
Loss at iteration 890 : 0.10435481369495392
Loss at iteration 900 : 0.09196259826421738
Loss at iteration 910 : 0.12491074204444885
Loss at iteration 920 : 0.10955717414617538
Loss at iteration 930 : 0.10393320769071579
Loss at iteration 940 : 0.1263723224401474
Loss at iteration 950 : 0.09218847751617432
Loss at iteration 960 : 0.08808517456054688
Loss at iteration 970 : 0.07918445765972137
Loss at iteration 980 : 0.08659642934799194
Loss at iteration 990 : 0.09481512010097504
Loss at iteration 1000 : 0.10011100023984909
Loss at iteration 1010 : 0.0806502103805542
Loss at iteration 1020 : 0.095335952937603
Loss at iteration 1030 : 0.0543854646384716
Loss at iteration 1040 : 0.0739133358001709
Loss at iteration 1050 : 0.08960656821727753
Loss at iteration 1060 : 0.11421559751033783
Loss at iteration 1070 : 0.05979461595416069
Loss at iteration 1080 : 0.07616027444601059
Loss at iteration 1090 : 0.0647704005241394
Loss at iteration 1100 : 0.0784907191991806
Loss at iteration 1110 : 0.07700696587562561
Loss at iteration 1120 : 0.09070532023906708
Loss at iteration 1130 : 0.06632685661315918
Loss at iteration 1140 : 0.10544289648532867
Loss at iteration 1150 : 0.05269857496023178
Loss at iteration 1160 : 0.06511948257684708
Loss at iteration 1170 : 0.09007932990789413
Loss at iteration 1180 : 0.05960007756948471
Loss at iteration 1190 : 0.059506308287382126
Loss at iteration 1200 : 0.07469435036182404
Loss at iteration 1210 : 0.0791206806898117
The SSIM Value is: 0.7108283042907715
The PSNR Value is: 21.122939936319987
the epoch is: 71
Loss at iteration 10 : 0.057235848158597946
Loss at iteration 20 : 0.10164248943328857
Loss at iteration 30 : 0.13067694008350372
Loss at iteration 40 : 0.06107981503009796
Loss at iteration 50 : 0.09059694409370422
Loss at iteration 60 : 0.08784067630767822
Loss at iteration 70 : 0.0793510377407074
Loss at iteration 80 : 0.060610368847846985
Loss at iteration 90 : 0.09278392791748047
Loss at iteration 100 : 0.13256680965423584
Loss at iteration 110 : 0.06995528191328049
Loss at iteration 120 : 0.06693568825721741
Loss at iteration 130 : 0.08553354442119598
Loss at iteration 140 : 0.0689910501241684
Loss at iteration 150 : 0.10413932800292969
Loss at iteration 160 : 0.09248902648687363
Loss at iteration 170 : 0.08812364190816879
Loss at iteration 180 : 0.09076188504695892
Loss at iteration 190 : 0.08833201974630356
Loss at iteration 200 : 0.10212050378322601
Loss at iteration 210 : 0.08195082098245621
Loss at iteration 220 : 0.07762559503316879
Loss at iteration 230 : 0.06894560158252716
Loss at iteration 240 : 0.13778477907180786
Loss at iteration 250 : 0.07331323623657227
Loss at iteration 260 : 0.12037752568721771
Loss at iteration 270 : 0.07357296347618103
Loss at iteration 280 : 0.09110447019338608
Loss at iteration 290 : 0.07599157094955444
Loss at iteration 300 : 0.07969372719526291
Loss at iteration 310 : 0.1131671667098999
Loss at iteration 320 : 0.06272188574075699
Loss at iteration 330 : 0.0628921315073967
Loss at iteration 340 : 0.0968288779258728
Loss at iteration 350 : 0.10377228260040283
Loss at iteration 360 : 0.0803341418504715
Loss at iteration 370 : 0.06414163112640381
Loss at iteration 380 : 0.08838357776403427
Loss at iteration 390 : 0.08456626534461975
Loss at iteration 400 : 0.047541916370391846
Loss at iteration 410 : 0.0788697898387909
Loss at iteration 420 : 0.08292092382907867
Loss at iteration 430 : 0.061612874269485474
Loss at iteration 440 : 0.059313248842954636
Loss at iteration 450 : 0.103865846991539
Loss at iteration 460 : 0.11952286213636398
Loss at iteration 470 : 0.10756812989711761
Loss at iteration 480 : 0.043047916144132614
Loss at iteration 490 : 0.06563863158226013
Loss at iteration 500 : 0.05726974457502365
Loss at iteration 510 : 0.1013917326927185
Loss at iteration 520 : 0.0769466906785965
Loss at iteration 530 : 0.08195105195045471
Loss at iteration 540 : 0.056475184857845306
Loss at iteration 550 : 0.11452369391918182
Loss at iteration 560 : 0.1126134991645813
Loss at iteration 570 : 0.15676748752593994
Loss at iteration 580 : 0.09361951053142548
Loss at iteration 590 : 0.08533331751823425
Loss at iteration 600 : 0.0874829813838005
Loss at iteration 610 : 0.11474262177944183
Loss at iteration 620 : 0.05388469249010086
Loss at iteration 630 : 0.08574434369802475
Loss at iteration 640 : 0.09453992545604706
Loss at iteration 650 : 0.054710596799850464
Loss at iteration 660 : 0.14953765273094177
Loss at iteration 670 : 0.0762043446302414
Loss at iteration 680 : 0.06911315023899078
Loss at iteration 690 : 0.10900389403104782
Loss at iteration 700 : 0.07104235887527466
Loss at iteration 710 : 0.08428027480840683
Loss at iteration 720 : 0.13805744051933289
Loss at iteration 730 : 0.12195686995983124
Loss at iteration 740 : 0.06338536739349365
Loss at iteration 750 : 0.08640088886022568
Loss at iteration 760 : 0.12556004524230957
Loss at iteration 770 : 0.07575896382331848
Loss at iteration 780 : 0.0857960432767868
Loss at iteration 790 : 0.09245167672634125
Loss at iteration 800 : 0.08516960591077805
Loss at iteration 810 : 0.1048610657453537
Loss at iteration 820 : 0.09386850893497467
Loss at iteration 830 : 0.09878380596637726
Loss at iteration 840 : 0.08603250980377197
Loss at iteration 850 : 0.06935545057058334
Loss at iteration 860 : 0.07836177945137024
Loss at iteration 870 : 0.08840109407901764
Loss at iteration 880 : 0.0889897346496582
Loss at iteration 890 : 0.07151457667350769
Loss at iteration 900 : 0.0906844362616539
Loss at iteration 910 : 0.08933626115322113
Loss at iteration 920 : 0.10000577569007874
Loss at iteration 930 : 0.09077057242393494
Loss at iteration 940 : 0.07876788079738617
Loss at iteration 950 : 0.09207150340080261
Loss at iteration 960 : 0.10383705794811249
Loss at iteration 970 : 0.05970263481140137
Loss at iteration 980 : 0.07658156752586365
Loss at iteration 990 : 0.08548563718795776
Loss at iteration 1000 : 0.07781383395195007
Loss at iteration 1010 : 0.07440151274204254
Loss at iteration 1020 : 0.06490322947502136
Loss at iteration 1030 : 0.05278493091464043
Loss at iteration 1040 : 0.10264914482831955
Loss at iteration 1050 : 0.07887915521860123
Loss at iteration 1060 : 0.09716321527957916
Loss at iteration 1070 : 0.08473022282123566
Loss at iteration 1080 : 0.09941335767507553
Loss at iteration 1090 : 0.06480079889297485
Loss at iteration 1100 : 0.08026038110256195
Loss at iteration 1110 : 0.09736615419387817
Loss at iteration 1120 : 0.11993080377578735
Loss at iteration 1130 : 0.10156845301389694
Loss at iteration 1140 : 0.08971067517995834
Loss at iteration 1150 : 0.05724534019827843
Loss at iteration 1160 : 0.06601256132125854
Loss at iteration 1170 : 0.076470747590065
Loss at iteration 1180 : 0.10552655160427094
Loss at iteration 1190 : 0.05111289024353027
Loss at iteration 1200 : 0.08168648183345795
Loss at iteration 1210 : 0.07743528485298157
The SSIM Value is: 0.7040048638979594
The PSNR Value is: 20.83119265238444
the epoch is: 72
Loss at iteration 10 : 0.0739474818110466
Loss at iteration 20 : 0.06333069503307343
Loss at iteration 30 : 0.050894446671009064
Loss at iteration 40 : 0.04804393649101257
Loss at iteration 50 : 0.08212381601333618
Loss at iteration 60 : 0.07285553961992264
Loss at iteration 70 : 0.06896548718214035
Loss at iteration 80 : 0.04326469823718071
Loss at iteration 90 : 0.07777229696512222
Loss at iteration 100 : 0.0746569037437439
Loss at iteration 110 : 0.08919726312160492
Loss at iteration 120 : 0.09633511304855347
Loss at iteration 130 : 0.06670267879962921
Loss at iteration 140 : 0.08201650530099869
Loss at iteration 150 : 0.07508061826229095
Loss at iteration 160 : 0.10138349235057831
Loss at iteration 170 : 0.0800795704126358
Loss at iteration 180 : 0.10651317238807678
Loss at iteration 190 : 0.07145270705223083
Loss at iteration 200 : 0.0788404643535614
Loss at iteration 210 : 0.0630098208785057
Loss at iteration 220 : 0.1061519980430603
Loss at iteration 230 : 0.08198808133602142
Loss at iteration 240 : 0.09000655263662338
Loss at iteration 250 : 0.07064757496118546
Loss at iteration 260 : 0.11138107627630234
Loss at iteration 270 : 0.10243815183639526
Loss at iteration 280 : 0.05648810788989067
Loss at iteration 290 : 0.042309969663619995
Loss at iteration 300 : 0.07133200764656067
Loss at iteration 310 : 0.12868694961071014
Loss at iteration 320 : 0.07765749096870422
Loss at iteration 330 : 0.07431147992610931
Loss at iteration 340 : 0.06798388063907623
Loss at iteration 350 : 0.10020200908184052
Loss at iteration 360 : 0.10785709321498871
Loss at iteration 370 : 0.08914210647344589
Loss at iteration 380 : 0.08743112534284592
Loss at iteration 390 : 0.07841014862060547
Loss at iteration 400 : 0.07542672753334045
Loss at iteration 410 : 0.04714467376470566
Loss at iteration 420 : 0.1415783166885376
Loss at iteration 430 : 0.045539237558841705
Loss at iteration 440 : 0.08090805262327194
Loss at iteration 450 : 0.09285341203212738
Loss at iteration 460 : 0.09205514192581177
Loss at iteration 470 : 0.09886227548122406
Loss at iteration 480 : 0.11261244863271713
Loss at iteration 490 : 0.06073564291000366
Loss at iteration 500 : 0.10330371558666229
Loss at iteration 510 : 0.09374972432851791
Loss at iteration 520 : 0.08403022587299347
Loss at iteration 530 : 0.07313334941864014
Loss at iteration 540 : 0.0878252238035202
Loss at iteration 550 : 0.109428271651268
Loss at iteration 560 : 0.08563446998596191
Loss at iteration 570 : 0.12167537212371826
Loss at iteration 580 : 0.047960638999938965
Loss at iteration 590 : 0.06868283450603485
Loss at iteration 600 : 0.10737445205450058
Loss at iteration 610 : 0.07707034796476364
Loss at iteration 620 : 0.07450155913829803
Loss at iteration 630 : 0.09544286131858826
Loss at iteration 640 : 0.08059920370578766
Loss at iteration 650 : 0.06626742333173752
Loss at iteration 660 : 0.0502106249332428
Loss at iteration 670 : 0.08332908153533936
Loss at iteration 680 : 0.05182506889104843
Loss at iteration 690 : 0.06367422640323639
Loss at iteration 700 : 0.10570649802684784
Loss at iteration 710 : 0.08870232105255127
Loss at iteration 720 : 0.09729188680648804
Loss at iteration 730 : 0.09880009293556213
Loss at iteration 740 : 0.0896163135766983
Loss at iteration 750 : 0.07158541679382324
Loss at iteration 760 : 0.09580477327108383
Loss at iteration 770 : 0.06813535839319229
Loss at iteration 780 : 0.08527378737926483
Loss at iteration 790 : 0.08603663742542267
Loss at iteration 800 : 0.11080425977706909
Loss at iteration 810 : 0.10083240270614624
Loss at iteration 820 : 0.08650307357311249
Loss at iteration 830 : 0.13108080625534058
Loss at iteration 840 : 0.11613935232162476
Loss at iteration 850 : 0.08144944906234741
Loss at iteration 860 : 0.06334959715604782
Loss at iteration 870 : 0.07973438501358032
Loss at iteration 880 : 0.05229827016592026
Loss at iteration 890 : 0.12120026350021362
Loss at iteration 900 : 0.0526868961751461
Loss at iteration 910 : 0.07881566882133484
Loss at iteration 920 : 0.06455741822719574
Loss at iteration 930 : 0.08409328758716583
Loss at iteration 940 : 0.0617065504193306
Loss at iteration 950 : 0.1051800474524498
Loss at iteration 960 : 0.08959093689918518
Loss at iteration 970 : 0.09534281492233276
Loss at iteration 980 : 0.07306236773729324
Loss at iteration 990 : 0.05936269089579582
Loss at iteration 1000 : 0.0939972996711731
Loss at iteration 1010 : 0.081497423350811
Loss at iteration 1020 : 0.08079361915588379
Loss at iteration 1030 : 0.0982644185423851
Loss at iteration 1040 : 0.096445232629776
Loss at iteration 1050 : 0.06564364582300186
Loss at iteration 1060 : 0.05677551031112671
Loss at iteration 1070 : 0.05815830081701279
Loss at iteration 1080 : 0.06414070725440979
Loss at iteration 1090 : 0.08577325195074081
Loss at iteration 1100 : 0.0891326516866684
Loss at iteration 1110 : 0.155984029173851
Loss at iteration 1120 : 0.10811486840248108
Loss at iteration 1130 : 0.09107299894094467
Loss at iteration 1140 : 0.08455990254878998
Loss at iteration 1150 : 0.08997037261724472
Loss at iteration 1160 : 0.08522383868694305
Loss at iteration 1170 : 0.04070701450109482
Loss at iteration 1180 : 0.1495417207479477
Loss at iteration 1190 : 0.06961061805486679
Loss at iteration 1200 : 0.10045789182186127
Loss at iteration 1210 : 0.08451702445745468
The SSIM Value is: 0.7010695517063141
The PSNR Value is: 20.490468215942382
the epoch is: 73
Loss at iteration 10 : 0.13016919791698456
Loss at iteration 20 : 0.12237655371427536
Loss at iteration 30 : 0.09990956634283066
Loss at iteration 40 : 0.1059286817908287
Loss at iteration 50 : 0.12825028598308563
Loss at iteration 60 : 0.08474089205265045
Loss at iteration 70 : 0.08352857083082199
Loss at iteration 80 : 0.0703469067811966
Loss at iteration 90 : 0.12797725200653076
Loss at iteration 100 : 0.05803212523460388
Loss at iteration 110 : 0.09717230498790741
Loss at iteration 120 : 0.06382321566343307
Loss at iteration 130 : 0.06249726936221123
Loss at iteration 140 : 0.08778651058673859
Loss at iteration 150 : 0.07295612245798111
Loss at iteration 160 : 0.07418843358755112
Loss at iteration 170 : 0.10484230518341064
Loss at iteration 180 : 0.052870169281959534
Loss at iteration 190 : 0.0834776908159256
Loss at iteration 200 : 0.10098519921302795
Loss at iteration 210 : 0.09895837306976318
Loss at iteration 220 : 0.15006069839000702
Loss at iteration 230 : 0.09754522889852524
Loss at iteration 240 : 0.06857328116893768
Loss at iteration 250 : 0.17459766566753387
Loss at iteration 260 : 0.09180846065282822
Loss at iteration 270 : 0.07748168706893921
Loss at iteration 280 : 0.07205275446176529
Loss at iteration 290 : 0.08044455945491791
Loss at iteration 300 : 0.11275067180395126
Loss at iteration 310 : 0.06351914256811142
Loss at iteration 320 : 0.07677652686834335
Loss at iteration 330 : 0.11320306360721588
Loss at iteration 340 : 0.0863720178604126
Loss at iteration 350 : 0.09300466626882553
Loss at iteration 360 : 0.09001921117305756
Loss at iteration 370 : 0.11371465772390366
Loss at iteration 380 : 0.08459948748350143
Loss at iteration 390 : 0.1216665655374527
Loss at iteration 400 : 0.07050515711307526
Loss at iteration 410 : 0.08098278939723969
Loss at iteration 420 : 0.09365823119878769
Loss at iteration 430 : 0.07271596044301987
Loss at iteration 440 : 0.05267570540308952
Loss at iteration 450 : 0.08629673719406128
Loss at iteration 460 : 0.10081874579191208
Loss at iteration 470 : 0.1077149361371994
Loss at iteration 480 : 0.08078134059906006
Loss at iteration 490 : 0.11138045787811279
Loss at iteration 500 : 0.12605683505535126
Loss at iteration 510 : 0.07433699816465378
Loss at iteration 520 : 0.07759498059749603
Loss at iteration 530 : 0.11529429256916046
Loss at iteration 540 : 0.08501347154378891
Loss at iteration 550 : 0.10348081588745117
Loss at iteration 560 : 0.10344234108924866
Loss at iteration 570 : 0.07168551534414291
Loss at iteration 580 : 0.0966944545507431
Loss at iteration 590 : 0.03116725943982601
Loss at iteration 600 : 0.10322052240371704
Loss at iteration 610 : 0.10645830631256104
Loss at iteration 620 : 0.11608409881591797
Loss at iteration 630 : 0.1144232377409935
Loss at iteration 640 : 0.0935308188199997
Loss at iteration 650 : 0.06999863684177399
Loss at iteration 660 : 0.08679740130901337
Loss at iteration 670 : 0.11580520868301392
Loss at iteration 680 : 0.05042582005262375
Loss at iteration 690 : 0.0758601650595665
Loss at iteration 700 : 0.10480072349309921
Loss at iteration 710 : 0.1553587168455124
Loss at iteration 720 : 0.06941822171211243
Loss at iteration 730 : 0.059566497802734375
Loss at iteration 740 : 0.06826908886432648
Loss at iteration 750 : 0.07142914831638336
Loss at iteration 760 : 0.07711462676525116
Loss at iteration 770 : 0.08311592787504196
Loss at iteration 780 : 0.06997208297252655
Loss at iteration 790 : 0.07420787215232849
Loss at iteration 800 : 0.04316598176956177
Loss at iteration 810 : 0.07399271428585052
Loss at iteration 820 : 0.11764299124479294
Loss at iteration 830 : 0.06922685354948044
Loss at iteration 840 : 0.08173312246799469
Loss at iteration 850 : 0.05683727562427521
Loss at iteration 860 : 0.0892815887928009
Loss at iteration 870 : 0.0775875598192215
Loss at iteration 880 : 0.08597230911254883
Loss at iteration 890 : 0.07468371838331223
Loss at iteration 900 : 0.10280568897724152
Loss at iteration 910 : 0.12340837717056274
Loss at iteration 920 : 0.11369697749614716
Loss at iteration 930 : 0.08486386388540268
Loss at iteration 940 : 0.1129336804151535
Loss at iteration 950 : 0.10443054139614105
Loss at iteration 960 : 0.08577662706375122
Loss at iteration 970 : 0.09828498214483261
Loss at iteration 980 : 0.09046493470668793
Loss at iteration 990 : 0.07724948972463608
Loss at iteration 1000 : 0.0743323266506195
Loss at iteration 1010 : 0.06902823597192764
Loss at iteration 1020 : 0.08527307957410812
Loss at iteration 1030 : 0.08333676308393478
Loss at iteration 1040 : 0.03399945795536041
Loss at iteration 1050 : 0.0806940421462059
Loss at iteration 1060 : 0.09674098342657089
Loss at iteration 1070 : 0.09650278091430664
Loss at iteration 1080 : 0.10405535995960236
Loss at iteration 1090 : 0.08557817339897156
Loss at iteration 1100 : 0.12291735410690308
Loss at iteration 1110 : 0.048215508460998535
Loss at iteration 1120 : 0.06957197189331055
Loss at iteration 1130 : 0.09127627313137054
Loss at iteration 1140 : 0.09298907965421677
Loss at iteration 1150 : 0.08065564930438995
Loss at iteration 1160 : 0.05502206087112427
Loss at iteration 1170 : 0.07845565676689148
Loss at iteration 1180 : 0.04704279825091362
Loss at iteration 1190 : 0.06868100166320801
Loss at iteration 1200 : 0.09129943698644638
Loss at iteration 1210 : 0.06306078284978867
The SSIM Value is: 0.7101740797360738
The PSNR Value is: 21.36442337036133
the epoch is: 74
Loss at iteration 10 : 0.11002738773822784
Loss at iteration 20 : 0.09003635495901108
Loss at iteration 30 : 0.09858159720897675
Loss at iteration 40 : 0.1066102534532547
Loss at iteration 50 : 0.08333977311849594
Loss at iteration 60 : 0.08543461561203003
Loss at iteration 70 : 0.05184987187385559
Loss at iteration 80 : 0.08156005293130875
Loss at iteration 90 : 0.1038815826177597
Loss at iteration 100 : 0.09840276092290878
Loss at iteration 110 : 0.09708137810230255
Loss at iteration 120 : 0.09198717772960663
Loss at iteration 130 : 0.07748191058635712
Loss at iteration 140 : 0.048559751361608505
Loss at iteration 150 : 0.09015508741140366
Loss at iteration 160 : 0.0834939107298851
Loss at iteration 170 : 0.07514425367116928
Loss at iteration 180 : 0.08954104781150818
Loss at iteration 190 : 0.11735990643501282
Loss at iteration 200 : 0.1110483855009079
Loss at iteration 210 : 0.05093234032392502
Loss at iteration 220 : 0.07372959703207016
Loss at iteration 230 : 0.11143602430820465
Loss at iteration 240 : 0.08877619355916977
Loss at iteration 250 : 0.050024356693029404
Loss at iteration 260 : 0.14073054492473602
Loss at iteration 270 : 0.05829782783985138
Loss at iteration 280 : 0.1331767439842224
Loss at iteration 290 : 0.07691122591495514
Loss at iteration 300 : 0.06966152787208557
Loss at iteration 310 : 0.11451135575771332
Loss at iteration 320 : 0.10376101732254028
Loss at iteration 330 : 0.060478828847408295
Loss at iteration 340 : 0.0731654018163681
Loss at iteration 350 : 0.08404618501663208
Loss at iteration 360 : 0.07337938249111176
Loss at iteration 370 : 0.12440847605466843
Loss at iteration 380 : 0.0660882517695427
Loss at iteration 390 : 0.06640685349702835
Loss at iteration 400 : 0.08691583573818207
Loss at iteration 410 : 0.1188507229089737
Loss at iteration 420 : 0.10045546293258667
Loss at iteration 430 : 0.1076686903834343
Loss at iteration 440 : 0.06948786228895187
Loss at iteration 450 : 0.08418136090040207
Loss at iteration 460 : 0.0808263048529625
Loss at iteration 470 : 0.07898025959730148
Loss at iteration 480 : 0.07449060678482056
Loss at iteration 490 : 0.0727996826171875
Loss at iteration 500 : 0.0804438441991806
Loss at iteration 510 : 0.0647326335310936
Loss at iteration 520 : 0.09722903370857239
Loss at iteration 530 : 0.07664406299591064
Loss at iteration 540 : 0.12090270966291428
Loss at iteration 550 : 0.10068480670452118
Loss at iteration 560 : 0.0897049605846405
Loss at iteration 570 : 0.07694245129823685
Loss at iteration 580 : 0.039230939000844955
Loss at iteration 590 : 0.07822167873382568
Loss at iteration 600 : 0.08472182601690292
Loss at iteration 610 : 0.06328563392162323
Loss at iteration 620 : 0.04896160960197449
Loss at iteration 630 : 0.09229495376348495
Loss at iteration 640 : 0.09409654140472412
Loss at iteration 650 : 0.06961774826049805
Loss at iteration 660 : 0.09985654056072235
Loss at iteration 670 : 0.06454899907112122
Loss at iteration 680 : 0.20371145009994507
Loss at iteration 690 : 0.10113205760717392
Loss at iteration 700 : 0.07702973484992981
Loss at iteration 710 : 0.07745829224586487
Loss at iteration 720 : 0.05851462483406067
Loss at iteration 730 : 0.11727064847946167
Loss at iteration 740 : 0.1140180453658104
Loss at iteration 750 : 0.1054951399564743
Loss at iteration 760 : 0.08652256429195404
Loss at iteration 770 : 0.14862242341041565
Loss at iteration 780 : 0.0496576651930809
Loss at iteration 790 : 0.08576416969299316
Loss at iteration 800 : 0.13250941038131714
Loss at iteration 810 : 0.09315353631973267
Loss at iteration 820 : 0.06998572498559952
Loss at iteration 830 : 0.06347830593585968
Loss at iteration 840 : 0.09885065257549286
Loss at iteration 850 : 0.061791371554136276
Loss at iteration 860 : 0.05314047262072563
Loss at iteration 870 : 0.09207674860954285
Loss at iteration 880 : 0.06628647446632385
Loss at iteration 890 : 0.06479925662279129
Loss at iteration 900 : 0.08922957628965378
Loss at iteration 910 : 0.1503705233335495
Loss at iteration 920 : 0.09133803844451904
Loss at iteration 930 : 0.052213072776794434
Loss at iteration 940 : 0.06552198529243469
Loss at iteration 950 : 0.12500515580177307
Loss at iteration 960 : 0.11413644254207611
Loss at iteration 970 : 0.09190823882818222
Loss at iteration 980 : 0.12548905611038208
Loss at iteration 990 : 0.08828365057706833
Loss at iteration 1000 : 0.06665138900279999
Loss at iteration 1010 : 0.06473291665315628
Loss at iteration 1020 : 0.09746581315994263
Loss at iteration 1030 : 0.08363427966833115
Loss at iteration 1040 : 0.0807056874036789
Loss at iteration 1050 : 0.03962510824203491
Loss at iteration 1060 : 0.10915432870388031
Loss at iteration 1070 : 0.06283186376094818
Loss at iteration 1080 : 0.07982754707336426
Loss at iteration 1090 : 0.05438448116183281
Loss at iteration 1100 : 0.07084415853023529
Loss at iteration 1110 : 0.08810721337795258
Loss at iteration 1120 : 0.07192663848400116
Loss at iteration 1130 : 0.10806472599506378
Loss at iteration 1140 : 0.07841649651527405
Loss at iteration 1150 : 0.07958382368087769
Loss at iteration 1160 : 0.11407359689474106
Loss at iteration 1170 : 0.11536320298910141
Loss at iteration 1180 : 0.10580986738204956
Loss at iteration 1190 : 0.06238723546266556
Loss at iteration 1200 : 0.07858067750930786
Loss at iteration 1210 : 0.11635236442089081
The SSIM Value is: 0.7063458502292633
The PSNR Value is: 20.588156763712565
the epoch is: 75
Loss at iteration 10 : 0.06003260612487793
Loss at iteration 20 : 0.0723157674074173
Loss at iteration 30 : 0.12242462486028671
Loss at iteration 40 : 0.06713450700044632
Loss at iteration 50 : 0.05257305130362511
Loss at iteration 60 : 0.08216233551502228
Loss at iteration 70 : 0.06944000720977783
Loss at iteration 80 : 0.09053989499807358
Loss at iteration 90 : 0.06990507990121841
Loss at iteration 100 : 0.12334667146205902
Loss at iteration 110 : 0.04896872490644455
Loss at iteration 120 : 0.09892982244491577
Loss at iteration 130 : 0.09555565565824509
Loss at iteration 140 : 0.0683809444308281
Loss at iteration 150 : 0.08733546733856201
Loss at iteration 160 : 0.10225071012973785
Loss at iteration 170 : 0.11136762797832489
Loss at iteration 180 : 0.06471946090459824
Loss at iteration 190 : 0.05612514540553093
Loss at iteration 200 : 0.04766790196299553
Loss at iteration 210 : 0.11026676744222641
Loss at iteration 220 : 0.0718456357717514
Loss at iteration 230 : 0.12765376269817352
Loss at iteration 240 : 0.06370168924331665
Loss at iteration 250 : 0.04808977246284485
Loss at iteration 260 : 0.09589719772338867
Loss at iteration 270 : 0.07233986258506775
Loss at iteration 280 : 0.09008821099996567
Loss at iteration 290 : 0.11511509120464325
Loss at iteration 300 : 0.07594877481460571
Loss at iteration 310 : 0.0842168852686882
Loss at iteration 320 : 0.06696899980306625
Loss at iteration 330 : 0.05956333875656128
Loss at iteration 340 : 0.06736113876104355
Loss at iteration 350 : 0.060146015137434006
Loss at iteration 360 : 0.05831236392259598
Loss at iteration 370 : 0.08054219186306
Loss at iteration 380 : 0.07182049751281738
Loss at iteration 390 : 0.03922799229621887
Loss at iteration 400 : 0.08393891900777817
Loss at iteration 410 : 0.07741760462522507
Loss at iteration 420 : 0.07851166278123856
Loss at iteration 430 : 0.07268068194389343
Loss at iteration 440 : 0.06858499348163605
Loss at iteration 450 : 0.0791688859462738
Loss at iteration 460 : 0.07176388055086136
Loss at iteration 470 : 0.10122670233249664
Loss at iteration 480 : 0.10062035173177719
Loss at iteration 490 : 0.058598097413778305
Loss at iteration 500 : 0.11969979852437973
Loss at iteration 510 : 0.14285308122634888
Loss at iteration 520 : 0.09864326566457748
Loss at iteration 530 : 0.07345074415206909
Loss at iteration 540 : 0.08602418750524521
Loss at iteration 550 : 0.06965260952711105
Loss at iteration 560 : 0.08722635358572006
Loss at iteration 570 : 0.09481900930404663
Loss at iteration 580 : 0.05558241531252861
Loss at iteration 590 : 0.1030544862151146
Loss at iteration 600 : 0.05557722970843315
Loss at iteration 610 : 0.08690859377384186
Loss at iteration 620 : 0.09465418756008148
Loss at iteration 630 : 0.11321379989385605
Loss at iteration 640 : 0.06998615711927414
Loss at iteration 650 : 0.07795287668704987
Loss at iteration 660 : 0.053031206130981445
Loss at iteration 670 : 0.09799741208553314
Loss at iteration 680 : 0.0531502291560173
Loss at iteration 690 : 0.07058469951152802
Loss at iteration 700 : 0.06982539594173431
Loss at iteration 710 : 0.10629010200500488
Loss at iteration 720 : 0.06200842186808586
Loss at iteration 730 : 0.11166443675756454
Loss at iteration 740 : 0.06211152672767639
Loss at iteration 750 : 0.0510948970913887
Loss at iteration 760 : 0.10858523845672607
Loss at iteration 770 : 0.08982893079519272
Loss at iteration 780 : 0.08989566564559937
Loss at iteration 790 : 0.07452861964702606
Loss at iteration 800 : 0.09950321912765503
Loss at iteration 810 : 0.11101748794317245
Loss at iteration 820 : 0.09080009162425995
Loss at iteration 830 : 0.09466366469860077
Loss at iteration 840 : 0.10562469810247421
Loss at iteration 850 : 0.05499395355582237
Loss at iteration 860 : 0.03753096982836723
Loss at iteration 870 : 0.09146108478307724
Loss at iteration 880 : 0.047861646860837936
Loss at iteration 890 : 0.08512088656425476
Loss at iteration 900 : 0.05362090468406677
Loss at iteration 910 : 0.12546885013580322
Loss at iteration 920 : 0.09361211955547333
Loss at iteration 930 : 0.09823646396398544
Loss at iteration 940 : 0.0974382683634758
Loss at iteration 950 : 0.0595322847366333
Loss at iteration 960 : 0.11689873784780502
Loss at iteration 970 : 0.0808095782995224
Loss at iteration 980 : 0.08346158266067505
Loss at iteration 990 : 0.05656997486948967
Loss at iteration 1000 : 0.10103213787078857
Loss at iteration 1010 : 0.10320930927991867
Loss at iteration 1020 : 0.12026824802160263
Loss at iteration 1030 : 0.07560693472623825
Loss at iteration 1040 : 0.1267419159412384
Loss at iteration 1050 : 0.10922229290008545
Loss at iteration 1060 : 0.08522053062915802
Loss at iteration 1070 : 0.09397946298122406
Loss at iteration 1080 : 0.06135236471891403
Loss at iteration 1090 : 0.1028372272849083
Loss at iteration 1100 : 0.07990461587905884
Loss at iteration 1110 : 0.12395000457763672
Loss at iteration 1120 : 0.08790158480405807
Loss at iteration 1130 : 0.07018475979566574
Loss at iteration 1140 : 0.08473853766918182
Loss at iteration 1150 : 0.1000530943274498
Loss at iteration 1160 : 0.058137279003858566
Loss at iteration 1170 : 0.06958810985088348
Loss at iteration 1180 : 0.05674642324447632
Loss at iteration 1190 : 0.03696015477180481
Loss at iteration 1200 : 0.10905212163925171
Loss at iteration 1210 : 0.1128208339214325
The SSIM Value is: 0.7071152369181315
The PSNR Value is: 20.93339703877767
the epoch is: 76
Loss at iteration 10 : 0.10328584909439087
Loss at iteration 20 : 0.09744344651699066
Loss at iteration 30 : 0.07322856038808823
Loss at iteration 40 : 0.05961121246218681
Loss at iteration 50 : 0.0699114128947258
Loss at iteration 60 : 0.10176233947277069
Loss at iteration 70 : 0.06524576991796494
Loss at iteration 80 : 0.14133743941783905
Loss at iteration 90 : 0.12602421641349792
Loss at iteration 100 : 0.06370803713798523
Loss at iteration 110 : 0.04747886210680008
Loss at iteration 120 : 0.05023862421512604
Loss at iteration 130 : 0.10437990725040436
Loss at iteration 140 : 0.04461060091853142
Loss at iteration 150 : 0.08904910087585449
Loss at iteration 160 : 0.09164812415838242
Loss at iteration 170 : 0.056535594165325165
Loss at iteration 180 : 0.09084300696849823
Loss at iteration 190 : 0.06442239135503769
Loss at iteration 200 : 0.12358264625072479
Loss at iteration 210 : 0.05671795457601547
Loss at iteration 220 : 0.07204325497150421
Loss at iteration 230 : 0.06405390799045563
Loss at iteration 240 : 0.11688484251499176
Loss at iteration 250 : 0.0776546522974968
Loss at iteration 260 : 0.10831911861896515
Loss at iteration 270 : 0.07760557532310486
Loss at iteration 280 : 0.09696565568447113
Loss at iteration 290 : 0.10353128612041473
Loss at iteration 300 : 0.11435702443122864
Loss at iteration 310 : 0.1148582398891449
Loss at iteration 320 : 0.113779217004776
Loss at iteration 330 : 0.045847319066524506
Loss at iteration 340 : 0.08013378828763962
Loss at iteration 350 : 0.08701908588409424
Loss at iteration 360 : 0.12267785519361496
Loss at iteration 370 : 0.11272285878658295
Loss at iteration 380 : 0.09122293442487717
Loss at iteration 390 : 0.0831005647778511
Loss at iteration 400 : 0.11904623359441757
Loss at iteration 410 : 0.12949147820472717
Loss at iteration 420 : 0.09304066002368927
Loss at iteration 430 : 0.05926872417330742
Loss at iteration 440 : 0.10765498876571655
Loss at iteration 450 : 0.069810651242733
Loss at iteration 460 : 0.09378288686275482
Loss at iteration 470 : 0.0685240849852562
Loss at iteration 480 : 0.08985091745853424
Loss at iteration 490 : 0.08229447901248932
Loss at iteration 500 : 0.10180451720952988
Loss at iteration 510 : 0.07549719512462616
Loss at iteration 520 : 0.08210739493370056
Loss at iteration 530 : 0.057313352823257446
Loss at iteration 540 : 0.07476656138896942
Loss at iteration 550 : 0.12064963579177856
Loss at iteration 560 : 0.10095228999853134
Loss at iteration 570 : 0.06226767599582672
Loss at iteration 580 : 0.09098919481039047
Loss at iteration 590 : 0.04134032875299454
Loss at iteration 600 : 0.129375621676445
Loss at iteration 610 : 0.09020774066448212
Loss at iteration 620 : 0.07540131360292435
Loss at iteration 630 : 0.05277588218450546
Loss at iteration 640 : 0.11259018629789352
Loss at iteration 650 : 0.1241157278418541
Loss at iteration 660 : 0.09744091331958771
Loss at iteration 670 : 0.15851803123950958
Loss at iteration 680 : 0.10411366075277328
Loss at iteration 690 : 0.16686181724071503
Loss at iteration 700 : 0.06984005868434906
Loss at iteration 710 : 0.059776000678539276
Loss at iteration 720 : 0.07188986241817474
Loss at iteration 730 : 0.09022286534309387
Loss at iteration 740 : 0.08240148425102234
Loss at iteration 750 : 0.06366924941539764
Loss at iteration 760 : 0.07998156547546387
Loss at iteration 770 : 0.04722316190600395
Loss at iteration 780 : 0.047744475305080414
Loss at iteration 790 : 0.08134577423334122
Loss at iteration 800 : 0.09059484302997589
Loss at iteration 810 : 0.06370815634727478
Loss at iteration 820 : 0.11379673331975937
Loss at iteration 830 : 0.13402965664863586
Loss at iteration 840 : 0.1069134920835495
Loss at iteration 850 : 0.04942574352025986
Loss at iteration 860 : 0.08584447205066681
Loss at iteration 870 : 0.09849236905574799
Loss at iteration 880 : 0.10977112501859665
Loss at iteration 890 : 0.07947180420160294
Loss at iteration 900 : 0.08095043152570724
Loss at iteration 910 : 0.07690328359603882
Loss at iteration 920 : 0.10432712733745575
Loss at iteration 930 : 0.07590587437152863
Loss at iteration 940 : 0.07268461585044861
Loss at iteration 950 : 0.08881270885467529
Loss at iteration 960 : 0.06200747936964035
Loss at iteration 970 : 0.08515268564224243
Loss at iteration 980 : 0.09598687291145325
Loss at iteration 990 : 0.052724987268447876
Loss at iteration 1000 : 0.10703577101230621
Loss at iteration 1010 : 0.11780482530593872
Loss at iteration 1020 : 0.06919005513191223
Loss at iteration 1030 : 0.10934149473905563
Loss at iteration 1040 : 0.12168445438146591
Loss at iteration 1050 : 0.10026180744171143
Loss at iteration 1060 : 0.06340722739696503
Loss at iteration 1070 : 0.10884645581245422
Loss at iteration 1080 : 0.09083127230405807
Loss at iteration 1090 : 0.11143611371517181
Loss at iteration 1100 : 0.05843812972307205
Loss at iteration 1110 : 0.07581166923046112
Loss at iteration 1120 : 0.11075025796890259
Loss at iteration 1130 : 0.07327383756637573
Loss at iteration 1140 : 0.09359204769134521
Loss at iteration 1150 : 0.05822017788887024
Loss at iteration 1160 : 0.03731738403439522
Loss at iteration 1170 : 0.09175822138786316
Loss at iteration 1180 : 0.09644439071416855
Loss at iteration 1190 : 0.07702064514160156
Loss at iteration 1200 : 0.0857144445180893
Loss at iteration 1210 : 0.10607610642910004
The SSIM Value is: 0.7078657944997152
The PSNR Value is: 21.004767608642577
the epoch is: 77
Loss at iteration 10 : 0.11313709616661072
Loss at iteration 20 : 0.0986185073852539
Loss at iteration 30 : 0.08964791148900986
Loss at iteration 40 : 0.05385869741439819
Loss at iteration 50 : 0.09821927547454834
Loss at iteration 60 : 0.09772174060344696
Loss at iteration 70 : 0.12170540541410446
Loss at iteration 80 : 0.116130530834198
Loss at iteration 90 : 0.07743104547262192
Loss at iteration 100 : 0.07073260843753815
Loss at iteration 110 : 0.06552918255329132
Loss at iteration 120 : 0.05857427045702934
Loss at iteration 130 : 0.10800996422767639
Loss at iteration 140 : 0.049135368317365646
Loss at iteration 150 : 0.09268661588430405
Loss at iteration 160 : 0.08385135978460312
Loss at iteration 170 : 0.06893109530210495
Loss at iteration 180 : 0.07704511284828186
Loss at iteration 190 : 0.07648320496082306
Loss at iteration 200 : 0.06382863968610764
Loss at iteration 210 : 0.08222140371799469
Loss at iteration 220 : 0.08315762132406235
Loss at iteration 230 : 0.08537282794713974
Loss at iteration 240 : 0.0896303579211235
Loss at iteration 250 : 0.09906735271215439
Loss at iteration 260 : 0.11576034873723984
Loss at iteration 270 : 0.06185116618871689
Loss at iteration 280 : 0.08617478609085083
Loss at iteration 290 : 0.10238151997327805
Loss at iteration 300 : 0.14130178093910217
Loss at iteration 310 : 0.095340296626091
Loss at iteration 320 : 0.07559145241975784
Loss at iteration 330 : 0.09590491652488708
Loss at iteration 340 : 0.04281310364603996
Loss at iteration 350 : 0.05547001585364342
Loss at iteration 360 : 0.06627446413040161
Loss at iteration 370 : 0.10081305354833603
Loss at iteration 380 : 0.12782122194766998
Loss at iteration 390 : 0.10596619546413422
Loss at iteration 400 : 0.08250712603330612
Loss at iteration 410 : 0.07900984585285187
Loss at iteration 420 : 0.061610445380210876
Loss at iteration 430 : 0.0732172280550003
Loss at iteration 440 : 0.055820971727371216
Loss at iteration 450 : 0.08092981576919556
Loss at iteration 460 : 0.05687938258051872
Loss at iteration 470 : 0.0953095555305481
Loss at iteration 480 : 0.06622207164764404
Loss at iteration 490 : 0.09684443473815918
Loss at iteration 500 : 0.07535907626152039
Loss at iteration 510 : 0.11894645541906357
Loss at iteration 520 : 0.064950130879879
Loss at iteration 530 : 0.07530325651168823
Loss at iteration 540 : 0.0806444063782692
Loss at iteration 550 : 0.09094242751598358
Loss at iteration 560 : 0.09427875280380249
Loss at iteration 570 : 0.0779217928647995
Loss at iteration 580 : 0.06061483174562454
Loss at iteration 590 : 0.09380712360143661
Loss at iteration 600 : 0.0576019212603569
Loss at iteration 610 : 0.10305356234312057
Loss at iteration 620 : 0.08567595481872559
Loss at iteration 630 : 0.08915035426616669
Loss at iteration 640 : 0.05899599939584732
Loss at iteration 650 : 0.12667739391326904
Loss at iteration 660 : 0.10384198278188705
Loss at iteration 670 : 0.09590413421392441
Loss at iteration 680 : 0.07730720937252045
Loss at iteration 690 : 0.09083043038845062
Loss at iteration 700 : 0.08356644213199615
Loss at iteration 710 : 0.08672138303518295
Loss at iteration 720 : 0.04375430941581726
Loss at iteration 730 : 0.07161202281713486
Loss at iteration 740 : 0.13726022839546204
Loss at iteration 750 : 0.11922676861286163
Loss at iteration 760 : 0.08364273607730865
Loss at iteration 770 : 0.07514512538909912
Loss at iteration 780 : 0.12452705949544907
Loss at iteration 790 : 0.05640590190887451
Loss at iteration 800 : 0.06462472677230835
Loss at iteration 810 : 0.10313849151134491
Loss at iteration 820 : 0.06504452228546143
Loss at iteration 830 : 0.0975039005279541
Loss at iteration 840 : 0.04626186937093735
Loss at iteration 850 : 0.06493362784385681
Loss at iteration 860 : 0.0959395319223404
Loss at iteration 870 : 0.055742375552654266
Loss at iteration 880 : 0.07610570639371872
Loss at iteration 890 : 0.059295654296875
Loss at iteration 900 : 0.07234318554401398
Loss at iteration 910 : 0.0629921704530716
Loss at iteration 920 : 0.09298233687877655
Loss at iteration 930 : 0.1211184486746788
Loss at iteration 940 : 0.06008550524711609
Loss at iteration 950 : 0.05377615615725517
Loss at iteration 960 : 0.06630105525255203
Loss at iteration 970 : 0.09799584001302719
Loss at iteration 980 : 0.08006738871335983
Loss at iteration 990 : 0.052329596132040024
Loss at iteration 1000 : 0.08492153137922287
Loss at iteration 1010 : 0.08124825358390808
Loss at iteration 1020 : 0.08900104463100433
Loss at iteration 1030 : 0.09469681978225708
Loss at iteration 1040 : 0.07655653357505798
Loss at iteration 1050 : 0.04395186901092529
Loss at iteration 1060 : 0.04565005004405975
Loss at iteration 1070 : 0.06475597620010376
Loss at iteration 1080 : 0.06799199432134628
Loss at iteration 1090 : 0.09754666686058044
Loss at iteration 1100 : 0.060523610562086105
Loss at iteration 1110 : 0.04401097819209099
Loss at iteration 1120 : 0.08562567085027695
Loss at iteration 1130 : 0.07453816384077072
Loss at iteration 1140 : 0.053366199135780334
Loss at iteration 1150 : 0.08052076399326324
Loss at iteration 1160 : 0.07582928240299225
Loss at iteration 1170 : 0.0936710461974144
Loss at iteration 1180 : 0.1245131716132164
Loss at iteration 1190 : 0.0811220109462738
Loss at iteration 1200 : 0.09045162051916122
Loss at iteration 1210 : 0.04953718185424805
The SSIM Value is: 0.7065076251824697
The PSNR Value is: 20.550352478027342
the epoch is: 78
Loss at iteration 10 : 0.06557624787092209
Loss at iteration 20 : 0.11094190925359726
Loss at iteration 30 : 0.098436139523983
Loss at iteration 40 : 0.07233576476573944
Loss at iteration 50 : 0.08360444754362106
Loss at iteration 60 : 0.07830297946929932
Loss at iteration 70 : 0.05901230499148369
Loss at iteration 80 : 0.06300683319568634
Loss at iteration 90 : 0.13733509182929993
Loss at iteration 100 : 0.09376722574234009
Loss at iteration 110 : 0.0973053127527237
Loss at iteration 120 : 0.08739980310201645
Loss at iteration 130 : 0.07411080598831177
Loss at iteration 140 : 0.07427119463682175
Loss at iteration 150 : 0.10744571685791016
Loss at iteration 160 : 0.10525348037481308
Loss at iteration 170 : 0.07551781833171844
Loss at iteration 180 : 0.07309053838253021
Loss at iteration 190 : 0.07735367119312286
Loss at iteration 200 : 0.0722014307975769
Loss at iteration 210 : 0.08113961666822433
Loss at iteration 220 : 0.08821980655193329
Loss at iteration 230 : 0.07604857534170151
Loss at iteration 240 : 0.1347598284482956
Loss at iteration 250 : 0.056946855038404465
Loss at iteration 260 : 0.06769825518131256
Loss at iteration 270 : 0.09816288948059082
Loss at iteration 280 : 0.08169174194335938
Loss at iteration 290 : 0.0656246617436409
Loss at iteration 300 : 0.08658390492200851
Loss at iteration 310 : 0.11355425417423248
Loss at iteration 320 : 0.1057487279176712
Loss at iteration 330 : 0.05911066383123398
Loss at iteration 340 : 0.09005534648895264
Loss at iteration 350 : 0.06888820230960846
Loss at iteration 360 : 0.05821788311004639
Loss at iteration 370 : 0.07802893966436386
Loss at iteration 380 : 0.0828256756067276
Loss at iteration 390 : 0.1098322793841362
Loss at iteration 400 : 0.07992731034755707
Loss at iteration 410 : 0.06746487319469452
Loss at iteration 420 : 0.09100400656461716
Loss at iteration 430 : 0.07906297594308853
Loss at iteration 440 : 0.07469933480024338
Loss at iteration 450 : 0.043404798954725266
Loss at iteration 460 : 0.07534892857074738
Loss at iteration 470 : 0.13399842381477356
Loss at iteration 480 : 0.12016334384679794
Loss at iteration 490 : 0.0853181704878807
Loss at iteration 500 : 0.09014197438955307
Loss at iteration 510 : 0.08486752212047577
Loss at iteration 520 : 0.052690327167510986
Loss at iteration 530 : 0.09371133893728256
Loss at iteration 540 : 0.11635485291481018
Loss at iteration 550 : 0.08157657086849213
Loss at iteration 560 : 0.07743771374225616
Loss at iteration 570 : 0.09521238505840302
Loss at iteration 580 : 0.08815394341945648
Loss at iteration 590 : 0.06922838091850281
Loss at iteration 600 : 0.07027895748615265
Loss at iteration 610 : 0.0702831894159317
Loss at iteration 620 : 0.10221616923809052
Loss at iteration 630 : 0.1009274572134018
Loss at iteration 640 : 0.09619618952274323
Loss at iteration 650 : 0.06962146610021591
Loss at iteration 660 : 0.12123161554336548
Loss at iteration 670 : 0.0688256248831749
Loss at iteration 680 : 0.08972112834453583
Loss at iteration 690 : 0.09381013363599777
Loss at iteration 700 : 0.12266503274440765
Loss at iteration 710 : 0.06189270317554474
Loss at iteration 720 : 0.06792405992746353
Loss at iteration 730 : 0.103839211165905
Loss at iteration 740 : 0.0998762845993042
Loss at iteration 750 : 0.09923750162124634
Loss at iteration 760 : 0.07140763103961945
Loss at iteration 770 : 0.06151074916124344
Loss at iteration 780 : 0.04851904511451721
Loss at iteration 790 : 0.06950187683105469
Loss at iteration 800 : 0.09259156882762909
Loss at iteration 810 : 0.06687773764133453
Loss at iteration 820 : 0.0867607444524765
Loss at iteration 830 : 0.051214467734098434
Loss at iteration 840 : 0.05673111602663994
Loss at iteration 850 : 0.05960614234209061
Loss at iteration 860 : 0.08038800209760666
Loss at iteration 870 : 0.06721974164247513
Loss at iteration 880 : 0.1356736719608307
Loss at iteration 890 : 0.07783862203359604
Loss at iteration 900 : 0.08031493425369263
Loss at iteration 910 : 0.11424880474805832
Loss at iteration 920 : 0.10292771458625793
Loss at iteration 930 : 0.06080445647239685
Loss at iteration 940 : 0.07325684279203415
Loss at iteration 950 : 0.05152706429362297
Loss at iteration 960 : 0.1285310685634613
Loss at iteration 970 : 0.07468913495540619
Loss at iteration 980 : 0.06326110661029816
Loss at iteration 990 : 0.09921007603406906
Loss at iteration 1000 : 0.09578823298215866
Loss at iteration 1010 : 0.06897929310798645
Loss at iteration 1020 : 0.11746291816234589
Loss at iteration 1030 : 0.12261664122343063
Loss at iteration 1040 : 0.08123498409986496
Loss at iteration 1050 : 0.11463507264852524
Loss at iteration 1060 : 0.08757668733596802
Loss at iteration 1070 : 0.11078837513923645
Loss at iteration 1080 : 0.09797816723585129
Loss at iteration 1090 : 0.11247118562459946
Loss at iteration 1100 : 0.07109388709068298
Loss at iteration 1110 : 0.11217417567968369
Loss at iteration 1120 : 0.15921996533870697
Loss at iteration 1130 : 0.05720355361700058
Loss at iteration 1140 : 0.06839419901371002
Loss at iteration 1150 : 0.09614728391170502
Loss at iteration 1160 : 0.0942811667919159
Loss at iteration 1170 : 0.08067555725574493
Loss at iteration 1180 : 0.07890293002128601
Loss at iteration 1190 : 0.06481508910655975
Loss at iteration 1200 : 0.08658139407634735
Loss at iteration 1210 : 0.06957700848579407
The SSIM Value is: 0.7097729921340943
The PSNR Value is: 20.885343297322592
the epoch is: 79
Loss at iteration 10 : 0.0798412412405014
Loss at iteration 20 : 0.0765921026468277
Loss at iteration 30 : 0.08022741973400116
Loss at iteration 40 : 0.06716176122426987
Loss at iteration 50 : 0.08120261877775192
Loss at iteration 60 : 0.06426072120666504
Loss at iteration 70 : 0.07259371131658554
Loss at iteration 80 : 0.07552169263362885
Loss at iteration 90 : 0.10668711364269257
Loss at iteration 100 : 0.08958853781223297
Loss at iteration 110 : 0.0713605135679245
Loss at iteration 120 : 0.05231577157974243
Loss at iteration 130 : 0.07602685689926147
Loss at iteration 140 : 0.14333708584308624
Loss at iteration 150 : 0.08693540096282959
Loss at iteration 160 : 0.10399395227432251
Loss at iteration 170 : 0.07439349591732025
Loss at iteration 180 : 0.06029354780912399
Loss at iteration 190 : 0.10987897962331772
Loss at iteration 200 : 0.0808669924736023
Loss at iteration 210 : 0.05456767603754997
Loss at iteration 220 : 0.10861452668905258
Loss at iteration 230 : 0.07284067571163177
Loss at iteration 240 : 0.12301071733236313
Loss at iteration 250 : 0.06672796607017517
Loss at iteration 260 : 0.13762806355953217
Loss at iteration 270 : 0.0936773270368576
Loss at iteration 280 : 0.0677056759595871
Loss at iteration 290 : 0.08308424055576324
Loss at iteration 300 : 0.1115545853972435
Loss at iteration 310 : 0.09251129627227783
Loss at iteration 320 : 0.0682908147573471
Loss at iteration 330 : 0.1348925530910492
Loss at iteration 340 : 0.07479430735111237
Loss at iteration 350 : 0.13341878354549408
Loss at iteration 360 : 0.0954437404870987
Loss at iteration 370 : 0.07764187455177307
Loss at iteration 380 : 0.05976657196879387
Loss at iteration 390 : 0.04404269903898239
Loss at iteration 400 : 0.08327445387840271
Loss at iteration 410 : 0.10234761238098145
Loss at iteration 420 : 0.09366676211357117
Loss at iteration 430 : 0.08783477544784546
Loss at iteration 440 : 0.09089824557304382
Loss at iteration 450 : 0.09790408611297607
Loss at iteration 460 : 0.1006714329123497
Loss at iteration 470 : 0.07676763832569122
Loss at iteration 480 : 0.07727059721946716
Loss at iteration 490 : 0.0877978652715683
Loss at iteration 500 : 0.09143435955047607
Loss at iteration 510 : 0.08767464011907578
Loss at iteration 520 : 0.0576980821788311
Loss at iteration 530 : 0.06571198254823685
Loss at iteration 540 : 0.09220781177282333
Loss at iteration 550 : 0.08566243946552277
Loss at iteration 560 : 0.0870814174413681
Loss at iteration 570 : 0.08598440140485764
Loss at iteration 580 : 0.05195704102516174
Loss at iteration 590 : 0.0822620615363121
Loss at iteration 600 : 0.055756695568561554
Loss at iteration 610 : 0.09734545648097992
Loss at iteration 620 : 0.1454484611749649
Loss at iteration 630 : 0.08937191963195801
Loss at iteration 640 : 0.11105874180793762
Loss at iteration 650 : 0.11361958086490631
Loss at iteration 660 : 0.06585268676280975
Loss at iteration 670 : 0.08876615762710571
Loss at iteration 680 : 0.08630087226629257
Loss at iteration 690 : 0.08585309982299805
Loss at iteration 700 : 0.09256406873464584
Loss at iteration 710 : 0.07513175904750824
Loss at iteration 720 : 0.08056499063968658
Loss at iteration 730 : 0.11357244104146957
Loss at iteration 740 : 0.06214689463376999
Loss at iteration 750 : 0.0840858668088913
Loss at iteration 760 : 0.054250799119472504
Loss at iteration 770 : 0.062475115060806274
Loss at iteration 780 : 0.06190694868564606
Loss at iteration 790 : 0.08240142464637756
Loss at iteration 800 : 0.11995633691549301
Loss at iteration 810 : 0.07652462273836136
Loss at iteration 820 : 0.05216192454099655
Loss at iteration 830 : 0.10505707561969757
Loss at iteration 840 : 0.044665999710559845
Loss at iteration 850 : 0.05598854273557663
Loss at iteration 860 : 0.09677629917860031
Loss at iteration 870 : 0.122919961810112
Loss at iteration 880 : 0.051568251103162766
Loss at iteration 890 : 0.05247003957629204
Loss at iteration 900 : 0.08203323185443878
Loss at iteration 910 : 0.05233001708984375
Loss at iteration 920 : 0.08824020624160767
Loss at iteration 930 : 0.09355781972408295
Loss at iteration 940 : 0.10277148336172104
Loss at iteration 950 : 0.06017875671386719
Loss at iteration 960 : 0.09078632295131683
Loss at iteration 970 : 0.05384506285190582
Loss at iteration 980 : 0.10546847432851791
Loss at iteration 990 : 0.06633293628692627
Loss at iteration 1000 : 0.10471959412097931
Loss at iteration 1010 : 0.12246361374855042
Loss at iteration 1020 : 0.12834390997886658
Loss at iteration 1030 : 0.05517026409506798
Loss at iteration 1040 : 0.04286639019846916
Loss at iteration 1050 : 0.0976061075925827
Loss at iteration 1060 : 0.12820032238960266
Loss at iteration 1070 : 0.08993285149335861
Loss at iteration 1080 : 0.06271861493587494
Loss at iteration 1090 : 0.07326309382915497
Loss at iteration 1100 : 0.10732953250408173
Loss at iteration 1110 : 0.09069722890853882
Loss at iteration 1120 : 0.10571092367172241
Loss at iteration 1130 : 0.07477610558271408
Loss at iteration 1140 : 0.07020404934883118
Loss at iteration 1150 : 0.06085506081581116
Loss at iteration 1160 : 0.10299673676490784
Loss at iteration 1170 : 0.09282954782247543
Loss at iteration 1180 : 0.10642798244953156
Loss at iteration 1190 : 0.06445998698472977
Loss at iteration 1200 : 0.05630221962928772
Loss at iteration 1210 : 0.07805009931325912
The SSIM Value is: 0.7108259677886963
The PSNR Value is: 21.303079223632814
the epoch is: 80
Loss at iteration 10 : 0.05299174040555954
Loss at iteration 20 : 0.0771092101931572
Loss at iteration 30 : 0.06362445652484894
Loss at iteration 40 : 0.04089992493391037
Loss at iteration 50 : 0.07096754014492035
Loss at iteration 60 : 0.07596132159233093
Loss at iteration 70 : 0.07660441100597382
Loss at iteration 80 : 0.09480845928192139
Loss at iteration 90 : 0.06866899132728577
Loss at iteration 100 : 0.06242665648460388
Loss at iteration 110 : 0.06555956602096558
Loss at iteration 120 : 0.09754034131765366
Loss at iteration 130 : 0.05328219383955002
Loss at iteration 140 : 0.07126826792955399
Loss at iteration 150 : 0.12556320428848267
Loss at iteration 160 : 0.08240729570388794
Loss at iteration 170 : 0.0889245867729187
Loss at iteration 180 : 0.09271150827407837
Loss at iteration 190 : 0.08525528013706207
Loss at iteration 200 : 0.06781228631734848
Loss at iteration 210 : 0.10941274464130402
Loss at iteration 220 : 0.10026391595602036
Loss at iteration 230 : 0.07776995003223419
Loss at iteration 240 : 0.07623972743749619
Loss at iteration 250 : 0.10277384519577026
Loss at iteration 260 : 0.07987707853317261
Loss at iteration 270 : 0.09540119022130966
Loss at iteration 280 : 0.11125542968511581
Loss at iteration 290 : 0.09590917825698853
Loss at iteration 300 : 0.09198220074176788
Loss at iteration 310 : 0.06697526574134827
Loss at iteration 320 : 0.1429625153541565
Loss at iteration 330 : 0.0894247516989708
Loss at iteration 340 : 0.06881321966648102
Loss at iteration 350 : 0.06979264318943024
Loss at iteration 360 : 0.06078104302287102
Loss at iteration 370 : 0.11602484434843063
Loss at iteration 380 : 0.07524403184652328
Loss at iteration 390 : 0.08024071156978607
Loss at iteration 400 : 0.08103740215301514
Loss at iteration 410 : 0.07453739643096924
Loss at iteration 420 : 0.08497685194015503
Loss at iteration 430 : 0.09570799767971039
Loss at iteration 440 : 0.062371134757995605
Loss at iteration 450 : 0.05072387307882309
Loss at iteration 460 : 0.05166744813323021
Loss at iteration 470 : 0.07537373900413513
Loss at iteration 480 : 0.05424634367227554
Loss at iteration 490 : 0.07666772603988647
Loss at iteration 500 : 0.0765732079744339
Loss at iteration 510 : 0.09952940791845322
Loss at iteration 520 : 0.08829724788665771
Loss at iteration 530 : 0.08945979923009872
Loss at iteration 540 : 0.16150373220443726
Loss at iteration 550 : 0.12191708385944366
Loss at iteration 560 : 0.06660085171461105
Loss at iteration 570 : 0.07580411434173584
Loss at iteration 580 : 0.07136594504117966
Loss at iteration 590 : 0.056150685995817184
Loss at iteration 600 : 0.06445039808750153
Loss at iteration 610 : 0.10518016666173935
Loss at iteration 620 : 0.08862332999706268
Loss at iteration 630 : 0.101426862180233
Loss at iteration 640 : 0.05840235576033592
Loss at iteration 650 : 0.08244284242391586
Loss at iteration 660 : 0.09888370335102081
Loss at iteration 670 : 0.06795358657836914
Loss at iteration 680 : 0.12999457120895386
Loss at iteration 690 : 0.12315937131643295
Loss at iteration 700 : 0.04829886183142662
Loss at iteration 710 : 0.10738100856542587
Loss at iteration 720 : 0.10011798143386841
Loss at iteration 730 : 0.09212736785411835
Loss at iteration 740 : 0.11646604537963867
Loss at iteration 750 : 0.09366950392723083
Loss at iteration 760 : 0.07568980753421783
Loss at iteration 770 : 0.06067534536123276
Loss at iteration 780 : 0.08140085637569427
Loss at iteration 790 : 0.08133064210414886
Loss at iteration 800 : 0.1337338536977768
Loss at iteration 810 : 0.06257700175046921
Loss at iteration 820 : 0.08544042706489563
Loss at iteration 830 : 0.04910699650645256
Loss at iteration 840 : 0.06466111540794373
Loss at iteration 850 : 0.09468939155340195
Loss at iteration 860 : 0.08112873882055283
Loss at iteration 870 : 0.08303940296173096
Loss at iteration 880 : 0.0672493577003479
Loss at iteration 890 : 0.07083696871995926
Loss at iteration 900 : 0.08487492054700851
Loss at iteration 910 : 0.08181846141815186
Loss at iteration 920 : 0.14418689906597137
Loss at iteration 930 : 0.05412556976079941
Loss at iteration 940 : 0.11600939929485321
Loss at iteration 950 : 0.05359882861375809
Loss at iteration 960 : 0.07604596018791199
Loss at iteration 970 : 0.10509293526411057
Loss at iteration 980 : 0.06147713214159012
Loss at iteration 990 : 0.06502068042755127
Loss at iteration 1000 : 0.06144918128848076
Loss at iteration 1010 : 0.0501188300549984
Loss at iteration 1020 : 0.06985495239496231
Loss at iteration 1030 : 0.07592825591564178
Loss at iteration 1040 : 0.07001715898513794
Loss at iteration 1050 : 0.0668804869055748
Loss at iteration 1060 : 0.06925829499959946
Loss at iteration 1070 : 0.06582638621330261
Loss at iteration 1080 : 0.06563182175159454
Loss at iteration 1090 : 0.0626230239868164
Loss at iteration 1100 : 0.03997519239783287
Loss at iteration 1110 : 0.05612775683403015
Loss at iteration 1120 : 0.10295270383358002
Loss at iteration 1130 : 0.05865619331598282
Loss at iteration 1140 : 0.1125180646777153
Loss at iteration 1150 : 0.05166811868548393
Loss at iteration 1160 : 0.11832386255264282
Loss at iteration 1170 : 0.05936288833618164
Loss at iteration 1180 : 0.08080610632896423
Loss at iteration 1190 : 0.09350883215665817
Loss at iteration 1200 : 0.06515024602413177
Loss at iteration 1210 : 0.06566023081541061
The SSIM Value is: 0.7086485664049784
The PSNR Value is: 21.339912287394206
the epoch is: 81
Loss at iteration 10 : 0.0763603076338768
Loss at iteration 20 : 0.08913742005825043
Loss at iteration 30 : 0.06179577857255936
Loss at iteration 40 : 0.07229909300804138
Loss at iteration 50 : 0.10684192180633545
Loss at iteration 60 : 0.08953189849853516
Loss at iteration 70 : 0.08923399448394775
Loss at iteration 80 : 0.1371377855539322
Loss at iteration 90 : 0.06958688050508499
Loss at iteration 100 : 0.1096959337592125
Loss at iteration 110 : 0.07836581021547318
Loss at iteration 120 : 0.09537294507026672
Loss at iteration 130 : 0.12677960097789764
Loss at iteration 140 : 0.06460561603307724
Loss at iteration 150 : 0.07547956705093384
Loss at iteration 160 : 0.059785567224025726
Loss at iteration 170 : 0.1404544711112976
Loss at iteration 180 : 0.10086914151906967
Loss at iteration 190 : 0.04770616814494133
Loss at iteration 200 : 0.08181726187467575
Loss at iteration 210 : 0.08717207610607147
Loss at iteration 220 : 0.10096198320388794
Loss at iteration 230 : 0.10078983008861542
Loss at iteration 240 : 0.09285257756710052
Loss at iteration 250 : 0.11740376800298691
Loss at iteration 260 : 0.09993846714496613
Loss at iteration 270 : 0.06459705531597137
Loss at iteration 280 : 0.16297242045402527
Loss at iteration 290 : 0.06090746074914932
Loss at iteration 300 : 0.09674781560897827
Loss at iteration 310 : 0.0738181322813034
Loss at iteration 320 : 0.06363298743963242
Loss at iteration 330 : 0.0796755999326706
Loss at iteration 340 : 0.07242085039615631
Loss at iteration 350 : 0.06701882183551788
Loss at iteration 360 : 0.06663364917039871
Loss at iteration 370 : 0.11803613603115082
Loss at iteration 380 : 0.04956495761871338
Loss at iteration 390 : 0.04473919793963432
Loss at iteration 400 : 0.05947478115558624
Loss at iteration 410 : 0.08556344360113144
Loss at iteration 420 : 0.09273141622543335
Loss at iteration 430 : 0.07252918183803558
Loss at iteration 440 : 0.0886920914053917
Loss at iteration 450 : 0.09015904366970062
Loss at iteration 460 : 0.06004232168197632
Loss at iteration 470 : 0.11019140481948853
Loss at iteration 480 : 0.1331140697002411
Loss at iteration 490 : 0.12178400158882141
Loss at iteration 500 : 0.07679621875286102
Loss at iteration 510 : 0.057484086602926254
Loss at iteration 520 : 0.11570471525192261
Loss at iteration 530 : 0.08007745444774628
Loss at iteration 540 : 0.08270522207021713
Loss at iteration 550 : 0.08327873796224594
Loss at iteration 560 : 0.10605651140213013
Loss at iteration 570 : 0.13218241930007935
Loss at iteration 580 : 0.07743259519338608
Loss at iteration 590 : 0.07100959867238998
Loss at iteration 600 : 0.08573786914348602
Loss at iteration 610 : 0.08280064910650253
Loss at iteration 620 : 0.0747779831290245
Loss at iteration 630 : 0.07293558120727539
Loss at iteration 640 : 0.10458141565322876
Loss at iteration 650 : 0.06483044475317001
Loss at iteration 660 : 0.12356668710708618
Loss at iteration 670 : 0.0819951668381691
Loss at iteration 680 : 0.07021427154541016
Loss at iteration 690 : 0.09983953088521957
Loss at iteration 700 : 0.05538532882928848
Loss at iteration 710 : 0.10602953284978867
Loss at iteration 720 : 0.08214911818504333
Loss at iteration 730 : 0.09046795219182968
Loss at iteration 740 : 0.15512214601039886
Loss at iteration 750 : 0.09458321332931519
Loss at iteration 760 : 0.0810634046792984
Loss at iteration 770 : 0.07032156735658646
Loss at iteration 780 : 0.06932269781827927
Loss at iteration 790 : 0.05678321048617363
Loss at iteration 800 : 0.11156858503818512
Loss at iteration 810 : 0.09965535253286362
Loss at iteration 820 : 0.09473913908004761
Loss at iteration 830 : 0.08460315316915512
Loss at iteration 840 : 0.050438594073057175
Loss at iteration 850 : 0.09958867728710175
Loss at iteration 860 : 0.06714001297950745
Loss at iteration 870 : 0.08967810869216919
Loss at iteration 880 : 0.06339272111654282
Loss at iteration 890 : 0.07960943877696991
Loss at iteration 900 : 0.07310883700847626
Loss at iteration 910 : 0.0929979681968689
Loss at iteration 920 : 0.06486820429563522
Loss at iteration 930 : 0.06349240243434906
Loss at iteration 940 : 0.07409157603979111
Loss at iteration 950 : 0.05826423689723015
Loss at iteration 960 : 0.10922877490520477
Loss at iteration 970 : 0.06576111912727356
Loss at iteration 980 : 0.07714062929153442
Loss at iteration 990 : 0.04980506747961044
Loss at iteration 1000 : 0.0667528510093689
Loss at iteration 1010 : 0.08550276607275009
Loss at iteration 1020 : 0.0756056010723114
Loss at iteration 1030 : 0.061217568814754486
Loss at iteration 1040 : 0.092105433344841
Loss at iteration 1050 : 0.09330764412879944
Loss at iteration 1060 : 0.10193626582622528
Loss at iteration 1070 : 0.051671020686626434
Loss at iteration 1080 : 0.08843572437763214
Loss at iteration 1090 : 0.06900235265493393
Loss at iteration 1100 : 0.07497027516365051
Loss at iteration 1110 : 0.07804183661937714
Loss at iteration 1120 : 0.10093533992767334
Loss at iteration 1130 : 0.0709652453660965
Loss at iteration 1140 : 0.10939584672451019
Loss at iteration 1150 : 0.057514648884534836
Loss at iteration 1160 : 0.0857282280921936
Loss at iteration 1170 : 0.11494863033294678
Loss at iteration 1180 : 0.07730096578598022
Loss at iteration 1190 : 0.07849903404712677
Loss at iteration 1200 : 0.1281079649925232
Loss at iteration 1210 : 0.0695841908454895
The SSIM Value is: 0.7075691421826681
The PSNR Value is: 20.821679051717123
the epoch is: 82
Loss at iteration 10 : 0.09288015216588974
Loss at iteration 20 : 0.13475356996059418
Loss at iteration 30 : 0.07491461932659149
Loss at iteration 40 : 0.06390677392482758
Loss at iteration 50 : 0.10048525035381317
Loss at iteration 60 : 0.05802445858716965
Loss at iteration 70 : 0.046670593321323395
Loss at iteration 80 : 0.08136255294084549
Loss at iteration 90 : 0.1734364628791809
Loss at iteration 100 : 0.07507293671369553
Loss at iteration 110 : 0.15901342034339905
Loss at iteration 120 : 0.08893576264381409
Loss at iteration 130 : 0.10916890949010849
Loss at iteration 140 : 0.05897092819213867
Loss at iteration 150 : 0.062441565096378326
Loss at iteration 160 : 0.0820191502571106
Loss at iteration 170 : 0.1136656254529953
Loss at iteration 180 : 0.09640341252088547
Loss at iteration 190 : 0.07446305453777313
Loss at iteration 200 : 0.0913681611418724
Loss at iteration 210 : 0.09592753648757935
Loss at iteration 220 : 0.07224196195602417
Loss at iteration 230 : 0.06893803179264069
Loss at iteration 240 : 0.08422648906707764
Loss at iteration 250 : 0.07872800529003143
Loss at iteration 260 : 0.14812365174293518
Loss at iteration 270 : 0.078987255692482
Loss at iteration 280 : 0.1163889616727829
Loss at iteration 290 : 0.09286805242300034
Loss at iteration 300 : 0.13365787267684937
Loss at iteration 310 : 0.06901568174362183
Loss at iteration 320 : 0.08007682859897614
Loss at iteration 330 : 0.10130345821380615
Loss at iteration 340 : 0.07933461666107178
Loss at iteration 350 : 0.09648748487234116
Loss at iteration 360 : 0.06630145758390427
Loss at iteration 370 : 0.10440392792224884
Loss at iteration 380 : 0.06540030986070633
Loss at iteration 390 : 0.11012566089630127
Loss at iteration 400 : 0.06146402284502983
Loss at iteration 410 : 0.07114669680595398
Loss at iteration 420 : 0.07184877246618271
Loss at iteration 430 : 0.09884065389633179
Loss at iteration 440 : 0.09924620389938354
Loss at iteration 450 : 0.09159529209136963
Loss at iteration 460 : 0.1204829141497612
Loss at iteration 470 : 0.06871066242456436
Loss at iteration 480 : 0.06546985357999802
Loss at iteration 490 : 0.07568682730197906
Loss at iteration 500 : 0.056827664375305176
Loss at iteration 510 : 0.07111404836177826
Loss at iteration 520 : 0.06866075098514557
Loss at iteration 530 : 0.08859175443649292
Loss at iteration 540 : 0.07085039466619492
Loss at iteration 550 : 0.10024149715900421
Loss at iteration 560 : 0.08230926841497421
Loss at iteration 570 : 0.09286876767873764
Loss at iteration 580 : 0.10544358193874359
Loss at iteration 590 : 0.09219132363796234
Loss at iteration 600 : 0.11027765274047852
Loss at iteration 610 : 0.09738209843635559
Loss at iteration 620 : 0.07417874038219452
Loss at iteration 630 : 0.10687683522701263
Loss at iteration 640 : 0.11918863654136658
Loss at iteration 650 : 0.05751568078994751
Loss at iteration 660 : 0.094480499625206
Loss at iteration 670 : 0.05089227482676506
Loss at iteration 680 : 0.09180603921413422
Loss at iteration 690 : 0.07150363177061081
Loss at iteration 700 : 0.07186099886894226
Loss at iteration 710 : 0.08868270367383957
Loss at iteration 720 : 0.11024634540081024
Loss at iteration 730 : 0.07457046210765839
Loss at iteration 740 : 0.06604571640491486
Loss at iteration 750 : 0.07529714703559875
Loss at iteration 760 : 0.0868326872587204
Loss at iteration 770 : 0.11324500292539597
Loss at iteration 780 : 0.10098888725042343
Loss at iteration 790 : 0.06882654875516891
Loss at iteration 800 : 0.07570181787014008
Loss at iteration 810 : 0.05831262469291687
Loss at iteration 820 : 0.10120348632335663
Loss at iteration 830 : 0.07836510241031647
Loss at iteration 840 : 0.09253506362438202
Loss at iteration 850 : 0.05603765696287155
Loss at iteration 860 : 0.0860142931342125
Loss at iteration 870 : 0.049195922911167145
Loss at iteration 880 : 0.07245370745658875
Loss at iteration 890 : 0.053086064755916595
Loss at iteration 900 : 0.0960419774055481
Loss at iteration 910 : 0.0497121661901474
Loss at iteration 920 : 0.06967087835073471
Loss at iteration 930 : 0.09593909978866577
Loss at iteration 940 : 0.10109876096248627
Loss at iteration 950 : 0.07567784190177917
Loss at iteration 960 : 0.09531417489051819
Loss at iteration 970 : 0.08956321328878403
Loss at iteration 980 : 0.11072555184364319
Loss at iteration 990 : 0.08258042484521866
Loss at iteration 1000 : 0.06969387084245682
Loss at iteration 1010 : 0.1101142019033432
Loss at iteration 1020 : 0.13719429075717926
Loss at iteration 1030 : 0.10630204528570175
Loss at iteration 1040 : 0.09002551436424255
Loss at iteration 1050 : 0.06704749912023544
Loss at iteration 1060 : 0.08781114220619202
Loss at iteration 1070 : 0.10455440729856491
Loss at iteration 1080 : 0.08286438882350922
Loss at iteration 1090 : 0.10370384156703949
Loss at iteration 1100 : 0.07672889530658722
Loss at iteration 1110 : 0.0863015353679657
Loss at iteration 1120 : 0.05900152027606964
Loss at iteration 1130 : 0.06848695874214172
Loss at iteration 1140 : 0.07286699116230011
Loss at iteration 1150 : 0.10806448757648468
Loss at iteration 1160 : 0.07208149135112762
Loss at iteration 1170 : 0.12309511005878448
Loss at iteration 1180 : 0.07875596731901169
Loss at iteration 1190 : 0.09259657561779022
Loss at iteration 1200 : 0.1312996745109558
Loss at iteration 1210 : 0.07022381573915482
The SSIM Value is: 0.7107711593310039
The PSNR Value is: 21.12987518310547
the epoch is: 83
Loss at iteration 10 : 0.10668343305587769
Loss at iteration 20 : 0.07512006163597107
Loss at iteration 30 : 0.05032195895910263
Loss at iteration 40 : 0.09798131883144379
Loss at iteration 50 : 0.06959027051925659
Loss at iteration 60 : 0.10174058377742767
Loss at iteration 70 : 0.09061680734157562
Loss at iteration 80 : 0.06232726573944092
Loss at iteration 90 : 0.07048816978931427
Loss at iteration 100 : 0.08831192553043365
Loss at iteration 110 : 0.1025473102927208
Loss at iteration 120 : 0.07365145534276962
Loss at iteration 130 : 0.0993262380361557
Loss at iteration 140 : 0.07308336347341537
Loss at iteration 150 : 0.08319759368896484
Loss at iteration 160 : 0.06969107687473297
Loss at iteration 170 : 0.0911235511302948
Loss at iteration 180 : 0.06860337406396866
Loss at iteration 190 : 0.10098116099834442
Loss at iteration 200 : 0.06976556777954102
Loss at iteration 210 : 0.08852418512105942
Loss at iteration 220 : 0.09757940471172333
Loss at iteration 230 : 0.10640861839056015
Loss at iteration 240 : 0.10135102272033691
Loss at iteration 250 : 0.08193452656269073
Loss at iteration 260 : 0.08230946958065033
Loss at iteration 270 : 0.07528690993785858
Loss at iteration 280 : 0.07379704713821411
Loss at iteration 290 : 0.08671992272138596
Loss at iteration 300 : 0.05690610408782959
Loss at iteration 310 : 0.09333475679159164
Loss at iteration 320 : 0.08120522648096085
Loss at iteration 330 : 0.07831143587827682
Loss at iteration 340 : 0.07773228734731674
Loss at iteration 350 : 0.10073155164718628
Loss at iteration 360 : 0.08331584930419922
Loss at iteration 370 : 0.06169767677783966
Loss at iteration 380 : 0.0843597799539566
Loss at iteration 390 : 0.08851677179336548
Loss at iteration 400 : 0.09122636169195175
Loss at iteration 410 : 0.0739528089761734
Loss at iteration 420 : 0.11261971294879913
Loss at iteration 430 : 0.07700769603252411
Loss at iteration 440 : 0.07254889607429504
Loss at iteration 450 : 0.07902339845895767
Loss at iteration 460 : 0.07209593057632446
Loss at iteration 470 : 0.05997930467128754
Loss at iteration 480 : 0.06260719895362854
Loss at iteration 490 : 0.08941056579351425
Loss at iteration 500 : 0.07320098578929901
Loss at iteration 510 : 0.0650731772184372
Loss at iteration 520 : 0.06765221059322357
Loss at iteration 530 : 0.061182811856269836
Loss at iteration 540 : 0.08143918216228485
Loss at iteration 550 : 0.07466839253902435
Loss at iteration 560 : 0.13228274881839752
Loss at iteration 570 : 0.09806333482265472
Loss at iteration 580 : 0.09670576453208923
Loss at iteration 590 : 0.06453659385442734
Loss at iteration 600 : 0.061324235051870346
Loss at iteration 610 : 0.06963532418012619
Loss at iteration 620 : 0.057547446340322495
Loss at iteration 630 : 0.08762644231319427
Loss at iteration 640 : 0.09467580914497375
Loss at iteration 650 : 0.07739987224340439
Loss at iteration 660 : 0.08274111151695251
Loss at iteration 670 : 0.07714179158210754
Loss at iteration 680 : 0.09262563288211823
Loss at iteration 690 : 0.09631168842315674
Loss at iteration 700 : 0.06531701982021332
Loss at iteration 710 : 0.1346406191587448
Loss at iteration 720 : 0.0969662219285965
Loss at iteration 730 : 0.06926573812961578
Loss at iteration 740 : 0.053604476153850555
Loss at iteration 750 : 0.08624182641506195
Loss at iteration 760 : 0.10130386054515839
Loss at iteration 770 : 0.10438667237758636
Loss at iteration 780 : 0.08313147723674774
Loss at iteration 790 : 0.05785605311393738
Loss at iteration 800 : 0.146095409989357
Loss at iteration 810 : 0.08986936509609222
Loss at iteration 820 : 0.07354617118835449
Loss at iteration 830 : 0.09786432981491089
Loss at iteration 840 : 0.053078778088092804
Loss at iteration 850 : 0.0928223580121994
Loss at iteration 860 : 0.09185584634542465
Loss at iteration 870 : 0.08258765935897827
Loss at iteration 880 : 0.07183417677879333
Loss at iteration 890 : 0.08563526719808578
Loss at iteration 900 : 0.0842636227607727
Loss at iteration 910 : 0.0899076908826828
Loss at iteration 920 : 0.07436998188495636
Loss at iteration 930 : 0.1505613923072815
Loss at iteration 940 : 0.05416116118431091
Loss at iteration 950 : 0.06735959649085999
Loss at iteration 960 : 0.14180874824523926
Loss at iteration 970 : 0.08997496217489243
Loss at iteration 980 : 0.11136295646429062
Loss at iteration 990 : 0.05137013643980026
Loss at iteration 1000 : 0.0831756442785263
Loss at iteration 1010 : 0.06297989189624786
Loss at iteration 1020 : 0.09499325603246689
Loss at iteration 1030 : 0.10507043451070786
Loss at iteration 1040 : 0.0702272355556488
Loss at iteration 1050 : 0.10362422466278076
Loss at iteration 1060 : 0.1583605259656906
Loss at iteration 1070 : 0.1119096577167511
Loss at iteration 1080 : 0.1503986120223999
Loss at iteration 1090 : 0.07486381381750107
Loss at iteration 1100 : 0.11009544134140015
Loss at iteration 1110 : 0.09443782269954681
Loss at iteration 1120 : 0.07718327641487122
Loss at iteration 1130 : 0.09028979390859604
Loss at iteration 1140 : 0.09409302473068237
Loss at iteration 1150 : 0.05565453693270683
Loss at iteration 1160 : 0.07720714062452316
Loss at iteration 1170 : 0.07207681238651276
Loss at iteration 1180 : 0.11486691236495972
Loss at iteration 1190 : 0.04000021889805794
Loss at iteration 1200 : 0.049638938158750534
Loss at iteration 1210 : 0.06003478169441223
The SSIM Value is: 0.7048786183198293
The PSNR Value is: 20.699741045633953
the epoch is: 84
Loss at iteration 10 : 0.10462023317813873
Loss at iteration 20 : 0.09192049503326416
Loss at iteration 30 : 0.050156623125076294
Loss at iteration 40 : 0.07384222000837326
Loss at iteration 50 : 0.09215496480464935
Loss at iteration 60 : 0.11085180938243866
Loss at iteration 70 : 0.09899213165044785
Loss at iteration 80 : 0.10702034085988998
Loss at iteration 90 : 0.07611708343029022
Loss at iteration 100 : 0.08578044176101685
Loss at iteration 110 : 0.05591603368520737
Loss at iteration 120 : 0.10030057281255722
Loss at iteration 130 : 0.08725927770137787
Loss at iteration 140 : 0.08796259760856628
Loss at iteration 150 : 0.06558309495449066
Loss at iteration 160 : 0.07332246005535126
Loss at iteration 170 : 0.11198499798774719
Loss at iteration 180 : 0.061810191720724106
Loss at iteration 190 : 0.1101311668753624
Loss at iteration 200 : 0.10283569991588593
Loss at iteration 210 : 0.07932735234498978
Loss at iteration 220 : 0.13025608658790588
Loss at iteration 230 : 0.10082197189331055
Loss at iteration 240 : 0.07860764861106873
Loss at iteration 250 : 0.05670192837715149
Loss at iteration 260 : 0.05264843627810478
Loss at iteration 270 : 0.05952223390340805
Loss at iteration 280 : 0.10295450687408447
Loss at iteration 290 : 0.06847062706947327
Loss at iteration 300 : 0.09361221641302109
Loss at iteration 310 : 0.16373488306999207
Loss at iteration 320 : 0.09671585261821747
Loss at iteration 330 : 0.10856637358665466
Loss at iteration 340 : 0.05743005871772766
Loss at iteration 350 : 0.13021254539489746
Loss at iteration 360 : 0.07815610617399216
Loss at iteration 370 : 0.11414434760808945
Loss at iteration 380 : 0.04111011326313019
Loss at iteration 390 : 0.07794398069381714
Loss at iteration 400 : 0.07632025331258774
Loss at iteration 410 : 0.10440149158239365
Loss at iteration 420 : 0.10043879598379135
Loss at iteration 430 : 0.06673020124435425
Loss at iteration 440 : 0.05512978881597519
Loss at iteration 450 : 0.1187874972820282
Loss at iteration 460 : 0.06908519566059113
Loss at iteration 470 : 0.0865546390414238
Loss at iteration 480 : 0.11123107373714447
Loss at iteration 490 : 0.06977641582489014
Loss at iteration 500 : 0.08894121646881104
Loss at iteration 510 : 0.10852415859699249
Loss at iteration 520 : 0.05806156247854233
Loss at iteration 530 : 0.0903133898973465
Loss at iteration 540 : 0.10085910558700562
Loss at iteration 550 : 0.11540092527866364
Loss at iteration 560 : 0.07590027153491974
Loss at iteration 570 : 0.056542377918958664
Loss at iteration 580 : 0.06910853832960129
Loss at iteration 590 : 0.11326264590024948
Loss at iteration 600 : 0.10190337896347046
Loss at iteration 610 : 0.07292778044939041
Loss at iteration 620 : 0.11168110370635986
Loss at iteration 630 : 0.0832173228263855
Loss at iteration 640 : 0.06587409973144531
Loss at iteration 650 : 0.0738123282790184
Loss at iteration 660 : 0.09007482975721359
Loss at iteration 670 : 0.0768255814909935
Loss at iteration 680 : 0.0746464729309082
Loss at iteration 690 : 0.060139887034893036
Loss at iteration 700 : 0.07681651413440704
Loss at iteration 710 : 0.08841884136199951
Loss at iteration 720 : 0.0635722428560257
Loss at iteration 730 : 0.11551332473754883
Loss at iteration 740 : 0.11969765275716782
Loss at iteration 750 : 0.10936388373374939
Loss at iteration 760 : 0.06506828963756561
Loss at iteration 770 : 0.07265578210353851
Loss at iteration 780 : 0.05458568036556244
Loss at iteration 790 : 0.10556145012378693
Loss at iteration 800 : 0.07151828706264496
Loss at iteration 810 : 0.09932781755924225
Loss at iteration 820 : 0.09365105628967285
Loss at iteration 830 : 0.06411556899547577
Loss at iteration 840 : 0.06791043281555176
Loss at iteration 850 : 0.09330295026302338
Loss at iteration 860 : 0.08165880292654037
Loss at iteration 870 : 0.07866434752941132
Loss at iteration 880 : 0.11562218517065048
Loss at iteration 890 : 0.07167942821979523
Loss at iteration 900 : 0.05583065748214722
Loss at iteration 910 : 0.055847764015197754
Loss at iteration 920 : 0.06215512752532959
Loss at iteration 930 : 0.08694504201412201
Loss at iteration 940 : 0.054309405386447906
Loss at iteration 950 : 0.11771444976329803
Loss at iteration 960 : 0.061215996742248535
Loss at iteration 970 : 0.1019323319196701
Loss at iteration 980 : 0.10631276667118073
Loss at iteration 990 : 0.08460943400859833
Loss at iteration 1000 : 0.09497679769992828
Loss at iteration 1010 : 0.05944490060210228
Loss at iteration 1020 : 0.05108198523521423
Loss at iteration 1030 : 0.10941468179225922
Loss at iteration 1040 : 0.09313669800758362
Loss at iteration 1050 : 0.07983960211277008
Loss at iteration 1060 : 0.07203496992588043
Loss at iteration 1070 : 0.102731853723526
Loss at iteration 1080 : 0.10430314391851425
Loss at iteration 1090 : 0.1075531393289566
Loss at iteration 1100 : 0.11058169603347778
Loss at iteration 1110 : 0.07872511446475983
Loss at iteration 1120 : 0.09102706611156464
Loss at iteration 1130 : 0.058486100286245346
Loss at iteration 1140 : 0.06246374174952507
Loss at iteration 1150 : 0.06804563105106354
Loss at iteration 1160 : 0.10858490318059921
Loss at iteration 1170 : 0.12201729416847229
Loss at iteration 1180 : 0.07679064571857452
Loss at iteration 1190 : 0.07736276090145111
Loss at iteration 1200 : 0.11866281926631927
Loss at iteration 1210 : 0.10881319642066956
The SSIM Value is: 0.7130460898081462
The PSNR Value is: 20.95200958251953
the epoch is: 85
Loss at iteration 10 : 0.11572869122028351
Loss at iteration 20 : 0.11868755519390106
Loss at iteration 30 : 0.13385064899921417
Loss at iteration 40 : 0.058007411658763885
Loss at iteration 50 : 0.06696390360593796
Loss at iteration 60 : 0.1322363317012787
Loss at iteration 70 : 0.08218921720981598
Loss at iteration 80 : 0.08297443389892578
Loss at iteration 90 : 0.05409615859389305
Loss at iteration 100 : 0.05886228010058403
Loss at iteration 110 : 0.07034799456596375
Loss at iteration 120 : 0.0939483493566513
Loss at iteration 130 : 0.09214075654745102
Loss at iteration 140 : 0.09411461651325226
Loss at iteration 150 : 0.11433546245098114
Loss at iteration 160 : 0.07913665473461151
Loss at iteration 170 : 0.08241664618253708
Loss at iteration 180 : 0.07333322614431381
Loss at iteration 190 : 0.08819826692342758
Loss at iteration 200 : 0.1498938798904419
Loss at iteration 210 : 0.1225571483373642
Loss at iteration 220 : 0.09373639523983002
Loss at iteration 230 : 0.07034970819950104
Loss at iteration 240 : 0.08951312303543091
Loss at iteration 250 : 0.09775321185588837
Loss at iteration 260 : 0.057475075125694275
Loss at iteration 270 : 0.06606429815292358
Loss at iteration 280 : 0.05900200456380844
Loss at iteration 290 : 0.0768095999956131
Loss at iteration 300 : 0.07485684752464294
Loss at iteration 310 : 0.06774288415908813
Loss at iteration 320 : 0.0657491534948349
Loss at iteration 330 : 0.09508337825536728
Loss at iteration 340 : 0.10099440813064575
Loss at iteration 350 : 0.05829010531306267
Loss at iteration 360 : 0.09779714047908783
Loss at iteration 370 : 0.06813500076532364
Loss at iteration 380 : 0.07889950275421143
Loss at iteration 390 : 0.07558298110961914
Loss at iteration 400 : 0.07093292474746704
Loss at iteration 410 : 0.09389804303646088
Loss at iteration 420 : 0.10940252244472504
Loss at iteration 430 : 0.06840148568153381
Loss at iteration 440 : 0.07833787053823471
Loss at iteration 450 : 0.08389272540807724
Loss at iteration 460 : 0.08099117875099182
Loss at iteration 470 : 0.07732103765010834
Loss at iteration 480 : 0.12107979506254196
Loss at iteration 490 : 0.056303076446056366
Loss at iteration 500 : 0.08369708061218262
Loss at iteration 510 : 0.07851802557706833
Loss at iteration 520 : 0.06776843965053558
Loss at iteration 530 : 0.12068422883749008
Loss at iteration 540 : 0.07708592712879181
Loss at iteration 550 : 0.0859946757555008
Loss at iteration 560 : 0.1109740138053894
Loss at iteration 570 : 0.06756799668073654
Loss at iteration 580 : 0.07874301075935364
Loss at iteration 590 : 0.1694849729537964
Loss at iteration 600 : 0.09729945659637451
Loss at iteration 610 : 0.08381016552448273
Loss at iteration 620 : 0.09623970091342926
Loss at iteration 630 : 0.10541074723005295
Loss at iteration 640 : 0.08159320056438446
Loss at iteration 650 : 0.05212660878896713
Loss at iteration 660 : 0.06491640210151672
Loss at iteration 670 : 0.06043531000614166
Loss at iteration 680 : 0.10882844030857086
Loss at iteration 690 : 0.058725543320178986
Loss at iteration 700 : 0.09104979038238525
Loss at iteration 710 : 0.06741794943809509
Loss at iteration 720 : 0.07323844730854034
Loss at iteration 730 : 0.07009148597717285
Loss at iteration 740 : 0.07096629589796066
Loss at iteration 750 : 0.07418951392173767
Loss at iteration 760 : 0.0769166499376297
Loss at iteration 770 : 0.08440659940242767
Loss at iteration 780 : 0.10998964309692383
Loss at iteration 790 : 0.08162209391593933
Loss at iteration 800 : 0.0995170921087265
Loss at iteration 810 : 0.08061128854751587
Loss at iteration 820 : 0.054736413061618805
Loss at iteration 830 : 0.06986716389656067
Loss at iteration 840 : 0.07356397807598114
Loss at iteration 850 : 0.08529216051101685
Loss at iteration 860 : 0.1546815037727356
Loss at iteration 870 : 0.08959382027387619
Loss at iteration 880 : 0.08057968318462372
Loss at iteration 890 : 0.06448136270046234
Loss at iteration 900 : 0.07638388127088547
Loss at iteration 910 : 0.11028637737035751
Loss at iteration 920 : 0.11906018853187561
Loss at iteration 930 : 0.08353113383054733
Loss at iteration 940 : 0.08843891322612762
Loss at iteration 950 : 0.13649064302444458
Loss at iteration 960 : 0.07600834220647812
Loss at iteration 970 : 0.05970613285899162
Loss at iteration 980 : 0.1103971004486084
Loss at iteration 990 : 0.10156422853469849
Loss at iteration 1000 : 0.09055044502019882
Loss at iteration 1010 : 0.10024812817573547
Loss at iteration 1020 : 0.044677987694740295
Loss at iteration 1030 : 0.12130540609359741
Loss at iteration 1040 : 0.08787161111831665
Loss at iteration 1050 : 0.08070691674947739
Loss at iteration 1060 : 0.09422187507152557
Loss at iteration 1070 : 0.07425893843173981
Loss at iteration 1080 : 0.06419702619314194
Loss at iteration 1090 : 0.0761047974228859
Loss at iteration 1100 : 0.10094945132732391
Loss at iteration 1110 : 0.08622275292873383
Loss at iteration 1120 : 0.05989508330821991
Loss at iteration 1130 : 0.07710383832454681
Loss at iteration 1140 : 0.04941154271364212
Loss at iteration 1150 : 0.088006392121315
Loss at iteration 1160 : 0.08739682286977768
Loss at iteration 1170 : 0.11738055944442749
Loss at iteration 1180 : 0.07175508141517639
Loss at iteration 1190 : 0.11717759072780609
Loss at iteration 1200 : 0.11439859122037888
Loss at iteration 1210 : 0.0779988169670105
The SSIM Value is: 0.7065448641777039
The PSNR Value is: 21.14216537475586
the epoch is: 86
Loss at iteration 10 : 0.07933345437049866
Loss at iteration 20 : 0.08306995034217834
Loss at iteration 30 : 0.15031202137470245
Loss at iteration 40 : 0.10207611322402954
Loss at iteration 50 : 0.056983157992362976
Loss at iteration 60 : 0.10363024473190308
Loss at iteration 70 : 0.060144778341054916
Loss at iteration 80 : 0.07151477038860321
Loss at iteration 90 : 0.061126600950956345
Loss at iteration 100 : 0.11551584303379059
Loss at iteration 110 : 0.092148058116436
Loss at iteration 120 : 0.14398907124996185
Loss at iteration 130 : 0.07189753651618958
Loss at iteration 140 : 0.07600273936986923
Loss at iteration 150 : 0.06743128597736359
Loss at iteration 160 : 0.09398508071899414
Loss at iteration 170 : 0.06710164248943329
Loss at iteration 180 : 0.07812944054603577
Loss at iteration 190 : 0.0895925760269165
Loss at iteration 200 : 0.08552201092243195
Loss at iteration 210 : 0.08334676921367645
Loss at iteration 220 : 0.0894617885351181
Loss at iteration 230 : 0.11153286695480347
Loss at iteration 240 : 0.12245422601699829
Loss at iteration 250 : 0.11150704324245453
Loss at iteration 260 : 0.07079671323299408
Loss at iteration 270 : 0.0855368971824646
Loss at iteration 280 : 0.09254904836416245
Loss at iteration 290 : 0.05180674046278
Loss at iteration 300 : 0.06918057799339294
Loss at iteration 310 : 0.09803955256938934
Loss at iteration 320 : 0.12204472720623016
Loss at iteration 330 : 0.10195396840572357
Loss at iteration 340 : 0.10729040950536728
Loss at iteration 350 : 0.1098160669207573
Loss at iteration 360 : 0.06929643452167511
Loss at iteration 370 : 0.12868711352348328
Loss at iteration 380 : 0.05522775277495384
Loss at iteration 390 : 0.05519760772585869
Loss at iteration 400 : 0.0914635881781578
Loss at iteration 410 : 0.10625189542770386
Loss at iteration 420 : 0.07433430850505829
Loss at iteration 430 : 0.06194723770022392
Loss at iteration 440 : 0.09393193572759628
Loss at iteration 450 : 0.08005635440349579
Loss at iteration 460 : 0.20198851823806763
Loss at iteration 470 : 0.08741585910320282
Loss at iteration 480 : 0.13626442849636078
Loss at iteration 490 : 0.08538328111171722
Loss at iteration 500 : 0.0939074382185936
Loss at iteration 510 : 0.05738179013133049
Loss at iteration 520 : 0.04481588304042816
Loss at iteration 530 : 0.10005456209182739
Loss at iteration 540 : 0.10504232347011566
Loss at iteration 550 : 0.11662034690380096
Loss at iteration 560 : 0.07193823158740997
Loss at iteration 570 : 0.10035096108913422
Loss at iteration 580 : 0.11075682938098907
Loss at iteration 590 : 0.098555788397789
Loss at iteration 600 : 0.08113384991884232
Loss at iteration 610 : 0.10835924744606018
Loss at iteration 620 : 0.07514370232820511
Loss at iteration 630 : 0.10419859737157822
Loss at iteration 640 : 0.12827350199222565
Loss at iteration 650 : 0.09858840703964233
Loss at iteration 660 : 0.09253585338592529
Loss at iteration 670 : 0.0690060555934906
Loss at iteration 680 : 0.06909488141536713
Loss at iteration 690 : 0.09243806451559067
Loss at iteration 700 : 0.10911878943443298
Loss at iteration 710 : 0.05188997834920883
Loss at iteration 720 : 0.10452991724014282
Loss at iteration 730 : 0.06208089739084244
Loss at iteration 740 : 0.07961820065975189
Loss at iteration 750 : 0.06849434226751328
Loss at iteration 760 : 0.0549301914870739
Loss at iteration 770 : 0.06094275042414665
Loss at iteration 780 : 0.08466517925262451
Loss at iteration 790 : 0.08015652000904083
Loss at iteration 800 : 0.10253149271011353
Loss at iteration 810 : 0.0806984156370163
Loss at iteration 820 : 0.055510442703962326
Loss at iteration 830 : 0.08896148204803467
Loss at iteration 840 : 0.09625035524368286
Loss at iteration 850 : 0.0930977314710617
Loss at iteration 860 : 0.1508011668920517
Loss at iteration 870 : 0.168114572763443
Loss at iteration 880 : 0.06758430600166321
Loss at iteration 890 : 0.1406131237745285
Loss at iteration 900 : 0.0476035438477993
Loss at iteration 910 : 0.06890319287776947
Loss at iteration 920 : 0.0858994573354721
Loss at iteration 930 : 0.09871719777584076
Loss at iteration 940 : 0.05721570551395416
Loss at iteration 950 : 0.07405856251716614
Loss at iteration 960 : 0.07498263567686081
Loss at iteration 970 : 0.05176315829157829
Loss at iteration 980 : 0.08932194113731384
Loss at iteration 990 : 0.08886697888374329
Loss at iteration 1000 : 0.0768599733710289
Loss at iteration 1010 : 0.09208112210035324
Loss at iteration 1020 : 0.08413013070821762
Loss at iteration 1030 : 0.09368711709976196
Loss at iteration 1040 : 0.06923897564411163
Loss at iteration 1050 : 0.06509342789649963
Loss at iteration 1060 : 0.129766047000885
Loss at iteration 1070 : 0.040854908525943756
Loss at iteration 1080 : 0.09067230671644211
Loss at iteration 1090 : 0.08104857802391052
Loss at iteration 1100 : 0.07925823330879211
Loss at iteration 1110 : 0.07859385013580322
Loss at iteration 1120 : 0.05785326287150383
Loss at iteration 1130 : 0.07096180319786072
Loss at iteration 1140 : 0.06810431182384491
Loss at iteration 1150 : 0.07686303555965424
Loss at iteration 1160 : 0.10491494834423065
Loss at iteration 1170 : 0.10248434543609619
Loss at iteration 1180 : 0.07017116248607635
Loss at iteration 1190 : 0.08041086047887802
Loss at iteration 1200 : 0.07039918005466461
Loss at iteration 1210 : 0.11473092436790466
The SSIM Value is: 0.7100724577903748
The PSNR Value is: 20.931120936075846
the epoch is: 87
Loss at iteration 10 : 0.07327896356582642
Loss at iteration 20 : 0.07078045606613159
Loss at iteration 30 : 0.048858821392059326
Loss at iteration 40 : 0.054866157472133636
Loss at iteration 50 : 0.08036044985055923
Loss at iteration 60 : 0.05824977159500122
Loss at iteration 70 : 0.11392994970083237
Loss at iteration 80 : 0.053277116268873215
Loss at iteration 90 : 0.1371535062789917
Loss at iteration 100 : 0.10377129912376404
Loss at iteration 110 : 0.07728168368339539
Loss at iteration 120 : 0.07654551416635513
Loss at iteration 130 : 0.047847501933574677
Loss at iteration 140 : 0.04583960026502609
Loss at iteration 150 : 0.0836714506149292
Loss at iteration 160 : 0.04760551080107689
Loss at iteration 170 : 0.07263320684432983
Loss at iteration 180 : 0.07223901152610779
Loss at iteration 190 : 0.08676104247570038
Loss at iteration 200 : 0.06930273771286011
Loss at iteration 210 : 0.1601378172636032
Loss at iteration 220 : 0.0825815498828888
Loss at iteration 230 : 0.12380307912826538
Loss at iteration 240 : 0.10770682990550995
Loss at iteration 250 : 0.07598107308149338
Loss at iteration 260 : 0.08039991557598114
Loss at iteration 270 : 0.08375003188848495
Loss at iteration 280 : 0.06224612519145012
Loss at iteration 290 : 0.07505463808774948
Loss at iteration 300 : 0.06907390058040619
Loss at iteration 310 : 0.08119842410087585
Loss at iteration 320 : 0.10265299677848816
Loss at iteration 330 : 0.05439358204603195
Loss at iteration 340 : 0.07710061967372894
Loss at iteration 350 : 0.08802946656942368
Loss at iteration 360 : 0.06325103342533112
Loss at iteration 370 : 0.09316866844892502
Loss at iteration 380 : 0.10804548859596252
Loss at iteration 390 : 0.11179404705762863
Loss at iteration 400 : 0.0792241320014
Loss at iteration 410 : 0.040747832506895065
Loss at iteration 420 : 0.1038498505949974
Loss at iteration 430 : 0.05615074560046196
Loss at iteration 440 : 0.10202410817146301
Loss at iteration 450 : 0.12528811395168304
Loss at iteration 460 : 0.14483173191547394
Loss at iteration 470 : 0.06699217110872269
Loss at iteration 480 : 0.07439495623111725
Loss at iteration 490 : 0.07425451278686523
Loss at iteration 500 : 0.12126252055168152
Loss at iteration 510 : 0.07820171117782593
Loss at iteration 520 : 0.045788221061229706
Loss at iteration 530 : 0.06292282044887543
Loss at iteration 540 : 0.06688684225082397
Loss at iteration 550 : 0.07968947291374207
Loss at iteration 560 : 0.10084490478038788
Loss at iteration 570 : 0.06319225579500198
Loss at iteration 580 : 0.055731676518917084
Loss at iteration 590 : 0.10403583943843842
Loss at iteration 600 : 0.07843087613582611
Loss at iteration 610 : 0.08043883740901947
Loss at iteration 620 : 0.05385441705584526
Loss at iteration 630 : 0.10043077170848846
Loss at iteration 640 : 0.06637974083423615
Loss at iteration 650 : 0.08298835158348083
Loss at iteration 660 : 0.10211946815252304
Loss at iteration 670 : 0.08425003290176392
Loss at iteration 680 : 0.11059741675853729
Loss at iteration 690 : 0.10306887328624725
Loss at iteration 700 : 0.05291748046875
Loss at iteration 710 : 0.12081708759069443
Loss at iteration 720 : 0.079804927110672
Loss at iteration 730 : 0.07613854110240936
Loss at iteration 740 : 0.06196984648704529
Loss at iteration 750 : 0.08781005442142487
Loss at iteration 760 : 0.0747298002243042
Loss at iteration 770 : 0.07805626839399338
Loss at iteration 780 : 0.06894417107105255
Loss at iteration 790 : 0.10409023612737656
Loss at iteration 800 : 0.08212720602750778
Loss at iteration 810 : 0.10590986907482147
Loss at iteration 820 : 0.07886907458305359
Loss at iteration 830 : 0.07669618725776672
Loss at iteration 840 : 0.07658701390028
Loss at iteration 850 : 0.05611174553632736
Loss at iteration 860 : 0.07101523131132126
Loss at iteration 870 : 0.15184937417507172
Loss at iteration 880 : 0.08259312063455582
Loss at iteration 890 : 0.0907692238688469
Loss at iteration 900 : 0.092010997235775
Loss at iteration 910 : 0.10172441601753235
Loss at iteration 920 : 0.06602706015110016
Loss at iteration 930 : 0.10192929208278656
Loss at iteration 940 : 0.10639845579862595
Loss at iteration 950 : 0.05913524329662323
Loss at iteration 960 : 0.05270732566714287
Loss at iteration 970 : 0.08980071544647217
Loss at iteration 980 : 0.05943059176206589
Loss at iteration 990 : 0.08468841016292572
Loss at iteration 1000 : 0.10950437188148499
Loss at iteration 1010 : 0.06695901602506638
Loss at iteration 1020 : 0.05227884650230408
Loss at iteration 1030 : 0.08308872580528259
Loss at iteration 1040 : 0.07507966458797455
Loss at iteration 1050 : 0.05486574023962021
Loss at iteration 1060 : 0.062032394111156464
Loss at iteration 1070 : 0.08985553681850433
Loss at iteration 1080 : 0.10084700584411621
Loss at iteration 1090 : 0.08659534156322479
Loss at iteration 1100 : 0.1092027872800827
Loss at iteration 1110 : 0.09309607744216919
Loss at iteration 1120 : 0.07861706614494324
Loss at iteration 1130 : 0.10181918740272522
Loss at iteration 1140 : 0.08625929802656174
Loss at iteration 1150 : 0.09977632761001587
Loss at iteration 1160 : 0.08377182483673096
Loss at iteration 1170 : 0.09291119873523712
Loss at iteration 1180 : 0.08603176474571228
Loss at iteration 1190 : 0.10729742050170898
Loss at iteration 1200 : 0.06594949960708618
Loss at iteration 1210 : 0.07399280369281769
The SSIM Value is: 0.7125076949596405
The PSNR Value is: 21.335384368896484
the epoch is: 88
Loss at iteration 10 : 0.09967269748449326
Loss at iteration 20 : 0.06296145915985107
Loss at iteration 30 : 0.09675891697406769
Loss at iteration 40 : 0.06058726832270622
Loss at iteration 50 : 0.07113485783338547
Loss at iteration 60 : 0.07659544050693512
Loss at iteration 70 : 0.09826835989952087
Loss at iteration 80 : 0.10627584159374237
Loss at iteration 90 : 0.060821376740932465
Loss at iteration 100 : 0.08485329151153564
Loss at iteration 110 : 0.11771667003631592
Loss at iteration 120 : 0.04658973217010498
Loss at iteration 130 : 0.10810728371143341
Loss at iteration 140 : 0.11855357885360718
Loss at iteration 150 : 0.13237601518630981
Loss at iteration 160 : 0.12927499413490295
Loss at iteration 170 : 0.1159982904791832
Loss at iteration 180 : 0.08529972285032272
Loss at iteration 190 : 0.10118983685970306
Loss at iteration 200 : 0.05365389585494995
Loss at iteration 210 : 0.07939660549163818
Loss at iteration 220 : 0.11510168015956879
Loss at iteration 230 : 0.1345098316669464
Loss at iteration 240 : 0.07877972722053528
Loss at iteration 250 : 0.037683889269828796
Loss at iteration 260 : 0.09125667065382004
Loss at iteration 270 : 0.1048925518989563
Loss at iteration 280 : 0.06812790036201477
Loss at iteration 290 : 0.08860760927200317
Loss at iteration 300 : 0.10700693726539612
Loss at iteration 310 : 0.10523755848407745
Loss at iteration 320 : 0.09063642472028732
Loss at iteration 330 : 0.06837289780378342
Loss at iteration 340 : 0.06391959637403488
Loss at iteration 350 : 0.07830549031496048
Loss at iteration 360 : 0.11506396532058716
Loss at iteration 370 : 0.08440323919057846
Loss at iteration 380 : 0.06717421114444733
Loss at iteration 390 : 0.07136072963476181
Loss at iteration 400 : 0.13680832087993622
Loss at iteration 410 : 0.08702799677848816
Loss at iteration 420 : 0.09272146224975586
Loss at iteration 430 : 0.07394956052303314
Loss at iteration 440 : 0.08667401969432831
Loss at iteration 450 : 0.11432787030935287
Loss at iteration 460 : 0.09542617201805115
Loss at iteration 470 : 0.11681398004293442
Loss at iteration 480 : 0.08279383182525635
Loss at iteration 490 : 0.07525297999382019
Loss at iteration 500 : 0.09953512996435165
Loss at iteration 510 : 0.11595829576253891
Loss at iteration 520 : 0.10057942569255829
Loss at iteration 530 : 0.0743437185883522
Loss at iteration 540 : 0.061422280967235565
Loss at iteration 550 : 0.057551704347133636
Loss at iteration 560 : 0.0722518265247345
Loss at iteration 570 : 0.09895255416631699
Loss at iteration 580 : 0.0645446926355362
Loss at iteration 590 : 0.08505573868751526
Loss at iteration 600 : 0.11917193233966827
Loss at iteration 610 : 0.09828059375286102
Loss at iteration 620 : 0.11884213984012604
Loss at iteration 630 : 0.1121445894241333
Loss at iteration 640 : 0.04705337807536125
Loss at iteration 650 : 0.10908801853656769
Loss at iteration 660 : 0.06381860375404358
Loss at iteration 670 : 0.08739893138408661
Loss at iteration 680 : 0.06104274094104767
Loss at iteration 690 : 0.10471133887767792
Loss at iteration 700 : 0.08341047167778015
Loss at iteration 710 : 0.08542360365390778
Loss at iteration 720 : 0.08483888953924179
Loss at iteration 730 : 0.11707507073879242
Loss at iteration 740 : 0.06454357504844666
Loss at iteration 750 : 0.07494436949491501
Loss at iteration 760 : 0.10621385276317596
Loss at iteration 770 : 0.09197001904249191
Loss at iteration 780 : 0.07249531149864197
Loss at iteration 790 : 0.05453258752822876
Loss at iteration 800 : 0.08990930765867233
Loss at iteration 810 : 0.06748666614294052
Loss at iteration 820 : 0.13476473093032837
Loss at iteration 830 : 0.07342188060283661
Loss at iteration 840 : 0.09535323083400726
Loss at iteration 850 : 0.11719014495611191
Loss at iteration 860 : 0.05721927806735039
Loss at iteration 870 : 0.13026849925518036
Loss at iteration 880 : 0.09894520044326782
Loss at iteration 890 : 0.12204363942146301
Loss at iteration 900 : 0.058713581413030624
Loss at iteration 910 : 0.04654460400342941
Loss at iteration 920 : 0.08894677460193634
Loss at iteration 930 : 0.10568360984325409
Loss at iteration 940 : 0.08103826642036438
Loss at iteration 950 : 0.10361775010824203
Loss at iteration 960 : 0.0920264944434166
Loss at iteration 970 : 0.08241729438304901
Loss at iteration 980 : 0.12943820655345917
Loss at iteration 990 : 0.06413635611534119
Loss at iteration 1000 : 0.06570874154567719
Loss at iteration 1010 : 0.07706987112760544
Loss at iteration 1020 : 0.11328848451375961
Loss at iteration 1030 : 0.0761246532201767
Loss at iteration 1040 : 0.0792645588517189
Loss at iteration 1050 : 0.11463048309087753
Loss at iteration 1060 : 0.07086604833602905
Loss at iteration 1070 : 0.08364710211753845
Loss at iteration 1080 : 0.07794224470853806
Loss at iteration 1090 : 0.09993815422058105
Loss at iteration 1100 : 0.1275770515203476
Loss at iteration 1110 : 0.11317833513021469
Loss at iteration 1120 : 0.0722232386469841
Loss at iteration 1130 : 0.07958077639341354
Loss at iteration 1140 : 0.0846596360206604
Loss at iteration 1150 : 0.09083440154790878
Loss at iteration 1160 : 0.12069019675254822
Loss at iteration 1170 : 0.06138329952955246
Loss at iteration 1180 : 0.07465937733650208
Loss at iteration 1190 : 0.06750883162021637
Loss at iteration 1200 : 0.0796988308429718
Loss at iteration 1210 : 0.08287375420331955
The SSIM Value is: 0.7067089716593424
The PSNR Value is: 20.920678901672364
the epoch is: 89
Loss at iteration 10 : 0.11691316962242126
Loss at iteration 20 : 0.10023170709609985
Loss at iteration 30 : 0.03891037404537201
Loss at iteration 40 : 0.1222824975848198
Loss at iteration 50 : 0.09450050443410873
Loss at iteration 60 : 0.08494731783866882
Loss at iteration 70 : 0.09377899765968323
Loss at iteration 80 : 0.10708129405975342
Loss at iteration 90 : 0.08693470805883408
Loss at iteration 100 : 0.05199889466166496
Loss at iteration 110 : 0.06594042479991913
Loss at iteration 120 : 0.07484348863363266
Loss at iteration 130 : 0.092523954808712
Loss at iteration 140 : 0.14238250255584717
Loss at iteration 150 : 0.08860373497009277
Loss at iteration 160 : 0.06678178161382675
Loss at iteration 170 : 0.06921429932117462
Loss at iteration 180 : 0.06096607446670532
Loss at iteration 190 : 0.047126591205596924
Loss at iteration 200 : 0.08255205303430557
Loss at iteration 210 : 0.12769213318824768
Loss at iteration 220 : 0.08433331549167633
Loss at iteration 230 : 0.04308638721704483
Loss at iteration 240 : 0.08368216454982758
Loss at iteration 250 : 0.08514551818370819
Loss at iteration 260 : 0.0836414322257042
Loss at iteration 270 : 0.055080216377973557
Loss at iteration 280 : 0.05902191251516342
Loss at iteration 290 : 0.07954344898462296
Loss at iteration 300 : 0.0662841796875
Loss at iteration 310 : 0.12978866696357727
Loss at iteration 320 : 0.06842615455389023
Loss at iteration 330 : 0.09137966483831406
Loss at iteration 340 : 0.11410804837942123
Loss at iteration 350 : 0.07832454144954681
Loss at iteration 360 : 0.09190921485424042
Loss at iteration 370 : 0.09034067392349243
Loss at iteration 380 : 0.055150680243968964
Loss at iteration 390 : 0.08596398681402206
Loss at iteration 400 : 0.08517058938741684
Loss at iteration 410 : 0.06954366713762283
Loss at iteration 420 : 0.12728813290596008
Loss at iteration 430 : 0.14783623814582825
Loss at iteration 440 : 0.09570060670375824
Loss at iteration 450 : 0.05073840543627739
Loss at iteration 460 : 0.08485907316207886
Loss at iteration 470 : 0.07385529577732086
Loss at iteration 480 : 0.10243859887123108
Loss at iteration 490 : 0.04479286074638367
Loss at iteration 500 : 0.06355268508195877
Loss at iteration 510 : 0.0998193621635437
Loss at iteration 520 : 0.09340384602546692
Loss at iteration 530 : 0.11836101859807968
Loss at iteration 540 : 0.08051061630249023
Loss at iteration 550 : 0.09107046574354172
Loss at iteration 560 : 0.10425591468811035
Loss at iteration 570 : 0.07250922918319702
Loss at iteration 580 : 0.11308365315198898
Loss at iteration 590 : 0.07649089395999908
Loss at iteration 600 : 0.0856209397315979
Loss at iteration 610 : 0.084322988986969
Loss at iteration 620 : 0.12123213708400726
Loss at iteration 630 : 0.05374680086970329
Loss at iteration 640 : 0.12408983707427979
Loss at iteration 650 : 0.08564838021993637
Loss at iteration 660 : 0.03483419865369797
Loss at iteration 670 : 0.09795306622982025
Loss at iteration 680 : 0.09328778833150864
Loss at iteration 690 : 0.08018447458744049
Loss at iteration 700 : 0.07552140951156616
Loss at iteration 710 : 0.0653659924864769
Loss at iteration 720 : 0.07890456169843674
Loss at iteration 730 : 0.1215554028749466
Loss at iteration 740 : 0.08574079722166061
Loss at iteration 750 : 0.07192106544971466
Loss at iteration 760 : 0.09965705871582031
Loss at iteration 770 : 0.07614850997924805
Loss at iteration 780 : 0.08161665499210358
Loss at iteration 790 : 0.08213700354099274
Loss at iteration 800 : 0.13956771790981293
Loss at iteration 810 : 0.06230490654706955
Loss at iteration 820 : 0.08962875604629517
Loss at iteration 830 : 0.09250536561012268
Loss at iteration 840 : 0.113001249730587
Loss at iteration 850 : 0.0650271326303482
Loss at iteration 860 : 0.08029957115650177
Loss at iteration 870 : 0.11552873253822327
Loss at iteration 880 : 0.05286716669797897
Loss at iteration 890 : 0.08841340243816376
Loss at iteration 900 : 0.06969253718852997
Loss at iteration 910 : 0.09247520565986633
Loss at iteration 920 : 0.06934643536806107
Loss at iteration 930 : 0.09081950038671494
Loss at iteration 940 : 0.0514831617474556
Loss at iteration 950 : 0.06574007868766785
Loss at iteration 960 : 0.08196936547756195
Loss at iteration 970 : 0.06783738732337952
Loss at iteration 980 : 0.0660165548324585
Loss at iteration 990 : 0.058887582272291183
Loss at iteration 1000 : 0.07955197989940643
Loss at iteration 1010 : 0.07414181530475616
Loss at iteration 1020 : 0.08534958958625793
Loss at iteration 1030 : 0.061417967081069946
Loss at iteration 1040 : 0.125344380736351
Loss at iteration 1050 : 0.10661106556653976
Loss at iteration 1060 : 0.1256575584411621
Loss at iteration 1070 : 0.06358498334884644
Loss at iteration 1080 : 0.10245385766029358
Loss at iteration 1090 : 0.0800323635339737
Loss at iteration 1100 : 0.08685895055532455
Loss at iteration 1110 : 0.1026817262172699
Loss at iteration 1120 : 0.09861477464437485
Loss at iteration 1130 : 0.11877040565013885
Loss at iteration 1140 : 0.09781964123249054
Loss at iteration 1150 : 0.11902984231710434
Loss at iteration 1160 : 0.048976294696331024
Loss at iteration 1170 : 0.06464223563671112
Loss at iteration 1180 : 0.08993634581565857
Loss at iteration 1190 : 0.06316401809453964
Loss at iteration 1200 : 0.07939353585243225
Loss at iteration 1210 : 0.06364932656288147
The SSIM Value is: 0.7034903426965078
The PSNR Value is: 20.52674566904704
the epoch is: 90
Loss at iteration 10 : 0.10548876225948334
Loss at iteration 20 : 0.046396493911743164
Loss at iteration 30 : 0.09981745481491089
Loss at iteration 40 : 0.1171395406126976
Loss at iteration 50 : 0.08281877636909485
Loss at iteration 60 : 0.11655151844024658
Loss at iteration 70 : 0.12499924749135971
Loss at iteration 80 : 0.0810043215751648
Loss at iteration 90 : 0.0778634175658226
Loss at iteration 100 : 0.10459186881780624
Loss at iteration 110 : 0.060565732419490814
Loss at iteration 120 : 0.09716307371854782
Loss at iteration 130 : 0.06938547641038895
Loss at iteration 140 : 0.11758019775152206
Loss at iteration 150 : 0.09291800111532211
Loss at iteration 160 : 0.0692262351512909
Loss at iteration 170 : 0.07329697906970978
Loss at iteration 180 : 0.06808731704950333
Loss at iteration 190 : 0.09803056716918945
Loss at iteration 200 : 0.1076340302824974
Loss at iteration 210 : 0.08101880550384521
Loss at iteration 220 : 0.09057486057281494
Loss at iteration 230 : 0.08354207128286362
Loss at iteration 240 : 0.08352254331111908
Loss at iteration 250 : 0.0962543711066246
Loss at iteration 260 : 0.09220895171165466
Loss at iteration 270 : 0.11645450443029404
Loss at iteration 280 : 0.07862314581871033
Loss at iteration 290 : 0.07580530643463135
Loss at iteration 300 : 0.06949862092733383
Loss at iteration 310 : 0.08496570587158203
Loss at iteration 320 : 0.09925620257854462
Loss at iteration 330 : 0.09577982127666473
Loss at iteration 340 : 0.0717444121837616
Loss at iteration 350 : 0.07785869389772415
Loss at iteration 360 : 0.10027194023132324
Loss at iteration 370 : 0.09831361472606659
Loss at iteration 380 : 0.08951444923877716
Loss at iteration 390 : 0.06480923295021057
Loss at iteration 400 : 0.06781359761953354
Loss at iteration 410 : 0.05307474359869957
Loss at iteration 420 : 0.08253642171621323
Loss at iteration 430 : 0.06845565885305405
Loss at iteration 440 : 0.07249660789966583
Loss at iteration 450 : 0.08477797359228134
Loss at iteration 460 : 0.05586367845535278
Loss at iteration 470 : 0.08266454935073853
Loss at iteration 480 : 0.06362827122211456
Loss at iteration 490 : 0.06761607527732849
Loss at iteration 500 : 0.08347251266241074
Loss at iteration 510 : 0.055606089532375336
Loss at iteration 520 : 0.10107329487800598
Loss at iteration 530 : 0.0675559937953949
Loss at iteration 540 : 0.05309547483921051
Loss at iteration 550 : 0.08390351384878159
Loss at iteration 560 : 0.10418663918972015
Loss at iteration 570 : 0.07834865152835846
Loss at iteration 580 : 0.06522054970264435
Loss at iteration 590 : 0.07204392552375793
Loss at iteration 600 : 0.06643727421760559
Loss at iteration 610 : 0.09825244545936584
Loss at iteration 620 : 0.05386320501565933
Loss at iteration 630 : 0.06677956879138947
Loss at iteration 640 : 0.07727877050638199
Loss at iteration 650 : 0.10312603414058685
Loss at iteration 660 : 0.06524497270584106
Loss at iteration 670 : 0.07909880578517914
Loss at iteration 680 : 0.06953820586204529
Loss at iteration 690 : 0.09413640201091766
Loss at iteration 700 : 0.09719076007604599
Loss at iteration 710 : 0.09259733557701111
Loss at iteration 720 : 0.05396242067217827
Loss at iteration 730 : 0.09586641192436218
Loss at iteration 740 : 0.061926692724227905
Loss at iteration 750 : 0.1622873991727829
Loss at iteration 760 : 0.08022068440914154
Loss at iteration 770 : 0.07276776432991028
Loss at iteration 780 : 0.08817887306213379
Loss at iteration 790 : 0.07964228093624115
Loss at iteration 800 : 0.08817122876644135
Loss at iteration 810 : 0.11627557873725891
Loss at iteration 820 : 0.1123330295085907
Loss at iteration 830 : 0.06210116297006607
Loss at iteration 840 : 0.0703730583190918
Loss at iteration 850 : 0.08570029586553574
Loss at iteration 860 : 0.05642443895339966
Loss at iteration 870 : 0.07506851851940155
Loss at iteration 880 : 0.06966382265090942
Loss at iteration 890 : 0.08484890311956406
Loss at iteration 900 : 0.07157017290592194
Loss at iteration 910 : 0.09460020065307617
Loss at iteration 920 : 0.10468181222677231
Loss at iteration 930 : 0.08600407838821411
Loss at iteration 940 : 0.13112956285476685
Loss at iteration 950 : 0.09839761257171631
Loss at iteration 960 : 0.11129479110240936
Loss at iteration 970 : 0.06428728997707367
Loss at iteration 980 : 0.10146331042051315
Loss at iteration 990 : 0.07284365594387054
Loss at iteration 1000 : 0.07043957710266113
Loss at iteration 1010 : 0.08905598521232605
Loss at iteration 1020 : 0.07076610624790192
Loss at iteration 1030 : 0.06942908465862274
Loss at iteration 1040 : 0.09160154312849045
Loss at iteration 1050 : 0.06478776037693024
Loss at iteration 1060 : 0.06250350177288055
Loss at iteration 1070 : 0.09681707620620728
Loss at iteration 1080 : 0.11005856841802597
Loss at iteration 1090 : 0.08175190538167953
Loss at iteration 1100 : 0.10044877231121063
Loss at iteration 1110 : 0.09513145685195923
Loss at iteration 1120 : 0.09832355380058289
Loss at iteration 1130 : 0.05895143747329712
Loss at iteration 1140 : 0.06339229643344879
Loss at iteration 1150 : 0.08103185147047043
Loss at iteration 1160 : 0.05637314170598984
Loss at iteration 1170 : 0.08148735016584396
Loss at iteration 1180 : 0.10146811604499817
Loss at iteration 1190 : 0.07675513625144958
Loss at iteration 1200 : 0.082505002617836
Loss at iteration 1210 : 0.10018064826726913
The SSIM Value is: 0.7095351099967957
The PSNR Value is: 20.514338556925455
the epoch is: 91
Loss at iteration 10 : 0.09649620950222015
Loss at iteration 20 : 0.10538216680288315
Loss at iteration 30 : 0.061672329902648926
Loss at iteration 40 : 0.08180809020996094
Loss at iteration 50 : 0.046207211911678314
Loss at iteration 60 : 0.09036639332771301
Loss at iteration 70 : 0.058886878192424774
Loss at iteration 80 : 0.09675216674804688
Loss at iteration 90 : 0.09388493746519089
Loss at iteration 100 : 0.060867901891469955
Loss at iteration 110 : 0.06835764646530151
Loss at iteration 120 : 0.08191268146038055
Loss at iteration 130 : 0.08491337299346924
Loss at iteration 140 : 0.07212869822978973
Loss at iteration 150 : 0.08879856765270233
Loss at iteration 160 : 0.06886085122823715
Loss at iteration 170 : 0.09207428991794586
Loss at iteration 180 : 0.10145600140094757
Loss at iteration 190 : 0.08668188750743866
Loss at iteration 200 : 0.0640384778380394
Loss at iteration 210 : 0.05281330645084381
Loss at iteration 220 : 0.0875253900885582
Loss at iteration 230 : 0.1290997564792633
Loss at iteration 240 : 0.05970485135912895
Loss at iteration 250 : 0.0630999207496643
Loss at iteration 260 : 0.1381947100162506
Loss at iteration 270 : 0.08129434287548065
Loss at iteration 280 : 0.10991929471492767
Loss at iteration 290 : 0.06453163921833038
Loss at iteration 300 : 0.068152517080307
Loss at iteration 310 : 0.0781523734331131
Loss at iteration 320 : 0.0807485282421112
Loss at iteration 330 : 0.04622462019324303
Loss at iteration 340 : 0.0719098225235939
Loss at iteration 350 : 0.1160506159067154
Loss at iteration 360 : 0.06298485398292542
Loss at iteration 370 : 0.08235439658164978
Loss at iteration 380 : 0.07698192447423935
Loss at iteration 390 : 0.09872689843177795
Loss at iteration 400 : 0.09853661060333252
Loss at iteration 410 : 0.08450756967067719
Loss at iteration 420 : 0.06080504506826401
Loss at iteration 430 : 0.10368359088897705
Loss at iteration 440 : 0.09026801586151123
Loss at iteration 450 : 0.06638538837432861
Loss at iteration 460 : 0.09794256091117859
Loss at iteration 470 : 0.08613140881061554
Loss at iteration 480 : 0.08978782594203949
Loss at iteration 490 : 0.0637207180261612
Loss at iteration 500 : 0.08039367198944092
Loss at iteration 510 : 0.10691969096660614
Loss at iteration 520 : 0.07278241217136383
Loss at iteration 530 : 0.08039363473653793
Loss at iteration 540 : 0.048176996409893036
Loss at iteration 550 : 0.07153439521789551
Loss at iteration 560 : 0.10271989554166794
Loss at iteration 570 : 0.0838969349861145
Loss at iteration 580 : 0.06667771190404892
Loss at iteration 590 : 0.06897637248039246
Loss at iteration 600 : 0.09319459646940231
Loss at iteration 610 : 0.12442447990179062
Loss at iteration 620 : 0.0946316346526146
Loss at iteration 630 : 0.07708216458559036
Loss at iteration 640 : 0.0884021520614624
Loss at iteration 650 : 0.10037025064229965
Loss at iteration 660 : 0.06542346626520157
Loss at iteration 670 : 0.08942700922489166
Loss at iteration 680 : 0.07163427770137787
Loss at iteration 690 : 0.09809410572052002
Loss at iteration 700 : 0.10578124970197678
Loss at iteration 710 : 0.12718790769577026
Loss at iteration 720 : 0.07158061861991882
Loss at iteration 730 : 0.0788436084985733
Loss at iteration 740 : 0.06189587712287903
Loss at iteration 750 : 0.08012206852436066
Loss at iteration 760 : 0.07771647721529007
Loss at iteration 770 : 0.12080442160367966
Loss at iteration 780 : 0.04751332849264145
Loss at iteration 790 : 0.059286005795001984
Loss at iteration 800 : 0.07712775468826294
Loss at iteration 810 : 0.07537993043661118
Loss at iteration 820 : 0.09013088792562485
Loss at iteration 830 : 0.08229392021894455
Loss at iteration 840 : 0.09303534030914307
Loss at iteration 850 : 0.11952316761016846
Loss at iteration 860 : 0.05070776864886284
Loss at iteration 870 : 0.125146746635437
Loss at iteration 880 : 0.07340125739574432
Loss at iteration 890 : 0.12880760431289673
Loss at iteration 900 : 0.07842127978801727
Loss at iteration 910 : 0.09699597954750061
Loss at iteration 920 : 0.11573457717895508
Loss at iteration 930 : 0.07732626050710678
Loss at iteration 940 : 0.08507256954908371
Loss at iteration 950 : 0.08419297635555267
Loss at iteration 960 : 0.05689183995127678
Loss at iteration 970 : 0.10269464552402496
Loss at iteration 980 : 0.09827548265457153
Loss at iteration 990 : 0.05767660588026047
Loss at iteration 1000 : 0.0453057698905468
Loss at iteration 1010 : 0.07024091482162476
Loss at iteration 1020 : 0.11627160757780075
Loss at iteration 1030 : 0.10670258849859238
Loss at iteration 1040 : 0.07721281051635742
Loss at iteration 1050 : 0.09450723975896835
Loss at iteration 1060 : 0.1120966300368309
Loss at iteration 1070 : 0.08312193304300308
Loss at iteration 1080 : 0.06984364241361618
Loss at iteration 1090 : 0.04374421387910843
Loss at iteration 1100 : 0.06607623398303986
Loss at iteration 1110 : 0.061910830438137054
Loss at iteration 1120 : 0.09237228333950043
Loss at iteration 1130 : 0.10942979156970978
Loss at iteration 1140 : 0.07710863649845123
Loss at iteration 1150 : 0.08176955580711365
Loss at iteration 1160 : 0.11408566683530807
Loss at iteration 1170 : 0.06629183143377304
Loss at iteration 1180 : 0.11656123399734497
Loss at iteration 1190 : 0.096005879342556
Loss at iteration 1200 : 0.06937752664089203
Loss at iteration 1210 : 0.06217937171459198
The SSIM Value is: 0.7069850027561188
The PSNR Value is: 20.817831548055015
the epoch is: 92
Loss at iteration 10 : 0.05816635116934776
Loss at iteration 20 : 0.060746949166059494
Loss at iteration 30 : 0.08755757659673691
Loss at iteration 40 : 0.07631096243858337
Loss at iteration 50 : 0.08479952812194824
Loss at iteration 60 : 0.08938255161046982
Loss at iteration 70 : 0.09933765977621078
Loss at iteration 80 : 0.09807749837636948
Loss at iteration 90 : 0.07498219609260559
Loss at iteration 100 : 0.10098802298307419
Loss at iteration 110 : 0.08062504231929779
Loss at iteration 120 : 0.08479845523834229
Loss at iteration 130 : 0.08409073203802109
Loss at iteration 140 : 0.07149872183799744
Loss at iteration 150 : 0.0910552591085434
Loss at iteration 160 : 0.08700710535049438
Loss at iteration 170 : 0.09897428005933762
Loss at iteration 180 : 0.06598474830389023
Loss at iteration 190 : 0.0727902352809906
Loss at iteration 200 : 0.04905559867620468
Loss at iteration 210 : 0.12577420473098755
Loss at iteration 220 : 0.0676339864730835
Loss at iteration 230 : 0.09736430644989014
Loss at iteration 240 : 0.07566022127866745
Loss at iteration 250 : 0.09888383001089096
Loss at iteration 260 : 0.13537953794002533
Loss at iteration 270 : 0.07819031178951263
Loss at iteration 280 : 0.07246758788824081
Loss at iteration 290 : 0.06063874810934067
Loss at iteration 300 : 0.09750431776046753
Loss at iteration 310 : 0.09803488105535507
Loss at iteration 320 : 0.08049637079238892
Loss at iteration 330 : 0.12500175833702087
Loss at iteration 340 : 0.08231544494628906
Loss at iteration 350 : 0.09381377696990967
Loss at iteration 360 : 0.07958540320396423
Loss at iteration 370 : 0.12392094731330872
Loss at iteration 380 : 0.06974465399980545
Loss at iteration 390 : 0.049398988485336304
Loss at iteration 400 : 0.09806668758392334
Loss at iteration 410 : 0.05276504158973694
Loss at iteration 420 : 0.09519346803426743
Loss at iteration 430 : 0.09885381907224655
Loss at iteration 440 : 0.0548069030046463
Loss at iteration 450 : 0.06293286383152008
Loss at iteration 460 : 0.09161564707756042
Loss at iteration 470 : 0.09318071603775024
Loss at iteration 480 : 0.06817015260457993
Loss at iteration 490 : 0.061960369348526
Loss at iteration 500 : 0.0743044763803482
Loss at iteration 510 : 0.06678316742181778
Loss at iteration 520 : 0.10788273066282272
Loss at iteration 530 : 0.11559814214706421
Loss at iteration 540 : 0.093288853764534
Loss at iteration 550 : 0.06881194561719894
Loss at iteration 560 : 0.08259770274162292
Loss at iteration 570 : 0.06451237946748734
Loss at iteration 580 : 0.06164398044347763
Loss at iteration 590 : 0.12273072451353073
Loss at iteration 600 : 0.07417525351047516
Loss at iteration 610 : 0.0675581842660904
Loss at iteration 620 : 0.10006802529096603
Loss at iteration 630 : 0.0502404049038887
Loss at iteration 640 : 0.051483578979969025
Loss at iteration 650 : 0.10482438653707504
Loss at iteration 660 : 0.07788845151662827
Loss at iteration 670 : 0.07777943462133408
Loss at iteration 680 : 0.07833452522754669
Loss at iteration 690 : 0.07408126443624496
Loss at iteration 700 : 0.09608089178800583
Loss at iteration 710 : 0.0803886353969574
Loss at iteration 720 : 0.051864735782146454
Loss at iteration 730 : 0.07438862323760986
Loss at iteration 740 : 0.0562509223818779
Loss at iteration 750 : 0.09279286116361618
Loss at iteration 760 : 0.04790683835744858
Loss at iteration 770 : 0.06793971359729767
Loss at iteration 780 : 0.1067984327673912
Loss at iteration 790 : 0.08404355496168137
Loss at iteration 800 : 0.04914387688040733
Loss at iteration 810 : 0.08012906461954117
Loss at iteration 820 : 0.06208207458257675
Loss at iteration 830 : 0.17254361510276794
Loss at iteration 840 : 0.07444392144680023
Loss at iteration 850 : 0.09692931175231934
Loss at iteration 860 : 0.10918769240379333
Loss at iteration 870 : 0.0910722017288208
Loss at iteration 880 : 0.08596734702587128
Loss at iteration 890 : 0.09678901731967926
Loss at iteration 900 : 0.12226846069097519
Loss at iteration 910 : 0.06355886161327362
Loss at iteration 920 : 0.04871705174446106
Loss at iteration 930 : 0.1110823005437851
Loss at iteration 940 : 0.051955368369817734
Loss at iteration 950 : 0.05408139154314995
Loss at iteration 960 : 0.040776126086711884
Loss at iteration 970 : 0.07981538027524948
Loss at iteration 980 : 0.10533712804317474
Loss at iteration 990 : 0.06916370242834091
Loss at iteration 1000 : 0.08595039695501328
Loss at iteration 1010 : 0.08586343377828598
Loss at iteration 1020 : 0.08224606513977051
Loss at iteration 1030 : 0.05694074183702469
Loss at iteration 1040 : 0.06810463964939117
Loss at iteration 1050 : 0.09167899936437607
Loss at iteration 1060 : 0.10626636445522308
Loss at iteration 1070 : 0.05064184591174126
Loss at iteration 1080 : 0.05865480750799179
Loss at iteration 1090 : 0.0596751868724823
Loss at iteration 1100 : 0.06137123703956604
Loss at iteration 1110 : 0.12374871969223022
Loss at iteration 1120 : 0.0770135372877121
Loss at iteration 1130 : 0.08662202209234238
Loss at iteration 1140 : 0.06117004156112671
Loss at iteration 1150 : 0.0937863439321518
Loss at iteration 1160 : 0.09581585973501205
Loss at iteration 1170 : 0.07725530862808228
Loss at iteration 1180 : 0.07735129445791245
Loss at iteration 1190 : 0.06653200089931488
Loss at iteration 1200 : 0.05910022556781769
Loss at iteration 1210 : 0.07252661883831024
The SSIM Value is: 0.7134693066279093
The PSNR Value is: 20.70404167175293
the epoch is: 93
Loss at iteration 10 : 0.039688590914011
Loss at iteration 20 : 0.09261888265609741
Loss at iteration 30 : 0.09479831159114838
Loss at iteration 40 : 0.07856841385364532
Loss at iteration 50 : 0.12400776147842407
Loss at iteration 60 : 0.10958018898963928
Loss at iteration 70 : 0.09639136493206024
Loss at iteration 80 : 0.050425704568624496
Loss at iteration 90 : 0.07328973710536957
Loss at iteration 100 : 0.1184396967291832
Loss at iteration 110 : 0.07548119127750397
Loss at iteration 120 : 0.08895111083984375
Loss at iteration 130 : 0.06220078468322754
Loss at iteration 140 : 0.06271211057901382
Loss at iteration 150 : 0.05103275179862976
Loss at iteration 160 : 0.08297568559646606
Loss at iteration 170 : 0.09724408388137817
Loss at iteration 180 : 0.06593246012926102
Loss at iteration 190 : 0.05993103235960007
Loss at iteration 200 : 0.08738043904304504
Loss at iteration 210 : 0.06187039241194725
Loss at iteration 220 : 0.0988263413310051
Loss at iteration 230 : 0.07492385804653168
Loss at iteration 240 : 0.07278742641210556
Loss at iteration 250 : 0.0861039012670517
Loss at iteration 260 : 0.12754052877426147
Loss at iteration 270 : 0.0961361676454544
Loss at iteration 280 : 0.06359603255987167
Loss at iteration 290 : 0.10323765128850937
Loss at iteration 300 : 0.061215247958898544
Loss at iteration 310 : 0.07526765763759613
Loss at iteration 320 : 0.1116248369216919
Loss at iteration 330 : 0.10091125220060349
Loss at iteration 340 : 0.1313449740409851
Loss at iteration 350 : 0.06716896593570709
Loss at iteration 360 : 0.07159397751092911
Loss at iteration 370 : 0.07217872887849808
Loss at iteration 380 : 0.1190929040312767
Loss at iteration 390 : 0.07234696298837662
Loss at iteration 400 : 0.09443553537130356
Loss at iteration 410 : 0.046521902084350586
Loss at iteration 420 : 0.1080518588423729
Loss at iteration 430 : 0.05145224928855896
Loss at iteration 440 : 0.07855471968650818
Loss at iteration 450 : 0.07515644282102585
Loss at iteration 460 : 0.11617839336395264
Loss at iteration 470 : 0.05975900590419769
Loss at iteration 480 : 0.1875719279050827
Loss at iteration 490 : 0.11022770404815674
Loss at iteration 500 : 0.09366361796855927
Loss at iteration 510 : 0.07922370731830597
Loss at iteration 520 : 0.08094324171543121
Loss at iteration 530 : 0.06250014901161194
Loss at iteration 540 : 0.06495137512683868
Loss at iteration 550 : 0.06632304191589355
Loss at iteration 560 : 0.07371044903993607
Loss at iteration 570 : 0.08141503483057022
Loss at iteration 580 : 0.07840332388877869
Loss at iteration 590 : 0.13519801199436188
Loss at iteration 600 : 0.07052313536405563
Loss at iteration 610 : 0.095900759100914
Loss at iteration 620 : 0.05406903475522995
Loss at iteration 630 : 0.0848878026008606
Loss at iteration 640 : 0.062375232577323914
Loss at iteration 650 : 0.06218117102980614
Loss at iteration 660 : 0.09698545932769775
Loss at iteration 670 : 0.07093006372451782
Loss at iteration 680 : 0.11990407109260559
Loss at iteration 690 : 0.07528779655694962
Loss at iteration 700 : 0.08500491082668304
Loss at iteration 710 : 0.05614146590232849
Loss at iteration 720 : 0.07116000354290009
Loss at iteration 730 : 0.07227995991706848
Loss at iteration 740 : 0.08094336837530136
Loss at iteration 750 : 0.06557752192020416
Loss at iteration 760 : 0.0985758900642395
Loss at iteration 770 : 0.11170078814029694
Loss at iteration 780 : 0.07955126464366913
Loss at iteration 790 : 0.12836799025535583
Loss at iteration 800 : 0.11877681314945221
Loss at iteration 810 : 0.11613451689481735
Loss at iteration 820 : 0.08493723720312119
Loss at iteration 830 : 0.07353613525629044
Loss at iteration 840 : 0.10560055822134018
Loss at iteration 850 : 0.07966633886098862
Loss at iteration 860 : 0.09483464807271957
Loss at iteration 870 : 0.09461987018585205
Loss at iteration 880 : 0.07106510549783707
Loss at iteration 890 : 0.05934356898069382
Loss at iteration 900 : 0.14093062281608582
Loss at iteration 910 : 0.07014888525009155
Loss at iteration 920 : 0.10365640372037888
Loss at iteration 930 : 0.07947415858507156
Loss at iteration 940 : 0.084409698843956
Loss at iteration 950 : 0.09508781135082245
Loss at iteration 960 : 0.06888097524642944
Loss at iteration 970 : 0.07269193977117538
Loss at iteration 980 : 0.07010336965322495
Loss at iteration 990 : 0.06239505112171173
Loss at iteration 1000 : 0.10775244235992432
Loss at iteration 1010 : 0.07326828688383102
Loss at iteration 1020 : 0.08703136444091797
Loss at iteration 1030 : 0.08070899546146393
Loss at iteration 1040 : 0.059783510863780975
Loss at iteration 1050 : 0.07151808589696884
Loss at iteration 1060 : 0.0742511972784996
Loss at iteration 1070 : 0.06071416288614273
Loss at iteration 1080 : 0.08898898214101791
Loss at iteration 1090 : 0.08238264173269272
Loss at iteration 1100 : 0.06359313428401947
Loss at iteration 1110 : 0.06346498429775238
Loss at iteration 1120 : 0.06880410015583038
Loss at iteration 1130 : 0.08357834815979004
Loss at iteration 1140 : 0.0825156420469284
Loss at iteration 1150 : 0.0954076498746872
Loss at iteration 1160 : 0.07431819289922714
Loss at iteration 1170 : 0.11923598498106003
Loss at iteration 1180 : 0.07707082480192184
Loss at iteration 1190 : 0.09653060883283615
Loss at iteration 1200 : 0.07540793716907501
Loss at iteration 1210 : 0.09139657020568848
The SSIM Value is: 0.7118009269237519
The PSNR Value is: 21.033673095703126
the epoch is: 94
Loss at iteration 10 : 0.07460816949605942
Loss at iteration 20 : 0.09373593330383301
Loss at iteration 30 : 0.10153183341026306
Loss at iteration 40 : 0.08643929660320282
Loss at iteration 50 : 0.13737308979034424
Loss at iteration 60 : 0.08470737934112549
Loss at iteration 70 : 0.06232403963804245
Loss at iteration 80 : 0.061340849846601486
Loss at iteration 90 : 0.14416365325450897
Loss at iteration 100 : 0.07852789014577866
Loss at iteration 110 : 0.09810992330312729
Loss at iteration 120 : 0.09196323156356812
Loss at iteration 130 : 0.04891364276409149
Loss at iteration 140 : 0.11674605309963226
Loss at iteration 150 : 0.1225559338927269
Loss at iteration 160 : 0.0769699215888977
Loss at iteration 170 : 0.09125861525535583
Loss at iteration 180 : 0.07668652385473251
Loss at iteration 190 : 0.06959877908229828
Loss at iteration 200 : 0.08813691139221191
Loss at iteration 210 : 0.04628368467092514
Loss at iteration 220 : 0.06008901447057724
Loss at iteration 230 : 0.07996422052383423
Loss at iteration 240 : 0.06714759767055511
Loss at iteration 250 : 0.09124507755041122
Loss at iteration 260 : 0.1243634968996048
Loss at iteration 270 : 0.10228127241134644
Loss at iteration 280 : 0.05171015113592148
Loss at iteration 290 : 0.1145138368010521
Loss at iteration 300 : 0.09416171163320541
Loss at iteration 310 : 0.10495300590991974
Loss at iteration 320 : 0.08800244331359863
Loss at iteration 330 : 0.04681051895022392
Loss at iteration 340 : 0.05571408197283745
Loss at iteration 350 : 0.06473570317029953
Loss at iteration 360 : 0.07278480380773544
Loss at iteration 370 : 0.09204166382551193
Loss at iteration 380 : 0.08386536687612534
Loss at iteration 390 : 0.05192277953028679
Loss at iteration 400 : 0.08712109923362732
Loss at iteration 410 : 0.08987843990325928
Loss at iteration 420 : 0.10065453499555588
Loss at iteration 430 : 0.09639452397823334
Loss at iteration 440 : 0.08147019147872925
Loss at iteration 450 : 0.09774281829595566
Loss at iteration 460 : 0.06075230985879898
Loss at iteration 470 : 0.061060696840286255
Loss at iteration 480 : 0.05788367986679077
Loss at iteration 490 : 0.0777682363986969
Loss at iteration 500 : 0.05887570232152939
Loss at iteration 510 : 0.039219729602336884
Loss at iteration 520 : 0.08458521217107773
Loss at iteration 530 : 0.0674973726272583
Loss at iteration 540 : 0.0854397565126419
Loss at iteration 550 : 0.08028531819581985
Loss at iteration 560 : 0.08755552023649216
Loss at iteration 570 : 0.07815028727054596
Loss at iteration 580 : 0.09186842292547226
Loss at iteration 590 : 0.11476743221282959
Loss at iteration 600 : 0.08054424822330475
Loss at iteration 610 : 0.045243632048368454
Loss at iteration 620 : 0.13111309707164764
Loss at iteration 630 : 0.0928938090801239
Loss at iteration 640 : 0.07806342095136642
Loss at iteration 650 : 0.08767317235469818
Loss at iteration 660 : 0.09934614598751068
Loss at iteration 670 : 0.0785583108663559
Loss at iteration 680 : 0.08337828516960144
Loss at iteration 690 : 0.08933186531066895
Loss at iteration 700 : 0.11741864681243896
Loss at iteration 710 : 0.052716221660375595
Loss at iteration 720 : 0.048659250140190125
Loss at iteration 730 : 0.0703241229057312
Loss at iteration 740 : 0.10395600646734238
Loss at iteration 750 : 0.09240067005157471
Loss at iteration 760 : 0.1451033055782318
Loss at iteration 770 : 0.10031380504369736
Loss at iteration 780 : 0.0721387267112732
Loss at iteration 790 : 0.1275528222322464
Loss at iteration 800 : 0.07315550744533539
Loss at iteration 810 : 0.07121340930461884
Loss at iteration 820 : 0.10404407978057861
Loss at iteration 830 : 0.12696966528892517
Loss at iteration 840 : 0.09392314404249191
Loss at iteration 850 : 0.11836575716733932
Loss at iteration 860 : 0.07840969413518906
Loss at iteration 870 : 0.10889993607997894
Loss at iteration 880 : 0.05680033192038536
Loss at iteration 890 : 0.072060227394104
Loss at iteration 900 : 0.1234649121761322
Loss at iteration 910 : 0.1301095336675644
Loss at iteration 920 : 0.14916285872459412
Loss at iteration 930 : 0.12220524251461029
Loss at iteration 940 : 0.04518505558371544
Loss at iteration 950 : 0.10745891183614731
Loss at iteration 960 : 0.08481602370738983
Loss at iteration 970 : 0.09810017049312592
Loss at iteration 980 : 0.04590102285146713
Loss at iteration 990 : 0.06822040677070618
Loss at iteration 1000 : 0.10348212718963623
Loss at iteration 1010 : 0.08269872516393661
Loss at iteration 1020 : 0.0759124755859375
Loss at iteration 1030 : 0.09863494336605072
Loss at iteration 1040 : 0.13647547364234924
Loss at iteration 1050 : 0.05943891033530235
Loss at iteration 1060 : 0.061711691319942474
Loss at iteration 1070 : 0.10741107165813446
Loss at iteration 1080 : 0.09335621446371078
Loss at iteration 1090 : 0.0914468765258789
Loss at iteration 1100 : 0.1248776912689209
Loss at iteration 1110 : 0.07447941601276398
Loss at iteration 1120 : 0.08442041277885437
Loss at iteration 1130 : 0.10256471484899521
Loss at iteration 1140 : 0.0787460058927536
Loss at iteration 1150 : 0.10220795124769211
Loss at iteration 1160 : 0.08707885444164276
Loss at iteration 1170 : 0.08371319621801376
Loss at iteration 1180 : 0.03655814006924629
Loss at iteration 1190 : 0.09254249185323715
Loss at iteration 1200 : 0.09590041637420654
Loss at iteration 1210 : 0.08798770606517792
The SSIM Value is: 0.7068434039751689
The PSNR Value is: 20.28174743652344
the epoch is: 95
Loss at iteration 10 : 0.09645964205265045
Loss at iteration 20 : 0.07994150370359421
Loss at iteration 30 : 0.05760199949145317
Loss at iteration 40 : 0.11491373181343079
Loss at iteration 50 : 0.08122824877500534
Loss at iteration 60 : 0.060475390404462814
Loss at iteration 70 : 0.10358011722564697
Loss at iteration 80 : 0.11671429872512817
Loss at iteration 90 : 0.0842486247420311
Loss at iteration 100 : 0.14876306056976318
Loss at iteration 110 : 0.07297247648239136
Loss at iteration 120 : 0.08115430921316147
Loss at iteration 130 : 0.07257716357707977
Loss at iteration 140 : 0.04396481066942215
Loss at iteration 150 : 0.07783988118171692
Loss at iteration 160 : 0.12109196186065674
Loss at iteration 170 : 0.09273233264684677
Loss at iteration 180 : 0.07995650172233582
Loss at iteration 190 : 0.04179059714078903
Loss at iteration 200 : 0.07477149367332458
Loss at iteration 210 : 0.05803385004401207
Loss at iteration 220 : 0.086997389793396
Loss at iteration 230 : 0.07787175476551056
Loss at iteration 240 : 0.06209726631641388
Loss at iteration 250 : 0.10701082646846771
Loss at iteration 260 : 0.06001005321741104
Loss at iteration 270 : 0.05338796228170395
Loss at iteration 280 : 0.14297059178352356
Loss at iteration 290 : 0.04951823502779007
Loss at iteration 300 : 0.0926273912191391
Loss at iteration 310 : 0.05829325318336487
Loss at iteration 320 : 0.13775326311588287
Loss at iteration 330 : 0.09654747694730759
Loss at iteration 340 : 0.07860948890447617
Loss at iteration 350 : 0.09686358273029327
Loss at iteration 360 : 0.10170239210128784
Loss at iteration 370 : 0.059196095913648605
Loss at iteration 380 : 0.11499917507171631
Loss at iteration 390 : 0.07679575681686401
Loss at iteration 400 : 0.10232752561569214
Loss at iteration 410 : 0.06658503413200378
Loss at iteration 420 : 0.07294751703739166
Loss at iteration 430 : 0.09638939797878265
Loss at iteration 440 : 0.08150389045476913
Loss at iteration 450 : 0.08012787997722626
Loss at iteration 460 : 0.11517217010259628
Loss at iteration 470 : 0.06536979973316193
Loss at iteration 480 : 0.08657314628362656
Loss at iteration 490 : 0.0616314634680748
Loss at iteration 500 : 0.0585421547293663
Loss at iteration 510 : 0.12046892940998077
Loss at iteration 520 : 0.08312519639730453
Loss at iteration 530 : 0.08301356434822083
Loss at iteration 540 : 0.056836605072021484
Loss at iteration 550 : 0.09627765417098999
Loss at iteration 560 : 0.07774509489536285
Loss at iteration 570 : 0.06739936769008636
Loss at iteration 580 : 0.09410956501960754
Loss at iteration 590 : 0.1253243088722229
Loss at iteration 600 : 0.06634008139371872
Loss at iteration 610 : 0.05963079631328583
Loss at iteration 620 : 0.09958183765411377
Loss at iteration 630 : 0.0780203565955162
Loss at iteration 640 : 0.11615460366010666
Loss at iteration 650 : 0.11641141027212143
Loss at iteration 660 : 0.06810998171567917
Loss at iteration 670 : 0.15581968426704407
Loss at iteration 680 : 0.09334244579076767
Loss at iteration 690 : 0.09140574187040329
Loss at iteration 700 : 0.08470338582992554
Loss at iteration 710 : 0.07133042812347412
Loss at iteration 720 : 0.09299011528491974
Loss at iteration 730 : 0.18987804651260376
Loss at iteration 740 : 0.12283368408679962
Loss at iteration 750 : 0.11820721626281738
Loss at iteration 760 : 0.1238064169883728
Loss at iteration 770 : 0.1337827742099762
Loss at iteration 780 : 0.08573411405086517
Loss at iteration 790 : 0.09282448142766953
Loss at iteration 800 : 0.08294736593961716
Loss at iteration 810 : 0.11719175428152084
Loss at iteration 820 : 0.07600268721580505
Loss at iteration 830 : 0.10111089795827866
Loss at iteration 840 : 0.05054398626089096
Loss at iteration 850 : 0.06585203856229782
Loss at iteration 860 : 0.05536417290568352
Loss at iteration 870 : 0.0590965561568737
Loss at iteration 880 : 0.05180761218070984
Loss at iteration 890 : 0.08328120410442352
Loss at iteration 900 : 0.11944344639778137
Loss at iteration 910 : 0.07009994983673096
Loss at iteration 920 : 0.07903732359409332
Loss at iteration 930 : 0.06276148557662964
Loss at iteration 940 : 0.07972416281700134
Loss at iteration 950 : 0.13238069415092468
Loss at iteration 960 : 0.11514389514923096
Loss at iteration 970 : 0.10632297396659851
Loss at iteration 980 : 0.0912310928106308
Loss at iteration 990 : 0.07304553687572479
Loss at iteration 1000 : 0.0631132498383522
Loss at iteration 1010 : 0.07347412407398224
Loss at iteration 1020 : 0.09644194692373276
Loss at iteration 1030 : 0.10266298055648804
Loss at iteration 1040 : 0.09211663901805878
Loss at iteration 1050 : 0.08539748191833496
Loss at iteration 1060 : 0.1138366311788559
Loss at iteration 1070 : 0.06714541465044022
Loss at iteration 1080 : 0.04941385239362717
Loss at iteration 1090 : 0.0693916529417038
Loss at iteration 1100 : 0.0727647989988327
Loss at iteration 1110 : 0.06783320009708405
Loss at iteration 1120 : 0.0512877032160759
Loss at iteration 1130 : 0.06578975915908813
Loss at iteration 1140 : 0.10162653774023056
Loss at iteration 1150 : 0.11966892331838608
Loss at iteration 1160 : 0.08945643901824951
Loss at iteration 1170 : 0.053490690886974335
Loss at iteration 1180 : 0.0947987362742424
Loss at iteration 1190 : 0.11617635190486908
Loss at iteration 1200 : 0.07450157403945923
Loss at iteration 1210 : 0.14235609769821167
The SSIM Value is: 0.7025981326897939
The PSNR Value is: 20.56062037150065
the epoch is: 96
Loss at iteration 10 : 0.14154593646526337
Loss at iteration 20 : 0.07837001979351044
Loss at iteration 30 : 0.07475917041301727
Loss at iteration 40 : 0.0645245760679245
Loss at iteration 50 : 0.06595943868160248
Loss at iteration 60 : 0.10194049775600433
Loss at iteration 70 : 0.07150445878505707
Loss at iteration 80 : 0.10539978742599487
Loss at iteration 90 : 0.07925944775342941
Loss at iteration 100 : 0.06034617871046066
Loss at iteration 110 : 0.07525299489498138
Loss at iteration 120 : 0.08635623753070831
Loss at iteration 130 : 0.06144062429666519
Loss at iteration 140 : 0.07053767144680023
Loss at iteration 150 : 0.05432277172803879
Loss at iteration 160 : 0.08972878754138947
Loss at iteration 170 : 0.10298864543437958
Loss at iteration 180 : 0.06136569380760193
Loss at iteration 190 : 0.09853862226009369
Loss at iteration 200 : 0.08817078173160553
Loss at iteration 210 : 0.09859961271286011
Loss at iteration 220 : 0.0836145207285881
Loss at iteration 230 : 0.13899511098861694
Loss at iteration 240 : 0.10732875764369965
Loss at iteration 250 : 0.06131993234157562
Loss at iteration 260 : 0.06831382215023041
Loss at iteration 270 : 0.0523415207862854
Loss at iteration 280 : 0.07002782821655273
Loss at iteration 290 : 0.05328606814146042
Loss at iteration 300 : 0.0896228551864624
Loss at iteration 310 : 0.09314756095409393
Loss at iteration 320 : 0.11332714557647705
Loss at iteration 330 : 0.05405210703611374
Loss at iteration 340 : 0.1167052835226059
Loss at iteration 350 : 0.09798853099346161
Loss at iteration 360 : 0.09618528932332993
Loss at iteration 370 : 0.08568205684423447
Loss at iteration 380 : 0.09244976937770844
Loss at iteration 390 : 0.07350657880306244
Loss at iteration 400 : 0.11845456808805466
Loss at iteration 410 : 0.07204142212867737
Loss at iteration 420 : 0.1305028200149536
Loss at iteration 430 : 0.07871821522712708
Loss at iteration 440 : 0.10950471460819244
Loss at iteration 450 : 0.03966924920678139
Loss at iteration 460 : 0.12806633114814758
Loss at iteration 470 : 0.08871018886566162
Loss at iteration 480 : 0.06501496583223343
Loss at iteration 490 : 0.06640487164258957
Loss at iteration 500 : 0.08018716424703598
Loss at iteration 510 : 0.10186819732189178
Loss at iteration 520 : 0.10114884376525879
Loss at iteration 530 : 0.08982931077480316
Loss at iteration 540 : 0.06797079741954803
Loss at iteration 550 : 0.13818742334842682
Loss at iteration 560 : 0.03845929354429245
Loss at iteration 570 : 0.08087945729494095
Loss at iteration 580 : 0.11190174520015717
Loss at iteration 590 : 0.0809195265173912
Loss at iteration 600 : 0.07567725330591202
Loss at iteration 610 : 0.13423371315002441
Loss at iteration 620 : 0.09804821014404297
Loss at iteration 630 : 0.07249359786510468
Loss at iteration 640 : 0.08260603994131088
Loss at iteration 650 : 0.05695648118853569
Loss at iteration 660 : 0.15517280995845795
Loss at iteration 670 : 0.05648210644721985
Loss at iteration 680 : 0.08714946359395981
Loss at iteration 690 : 0.07194840908050537
Loss at iteration 700 : 0.09664909541606903
Loss at iteration 710 : 0.09033124148845673
Loss at iteration 720 : 0.12497316300868988
Loss at iteration 730 : 0.11915843933820724
Loss at iteration 740 : 0.0756748616695404
Loss at iteration 750 : 0.19092373549938202
Loss at iteration 760 : 0.0694756954908371
Loss at iteration 770 : 0.07551407814025879
Loss at iteration 780 : 0.11582112312316895
Loss at iteration 790 : 0.14720600843429565
Loss at iteration 800 : 0.07038125395774841
Loss at iteration 810 : 0.05039161443710327
Loss at iteration 820 : 0.05644380301237106
Loss at iteration 830 : 0.06074220687150955
Loss at iteration 840 : 0.06929092109203339
Loss at iteration 850 : 0.0732744038105011
Loss at iteration 860 : 0.062343597412109375
Loss at iteration 870 : 0.0496431365609169
Loss at iteration 880 : 0.07272827625274658
Loss at iteration 890 : 0.08240055292844772
Loss at iteration 900 : 0.07164239138364792
Loss at iteration 910 : 0.07091564685106277
Loss at iteration 920 : 0.04616537690162659
Loss at iteration 930 : 0.10428058356046677
Loss at iteration 940 : 0.09702207893133163
Loss at iteration 950 : 0.08482363075017929
Loss at iteration 960 : 0.09119582176208496
Loss at iteration 970 : 0.0596269816160202
Loss at iteration 980 : 0.04775021970272064
Loss at iteration 990 : 0.05937430262565613
Loss at iteration 1000 : 0.04078378528356552
Loss at iteration 1010 : 0.04333297163248062
Loss at iteration 1020 : 0.096094511449337
Loss at iteration 1030 : 0.1056867465376854
Loss at iteration 1040 : 0.07701075077056885
Loss at iteration 1050 : 0.07983656227588654
Loss at iteration 1060 : 0.09783096611499786
Loss at iteration 1070 : 0.09814950823783875
Loss at iteration 1080 : 0.0836641937494278
Loss at iteration 1090 : 0.10616307705640793
Loss at iteration 1100 : 0.13126707077026367
Loss at iteration 1110 : 0.10186126083135605
Loss at iteration 1120 : 0.05924123153090477
Loss at iteration 1130 : 0.09342654049396515
Loss at iteration 1140 : 0.07577753067016602
Loss at iteration 1150 : 0.09958398342132568
Loss at iteration 1160 : 0.05885111540555954
Loss at iteration 1170 : 0.10994325578212738
Loss at iteration 1180 : 0.08763408660888672
Loss at iteration 1190 : 0.08314155042171478
Loss at iteration 1200 : 0.1010650098323822
Loss at iteration 1210 : 0.1027868390083313
The SSIM Value is: 0.7119989494482676
The PSNR Value is: 21.228092829386394
the epoch is: 97
Loss at iteration 10 : 0.058078423142433167
Loss at iteration 20 : 0.06741099059581757
Loss at iteration 30 : 0.0879053920507431
Loss at iteration 40 : 0.050744228065013885
Loss at iteration 50 : 0.09399627149105072
Loss at iteration 60 : 0.07732711732387543
Loss at iteration 70 : 0.06661522388458252
Loss at iteration 80 : 0.07249581068754196
Loss at iteration 90 : 0.058745548129081726
Loss at iteration 100 : 0.07515914738178253
Loss at iteration 110 : 0.08941030502319336
Loss at iteration 120 : 0.1147557944059372
Loss at iteration 130 : 0.07182769477367401
Loss at iteration 140 : 0.039414867758750916
Loss at iteration 150 : 0.06457911431789398
Loss at iteration 160 : 0.08168745040893555
Loss at iteration 170 : 0.09987922012805939
Loss at iteration 180 : 0.10052508115768433
Loss at iteration 190 : 0.05325247347354889
Loss at iteration 200 : 0.12503576278686523
Loss at iteration 210 : 0.09392410516738892
Loss at iteration 220 : 0.0756482183933258
Loss at iteration 230 : 0.09812837839126587
Loss at iteration 240 : 0.05624743551015854
Loss at iteration 250 : 0.13732676208019257
Loss at iteration 260 : 0.07628320902585983
Loss at iteration 270 : 0.09577678889036179
Loss at iteration 280 : 0.08306126296520233
Loss at iteration 290 : 0.07171711325645447
Loss at iteration 300 : 0.11839427798986435
Loss at iteration 310 : 0.0858466848731041
Loss at iteration 320 : 0.07105818390846252
Loss at iteration 330 : 0.07949172705411911
Loss at iteration 340 : 0.05004195496439934
Loss at iteration 350 : 0.07340054959058762
Loss at iteration 360 : 0.041820697486400604
Loss at iteration 370 : 0.08047036826610565
Loss at iteration 380 : 0.09831510484218597
Loss at iteration 390 : 0.059788040816783905
Loss at iteration 400 : 0.07482188194990158
Loss at iteration 410 : 0.04694663733243942
Loss at iteration 420 : 0.0822184607386589
Loss at iteration 430 : 0.1028895229101181
Loss at iteration 440 : 0.0903480052947998
Loss at iteration 450 : 0.08060099929571152
Loss at iteration 460 : 0.04084158316254616
Loss at iteration 470 : 0.11280225217342377
Loss at iteration 480 : 0.07774823904037476
Loss at iteration 490 : 0.0935971587896347
Loss at iteration 500 : 0.12535886466503143
Loss at iteration 510 : 0.12320523709058762
Loss at iteration 520 : 0.0928952619433403
Loss at iteration 530 : 0.09681196510791779
Loss at iteration 540 : 0.130414217710495
Loss at iteration 550 : 0.10873579978942871
Loss at iteration 560 : 0.06420879065990448
Loss at iteration 570 : 0.13166378438472748
Loss at iteration 580 : 0.04646851122379303
Loss at iteration 590 : 0.07054336369037628
Loss at iteration 600 : 0.039528921246528625
Loss at iteration 610 : 0.06604816764593124
Loss at iteration 620 : 0.06797458976507187
Loss at iteration 630 : 0.09591054171323776
Loss at iteration 640 : 0.08986219018697739
Loss at iteration 650 : 0.11777366697788239
Loss at iteration 660 : 0.10522407293319702
Loss at iteration 670 : 0.08372144401073456
Loss at iteration 680 : 0.10236676782369614
Loss at iteration 690 : 0.09393833577632904
Loss at iteration 700 : 0.06584499776363373
Loss at iteration 710 : 0.08599274605512619
Loss at iteration 720 : 0.07861854881048203
Loss at iteration 730 : 0.09180980175733566
Loss at iteration 740 : 0.1496717780828476
Loss at iteration 750 : 0.08595936745405197
Loss at iteration 760 : 0.1515742540359497
Loss at iteration 770 : 0.1027969941496849
Loss at iteration 780 : 0.09285296499729156
Loss at iteration 790 : 0.10039308667182922
Loss at iteration 800 : 0.05760248750448227
Loss at iteration 810 : 0.06567320227622986
Loss at iteration 820 : 0.0760626494884491
Loss at iteration 830 : 0.0656522810459137
Loss at iteration 840 : 0.07553841918706894
Loss at iteration 850 : 0.07651737332344055
Loss at iteration 860 : 0.09710526466369629
Loss at iteration 870 : 0.0897531509399414
Loss at iteration 880 : 0.07692908495664597
Loss at iteration 890 : 0.08964147418737411
Loss at iteration 900 : 0.04945157468318939
Loss at iteration 910 : 0.10211805999279022
Loss at iteration 920 : 0.06861862540245056
Loss at iteration 930 : 0.0755961686372757
Loss at iteration 940 : 0.07209533452987671
Loss at iteration 950 : 0.09199285507202148
Loss at iteration 960 : 0.12686219811439514
Loss at iteration 970 : 0.10206076502799988
Loss at iteration 980 : 0.1118493527173996
Loss at iteration 990 : 0.07408060133457184
Loss at iteration 1000 : 0.10268693417310715
Loss at iteration 1010 : 0.08574587106704712
Loss at iteration 1020 : 0.1521655172109604
Loss at iteration 1030 : 0.08166724443435669
Loss at iteration 1040 : 0.10618798434734344
Loss at iteration 1050 : 0.13547483086585999
Loss at iteration 1060 : 0.11345987766981125
Loss at iteration 1070 : 0.10713072121143341
Loss at iteration 1080 : 0.11821502447128296
Loss at iteration 1090 : 0.10527689754962921
Loss at iteration 1100 : 0.09973591566085815
Loss at iteration 1110 : 0.11152312159538269
Loss at iteration 1120 : 0.07579627633094788
Loss at iteration 1130 : 0.06573894619941711
Loss at iteration 1140 : 0.09484048932790756
Loss at iteration 1150 : 0.10015945136547089
Loss at iteration 1160 : 0.0889911949634552
Loss at iteration 1170 : 0.09513771533966064
Loss at iteration 1180 : 0.07790056616067886
Loss at iteration 1190 : 0.06649868935346603
Loss at iteration 1200 : 0.1602993756532669
Loss at iteration 1210 : 0.0919179767370224
The SSIM Value is: 0.7085579415162404
The PSNR Value is: 20.63646062215169
the epoch is: 98
Loss at iteration 10 : 0.08133690059185028
Loss at iteration 20 : 0.07778619229793549
Loss at iteration 30 : 0.05413048714399338
Loss at iteration 40 : 0.07361960411071777
Loss at iteration 50 : 0.12506520748138428
Loss at iteration 60 : 0.07520084828138351
Loss at iteration 70 : 0.108790323138237
Loss at iteration 80 : 0.0633290559053421
Loss at iteration 90 : 0.05869245529174805
Loss at iteration 100 : 0.09783603996038437
Loss at iteration 110 : 0.10260819643735886
Loss at iteration 120 : 0.06373432278633118
Loss at iteration 130 : 0.052856262773275375
Loss at iteration 140 : 0.0538802370429039
Loss at iteration 150 : 0.04402151703834534
Loss at iteration 160 : 0.10011424124240875
Loss at iteration 170 : 0.06968139111995697
Loss at iteration 180 : 0.09393306821584702
Loss at iteration 190 : 0.0829901248216629
Loss at iteration 200 : 0.07766927778720856
Loss at iteration 210 : 0.08031540364027023
Loss at iteration 220 : 0.08037278056144714
Loss at iteration 230 : 0.11008709669113159
Loss at iteration 240 : 0.11672570556402206
Loss at iteration 250 : 0.06155437231063843
Loss at iteration 260 : 0.10118797421455383
Loss at iteration 270 : 0.0489862784743309
Loss at iteration 280 : 0.10357816517353058
Loss at iteration 290 : 0.04896237701177597
Loss at iteration 300 : 0.09338915348052979
Loss at iteration 310 : 0.08809475600719452
Loss at iteration 320 : 0.10310405492782593
Loss at iteration 330 : 0.07727443426847458
Loss at iteration 340 : 0.07840382307767868
Loss at iteration 350 : 0.06234198436141014
Loss at iteration 360 : 0.07598574459552765
Loss at iteration 370 : 0.06713872402906418
Loss at iteration 380 : 0.11746586859226227
Loss at iteration 390 : 0.09438013285398483
Loss at iteration 400 : 0.08450436592102051
Loss at iteration 410 : 0.12133382260799408
Loss at iteration 420 : 0.10616324841976166
Loss at iteration 430 : 0.05783936381340027
Loss at iteration 440 : 0.0965002179145813
Loss at iteration 450 : 0.06388632953166962
Loss at iteration 460 : 0.05676376819610596
Loss at iteration 470 : 0.09542461484670639
Loss at iteration 480 : 0.11157405376434326
Loss at iteration 490 : 0.070940300822258
Loss at iteration 500 : 0.08145943284034729
Loss at iteration 510 : 0.068642258644104
Loss at iteration 520 : 0.07190104573965073
Loss at iteration 530 : 0.05916702747344971
Loss at iteration 540 : 0.06467199325561523
Loss at iteration 550 : 0.10109291970729828
Loss at iteration 560 : 0.09812565892934799
Loss at iteration 570 : 0.08281904458999634
Loss at iteration 580 : 0.06030476093292236
Loss at iteration 590 : 0.07523970305919647
Loss at iteration 600 : 0.10171294212341309
Loss at iteration 610 : 0.0784609243273735
Loss at iteration 620 : 0.07875056564807892
Loss at iteration 630 : 0.09153127670288086
Loss at iteration 640 : 0.072868213057518
Loss at iteration 650 : 0.08509407937526703
Loss at iteration 660 : 0.06853388249874115
Loss at iteration 670 : 0.0824088454246521
Loss at iteration 680 : 0.09840576350688934
Loss at iteration 690 : 0.0715392529964447
Loss at iteration 700 : 0.1200605034828186
Loss at iteration 710 : 0.08053352683782578
Loss at iteration 720 : 0.08736070990562439
Loss at iteration 730 : 0.06924867630004883
Loss at iteration 740 : 0.06274674832820892
Loss at iteration 750 : 0.07787061482667923
Loss at iteration 760 : 0.13305017352104187
Loss at iteration 770 : 0.11420725286006927
Loss at iteration 780 : 0.06400555372238159
Loss at iteration 790 : 0.0947938784956932
Loss at iteration 800 : 0.08942117542028427
Loss at iteration 810 : 0.08486318588256836
Loss at iteration 820 : 0.14418679475784302
Loss at iteration 830 : 0.07363429665565491
Loss at iteration 840 : 0.13461647927761078
Loss at iteration 850 : 0.08119900524616241
Loss at iteration 860 : 0.05445452779531479
Loss at iteration 870 : 0.06932295113801956
Loss at iteration 880 : 0.07301479578018188
Loss at iteration 890 : 0.062021441757678986
Loss at iteration 900 : 0.07406432181596756
Loss at iteration 910 : 0.06992584466934204
Loss at iteration 920 : 0.07792609930038452
Loss at iteration 930 : 0.07659024000167847
Loss at iteration 940 : 0.13322892785072327
Loss at iteration 950 : 0.10369031131267548
Loss at iteration 960 : 0.10614638030529022
Loss at iteration 970 : 0.09338080883026123
Loss at iteration 980 : 0.08986257016658783
Loss at iteration 990 : 0.042057931423187256
Loss at iteration 1000 : 0.06473194807767868
Loss at iteration 1010 : 0.12234299629926682
Loss at iteration 1020 : 0.09241229295730591
Loss at iteration 1030 : 0.09778226912021637
Loss at iteration 1040 : 0.10309915989637375
Loss at iteration 1050 : 0.08134110271930695
Loss at iteration 1060 : 0.07788941264152527
Loss at iteration 1070 : 0.12077423185110092
Loss at iteration 1080 : 0.08026957511901855
Loss at iteration 1090 : 0.06934909522533417
Loss at iteration 1100 : 0.09541736543178558
Loss at iteration 1110 : 0.07457002252340317
Loss at iteration 1120 : 0.06408475339412689
Loss at iteration 1130 : 0.11070644855499268
Loss at iteration 1140 : 0.0975765809416771
Loss at iteration 1150 : 0.08964034169912338
Loss at iteration 1160 : 0.09833389520645142
Loss at iteration 1170 : 0.1295011043548584
Loss at iteration 1180 : 0.11213362216949463
Loss at iteration 1190 : 0.07205809652805328
Loss at iteration 1200 : 0.07705849409103394
Loss at iteration 1210 : 0.09940756857395172
The SSIM Value is: 0.7060961703459422
The PSNR Value is: 20.638631057739257
the epoch is: 99
Loss at iteration 10 : 0.07893305271863937
Loss at iteration 20 : 0.08447333425283432
Loss at iteration 30 : 0.06793539226055145
Loss at iteration 40 : 0.089257150888443
Loss at iteration 50 : 0.05942421406507492
Loss at iteration 60 : 0.10948006808757782
Loss at iteration 70 : 0.09702890366315842
Loss at iteration 80 : 0.08666225522756577
Loss at iteration 90 : 0.13982699811458588
Loss at iteration 100 : 0.0712631493806839
Loss at iteration 110 : 0.064706951379776
Loss at iteration 120 : 0.057277482002973557
Loss at iteration 130 : 0.062029268592596054
Loss at iteration 140 : 0.07160317152738571
Loss at iteration 150 : 0.05270720273256302
Loss at iteration 160 : 0.0915384516119957
Loss at iteration 170 : 0.07751215994358063
Loss at iteration 180 : 0.11887817084789276
Loss at iteration 190 : 0.08117196708917618
Loss at iteration 200 : 0.1263451874256134
Loss at iteration 210 : 0.07528579235076904
Loss at iteration 220 : 0.10907323658466339
Loss at iteration 230 : 0.13002249598503113
Loss at iteration 240 : 0.08033303916454315
Loss at iteration 250 : 0.08261385560035706
Loss at iteration 260 : 0.10237010568380356
Loss at iteration 270 : 0.1017531007528305
Loss at iteration 280 : 0.050945818424224854
Loss at iteration 290 : 0.042081527411937714
Loss at iteration 300 : 0.11726394295692444
Loss at iteration 310 : 0.09689703583717346
Loss at iteration 320 : 0.09536075592041016
Loss at iteration 330 : 0.06499429047107697
Loss at iteration 340 : 0.10288657993078232
Loss at iteration 350 : 0.10094676911830902
Loss at iteration 360 : 0.0637090653181076
Loss at iteration 370 : 0.11824899166822433
Loss at iteration 380 : 0.10280120372772217
Loss at iteration 390 : 0.10947557538747787
Loss at iteration 400 : 0.12226095795631409
Loss at iteration 410 : 0.048366647213697433
Loss at iteration 420 : 0.10120314359664917
Loss at iteration 430 : 0.12156802415847778
Loss at iteration 440 : 0.06166147440671921
Loss at iteration 450 : 0.0829596221446991
Loss at iteration 460 : 0.053119342774152756
Loss at iteration 470 : 0.09198866784572601
Loss at iteration 480 : 0.10370521247386932
Loss at iteration 490 : 0.08603976666927338
Loss at iteration 500 : 0.04945787042379379
Loss at iteration 510 : 0.10983341932296753
Loss at iteration 520 : 0.04585812985897064
Loss at iteration 530 : 0.08079584687948227
Loss at iteration 540 : 0.06263738870620728
Loss at iteration 550 : 0.08377975225448608
Loss at iteration 560 : 0.09234987199306488
Loss at iteration 570 : 0.12383830547332764
Loss at iteration 580 : 0.057569172233343124
Loss at iteration 590 : 0.0994013100862503
Loss at iteration 600 : 0.061397116631269455
Loss at iteration 610 : 0.05257134884595871
Loss at iteration 620 : 0.06153823062777519
Loss at iteration 630 : 0.06623508036136627
Loss at iteration 640 : 0.08955663442611694
Loss at iteration 650 : 0.1038556843996048
Loss at iteration 660 : 0.04943057894706726
Loss at iteration 670 : 0.0842217430472374
Loss at iteration 680 : 0.055384110659360886
Loss at iteration 690 : 0.1348496973514557
Loss at iteration 700 : 0.08753339946269989
Loss at iteration 710 : 0.10530783981084824
Loss at iteration 720 : 0.10215004533529282
Loss at iteration 730 : 0.08437567949295044
Loss at iteration 740 : 0.10463203489780426
Loss at iteration 750 : 0.06921933591365814
Loss at iteration 760 : 0.08717627823352814
Loss at iteration 770 : 0.0768219456076622
Loss at iteration 780 : 0.09330805391073227
Loss at iteration 790 : 0.044374577701091766
Loss at iteration 800 : 0.08316782861948013
Loss at iteration 810 : 0.07971575856208801
Loss at iteration 820 : 0.07142776250839233
Loss at iteration 830 : 0.06992845237255096
Loss at iteration 840 : 0.0876879021525383
Loss at iteration 850 : 0.06586019694805145
Loss at iteration 860 : 0.11836124956607819
Loss at iteration 870 : 0.1003374457359314
Loss at iteration 880 : 0.11036637425422668
Loss at iteration 890 : 0.06649202108383179
Loss at iteration 900 : 0.0702197328209877
Loss at iteration 910 : 0.06956951320171356
Loss at iteration 920 : 0.08292457461357117
Loss at iteration 930 : 0.05474741384387016
Loss at iteration 940 : 0.137772798538208
Loss at iteration 950 : 0.07656606286764145
Loss at iteration 960 : 0.058840155601501465
Loss at iteration 970 : 0.06458821892738342
Loss at iteration 980 : 0.05650437995791435
Loss at iteration 990 : 0.1111505776643753
Loss at iteration 1000 : 0.07768917083740234
Loss at iteration 1010 : 0.07603850215673447
Loss at iteration 1020 : 0.12705889344215393
Loss at iteration 1030 : 0.12312082946300507
Loss at iteration 1040 : 0.060891564935445786
Loss at iteration 1050 : 0.10807029902935028
Loss at iteration 1060 : 0.06379005312919617
Loss at iteration 1070 : 0.12328454852104187
Loss at iteration 1080 : 0.09807327389717102
Loss at iteration 1090 : 0.10616618394851685
Loss at iteration 1100 : 0.0924450159072876
Loss at iteration 1110 : 0.0949554294347763
Loss at iteration 1120 : 0.08252143859863281
Loss at iteration 1130 : 0.09638921916484833
Loss at iteration 1140 : 0.09017108380794525
Loss at iteration 1150 : 0.10732656717300415
Loss at iteration 1160 : 0.08239399641752243
Loss at iteration 1170 : 0.07351881265640259
Loss at iteration 1180 : 0.13484962284564972
Loss at iteration 1190 : 0.11034148931503296
Loss at iteration 1200 : 0.08270776271820068
Loss at iteration 1210 : 0.06475748866796494
The SSIM Value is: 0.7051244636376699
The PSNR Value is: 20.397016779581705
the epoch is: 100
Loss at iteration 10 : 0.10580471158027649
Loss at iteration 20 : 0.13878688216209412
Loss at iteration 30 : 0.11534503102302551
Loss at iteration 40 : 0.08255192637443542
Loss at iteration 50 : 0.08414755761623383
Loss at iteration 60 : 0.05175818130373955
Loss at iteration 70 : 0.07534781843423843
Loss at iteration 80 : 0.1208057850599289
Loss at iteration 90 : 0.08786952495574951
Loss at iteration 100 : 0.08200700581073761
Loss at iteration 110 : 0.0978492721915245
Loss at iteration 120 : 0.08004419505596161
Loss at iteration 130 : 0.1051352471113205
Loss at iteration 140 : 0.06704186648130417
Loss at iteration 150 : 0.044191353023052216
Loss at iteration 160 : 0.06363730877637863
Loss at iteration 170 : 0.07190896570682526
Loss at iteration 180 : 0.06030867621302605
Loss at iteration 190 : 0.07320083677768707
Loss at iteration 200 : 0.1058574914932251
Loss at iteration 210 : 0.08597996830940247
Loss at iteration 220 : 0.05087476596236229
Loss at iteration 230 : 0.10150280594825745
Loss at iteration 240 : 0.06255808472633362
Loss at iteration 250 : 0.09320545196533203
Loss at iteration 260 : 0.07540194690227509
Loss at iteration 270 : 0.08648715913295746
Loss at iteration 280 : 0.11587890237569809
Loss at iteration 290 : 0.06837010383605957
Loss at iteration 300 : 0.10064195841550827
Loss at iteration 310 : 0.12932002544403076
Loss at iteration 320 : 0.06716059893369675
Loss at iteration 330 : 0.05313420295715332
Loss at iteration 340 : 0.07640469819307327
Loss at iteration 350 : 0.09214267134666443
Loss at iteration 360 : 0.0890272781252861
Loss at iteration 370 : 0.06606651842594147
Loss at iteration 380 : 0.07933024317026138
Loss at iteration 390 : 0.09724603593349457
Loss at iteration 400 : 0.067020945250988
Loss at iteration 410 : 0.04660940170288086
Loss at iteration 420 : 0.07081174850463867
Loss at iteration 430 : 0.06535474956035614
Loss at iteration 440 : 0.08365237712860107
Loss at iteration 450 : 0.12986259162425995
Loss at iteration 460 : 0.0799713283777237
Loss at iteration 470 : 0.05418698117136955
Loss at iteration 480 : 0.05709582567214966
Loss at iteration 490 : 0.05014876276254654
Loss at iteration 500 : 0.06369583308696747
Loss at iteration 510 : 0.049678683280944824
Loss at iteration 520 : 0.07621630281209946
Loss at iteration 530 : 0.06264393776655197
Loss at iteration 540 : 0.146697998046875
Loss at iteration 550 : 0.05392449349164963
Loss at iteration 560 : 0.10246779769659042
Loss at iteration 570 : 0.08759009093046188
Loss at iteration 580 : 0.0710044726729393
Loss at iteration 590 : 0.1331866979598999
Loss at iteration 600 : 0.05612827464938164
Loss at iteration 610 : 0.1291307657957077
Loss at iteration 620 : 0.07925387471914291
Loss at iteration 630 : 0.04514265060424805
Loss at iteration 640 : 0.06637495011091232
Loss at iteration 650 : 0.08801262080669403
Loss at iteration 660 : 0.06701789051294327
Loss at iteration 670 : 0.06269203871488571
Loss at iteration 680 : 0.06200557202100754
Loss at iteration 690 : 0.07196390628814697
Loss at iteration 700 : 0.04912060126662254
Loss at iteration 710 : 0.08497463166713715
Loss at iteration 720 : 0.0935245007276535
Loss at iteration 730 : 0.10098513215780258
Loss at iteration 740 : 0.11857044696807861
Loss at iteration 750 : 0.09265798330307007
Loss at iteration 760 : 0.08714981377124786
Loss at iteration 770 : 0.06752261519432068
Loss at iteration 780 : 0.04184594005346298
Loss at iteration 790 : 0.09601988643407822
Loss at iteration 800 : 0.06808077543973923
Loss at iteration 810 : 0.08391734957695007
Loss at iteration 820 : 0.06721581518650055
Loss at iteration 830 : 0.13248535990715027
Loss at iteration 840 : 0.13324826955795288
Loss at iteration 850 : 0.0795564204454422
Loss at iteration 860 : 0.06292598694562912
Loss at iteration 870 : 0.12256574630737305
Loss at iteration 880 : 0.06807592511177063
Loss at iteration 890 : 0.15814794600009918
Loss at iteration 900 : 0.05647006630897522
Loss at iteration 910 : 0.11452360451221466
Loss at iteration 920 : 0.07704140990972519
Loss at iteration 930 : 0.08010375499725342
Loss at iteration 940 : 0.07041898369789124
Loss at iteration 950 : 0.06655240058898926
Loss at iteration 960 : 0.11398297548294067
Loss at iteration 970 : 0.08616757392883301
Loss at iteration 980 : 0.0718412846326828
Loss at iteration 990 : 0.0811176672577858
Loss at iteration 1000 : 0.07605810463428497
Loss at iteration 1010 : 0.06448625028133392
Loss at iteration 1020 : 0.07223501056432724
Loss at iteration 1030 : 0.053175728768110275
Loss at iteration 1040 : 0.06994171440601349
Loss at iteration 1050 : 0.08084374666213989
Loss at iteration 1060 : 0.045162688940763474
Loss at iteration 1070 : 0.07368014752864838
Loss at iteration 1080 : 0.09729059785604477
Loss at iteration 1090 : 0.049897562712430954
Loss at iteration 1100 : 0.1367236226797104
Loss at iteration 1110 : 0.07074320316314697
Loss at iteration 1120 : 0.10846000909805298
Loss at iteration 1130 : 0.09586262702941895
Loss at iteration 1140 : 0.07788452506065369
Loss at iteration 1150 : 0.08135657012462616
Loss at iteration 1160 : 0.05082124471664429
Loss at iteration 1170 : 0.10390779376029968
Loss at iteration 1180 : 0.08499209582805634
Loss at iteration 1190 : 0.08307015150785446
Loss at iteration 1200 : 0.11957027018070221
Loss at iteration 1210 : 0.13792262971401215
The SSIM Value is: 0.7144505381584167
The PSNR Value is: 21.127647654215494
the epoch is: 101
Loss at iteration 10 : 0.09923803806304932
Loss at iteration 20 : 0.08376597613096237
Loss at iteration 30 : 0.1100696325302124
Loss at iteration 40 : 0.04569568857550621
Loss at iteration 50 : 0.06303291022777557
Loss at iteration 60 : 0.06962890923023224
Loss at iteration 70 : 0.08503075689077377
Loss at iteration 80 : 0.07077987492084503
Loss at iteration 90 : 0.06915672868490219
Loss at iteration 100 : 0.06880369782447815
Loss at iteration 110 : 0.05179658532142639
Loss at iteration 120 : 0.07358065247535706
Loss at iteration 130 : 0.073878213763237
Loss at iteration 140 : 0.056744903326034546
Loss at iteration 150 : 0.05644988268613815
Loss at iteration 160 : 0.08003178238868713
Loss at iteration 170 : 0.07755924761295319
Loss at iteration 180 : 0.09951716661453247
Loss at iteration 190 : 0.05734127014875412
Loss at iteration 200 : 0.10559578984975815
Loss at iteration 210 : 0.08306774497032166
Loss at iteration 220 : 0.06348125636577606
Loss at iteration 230 : 0.07205122709274292
Loss at iteration 240 : 0.13598501682281494
Loss at iteration 250 : 0.07375699281692505
Loss at iteration 260 : 0.08067496120929718
Loss at iteration 270 : 0.08262117207050323
Loss at iteration 280 : 0.08152277022600174
Loss at iteration 290 : 0.08501198887825012
Loss at iteration 300 : 0.04838978871703148
Loss at iteration 310 : 0.060885753482580185
Loss at iteration 320 : 0.11250954866409302
Loss at iteration 330 : 0.05824143812060356
Loss at iteration 340 : 0.06226126477122307
Loss at iteration 350 : 0.09057071805000305
Loss at iteration 360 : 0.06771112978458405
Loss at iteration 370 : 0.08269993960857391
Loss at iteration 380 : 0.12731580436229706
Loss at iteration 390 : 0.07000575959682465
Loss at iteration 400 : 0.13153952360153198
Loss at iteration 410 : 0.0965648740530014
Loss at iteration 420 : 0.09487792104482651
Loss at iteration 430 : 0.0938146635890007
Loss at iteration 440 : 0.1189504861831665
Loss at iteration 450 : 0.06187981367111206
Loss at iteration 460 : 0.07211282849311829
Loss at iteration 470 : 0.1062578409910202
Loss at iteration 480 : 0.08397510647773743
Loss at iteration 490 : 0.09157265722751617
Loss at iteration 500 : 0.05138663575053215
Loss at iteration 510 : 0.09739935398101807
Loss at iteration 520 : 0.07296700775623322
Loss at iteration 530 : 0.08537738025188446
Loss at iteration 540 : 0.11100554466247559
Loss at iteration 550 : 0.08744210004806519
Loss at iteration 560 : 0.10051139444112778
Loss at iteration 570 : 0.05616573616862297
Loss at iteration 580 : 0.0944703221321106
Loss at iteration 590 : 0.08175800740718842
Loss at iteration 600 : 0.07771061360836029
Loss at iteration 610 : 0.08227276802062988
Loss at iteration 620 : 0.06987567245960236
Loss at iteration 630 : 0.054625146090984344
Loss at iteration 640 : 0.08850674331188202
Loss at iteration 650 : 0.09652018547058105
Loss at iteration 660 : 0.07937819510698318
Loss at iteration 670 : 0.07011847198009491
Loss at iteration 680 : 0.10037343204021454
Loss at iteration 690 : 0.10786150395870209
Loss at iteration 700 : 0.06598709523677826
Loss at iteration 710 : 0.145663782954216
Loss at iteration 720 : 0.08425136655569077
Loss at iteration 730 : 0.06463450193405151
Loss at iteration 740 : 0.07316223531961441
Loss at iteration 750 : 0.08762133121490479
Loss at iteration 760 : 0.09653272479772568
Loss at iteration 770 : 0.08302441984415054
Loss at iteration 780 : 0.056481800973415375
Loss at iteration 790 : 0.15582551062107086
Loss at iteration 800 : 0.07454060018062592
Loss at iteration 810 : 0.09135009348392487
Loss at iteration 820 : 0.11682498455047607
Loss at iteration 830 : 0.0674918070435524
Loss at iteration 840 : 0.07524974644184113
Loss at iteration 850 : 0.06406764686107635
Loss at iteration 860 : 0.04519054293632507
Loss at iteration 870 : 0.09388332813978195
Loss at iteration 880 : 0.07205584645271301
Loss at iteration 890 : 0.05828511342406273
Loss at iteration 900 : 0.04946671426296234
Loss at iteration 910 : 0.11057894676923752
Loss at iteration 920 : 0.08162067085504532
Loss at iteration 930 : 0.07364217936992645
Loss at iteration 940 : 0.11735935509204865
Loss at iteration 950 : 0.07741130143404007
Loss at iteration 960 : 0.056489210575819016
Loss at iteration 970 : 0.05319727957248688
Loss at iteration 980 : 0.08449747413396835
Loss at iteration 990 : 0.11401264369487762
Loss at iteration 1000 : 0.07795969396829605
Loss at iteration 1010 : 0.06568346917629242
Loss at iteration 1020 : 0.1044432520866394
Loss at iteration 1030 : 0.08775648474693298
Loss at iteration 1040 : 0.07424380630254745
Loss at iteration 1050 : 0.07209447026252747
Loss at iteration 1060 : 0.10993178933858871
Loss at iteration 1070 : 0.10810552537441254
Loss at iteration 1080 : 0.12187376618385315
Loss at iteration 1090 : 0.08853330463171005
Loss at iteration 1100 : 0.08953613042831421
Loss at iteration 1110 : 0.10635417699813843
Loss at iteration 1120 : 0.065230593085289
Loss at iteration 1130 : 0.07606537640094757
Loss at iteration 1140 : 0.07441803067922592
Loss at iteration 1150 : 0.0843491479754448
Loss at iteration 1160 : 0.08990810811519623
Loss at iteration 1170 : 0.07053517550230026
Loss at iteration 1180 : 0.09934709966182709
Loss at iteration 1190 : 0.07768458127975464
Loss at iteration 1200 : 0.089313805103302
Loss at iteration 1210 : 0.15508219599723816
The SSIM Value is: 0.7132893880208333
The PSNR Value is: 20.753747113545735
the epoch is: 102
Loss at iteration 10 : 0.04876192659139633
Loss at iteration 20 : 0.05829126387834549
Loss at iteration 30 : 0.08122119307518005
Loss at iteration 40 : 0.08173952996730804
Loss at iteration 50 : 0.14347466826438904
Loss at iteration 60 : 0.08214985579252243
Loss at iteration 70 : 0.07356998324394226
Loss at iteration 80 : 0.0893176794052124
Loss at iteration 90 : 0.07243533432483673
Loss at iteration 100 : 0.13229456543922424
Loss at iteration 110 : 0.08796791732311249
Loss at iteration 120 : 0.110105499625206
Loss at iteration 130 : 0.0803229957818985
Loss at iteration 140 : 0.06582432240247726
Loss at iteration 150 : 0.05289424583315849
Loss at iteration 160 : 0.12639546394348145
Loss at iteration 170 : 0.07862626761198044
Loss at iteration 180 : 0.12736673653125763
Loss at iteration 190 : 0.04337736964225769
Loss at iteration 200 : 0.10526268184185028
Loss at iteration 210 : 0.15154439210891724
Loss at iteration 220 : 0.07498031109571457
Loss at iteration 230 : 0.10316776484251022
Loss at iteration 240 : 0.0836944580078125
Loss at iteration 250 : 0.08155584335327148
Loss at iteration 260 : 0.06964975595474243
Loss at iteration 270 : 0.062454283237457275
Loss at iteration 280 : 0.08981140702962875
Loss at iteration 290 : 0.09162765741348267
Loss at iteration 300 : 0.06466685235500336
Loss at iteration 310 : 0.07988744229078293
Loss at iteration 320 : 0.07966490834951401
Loss at iteration 330 : 0.08893293142318726
Loss at iteration 340 : 0.08428361266851425
Loss at iteration 350 : 0.10797855257987976
Loss at iteration 360 : 0.05434954911470413
Loss at iteration 370 : 0.09732341021299362
Loss at iteration 380 : 0.10191874951124191
Loss at iteration 390 : 0.07665896415710449
Loss at iteration 400 : 0.08713008463382721
Loss at iteration 410 : 0.07259683310985565
Loss at iteration 420 : 0.08533237874507904
Loss at iteration 430 : 0.08363964408636093
Loss at iteration 440 : 0.08184945583343506
Loss at iteration 450 : 0.07634272426366806
Loss at iteration 460 : 0.08168677985668182
Loss at iteration 470 : 0.11305741965770721
Loss at iteration 480 : 0.05915180966258049
Loss at iteration 490 : 0.08734168857336044
Loss at iteration 500 : 0.07131689041852951
Loss at iteration 510 : 0.08842115104198456
Loss at iteration 520 : 0.11027269065380096
Loss at iteration 530 : 0.10348866879940033
Loss at iteration 540 : 0.05497996509075165
Loss at iteration 550 : 0.07890497893095016
Loss at iteration 560 : 0.08592425286769867
Loss at iteration 570 : 0.09320329129695892
Loss at iteration 580 : 0.06983043253421783
Loss at iteration 590 : 0.057198941707611084
Loss at iteration 600 : 0.09747525304555893
Loss at iteration 610 : 0.10305432975292206
Loss at iteration 620 : 0.10787563025951385
Loss at iteration 630 : 0.0803719013929367
Loss at iteration 640 : 0.11763133853673935
Loss at iteration 650 : 0.08328614383935928
Loss at iteration 660 : 0.08623550832271576
Loss at iteration 670 : 0.08250394463539124
Loss at iteration 680 : 0.07939886301755905
Loss at iteration 690 : 0.06582324206829071
Loss at iteration 700 : 0.0644523948431015
Loss at iteration 710 : 0.05744510889053345
Loss at iteration 720 : 0.1279788762331009
Loss at iteration 730 : 0.11678864806890488
Loss at iteration 740 : 0.09145419299602509
Loss at iteration 750 : 0.09215354174375534
Loss at iteration 760 : 0.07676593959331512
Loss at iteration 770 : 0.12188825011253357
Loss at iteration 780 : 0.07381851226091385
Loss at iteration 790 : 0.08212567865848541
Loss at iteration 800 : 0.07142491638660431
Loss at iteration 810 : 0.0801604688167572
Loss at iteration 820 : 0.06364721059799194
Loss at iteration 830 : 0.057312920689582825
Loss at iteration 840 : 0.08173368871212006
Loss at iteration 850 : 0.08391012251377106
Loss at iteration 860 : 0.08100380003452301
Loss at iteration 870 : 0.09949924796819687
Loss at iteration 880 : 0.10293718427419662
Loss at iteration 890 : 0.13211357593536377
Loss at iteration 900 : 0.147213414311409
Loss at iteration 910 : 0.0858198031783104
Loss at iteration 920 : 0.06548639386892319
Loss at iteration 930 : 0.09237529337406158
Loss at iteration 940 : 0.0923227071762085
Loss at iteration 950 : 0.09474008530378342
Loss at iteration 960 : 0.10757134854793549
Loss at iteration 970 : 0.06593391299247742
Loss at iteration 980 : 0.07687059044837952
Loss at iteration 990 : 0.10261818021535873
Loss at iteration 1000 : 0.14144453406333923
Loss at iteration 1010 : 0.11723405122756958
Loss at iteration 1020 : 0.06781074404716492
Loss at iteration 1030 : 0.09304182231426239
Loss at iteration 1040 : 0.09185664355754852
Loss at iteration 1050 : 0.13987168669700623
Loss at iteration 1060 : 0.12768495082855225
Loss at iteration 1070 : 0.10283078998327255
Loss at iteration 1080 : 0.11699976772069931
Loss at iteration 1090 : 0.07354167103767395
Loss at iteration 1100 : 0.06345239281654358
Loss at iteration 1110 : 0.05305829644203186
Loss at iteration 1120 : 0.07506915181875229
Loss at iteration 1130 : 0.07588589191436768
Loss at iteration 1140 : 0.050226449966430664
Loss at iteration 1150 : 0.09377666562795639
Loss at iteration 1160 : 0.13116872310638428
Loss at iteration 1170 : 0.0970613956451416
Loss at iteration 1180 : 0.08938020467758179
Loss at iteration 1190 : 0.12965193390846252
Loss at iteration 1200 : 0.1008959412574768
Loss at iteration 1210 : 0.07976359874010086
The SSIM Value is: 0.7061696767807006
The PSNR Value is: 20.708768145243326
the epoch is: 103
Loss at iteration 10 : 0.06892114132642746
Loss at iteration 20 : 0.05746190994977951
Loss at iteration 30 : 0.13520324230194092
Loss at iteration 40 : 0.06179846450686455
Loss at iteration 50 : 0.13387088477611542
Loss at iteration 60 : 0.060291215777397156
Loss at iteration 70 : 0.11019100993871689
Loss at iteration 80 : 0.11650405824184418
Loss at iteration 90 : 0.07461902499198914
Loss at iteration 100 : 0.11078130453824997
Loss at iteration 110 : 0.03878089040517807
Loss at iteration 120 : 0.09149898588657379
Loss at iteration 130 : 0.05973774939775467
Loss at iteration 140 : 0.0778939351439476
Loss at iteration 150 : 0.0700163021683693
Loss at iteration 160 : 0.05959376320242882
Loss at iteration 170 : 0.08710287511348724
Loss at iteration 180 : 0.06848645210266113
Loss at iteration 190 : 0.1282215416431427
Loss at iteration 200 : 0.09835448116064072
Loss at iteration 210 : 0.07268745452165604
Loss at iteration 220 : 0.08738201856613159
Loss at iteration 230 : 0.06171560287475586
Loss at iteration 240 : 0.05765625834465027
Loss at iteration 250 : 0.07347017526626587
Loss at iteration 260 : 0.10099822282791138
Loss at iteration 270 : 0.08832070231437683
Loss at iteration 280 : 0.0672680139541626
Loss at iteration 290 : 0.06309747695922852
Loss at iteration 300 : 0.07041923701763153
Loss at iteration 310 : 0.1051371842622757
Loss at iteration 320 : 0.1025707870721817
Loss at iteration 330 : 0.08894817531108856
Loss at iteration 340 : 0.09338953346014023
Loss at iteration 350 : 0.08351941406726837
Loss at iteration 360 : 0.08961635082960129
Loss at iteration 370 : 0.07502000778913498
Loss at iteration 380 : 0.09016531705856323
Loss at iteration 390 : 0.08004637062549591
Loss at iteration 400 : 0.08129917085170746
Loss at iteration 410 : 0.10360436141490936
Loss at iteration 420 : 0.10321277379989624
Loss at iteration 430 : 0.0882209837436676
Loss at iteration 440 : 0.07732202857732773
Loss at iteration 450 : 0.052580490708351135
Loss at iteration 460 : 0.15029889345169067
Loss at iteration 470 : 0.0733192190527916
Loss at iteration 480 : 0.12373121827840805
Loss at iteration 490 : 0.10123230516910553
Loss at iteration 500 : 0.08752527832984924
Loss at iteration 510 : 0.05841952562332153
Loss at iteration 520 : 0.07514563947916031
Loss at iteration 530 : 0.11228583753108978
Loss at iteration 540 : 0.0712558850646019
Loss at iteration 550 : 0.08843325078487396
Loss at iteration 560 : 0.06916320323944092
Loss at iteration 570 : 0.10553701967000961
Loss at iteration 580 : 0.056302331387996674
Loss at iteration 590 : 0.043737899512052536
Loss at iteration 600 : 0.1285960078239441
Loss at iteration 610 : 0.08808853477239609
Loss at iteration 620 : 0.11081766337156296
Loss at iteration 630 : 0.11175726354122162
Loss at iteration 640 : 0.054269686341285706
Loss at iteration 650 : 0.06923981755971909
Loss at iteration 660 : 0.0595565065741539
Loss at iteration 670 : 0.12583163380622864
Loss at iteration 680 : 0.08945484459400177
Loss at iteration 690 : 0.08623423427343369
Loss at iteration 700 : 0.11920579522848129
Loss at iteration 710 : 0.08406500518321991
Loss at iteration 720 : 0.10958125442266464
Loss at iteration 730 : 0.06631852686405182
Loss at iteration 740 : 0.14512863755226135
Loss at iteration 750 : 0.05881102383136749
Loss at iteration 760 : 0.0674184262752533
Loss at iteration 770 : 0.10780242085456848
Loss at iteration 780 : 0.0755840539932251
Loss at iteration 790 : 0.09935726970434189
Loss at iteration 800 : 0.08310502022504807
Loss at iteration 810 : 0.10031020641326904
Loss at iteration 820 : 0.0636974573135376
Loss at iteration 830 : 0.08857383579015732
Loss at iteration 840 : 0.08783094584941864
Loss at iteration 850 : 0.09664057195186615
Loss at iteration 860 : 0.11262953281402588
Loss at iteration 870 : 0.07970307767391205
Loss at iteration 880 : 0.08481580018997192
Loss at iteration 890 : 0.0988646149635315
Loss at iteration 900 : 0.0783526599407196
Loss at iteration 910 : 0.13237005472183228
Loss at iteration 920 : 0.08159039914608002
Loss at iteration 930 : 0.07934032380580902
Loss at iteration 940 : 0.071408711373806
Loss at iteration 950 : 0.08910252153873444
Loss at iteration 960 : 0.07289107888936996
Loss at iteration 970 : 0.1219169944524765
Loss at iteration 980 : 0.04986240714788437
Loss at iteration 990 : 0.08099376410245895
Loss at iteration 1000 : 0.13140743970870972
Loss at iteration 1010 : 0.12250000238418579
Loss at iteration 1020 : 0.07664503157138824
Loss at iteration 1030 : 0.0783175677061081
Loss at iteration 1040 : 0.09337571263313293
Loss at iteration 1050 : 0.056546442210674286
Loss at iteration 1060 : 0.09991991519927979
Loss at iteration 1070 : 0.1810363233089447
Loss at iteration 1080 : 0.09624835103750229
Loss at iteration 1090 : 0.06318508088588715
Loss at iteration 1100 : 0.07223493605852127
Loss at iteration 1110 : 0.07857149839401245
Loss at iteration 1120 : 0.049786828458309174
Loss at iteration 1130 : 0.06784793734550476
Loss at iteration 1140 : 0.11176641285419464
Loss at iteration 1150 : 0.08640716969966888
Loss at iteration 1160 : 0.09029679000377655
Loss at iteration 1170 : 0.07371237874031067
Loss at iteration 1180 : 0.06326611340045929
Loss at iteration 1190 : 0.11584541201591492
Loss at iteration 1200 : 0.10134471952915192
Loss at iteration 1210 : 0.07218905538320541
The SSIM Value is: 0.7113714317480723
The PSNR Value is: 20.911765162150065
the epoch is: 104
Loss at iteration 10 : 0.06173097342252731
Loss at iteration 20 : 0.07372790575027466
Loss at iteration 30 : 0.09351611137390137
Loss at iteration 40 : 0.06640179455280304
Loss at iteration 50 : 0.09308145940303802
Loss at iteration 60 : 0.11699293553829193
Loss at iteration 70 : 0.0884811133146286
Loss at iteration 80 : 0.060663722455501556
Loss at iteration 90 : 0.11690404266119003
Loss at iteration 100 : 0.0851348489522934
Loss at iteration 110 : 0.08345049619674683
Loss at iteration 120 : 0.07611465454101562
Loss at iteration 130 : 0.06341003626585007
Loss at iteration 140 : 0.13291554152965546
Loss at iteration 150 : 0.08608901500701904
Loss at iteration 160 : 0.0787678062915802
Loss at iteration 170 : 0.08444812148809433
Loss at iteration 180 : 0.06551004946231842
Loss at iteration 190 : 0.08633536100387573
Loss at iteration 200 : 0.05610888451337814
Loss at iteration 210 : 0.06834444403648376
Loss at iteration 220 : 0.08831783384084702
Loss at iteration 230 : 0.11202629655599594
Loss at iteration 240 : 0.11329446732997894
Loss at iteration 250 : 0.07377763837575912
Loss at iteration 260 : 0.12222543358802795
Loss at iteration 270 : 0.08331118524074554
Loss at iteration 280 : 0.09632134437561035
Loss at iteration 290 : 0.05777767300605774
Loss at iteration 300 : 0.06464128196239471
Loss at iteration 310 : 0.09555231034755707
Loss at iteration 320 : 0.08193405717611313
Loss at iteration 330 : 0.0994332879781723
Loss at iteration 340 : 0.10725532472133636
Loss at iteration 350 : 0.08869117498397827
Loss at iteration 360 : 0.07907593250274658
Loss at iteration 370 : 0.09485308825969696
Loss at iteration 380 : 0.10591438412666321
Loss at iteration 390 : 0.06397511065006256
Loss at iteration 400 : 0.06434446573257446
Loss at iteration 410 : 0.08105656504631042
Loss at iteration 420 : 0.11433606594800949
Loss at iteration 430 : 0.10446593165397644
Loss at iteration 440 : 0.07051203399896622
Loss at iteration 450 : 0.06045523285865784
Loss at iteration 460 : 0.06641902774572372
Loss at iteration 470 : 0.11018151044845581
Loss at iteration 480 : 0.059102680534124374
Loss at iteration 490 : 0.0872618556022644
Loss at iteration 500 : 0.07285811007022858
Loss at iteration 510 : 0.06790442019701004
Loss at iteration 520 : 0.056833963841199875
Loss at iteration 530 : 0.09701897948980331
Loss at iteration 540 : 0.047959357500076294
Loss at iteration 550 : 0.07884719222784042
Loss at iteration 560 : 0.06477643549442291
Loss at iteration 570 : 0.07999034225940704
Loss at iteration 580 : 0.06559251993894577
Loss at iteration 590 : 0.060552097856998444
Loss at iteration 600 : 0.08704172819852829
Loss at iteration 610 : 0.082135409116745
Loss at iteration 620 : 0.09495854377746582
Loss at iteration 630 : 0.06669953465461731
Loss at iteration 640 : 0.07442629337310791
Loss at iteration 650 : 0.07101011276245117
Loss at iteration 660 : 0.12004533410072327
Loss at iteration 670 : 0.10959653556346893
Loss at iteration 680 : 0.04745114967226982
Loss at iteration 690 : 0.06114904209971428
Loss at iteration 700 : 0.08683939278125763
Loss at iteration 710 : 0.11478706449270248
Loss at iteration 720 : 0.10087460279464722
Loss at iteration 730 : 0.07975836098194122
Loss at iteration 740 : 0.07928463071584702
Loss at iteration 750 : 0.14201152324676514
Loss at iteration 760 : 0.06125820428133011
Loss at iteration 770 : 0.11404633522033691
Loss at iteration 780 : 0.10471197217702866
Loss at iteration 790 : 0.14793846011161804
Loss at iteration 800 : 0.09767697751522064
Loss at iteration 810 : 0.07450078427791595
Loss at iteration 820 : 0.06437341123819351
Loss at iteration 830 : 0.09732818603515625
Loss at iteration 840 : 0.05997209995985031
Loss at iteration 850 : 0.09417852759361267
Loss at iteration 860 : 0.05325118824839592
Loss at iteration 870 : 0.09737420082092285
Loss at iteration 880 : 0.09527810662984848
Loss at iteration 890 : 0.05977347120642662
Loss at iteration 900 : 0.09210320562124252
Loss at iteration 910 : 0.08326444029808044
Loss at iteration 920 : 0.06210332363843918
Loss at iteration 930 : 0.11627505719661713
Loss at iteration 940 : 0.06309127062559128
Loss at iteration 950 : 0.08644475787878036
Loss at iteration 960 : 0.07018112391233444
Loss at iteration 970 : 0.06082291156053543
Loss at iteration 980 : 0.06276315450668335
Loss at iteration 990 : 0.08401524275541306
Loss at iteration 1000 : 0.04754802584648132
Loss at iteration 1010 : 0.07998345792293549
Loss at iteration 1020 : 0.11231794208288193
Loss at iteration 1030 : 0.11530812829732895
Loss at iteration 1040 : 0.08798624575138092
Loss at iteration 1050 : 0.11245685815811157
Loss at iteration 1060 : 0.07212387025356293
Loss at iteration 1070 : 0.10009568929672241
Loss at iteration 1080 : 0.10492634028196335
Loss at iteration 1090 : 0.09072990715503693
Loss at iteration 1100 : 0.10505156219005585
Loss at iteration 1110 : 0.08823289722204208
Loss at iteration 1120 : 0.09188177436590195
Loss at iteration 1130 : 0.0765402615070343
Loss at iteration 1140 : 0.10613994300365448
Loss at iteration 1150 : 0.06319080293178558
Loss at iteration 1160 : 0.09021872282028198
Loss at iteration 1170 : 0.09167289733886719
Loss at iteration 1180 : 0.0812184140086174
Loss at iteration 1190 : 0.051826030015945435
Loss at iteration 1200 : 0.07709968090057373
Loss at iteration 1210 : 0.11244276165962219
The SSIM Value is: 0.7124035557111105
The PSNR Value is: 20.966481272379557
the epoch is: 105
Loss at iteration 10 : 0.12604843080043793
Loss at iteration 20 : 0.05946297571063042
Loss at iteration 30 : 0.0628199577331543
Loss at iteration 40 : 0.09685613960027695
Loss at iteration 50 : 0.1202542632818222
Loss at iteration 60 : 0.09152542799711227
Loss at iteration 70 : 0.06924937665462494
Loss at iteration 80 : 0.052768997848033905
Loss at iteration 90 : 0.0690034031867981
Loss at iteration 100 : 0.060811202973127365
Loss at iteration 110 : 0.0768548771739006
Loss at iteration 120 : 0.1313004195690155
Loss at iteration 130 : 0.09648817032575607
Loss at iteration 140 : 0.04963582009077072
Loss at iteration 150 : 0.09050482511520386
Loss at iteration 160 : 0.061186518520116806
Loss at iteration 170 : 0.09004392474889755
Loss at iteration 180 : 0.07770934700965881
Loss at iteration 190 : 0.06414934992790222
Loss at iteration 200 : 0.09892110526561737
Loss at iteration 210 : 0.05092693492770195
Loss at iteration 220 : 0.14608749747276306
Loss at iteration 230 : 0.07312293350696564
Loss at iteration 240 : 0.10779453814029694
Loss at iteration 250 : 0.0870249792933464
Loss at iteration 260 : 0.0845509022474289
Loss at iteration 270 : 0.06833010166883469
Loss at iteration 280 : 0.08781193941831589
Loss at iteration 290 : 0.09316430985927582
Loss at iteration 300 : 0.06650488078594208
Loss at iteration 310 : 0.11264025419950485
Loss at iteration 320 : 0.07350652664899826
Loss at iteration 330 : 0.07523351907730103
Loss at iteration 340 : 0.09135770797729492
Loss at iteration 350 : 0.1082485169172287
Loss at iteration 360 : 0.09078111499547958
Loss at iteration 370 : 0.081025131046772
Loss at iteration 380 : 0.09031601250171661
Loss at iteration 390 : 0.07917337119579315
Loss at iteration 400 : 0.06688456982374191
Loss at iteration 410 : 0.09745503962039948
Loss at iteration 420 : 0.13194972276687622
Loss at iteration 430 : 0.14524012804031372
Loss at iteration 440 : 0.06359178572893143
Loss at iteration 450 : 0.09463578462600708
Loss at iteration 460 : 0.08420895040035248
Loss at iteration 470 : 0.10770733654499054
Loss at iteration 480 : 0.10509424656629562
Loss at iteration 490 : 0.16626159846782684
Loss at iteration 500 : 0.05235454440116882
Loss at iteration 510 : 0.06293847411870956
Loss at iteration 520 : 0.09409254044294357
Loss at iteration 530 : 0.15572264790534973
Loss at iteration 540 : 0.05753503367304802
Loss at iteration 550 : 0.10500690340995789
Loss at iteration 560 : 0.07352814823389053
Loss at iteration 570 : 0.05373702570796013
Loss at iteration 580 : 0.043983906507492065
Loss at iteration 590 : 0.06266234815120697
Loss at iteration 600 : 0.11651428043842316
Loss at iteration 610 : 0.09053849428892136
Loss at iteration 620 : 0.05928092449903488
Loss at iteration 630 : 0.09226544946432114
Loss at iteration 640 : 0.11572335660457611
Loss at iteration 650 : 0.07825037837028503
Loss at iteration 660 : 0.0536353699862957
Loss at iteration 670 : 0.07840383052825928
Loss at iteration 680 : 0.04552135989069939
Loss at iteration 690 : 0.07507209479808807
Loss at iteration 700 : 0.08759268373250961
Loss at iteration 710 : 0.13416022062301636
Loss at iteration 720 : 0.061601053923368454
Loss at iteration 730 : 0.0592692606151104
Loss at iteration 740 : 0.0960412472486496
Loss at iteration 750 : 0.08317090570926666
Loss at iteration 760 : 0.07733329385519028
Loss at iteration 770 : 0.06692138314247131
Loss at iteration 780 : 0.08392657339572906
Loss at iteration 790 : 0.04473778232932091
Loss at iteration 800 : 0.05985212326049805
Loss at iteration 810 : 0.06881218403577805
Loss at iteration 820 : 0.10156723111867905
Loss at iteration 830 : 0.06693392246961594
Loss at iteration 840 : 0.13370080292224884
Loss at iteration 850 : 0.08129533380270004
Loss at iteration 860 : 0.1012939065694809
Loss at iteration 870 : 0.054798971861600876
Loss at iteration 880 : 0.06445921957492828
Loss at iteration 890 : 0.1064339280128479
Loss at iteration 900 : 0.12055359780788422
Loss at iteration 910 : 0.09896735846996307
Loss at iteration 920 : 0.12065911293029785
Loss at iteration 930 : 0.08563487976789474
Loss at iteration 940 : 0.06839026510715485
Loss at iteration 950 : 0.13006888329982758
Loss at iteration 960 : 0.05401807278394699
Loss at iteration 970 : 0.07839594036340714
Loss at iteration 980 : 0.09776812046766281
Loss at iteration 990 : 0.06286606937646866
Loss at iteration 1000 : 0.0751187726855278
Loss at iteration 1010 : 0.09278550744056702
Loss at iteration 1020 : 0.0726342648267746
Loss at iteration 1030 : 0.06636376678943634
Loss at iteration 1040 : 0.09433013200759888
Loss at iteration 1050 : 0.11229857057332993
Loss at iteration 1060 : 0.09158957004547119
Loss at iteration 1070 : 0.07730330526828766
Loss at iteration 1080 : 0.08005876839160919
Loss at iteration 1090 : 0.1190502867102623
Loss at iteration 1100 : 0.15071356296539307
Loss at iteration 1110 : 0.0936439260840416
Loss at iteration 1120 : 0.056328170001506805
Loss at iteration 1130 : 0.0827236995100975
Loss at iteration 1140 : 0.1189315915107727
Loss at iteration 1150 : 0.07071201503276825
Loss at iteration 1160 : 0.10514571517705917
Loss at iteration 1170 : 0.08002181351184845
Loss at iteration 1180 : 0.09464626014232635
Loss at iteration 1190 : 0.09649713337421417
Loss at iteration 1200 : 0.06654248386621475
Loss at iteration 1210 : 0.09708484262228012
The SSIM Value is: 0.7101555903752644
The PSNR Value is: 20.911198552449545
the epoch is: 106
Loss at iteration 10 : 0.09197431057691574
Loss at iteration 20 : 0.04788763448596001
Loss at iteration 30 : 0.12275367230176926
Loss at iteration 40 : 0.0782383605837822
Loss at iteration 50 : 0.0427640825510025
Loss at iteration 60 : 0.04947309195995331
Loss at iteration 70 : 0.09007858484983444
Loss at iteration 80 : 0.1373669058084488
Loss at iteration 90 : 0.10558028519153595
Loss at iteration 100 : 0.07830789685249329
Loss at iteration 110 : 0.06613989174365997
Loss at iteration 120 : 0.09159957617521286
Loss at iteration 130 : 0.08908647298812866
Loss at iteration 140 : 0.08460208028554916
Loss at iteration 150 : 0.11426502466201782
Loss at iteration 160 : 0.1341085284948349
Loss at iteration 170 : 0.09519360959529877
Loss at iteration 180 : 0.09547547996044159
Loss at iteration 190 : 0.09255360066890717
Loss at iteration 200 : 0.0831383615732193
Loss at iteration 210 : 0.08400317281484604
Loss at iteration 220 : 0.09907214343547821
Loss at iteration 230 : 0.09698177874088287
Loss at iteration 240 : 0.07545454800128937
Loss at iteration 250 : 0.13896164298057556
Loss at iteration 260 : 0.09053249657154083
Loss at iteration 270 : 0.09036646783351898
Loss at iteration 280 : 0.04816010594367981
Loss at iteration 290 : 0.06555571407079697
Loss at iteration 300 : 0.090511754155159
Loss at iteration 310 : 0.1345648169517517
Loss at iteration 320 : 0.08459451049566269
Loss at iteration 330 : 0.07169859856367111
Loss at iteration 340 : 0.09039634466171265
Loss at iteration 350 : 0.09192407131195068
Loss at iteration 360 : 0.1108686551451683
Loss at iteration 370 : 0.07581479847431183
Loss at iteration 380 : 0.15048278868198395
Loss at iteration 390 : 0.08177267760038376
Loss at iteration 400 : 0.07860305905342102
Loss at iteration 410 : 0.09193738549947739
Loss at iteration 420 : 0.08015580475330353
Loss at iteration 430 : 0.08860073983669281
Loss at iteration 440 : 0.08545349538326263
Loss at iteration 450 : 0.05892624706029892
Loss at iteration 460 : 0.05133315920829773
Loss at iteration 470 : 0.09381543844938278
Loss at iteration 480 : 0.09764500707387924
Loss at iteration 490 : 0.12000185251235962
Loss at iteration 500 : 0.08813254535198212
Loss at iteration 510 : 0.09723933041095734
Loss at iteration 520 : 0.07764419913291931
Loss at iteration 530 : 0.07159863412380219
Loss at iteration 540 : 0.1243177056312561
Loss at iteration 550 : 0.0661717876791954
Loss at iteration 560 : 0.05441826581954956
Loss at iteration 570 : 0.08953140676021576
Loss at iteration 580 : 0.1078345775604248
Loss at iteration 590 : 0.11222025007009506
Loss at iteration 600 : 0.08129637688398361
Loss at iteration 610 : 0.08097486197948456
Loss at iteration 620 : 0.10276226699352264
Loss at iteration 630 : 0.07024913281202316
Loss at iteration 640 : 0.08204293996095657
Loss at iteration 650 : 0.11218052357435226
Loss at iteration 660 : 0.06061498075723648
Loss at iteration 670 : 0.05813407897949219
Loss at iteration 680 : 0.049342237412929535
Loss at iteration 690 : 0.10016243159770966
Loss at iteration 700 : 0.09211665391921997
Loss at iteration 710 : 0.15903861820697784
Loss at iteration 720 : 0.0813625156879425
Loss at iteration 730 : 0.09503334760665894
Loss at iteration 740 : 0.04984138533473015
Loss at iteration 750 : 0.10366153717041016
Loss at iteration 760 : 0.12350618839263916
Loss at iteration 770 : 0.11090069264173508
Loss at iteration 780 : 0.08694775402545929
Loss at iteration 790 : 0.08635152876377106
Loss at iteration 800 : 0.10708516091108322
Loss at iteration 810 : 0.11151643097400665
Loss at iteration 820 : 0.10594174265861511
Loss at iteration 830 : 0.06731004267930984
Loss at iteration 840 : 0.1434711515903473
Loss at iteration 850 : 0.1074393093585968
Loss at iteration 860 : 0.088627889752388
Loss at iteration 870 : 0.08934074640274048
Loss at iteration 880 : 0.0754411369562149
Loss at iteration 890 : 0.06695285439491272
Loss at iteration 900 : 0.08120264112949371
Loss at iteration 910 : 0.0977921411395073
Loss at iteration 920 : 0.15268731117248535
Loss at iteration 930 : 0.055709850043058395
Loss at iteration 940 : 0.060285620391368866
Loss at iteration 950 : 0.05450966954231262
Loss at iteration 960 : 0.09677653759717941
Loss at iteration 970 : 0.10793212056159973
Loss at iteration 980 : 0.07934406399726868
Loss at iteration 990 : 0.08064053952693939
Loss at iteration 1000 : 0.1518518030643463
Loss at iteration 1010 : 0.03813538700342178
Loss at iteration 1020 : 0.08143269270658493
Loss at iteration 1030 : 0.06346887350082397
Loss at iteration 1040 : 0.04869741201400757
Loss at iteration 1050 : 0.08677667379379272
Loss at iteration 1060 : 0.0864298865199089
Loss at iteration 1070 : 0.08921166509389877
Loss at iteration 1080 : 0.06787586212158203
Loss at iteration 1090 : 0.05964168906211853
Loss at iteration 1100 : 0.06647684425115585
Loss at iteration 1110 : 0.06067940592765808
Loss at iteration 1120 : 0.0685916543006897
Loss at iteration 1130 : 0.08847449719905853
Loss at iteration 1140 : 0.07388786971569061
Loss at iteration 1150 : 0.06242799013853073
Loss at iteration 1160 : 0.05175215005874634
Loss at iteration 1170 : 0.0722784548997879
Loss at iteration 1180 : 0.05666917562484741
Loss at iteration 1190 : 0.08078344911336899
Loss at iteration 1200 : 0.04143848270177841
Loss at iteration 1210 : 0.12823429703712463
The SSIM Value is: 0.7070016960302988
The PSNR Value is: 20.45260264078776
the epoch is: 107
Loss at iteration 10 : 0.0618133544921875
Loss at iteration 20 : 0.0691002830862999
Loss at iteration 30 : 0.04973018169403076
Loss at iteration 40 : 0.051767557859420776
Loss at iteration 50 : 0.10702782869338989
Loss at iteration 60 : 0.08256553113460541
Loss at iteration 70 : 0.07408680766820908
Loss at iteration 80 : 0.06517016887664795
Loss at iteration 90 : 0.055704694241285324
Loss at iteration 100 : 0.07877647876739502
Loss at iteration 110 : 0.12883660197257996
Loss at iteration 120 : 0.07046283781528473
Loss at iteration 130 : 0.08866497874259949
Loss at iteration 140 : 0.08508813381195068
Loss at iteration 150 : 0.12283935397863388
Loss at iteration 160 : 0.06759682297706604
Loss at iteration 170 : 0.09086118638515472
Loss at iteration 180 : 0.09034723043441772
Loss at iteration 190 : 0.10844181478023529
Loss at iteration 200 : 0.06035103648900986
Loss at iteration 210 : 0.07497460395097733
Loss at iteration 220 : 0.06421855837106705
Loss at iteration 230 : 0.0985969752073288
Loss at iteration 240 : 0.08676856756210327
Loss at iteration 250 : 0.13846057653427124
Loss at iteration 260 : 0.09296192228794098
Loss at iteration 270 : 0.0664261132478714
Loss at iteration 280 : 0.10595288872718811
Loss at iteration 290 : 0.10249810665845871
Loss at iteration 300 : 0.06868820637464523
Loss at iteration 310 : 0.17004691064357758
Loss at iteration 320 : 0.13488316535949707
Loss at iteration 330 : 0.07286469638347626
Loss at iteration 340 : 0.07322210818529129
Loss at iteration 350 : 0.0710277408361435
Loss at iteration 360 : 0.06180514022707939
Loss at iteration 370 : 0.09816309064626694
Loss at iteration 380 : 0.06394477188587189
Loss at iteration 390 : 0.06442810595035553
Loss at iteration 400 : 0.08926086127758026
Loss at iteration 410 : 0.0815252885222435
Loss at iteration 420 : 0.06690078973770142
Loss at iteration 430 : 0.11492203176021576
Loss at iteration 440 : 0.08281854540109634
Loss at iteration 450 : 0.10154864192008972
Loss at iteration 460 : 0.09300369024276733
Loss at iteration 470 : 0.07131022214889526
Loss at iteration 480 : 0.08610890805721283
Loss at iteration 490 : 0.12335950136184692
Loss at iteration 500 : 0.11294591426849365
Loss at iteration 510 : 0.09505552053451538
Loss at iteration 520 : 0.07806840538978577
Loss at iteration 530 : 0.12382213771343231
Loss at iteration 540 : 0.06862799823284149
Loss at iteration 550 : 0.07940410077571869
Loss at iteration 560 : 0.07309328764677048
Loss at iteration 570 : 0.06701309978961945
Loss at iteration 580 : 0.06319454312324524
Loss at iteration 590 : 0.08770152181386948
Loss at iteration 600 : 0.06292262673377991
Loss at iteration 610 : 0.04635787010192871
Loss at iteration 620 : 0.09320243448019028
Loss at iteration 630 : 0.08745117485523224
Loss at iteration 640 : 0.09362848103046417
Loss at iteration 650 : 0.08370085060596466
Loss at iteration 660 : 0.15677309036254883
Loss at iteration 670 : 0.09826408326625824
Loss at iteration 680 : 0.07911989837884903
Loss at iteration 690 : 0.09865584969520569
Loss at iteration 700 : 0.10764016956090927
Loss at iteration 710 : 0.058941684663295746
Loss at iteration 720 : 0.10394557565450668
Loss at iteration 730 : 0.08030880242586136
Loss at iteration 740 : 0.09371509402990341
Loss at iteration 750 : 0.053260624408721924
Loss at iteration 760 : 0.07460248470306396
Loss at iteration 770 : 0.07652419805526733
Loss at iteration 780 : 0.0713515505194664
Loss at iteration 790 : 0.054243218153715134
Loss at iteration 800 : 0.06871584057807922
Loss at iteration 810 : 0.0455264076590538
Loss at iteration 820 : 0.06142289936542511
Loss at iteration 830 : 0.10038916766643524
Loss at iteration 840 : 0.1241450160741806
Loss at iteration 850 : 0.10177920758724213
Loss at iteration 860 : 0.05689432844519615
Loss at iteration 870 : 0.0825607031583786
Loss at iteration 880 : 0.07674935460090637
Loss at iteration 890 : 0.07994311302900314
Loss at iteration 900 : 0.06858335435390472
Loss at iteration 910 : 0.05672292038798332
Loss at iteration 920 : 0.10433284938335419
Loss at iteration 930 : 0.06405496597290039
Loss at iteration 940 : 0.07349639385938644
Loss at iteration 950 : 0.06515771150588989
Loss at iteration 960 : 0.06141895055770874
Loss at iteration 970 : 0.08663102984428406
Loss at iteration 980 : 0.10207053273916245
Loss at iteration 990 : 0.13051755726337433
Loss at iteration 1000 : 0.10880263149738312
Loss at iteration 1010 : 0.09763851016759872
Loss at iteration 1020 : 0.06183648109436035
Loss at iteration 1030 : 0.10592617094516754
Loss at iteration 1040 : 0.0778070017695427
Loss at iteration 1050 : 0.08899393677711487
Loss at iteration 1060 : 0.1502775400876999
Loss at iteration 1070 : 0.08043903857469559
Loss at iteration 1080 : 0.09653340280056
Loss at iteration 1090 : 0.06767688691616058
Loss at iteration 1100 : 0.08758905529975891
Loss at iteration 1110 : 0.1083763837814331
Loss at iteration 1120 : 0.10255732387304306
Loss at iteration 1130 : 0.0982908234000206
Loss at iteration 1140 : 0.11355406045913696
Loss at iteration 1150 : 0.06979084014892578
Loss at iteration 1160 : 0.1241534948348999
Loss at iteration 1170 : 0.1043347418308258
Loss at iteration 1180 : 0.05495908111333847
Loss at iteration 1190 : 0.060816001147031784
Loss at iteration 1200 : 0.07329346239566803
Loss at iteration 1210 : 0.08419430255889893
The SSIM Value is: 0.7086019873619079
The PSNR Value is: 20.706943384806316
the epoch is: 108
Loss at iteration 10 : 0.05518236756324768
Loss at iteration 20 : 0.05219646543264389
Loss at iteration 30 : 0.0535089373588562
Loss at iteration 40 : 0.0908055230975151
Loss at iteration 50 : 0.08304336667060852
Loss at iteration 60 : 0.06476940214633942
Loss at iteration 70 : 0.0883961021900177
Loss at iteration 80 : 0.06383989751338959
Loss at iteration 90 : 0.1161450445652008
Loss at iteration 100 : 0.03621743246912956
Loss at iteration 110 : 0.07985233515501022
Loss at iteration 120 : 0.07708695530891418
Loss at iteration 130 : 0.11661456525325775
Loss at iteration 140 : 0.09346933662891388
Loss at iteration 150 : 0.0771787017583847
Loss at iteration 160 : 0.06640031188726425
Loss at iteration 170 : 0.10282817482948303
Loss at iteration 180 : 0.07370692491531372
Loss at iteration 190 : 0.09606808423995972
Loss at iteration 200 : 0.08172166347503662
Loss at iteration 210 : 0.14577855169773102
Loss at iteration 220 : 0.047534190118312836
Loss at iteration 230 : 0.07387112826108932
Loss at iteration 240 : 0.09485416859388351
Loss at iteration 250 : 0.06597694009542465
Loss at iteration 260 : 0.07599589973688126
Loss at iteration 270 : 0.1309567391872406
Loss at iteration 280 : 0.13084542751312256
Loss at iteration 290 : 0.12097762525081635
Loss at iteration 300 : 0.08507323265075684
Loss at iteration 310 : 0.11119058728218079
Loss at iteration 320 : 0.06292688101530075
Loss at iteration 330 : 0.07956825196743011
Loss at iteration 340 : 0.09969892352819443
Loss at iteration 350 : 0.05887323617935181
Loss at iteration 360 : 0.12186642736196518
Loss at iteration 370 : 0.07715030014514923
Loss at iteration 380 : 0.09877687692642212
Loss at iteration 390 : 0.10906656086444855
Loss at iteration 400 : 0.10852228105068207
Loss at iteration 410 : 0.08193199336528778
Loss at iteration 420 : 0.06047878414392471
Loss at iteration 430 : 0.041579924523830414
Loss at iteration 440 : 0.07277389615774155
Loss at iteration 450 : 0.09910540282726288
Loss at iteration 460 : 0.08894132077693939
Loss at iteration 470 : 0.1129496619105339
Loss at iteration 480 : 0.15911224484443665
Loss at iteration 490 : 0.07356976717710495
Loss at iteration 500 : 0.07605026662349701
Loss at iteration 510 : 0.07270680367946625
Loss at iteration 520 : 0.0934607982635498
Loss at iteration 530 : 0.05315002426505089
Loss at iteration 540 : 0.08261176943778992
Loss at iteration 550 : 0.11735649406909943
Loss at iteration 560 : 0.096478670835495
Loss at iteration 570 : 0.06658969819545746
Loss at iteration 580 : 0.09622826427221298
Loss at iteration 590 : 0.09446384757757187
Loss at iteration 600 : 0.08235566318035126
Loss at iteration 610 : 0.1023569107055664
Loss at iteration 620 : 0.11502623558044434
Loss at iteration 630 : 0.07901200652122498
Loss at iteration 640 : 0.061849795281887054
Loss at iteration 650 : 0.10543414950370789
Loss at iteration 660 : 0.079539954662323
Loss at iteration 670 : 0.09594686329364777
Loss at iteration 680 : 0.05899471044540405
Loss at iteration 690 : 0.10081924498081207
Loss at iteration 700 : 0.09673788398504257
Loss at iteration 710 : 0.07702609896659851
Loss at iteration 720 : 0.07640565931797028
Loss at iteration 730 : 0.081337571144104
Loss at iteration 740 : 0.11251004785299301
Loss at iteration 750 : 0.0883328914642334
Loss at iteration 760 : 0.10932543128728867
Loss at iteration 770 : 0.08109420537948608
Loss at iteration 780 : 0.07232837378978729
Loss at iteration 790 : 0.08723603188991547
Loss at iteration 800 : 0.1038355827331543
Loss at iteration 810 : 0.05179369077086449
Loss at iteration 820 : 0.08411119878292084
Loss at iteration 830 : 0.07387251406908035
Loss at iteration 840 : 0.09993880242109299
Loss at iteration 850 : 0.07230371236801147
Loss at iteration 860 : 0.09104561805725098
Loss at iteration 870 : 0.07985848188400269
Loss at iteration 880 : 0.1440081000328064
Loss at iteration 890 : 0.09187182039022446
Loss at iteration 900 : 0.06649947166442871
Loss at iteration 910 : 0.08738019317388535
Loss at iteration 920 : 0.09097995609045029
Loss at iteration 930 : 0.06426216661930084
Loss at iteration 940 : 0.07824894040822983
Loss at iteration 950 : 0.07908976823091507
Loss at iteration 960 : 0.08336646854877472
Loss at iteration 970 : 0.0950227826833725
Loss at iteration 980 : 0.11677910387516022
Loss at iteration 990 : 0.08816054463386536
Loss at iteration 1000 : 0.06452047824859619
Loss at iteration 1010 : 0.09571988880634308
Loss at iteration 1020 : 0.11485080420970917
Loss at iteration 1030 : 0.06908759474754333
Loss at iteration 1040 : 0.06039699539542198
Loss at iteration 1050 : 0.10375502705574036
Loss at iteration 1060 : 0.05019139498472214
Loss at iteration 1070 : 0.06461136043071747
Loss at iteration 1080 : 0.07603224366903305
Loss at iteration 1090 : 0.10665790736675262
Loss at iteration 1100 : 0.09834285825490952
Loss at iteration 1110 : 0.07732465863227844
Loss at iteration 1120 : 0.052106112241744995
Loss at iteration 1130 : 0.06957997381687164
Loss at iteration 1140 : 0.10014566034078598
Loss at iteration 1150 : 0.1066703200340271
Loss at iteration 1160 : 0.08507757633924484
Loss at iteration 1170 : 0.060028862208127975
Loss at iteration 1180 : 0.06685800850391388
Loss at iteration 1190 : 0.0912824273109436
Loss at iteration 1200 : 0.08804599940776825
Loss at iteration 1210 : 0.09976231306791306
The SSIM Value is: 0.7140779972076416
The PSNR Value is: 21.34015630086263
the epoch is: 109
Loss at iteration 10 : 0.0787576213479042
Loss at iteration 20 : 0.09271468222141266
Loss at iteration 30 : 0.06495368480682373
Loss at iteration 40 : 0.0422896072268486
Loss at iteration 50 : 0.07485552132129669
Loss at iteration 60 : 0.09896822273731232
Loss at iteration 70 : 0.14717388153076172
Loss at iteration 80 : 0.06810936331748962
Loss at iteration 90 : 0.05979067087173462
Loss at iteration 100 : 0.07860152423381805
Loss at iteration 110 : 0.0867244303226471
Loss at iteration 120 : 0.10127876698970795
Loss at iteration 130 : 0.1123083233833313
Loss at iteration 140 : 0.07651746273040771
Loss at iteration 150 : 0.11625806987285614
Loss at iteration 160 : 0.08554097265005112
Loss at iteration 170 : 0.04392659664154053
Loss at iteration 180 : 0.08738674968481064
Loss at iteration 190 : 0.06627042591571808
Loss at iteration 200 : 0.08401533216238022
Loss at iteration 210 : 0.050897326320409775
Loss at iteration 220 : 0.12452399730682373
Loss at iteration 230 : 0.06648284196853638
Loss at iteration 240 : 0.04196865111589432
Loss at iteration 250 : 0.048611924052238464
Loss at iteration 260 : 0.09182833135128021
Loss at iteration 270 : 0.051640089601278305
Loss at iteration 280 : 0.08345099538564682
Loss at iteration 290 : 0.06851227581501007
Loss at iteration 300 : 0.10397571325302124
Loss at iteration 310 : 0.042606767266988754
Loss at iteration 320 : 0.05914900824427605
Loss at iteration 330 : 0.05759499967098236
Loss at iteration 340 : 0.06718786805868149
Loss at iteration 350 : 0.08241095393896103
Loss at iteration 360 : 0.06784224510192871
Loss at iteration 370 : 0.0629400759935379
Loss at iteration 380 : 0.06945101916790009
Loss at iteration 390 : 0.054212912917137146
Loss at iteration 400 : 0.10437276214361191
Loss at iteration 410 : 0.07229892909526825
Loss at iteration 420 : 0.05784659832715988
Loss at iteration 430 : 0.08241119980812073
Loss at iteration 440 : 0.04479869455099106
Loss at iteration 450 : 0.0973736047744751
Loss at iteration 460 : 0.10312153398990631
Loss at iteration 470 : 0.06409484148025513
Loss at iteration 480 : 0.0964285135269165
Loss at iteration 490 : 0.05061127245426178
Loss at iteration 500 : 0.05916466563940048
Loss at iteration 510 : 0.10032647848129272
Loss at iteration 520 : 0.0876927599310875
Loss at iteration 530 : 0.07975541800260544
Loss at iteration 540 : 0.11255770176649094
Loss at iteration 550 : 0.06382879614830017
Loss at iteration 560 : 0.06044044718146324
Loss at iteration 570 : 0.05849601328372955
Loss at iteration 580 : 0.11840033531188965
Loss at iteration 590 : 0.07311899960041046
Loss at iteration 600 : 0.12480641901493073
Loss at iteration 610 : 0.09848828613758087
Loss at iteration 620 : 0.09989288449287415
Loss at iteration 630 : 0.054041922092437744
Loss at iteration 640 : 0.06652534008026123
Loss at iteration 650 : 0.05252279341220856
Loss at iteration 660 : 0.12542784214019775
Loss at iteration 670 : 0.10402830690145493
Loss at iteration 680 : 0.10250496119260788
Loss at iteration 690 : 0.12661579251289368
Loss at iteration 700 : 0.06890729069709778
Loss at iteration 710 : 0.08110711723566055
Loss at iteration 720 : 0.1523168683052063
Loss at iteration 730 : 0.04853273555636406
Loss at iteration 740 : 0.12891298532485962
Loss at iteration 750 : 0.13624155521392822
Loss at iteration 760 : 0.1059793084859848
Loss at iteration 770 : 0.08610370010137558
Loss at iteration 780 : 0.05736093968153
Loss at iteration 790 : 0.08064742386341095
Loss at iteration 800 : 0.04802681505680084
Loss at iteration 810 : 0.08786066621541977
Loss at iteration 820 : 0.10157522559165955
Loss at iteration 830 : 0.058840446174144745
Loss at iteration 840 : 0.11150592565536499
Loss at iteration 850 : 0.07426517456769943
Loss at iteration 860 : 0.08851945400238037
Loss at iteration 870 : 0.07017320394515991
Loss at iteration 880 : 0.08726325631141663
Loss at iteration 890 : 0.1063610315322876
Loss at iteration 900 : 0.08214932680130005
Loss at iteration 910 : 0.0797562301158905
Loss at iteration 920 : 0.06093941256403923
Loss at iteration 930 : 0.07568052411079407
Loss at iteration 940 : 0.09840118885040283
Loss at iteration 950 : 0.0841493308544159
Loss at iteration 960 : 0.057954609394073486
Loss at iteration 970 : 0.07907208800315857
Loss at iteration 980 : 0.07431771606206894
Loss at iteration 990 : 0.07084842026233673
Loss at iteration 1000 : 0.07323676347732544
Loss at iteration 1010 : 0.10579576343297958
Loss at iteration 1020 : 0.061966054141521454
Loss at iteration 1030 : 0.11002953350543976
Loss at iteration 1040 : 0.05864943563938141
Loss at iteration 1050 : 0.10212593525648117
Loss at iteration 1060 : 0.14528828859329224
Loss at iteration 1070 : 0.07089923322200775
Loss at iteration 1080 : 0.1209026575088501
Loss at iteration 1090 : 0.0947834849357605
Loss at iteration 1100 : 0.07278299331665039
Loss at iteration 1110 : 0.06068764626979828
Loss at iteration 1120 : 0.07373689115047455
Loss at iteration 1130 : 0.10648443549871445
Loss at iteration 1140 : 0.0689043402671814
Loss at iteration 1150 : 0.07709836214780807
Loss at iteration 1160 : 0.09086217731237411
Loss at iteration 1170 : 0.12994307279586792
Loss at iteration 1180 : 0.08766239881515503
Loss at iteration 1190 : 0.0780591368675232
Loss at iteration 1200 : 0.05565681681036949
Loss at iteration 1210 : 0.07924558222293854
The SSIM Value is: 0.7113034009933472
The PSNR Value is: 20.847793579101562
the epoch is: 110
Loss at iteration 10 : 0.12226574122905731
Loss at iteration 20 : 0.08404695987701416
Loss at iteration 30 : 0.08683903515338898
Loss at iteration 40 : 0.06275606900453568
Loss at iteration 50 : 0.10714048147201538
Loss at iteration 60 : 0.07506780326366425
Loss at iteration 70 : 0.05379336327314377
Loss at iteration 80 : 0.08304034173488617
Loss at iteration 90 : 0.05362866446375847
Loss at iteration 100 : 0.047111134976148605
Loss at iteration 110 : 0.04374142736196518
Loss at iteration 120 : 0.059803348034620285
Loss at iteration 130 : 0.07933318614959717
Loss at iteration 140 : 0.05557098612189293
Loss at iteration 150 : 0.0788964331150055
Loss at iteration 160 : 0.1354072391986847
Loss at iteration 170 : 0.09814734756946564
Loss at iteration 180 : 0.08651238679885864
Loss at iteration 190 : 0.0891202837228775
Loss at iteration 200 : 0.0711761862039566
Loss at iteration 210 : 0.09663303941488266
Loss at iteration 220 : 0.08643227070569992
Loss at iteration 230 : 0.08115848898887634
Loss at iteration 240 : 0.056246399879455566
Loss at iteration 250 : 0.05943852663040161
Loss at iteration 260 : 0.08266224712133408
Loss at iteration 270 : 0.07281558215618134
Loss at iteration 280 : 0.08057674765586853
Loss at iteration 290 : 0.07939350605010986
Loss at iteration 300 : 0.04637819528579712
Loss at iteration 310 : 0.08138911426067352
Loss at iteration 320 : 0.08116565644741058
Loss at iteration 330 : 0.1028352677822113
Loss at iteration 340 : 0.13991670310497284
Loss at iteration 350 : 0.1129806861281395
Loss at iteration 360 : 0.08293546736240387
Loss at iteration 370 : 0.0643584355711937
Loss at iteration 380 : 0.09258449077606201
Loss at iteration 390 : 0.09581556171178818
Loss at iteration 400 : 0.08432531356811523
Loss at iteration 410 : 0.05742994695901871
Loss at iteration 420 : 0.09092068672180176
Loss at iteration 430 : 0.08608551323413849
Loss at iteration 440 : 0.06960444152355194
Loss at iteration 450 : 0.09400950372219086
Loss at iteration 460 : 0.12717148661613464
Loss at iteration 470 : 0.10732676088809967
Loss at iteration 480 : 0.08768664300441742
Loss at iteration 490 : 0.059620581567287445
Loss at iteration 500 : 0.0862177312374115
Loss at iteration 510 : 0.1012735366821289
Loss at iteration 520 : 0.09142957627773285
Loss at iteration 530 : 0.06036832183599472
Loss at iteration 540 : 0.09530576318502426
Loss at iteration 550 : 0.06811127066612244
Loss at iteration 560 : 0.08662645518779755
Loss at iteration 570 : 0.07914545387029648
Loss at iteration 580 : 0.07947853952646255
Loss at iteration 590 : 0.065326988697052
Loss at iteration 600 : 0.05955451354384422
Loss at iteration 610 : 0.10931126773357391
Loss at iteration 620 : 0.09187041968107224
Loss at iteration 630 : 0.06046856567263603
Loss at iteration 640 : 0.11204864829778671
Loss at iteration 650 : 0.08797454833984375
Loss at iteration 660 : 0.07160671055316925
Loss at iteration 670 : 0.07955890148878098
Loss at iteration 680 : 0.06995030492544174
Loss at iteration 690 : 0.08232329040765762
Loss at iteration 700 : 0.061382681131362915
Loss at iteration 710 : 0.10001926124095917
Loss at iteration 720 : 0.14010588824748993
Loss at iteration 730 : 0.06135860085487366
Loss at iteration 740 : 0.10782577842473984
Loss at iteration 750 : 0.07550117373466492
Loss at iteration 760 : 0.0973140224814415
Loss at iteration 770 : 0.09662480652332306
Loss at iteration 780 : 0.08396953344345093
Loss at iteration 790 : 0.1116916760802269
Loss at iteration 800 : 0.1263047158718109
Loss at iteration 810 : 0.06155167147517204
Loss at iteration 820 : 0.14738215506076813
Loss at iteration 830 : 0.10219219326972961
Loss at iteration 840 : 0.04150744527578354
Loss at iteration 850 : 0.11741908639669418
Loss at iteration 860 : 0.04366666078567505
Loss at iteration 870 : 0.0842168927192688
Loss at iteration 880 : 0.0579586885869503
Loss at iteration 890 : 0.06008194014430046
Loss at iteration 900 : 0.10746388882398605
Loss at iteration 910 : 0.07252316176891327
Loss at iteration 920 : 0.08722488582134247
Loss at iteration 930 : 0.10642220824956894
Loss at iteration 940 : 0.06167950481176376
Loss at iteration 950 : 0.13834477961063385
Loss at iteration 960 : 0.08453810214996338
Loss at iteration 970 : 0.1392994523048401
Loss at iteration 980 : 0.08758544921875
Loss at iteration 990 : 0.03447052091360092
Loss at iteration 1000 : 0.107685886323452
Loss at iteration 1010 : 0.09575651586055756
Loss at iteration 1020 : 0.07067005336284637
Loss at iteration 1030 : 0.10545159876346588
Loss at iteration 1040 : 0.08041851222515106
Loss at iteration 1050 : 0.0847206562757492
Loss at iteration 1060 : 0.1322886049747467
Loss at iteration 1070 : 0.07580399513244629
Loss at iteration 1080 : 0.07500769942998886
Loss at iteration 1090 : 0.06968140602111816
Loss at iteration 1100 : 0.07135798782110214
Loss at iteration 1110 : 0.1183415874838829
Loss at iteration 1120 : 0.0814770758152008
Loss at iteration 1130 : 0.061860084533691406
Loss at iteration 1140 : 0.10537253320217133
Loss at iteration 1150 : 0.07106395810842514
Loss at iteration 1160 : 0.07566727697849274
Loss at iteration 1170 : 0.04535963386297226
Loss at iteration 1180 : 0.10516919940710068
Loss at iteration 1190 : 0.12105540931224823
Loss at iteration 1200 : 0.11016305536031723
Loss at iteration 1210 : 0.07929199188947678
The SSIM Value is: 0.7080439110596974
The PSNR Value is: 20.811359786987303
the epoch is: 111
Loss at iteration 10 : 0.09453531354665756
Loss at iteration 20 : 0.06205899268388748
Loss at iteration 30 : 0.11365312337875366
Loss at iteration 40 : 0.09241203963756561
Loss at iteration 50 : 0.06983210146427155
Loss at iteration 60 : 0.08908112347126007
Loss at iteration 70 : 0.11640302836894989
Loss at iteration 80 : 0.0986708328127861
Loss at iteration 90 : 0.06355421990156174
Loss at iteration 100 : 0.08943583816289902
Loss at iteration 110 : 0.06375037133693695
Loss at iteration 120 : 0.06301044672727585
Loss at iteration 130 : 0.11307492852210999
Loss at iteration 140 : 0.058877911418676376
Loss at iteration 150 : 0.09121184796094894
Loss at iteration 160 : 0.08048732578754425
Loss at iteration 170 : 0.06326436251401901
Loss at iteration 180 : 0.08732336759567261
Loss at iteration 190 : 0.07273098826408386
Loss at iteration 200 : 0.07509252429008484
Loss at iteration 210 : 0.07118694484233856
Loss at iteration 220 : 0.07003112882375717
Loss at iteration 230 : 0.12044557183980942
Loss at iteration 240 : 0.08260927349328995
Loss at iteration 250 : 0.12039785087108612
Loss at iteration 260 : 0.0902634859085083
Loss at iteration 270 : 0.07510500401258469
Loss at iteration 280 : 0.07552126049995422
Loss at iteration 290 : 0.08251051604747772
Loss at iteration 300 : 0.0522528700530529
Loss at iteration 310 : 0.08128030598163605
Loss at iteration 320 : 0.057973407208919525
Loss at iteration 330 : 0.05572319030761719
Loss at iteration 340 : 0.09781736135482788
Loss at iteration 350 : 0.08934497833251953
Loss at iteration 360 : 0.1492968648672104
Loss at iteration 370 : 0.0725143700838089
Loss at iteration 380 : 0.05690523982048035
Loss at iteration 390 : 0.1312568187713623
Loss at iteration 400 : 0.06597438454627991
Loss at iteration 410 : 0.06655408442020416
Loss at iteration 420 : 0.10254824161529541
Loss at iteration 430 : 0.126933291554451
Loss at iteration 440 : 0.06925138831138611
Loss at iteration 450 : 0.06943900138139725
Loss at iteration 460 : 0.07077786326408386
Loss at iteration 470 : 0.10647502541542053
Loss at iteration 480 : 0.06389010697603226
Loss at iteration 490 : 0.07683941721916199
Loss at iteration 500 : 0.09217703342437744
Loss at iteration 510 : 0.05513026565313339
Loss at iteration 520 : 0.0713052749633789
Loss at iteration 530 : 0.10123497247695923
Loss at iteration 540 : 0.0821666345000267
Loss at iteration 550 : 0.06861484050750732
Loss at iteration 560 : 0.06171557307243347
Loss at iteration 570 : 0.06992217153310776
Loss at iteration 580 : 0.07991282641887665
Loss at iteration 590 : 0.098882757127285
Loss at iteration 600 : 0.06136849522590637
Loss at iteration 610 : 0.12392322719097137
Loss at iteration 620 : 0.09566164016723633
Loss at iteration 630 : 0.07597146928310394
Loss at iteration 640 : 0.05502372980117798
Loss at iteration 650 : 0.10712799429893494
Loss at iteration 660 : 0.07568804919719696
Loss at iteration 670 : 0.06606556475162506
Loss at iteration 680 : 0.037455711513757706
Loss at iteration 690 : 0.07642313092947006
Loss at iteration 700 : 0.07169249653816223
Loss at iteration 710 : 0.07463344931602478
Loss at iteration 720 : 0.08816251158714294
Loss at iteration 730 : 0.10182623565196991
Loss at iteration 740 : 0.07961825281381607
Loss at iteration 750 : 0.053839199244976044
Loss at iteration 760 : 0.05248615890741348
Loss at iteration 770 : 0.07258689403533936
Loss at iteration 780 : 0.09248687326908112
Loss at iteration 790 : 0.14198148250579834
Loss at iteration 800 : 0.06734783947467804
Loss at iteration 810 : 0.08783497661352158
Loss at iteration 820 : 0.0581885501742363
Loss at iteration 830 : 0.06650286912918091
Loss at iteration 840 : 0.07554903626441956
Loss at iteration 850 : 0.10149943828582764
Loss at iteration 860 : 0.05422968789935112
Loss at iteration 870 : 0.1111958771944046
Loss at iteration 880 : 0.10328018665313721
Loss at iteration 890 : 0.08708452433347702
Loss at iteration 900 : 0.13372746109962463
Loss at iteration 910 : 0.10577120631933212
Loss at iteration 920 : 0.0831681564450264
Loss at iteration 930 : 0.11224904656410217
Loss at iteration 940 : 0.048920147120952606
Loss at iteration 950 : 0.07259848713874817
Loss at iteration 960 : 0.07429675757884979
Loss at iteration 970 : 0.11981503665447235
Loss at iteration 980 : 0.03697746619582176
Loss at iteration 990 : 0.0779324620962143
Loss at iteration 1000 : 0.0943092554807663
Loss at iteration 1010 : 0.07044555991888046
Loss at iteration 1020 : 0.06279823184013367
Loss at iteration 1030 : 0.09083615988492966
Loss at iteration 1040 : 0.08331957459449768
Loss at iteration 1050 : 0.07691970467567444
Loss at iteration 1060 : 0.07912640273571014
Loss at iteration 1070 : 0.0872029960155487
Loss at iteration 1080 : 0.08190993964672089
Loss at iteration 1090 : 0.07191729545593262
Loss at iteration 1100 : 0.07462117075920105
Loss at iteration 1110 : 0.07649631053209305
Loss at iteration 1120 : 0.12408025562763214
Loss at iteration 1130 : 0.07670161128044128
Loss at iteration 1140 : 0.058350346982479095
Loss at iteration 1150 : 0.08290843665599823
Loss at iteration 1160 : 0.08715064823627472
Loss at iteration 1170 : 0.07176561653614044
Loss at iteration 1180 : 0.08515207469463348
Loss at iteration 1190 : 0.0768623948097229
Loss at iteration 1200 : 0.07335931062698364
Loss at iteration 1210 : 0.07600176334381104
The SSIM Value is: 0.7071820755799612
The PSNR Value is: 20.785934257507325
the epoch is: 112
Loss at iteration 10 : 0.07128522545099258
Loss at iteration 20 : 0.05417720600962639
Loss at iteration 30 : 0.07282472401857376
Loss at iteration 40 : 0.05490102991461754
Loss at iteration 50 : 0.10593120753765106
Loss at iteration 60 : 0.08988624811172485
Loss at iteration 70 : 0.0822582095861435
Loss at iteration 80 : 0.13419489562511444
Loss at iteration 90 : 0.08633629977703094
Loss at iteration 100 : 0.06808241456747055
Loss at iteration 110 : 0.08821996301412582
Loss at iteration 120 : 0.08775316178798676
Loss at iteration 130 : 0.07726481556892395
Loss at iteration 140 : 0.09558752179145813
Loss at iteration 150 : 0.1186966598033905
Loss at iteration 160 : 0.1328454464673996
Loss at iteration 170 : 0.08729653060436249
Loss at iteration 180 : 0.06877940148115158
Loss at iteration 190 : 0.12260928750038147
Loss at iteration 200 : 0.08920413255691528
Loss at iteration 210 : 0.0489993654191494
Loss at iteration 220 : 0.10391071438789368
Loss at iteration 230 : 0.05981389805674553
Loss at iteration 240 : 0.06759970635175705
Loss at iteration 250 : 0.14391958713531494
Loss at iteration 260 : 0.12581902742385864
Loss at iteration 270 : 0.0857638344168663
Loss at iteration 280 : 0.09707964956760406
Loss at iteration 290 : 0.11679121106863022
Loss at iteration 300 : 0.11226508021354675
Loss at iteration 310 : 0.12477662414312363
Loss at iteration 320 : 0.06772118806838989
Loss at iteration 330 : 0.07835426181554794
Loss at iteration 340 : 0.07020825147628784
Loss at iteration 350 : 0.06141000986099243
Loss at iteration 360 : 0.09337779134511948
Loss at iteration 370 : 0.10158436000347137
Loss at iteration 380 : 0.08344732969999313
Loss at iteration 390 : 0.10621075332164764
Loss at iteration 400 : 0.07844798266887665
Loss at iteration 410 : 0.13207241892814636
Loss at iteration 420 : 0.07631012052297592
Loss at iteration 430 : 0.09442772716283798
Loss at iteration 440 : 0.05166744440793991
Loss at iteration 450 : 0.05734863877296448
Loss at iteration 460 : 0.09970289468765259
Loss at iteration 470 : 0.08943551778793335
Loss at iteration 480 : 0.05956018716096878
Loss at iteration 490 : 0.09365697205066681
Loss at iteration 500 : 0.06010837480425835
Loss at iteration 510 : 0.089047372341156
Loss at iteration 520 : 0.060744211077690125
Loss at iteration 530 : 0.0816982090473175
Loss at iteration 540 : 0.07369504868984222
Loss at iteration 550 : 0.1439056247472763
Loss at iteration 560 : 0.09273767471313477
Loss at iteration 570 : 0.10107138007879257
Loss at iteration 580 : 0.09857344627380371
Loss at iteration 590 : 0.06664323061704636
Loss at iteration 600 : 0.060000233352184296
Loss at iteration 610 : 0.07811009883880615
Loss at iteration 620 : 0.07742448896169662
Loss at iteration 630 : 0.04702305048704147
Loss at iteration 640 : 0.10762862861156464
Loss at iteration 650 : 0.08765610307455063
Loss at iteration 660 : 0.14710883796215057
Loss at iteration 670 : 0.08678258955478668
Loss at iteration 680 : 0.05526312440633774
Loss at iteration 690 : 0.10111016035079956
Loss at iteration 700 : 0.06742480397224426
Loss at iteration 710 : 0.08107716590166092
Loss at iteration 720 : 0.07919659465551376
Loss at iteration 730 : 0.09582769870758057
Loss at iteration 740 : 0.056963544338941574
Loss at iteration 750 : 0.07355566322803497
Loss at iteration 760 : 0.09050541371107101
Loss at iteration 770 : 0.09012826532125473
Loss at iteration 780 : 0.09058201313018799
Loss at iteration 790 : 0.062334563583135605
Loss at iteration 800 : 0.06419679522514343
Loss at iteration 810 : 0.056687455624341965
Loss at iteration 820 : 0.07648798823356628
Loss at iteration 830 : 0.08856813609600067
Loss at iteration 840 : 0.07295563817024231
Loss at iteration 850 : 0.08888868987560272
Loss at iteration 860 : 0.11061587929725647
Loss at iteration 870 : 0.06788614392280579
Loss at iteration 880 : 0.0969209223985672
Loss at iteration 890 : 0.0691041424870491
Loss at iteration 900 : 0.13555312156677246
Loss at iteration 910 : 0.07024245709180832
Loss at iteration 920 : 0.09168600291013718
Loss at iteration 930 : 0.05005595088005066
Loss at iteration 940 : 0.08522666990756989
Loss at iteration 950 : 0.06687762588262558
Loss at iteration 960 : 0.12471215426921844
Loss at iteration 970 : 0.15275083482265472
Loss at iteration 980 : 0.10957030951976776
Loss at iteration 990 : 0.09870834648609161
Loss at iteration 1000 : 0.11376138776540756
Loss at iteration 1010 : 0.08942248672246933
Loss at iteration 1020 : 0.08662904798984528
Loss at iteration 1030 : 0.07905836403369904
Loss at iteration 1040 : 0.07197451591491699
Loss at iteration 1050 : 0.07191392779350281
Loss at iteration 1060 : 0.07951246947050095
Loss at iteration 1070 : 0.06404121965169907
Loss at iteration 1080 : 0.07957182824611664
Loss at iteration 1090 : 0.08676891028881073
Loss at iteration 1100 : 0.08873331546783447
Loss at iteration 1110 : 0.07445664703845978
Loss at iteration 1120 : 0.08639787882566452
Loss at iteration 1130 : 0.058881938457489014
Loss at iteration 1140 : 0.11581259965896606
Loss at iteration 1150 : 0.07208313047885895
Loss at iteration 1160 : 0.11321678012609482
Loss at iteration 1170 : 0.08286573737859726
Loss at iteration 1180 : 0.11190681904554367
Loss at iteration 1190 : 0.09263704717159271
Loss at iteration 1200 : 0.1083592027425766
Loss at iteration 1210 : 0.08515938371419907
The SSIM Value is: 0.712023812532425
The PSNR Value is: 21.05574747721354
the epoch is: 113
Loss at iteration 10 : 0.09038042277097702
Loss at iteration 20 : 0.07702141255140305
Loss at iteration 30 : 0.0792936459183693
Loss at iteration 40 : 0.09121763706207275
Loss at iteration 50 : 0.07472379505634308
Loss at iteration 60 : 0.13800020515918732
Loss at iteration 70 : 0.07153825461864471
Loss at iteration 80 : 0.07174570113420486
Loss at iteration 90 : 0.0993073433637619
Loss at iteration 100 : 0.08769330382347107
Loss at iteration 110 : 0.05492996424436569
Loss at iteration 120 : 0.06048068031668663
Loss at iteration 130 : 0.09268834441900253
Loss at iteration 140 : 0.07544531673192978
Loss at iteration 150 : 0.08884676545858383
Loss at iteration 160 : 0.0886581540107727
Loss at iteration 170 : 0.11465849727392197
Loss at iteration 180 : 0.08555041998624802
Loss at iteration 190 : 0.07800109684467316
Loss at iteration 200 : 0.10075709223747253
Loss at iteration 210 : 0.06983242183923721
Loss at iteration 220 : 0.06978169083595276
Loss at iteration 230 : 0.06529004871845245
Loss at iteration 240 : 0.07758622616529465
Loss at iteration 250 : 0.09261336922645569
Loss at iteration 260 : 0.05862938612699509
Loss at iteration 270 : 0.06277333199977875
Loss at iteration 280 : 0.055769916623830795
Loss at iteration 290 : 0.14112895727157593
Loss at iteration 300 : 0.07127655297517776
Loss at iteration 310 : 0.11372705549001694
Loss at iteration 320 : 0.08145032823085785
Loss at iteration 330 : 0.10918887704610825
Loss at iteration 340 : 0.10016509145498276
Loss at iteration 350 : 0.11348547786474228
Loss at iteration 360 : 0.0807153508067131
Loss at iteration 370 : 0.08786105364561081
Loss at iteration 380 : 0.08198843896389008
Loss at iteration 390 : 0.07567243278026581
Loss at iteration 400 : 0.09553884714841843
Loss at iteration 410 : 0.07697004079818726
Loss at iteration 420 : 0.05423706769943237
Loss at iteration 430 : 0.07073673605918884
Loss at iteration 440 : 0.08450055122375488
Loss at iteration 450 : 0.07161484658718109
Loss at iteration 460 : 0.05519614741206169
Loss at iteration 470 : 0.08170297741889954
Loss at iteration 480 : 0.0898902639746666
Loss at iteration 490 : 0.07234766334295273
Loss at iteration 500 : 0.053333502262830734
Loss at iteration 510 : 0.07285130023956299
Loss at iteration 520 : 0.08955448865890503
Loss at iteration 530 : 0.05443621799349785
Loss at iteration 540 : 0.08565224707126617
Loss at iteration 550 : 0.0714992806315422
Loss at iteration 560 : 0.08638709783554077
Loss at iteration 570 : 0.08261114358901978
Loss at iteration 580 : 0.08295741677284241
Loss at iteration 590 : 0.08615058660507202
Loss at iteration 600 : 0.04786219447851181
Loss at iteration 610 : 0.060419660061597824
Loss at iteration 620 : 0.090878427028656
Loss at iteration 630 : 0.05791609734296799
Loss at iteration 640 : 0.06397270411252975
Loss at iteration 650 : 0.05290485545992851
Loss at iteration 660 : 0.07258611917495728
Loss at iteration 670 : 0.12409339845180511
Loss at iteration 680 : 0.05596139281988144
Loss at iteration 690 : 0.0690678209066391
Loss at iteration 700 : 0.10947123169898987
Loss at iteration 710 : 0.10888025164604187
Loss at iteration 720 : 0.09409403800964355
Loss at iteration 730 : 0.11934731900691986
Loss at iteration 740 : 0.054469041526317596
Loss at iteration 750 : 0.07574924826622009
Loss at iteration 760 : 0.1276669055223465
Loss at iteration 770 : 0.059443503618240356
Loss at iteration 780 : 0.08225973695516586
Loss at iteration 790 : 0.06474217027425766
Loss at iteration 800 : 0.11371499300003052
Loss at iteration 810 : 0.048928193747997284
Loss at iteration 820 : 0.09448270499706268
Loss at iteration 830 : 0.09486864507198334
Loss at iteration 840 : 0.06699312478303909
Loss at iteration 850 : 0.05309414863586426
Loss at iteration 860 : 0.09574128687381744
Loss at iteration 870 : 0.07016067206859589
Loss at iteration 880 : 0.11421214044094086
Loss at iteration 890 : 0.10371385514736176
Loss at iteration 900 : 0.09634488075971603
Loss at iteration 910 : 0.0957319587469101
Loss at iteration 920 : 0.07555945217609406
Loss at iteration 930 : 0.09304909408092499
Loss at iteration 940 : 0.08622341603040695
Loss at iteration 950 : 0.07361733913421631
Loss at iteration 960 : 0.08883156627416611
Loss at iteration 970 : 0.056983839720487595
Loss at iteration 980 : 0.09283153712749481
Loss at iteration 990 : 0.12378521263599396
Loss at iteration 1000 : 0.09033821523189545
Loss at iteration 1010 : 0.0974549651145935
Loss at iteration 1020 : 0.055812470614910126
Loss at iteration 1030 : 0.11876022815704346
Loss at iteration 1040 : 0.10680679976940155
Loss at iteration 1050 : 0.0610000304877758
Loss at iteration 1060 : 0.09241031110286713
Loss at iteration 1070 : 0.11444803327322006
Loss at iteration 1080 : 0.0643380731344223
Loss at iteration 1090 : 0.049210064113140106
Loss at iteration 1100 : 0.08282501995563507
Loss at iteration 1110 : 0.06080726906657219
Loss at iteration 1120 : 0.05940457433462143
Loss at iteration 1130 : 0.08710744231939316
Loss at iteration 1140 : 0.10084380954504013
Loss at iteration 1150 : 0.09998001903295517
Loss at iteration 1160 : 0.10260488837957382
Loss at iteration 1170 : 0.0918918251991272
Loss at iteration 1180 : 0.06600800901651382
Loss at iteration 1190 : 0.08166999369859695
Loss at iteration 1200 : 0.08566361665725708
Loss at iteration 1210 : 0.06474963575601578
The SSIM Value is: 0.71555970509847
The PSNR Value is: 21.501842371622722
the highest SSIM value is: 21.501842371622722
the epoch is: 114
Loss at iteration 10 : 0.12414440512657166
Loss at iteration 20 : 0.08530642837285995
Loss at iteration 30 : 0.07254993915557861
Loss at iteration 40 : 0.08263710886240005
Loss at iteration 50 : 0.06402057409286499
Loss at iteration 60 : 0.09065893292427063
Loss at iteration 70 : 0.0969502180814743
Loss at iteration 80 : 0.10346077382564545
Loss at iteration 90 : 0.09915225207805634
Loss at iteration 100 : 0.09948398917913437
Loss at iteration 110 : 0.09255516529083252
Loss at iteration 120 : 0.05643998831510544
Loss at iteration 130 : 0.09464037418365479
Loss at iteration 140 : 0.09547226130962372
Loss at iteration 150 : 0.08942630141973495
Loss at iteration 160 : 0.13641269505023956
Loss at iteration 170 : 0.06612105667591095
Loss at iteration 180 : 0.10428035259246826
Loss at iteration 190 : 0.10939401388168335
Loss at iteration 200 : 0.12233098596334457
Loss at iteration 210 : 0.10077951848506927
Loss at iteration 220 : 0.0650511384010315
Loss at iteration 230 : 0.07729064673185349
Loss at iteration 240 : 0.08663731813430786
Loss at iteration 250 : 0.07055958360433578
Loss at iteration 260 : 0.05077798292040825
Loss at iteration 270 : 0.039915140718221664
Loss at iteration 280 : 0.10958418250083923
Loss at iteration 290 : 0.05458386242389679
Loss at iteration 300 : 0.07668209075927734
Loss at iteration 310 : 0.06771352887153625
Loss at iteration 320 : 0.060894161462783813
Loss at iteration 330 : 0.05018210411071777
Loss at iteration 340 : 0.070720374584198
Loss at iteration 350 : 0.08842611312866211
Loss at iteration 360 : 0.06604701280593872
Loss at iteration 370 : 0.07735142856836319
Loss at iteration 380 : 0.06291250884532928
Loss at iteration 390 : 0.10530009120702744
Loss at iteration 400 : 0.07837103307247162
Loss at iteration 410 : 0.07382611930370331
Loss at iteration 420 : 0.078000009059906
Loss at iteration 430 : 0.0842457041144371
Loss at iteration 440 : 0.08140328526496887
Loss at iteration 450 : 0.07775245606899261
Loss at iteration 460 : 0.08773235976696014
Loss at iteration 470 : 0.10930773615837097
Loss at iteration 480 : 0.07405775785446167
Loss at iteration 490 : 0.055775098502635956
Loss at iteration 500 : 0.08680596202611923
Loss at iteration 510 : 0.04789683222770691
Loss at iteration 520 : 0.08733169734477997
Loss at iteration 530 : 0.0906636118888855
Loss at iteration 540 : 0.05799072980880737
Loss at iteration 550 : 0.08402099460363388
Loss at iteration 560 : 0.10730451345443726
Loss at iteration 570 : 0.0788276344537735
Loss at iteration 580 : 0.07644931226968765
Loss at iteration 590 : 0.1150425598025322
Loss at iteration 600 : 0.11078476905822754
Loss at iteration 610 : 0.14528635144233704
Loss at iteration 620 : 0.10787173360586166
Loss at iteration 630 : 0.09234882891178131
Loss at iteration 640 : 0.09733480215072632
Loss at iteration 650 : 0.06578394025564194
Loss at iteration 660 : 0.12584301829338074
Loss at iteration 670 : 0.04063093289732933
Loss at iteration 680 : 0.061228882521390915
Loss at iteration 690 : 0.056671589612960815
Loss at iteration 700 : 0.06842832267284393
Loss at iteration 710 : 0.09425493329763412
Loss at iteration 720 : 0.10600809752941132
Loss at iteration 730 : 0.12227784097194672
Loss at iteration 740 : 0.11445055902004242
Loss at iteration 750 : 0.0757899060845375
Loss at iteration 760 : 0.07500752061605453
Loss at iteration 770 : 0.08360638469457626
Loss at iteration 780 : 0.16990023851394653
Loss at iteration 790 : 0.08707457035779953
Loss at iteration 800 : 0.13211123645305634
Loss at iteration 810 : 0.08639870584011078
Loss at iteration 820 : 0.09322112053632736
Loss at iteration 830 : 0.06899824738502502
Loss at iteration 840 : 0.060704439878463745
Loss at iteration 850 : 0.08873354643583298
Loss at iteration 860 : 0.09103018045425415
Loss at iteration 870 : 0.09371644258499146
Loss at iteration 880 : 0.05745004117488861
Loss at iteration 890 : 0.06141339987516403
Loss at iteration 900 : 0.10894676297903061
Loss at iteration 910 : 0.09180530160665512
Loss at iteration 920 : 0.08109482377767563
Loss at iteration 930 : 0.06447410583496094
Loss at iteration 940 : 0.07385221123695374
Loss at iteration 950 : 0.07482419908046722
Loss at iteration 960 : 0.08547678589820862
Loss at iteration 970 : 0.10908907651901245
Loss at iteration 980 : 0.0943499207496643
Loss at iteration 990 : 0.07206898927688599
Loss at iteration 1000 : 0.08027049154043198
Loss at iteration 1010 : 0.07962965965270996
Loss at iteration 1020 : 0.07141464948654175
Loss at iteration 1030 : 0.09382685273885727
Loss at iteration 1040 : 0.06340222805738449
Loss at iteration 1050 : 0.07115033268928528
Loss at iteration 1060 : 0.04600922390818596
Loss at iteration 1070 : 0.0807584822177887
Loss at iteration 1080 : 0.05253606289625168
Loss at iteration 1090 : 0.04848211258649826
Loss at iteration 1100 : 0.06058657169342041
Loss at iteration 1110 : 0.09607162326574326
Loss at iteration 1120 : 0.10078908503055573
Loss at iteration 1130 : 0.09056898951530457
Loss at iteration 1140 : 0.08581020683050156
Loss at iteration 1150 : 0.08974774926900864
Loss at iteration 1160 : 0.06019357591867447
Loss at iteration 1170 : 0.0872340202331543
Loss at iteration 1180 : 0.054607897996902466
Loss at iteration 1190 : 0.07053714990615845
Loss at iteration 1200 : 0.10120661556720734
Loss at iteration 1210 : 0.04360327124595642
The SSIM Value is: 0.7108915547529856
The PSNR Value is: 20.880722173055013
the epoch is: 115
Loss at iteration 10 : 0.07485340535640717
Loss at iteration 20 : 0.12311001867055893
Loss at iteration 30 : 0.0960993617773056
Loss at iteration 40 : 0.06656526029109955
Loss at iteration 50 : 0.06170127913355827
Loss at iteration 60 : 0.06918367743492126
Loss at iteration 70 : 0.10820209980010986
Loss at iteration 80 : 0.08150401711463928
Loss at iteration 90 : 0.09038525074720383
Loss at iteration 100 : 0.10185873508453369
Loss at iteration 110 : 0.10532071441411972
Loss at iteration 120 : 0.06685573607683182
Loss at iteration 130 : 0.03516587242484093
Loss at iteration 140 : 0.07845112681388855
Loss at iteration 150 : 0.12153381109237671
Loss at iteration 160 : 0.05662742257118225
Loss at iteration 170 : 0.06291323900222778
Loss at iteration 180 : 0.06560852378606796
Loss at iteration 190 : 0.06864479184150696
Loss at iteration 200 : 0.06523977965116501
Loss at iteration 210 : 0.1087263822555542
Loss at iteration 220 : 0.07792042195796967
Loss at iteration 230 : 0.04908987134695053
Loss at iteration 240 : 0.06288543343544006
Loss at iteration 250 : 0.0573246031999588
Loss at iteration 260 : 0.08578343689441681
Loss at iteration 270 : 0.08904451131820679
Loss at iteration 280 : 0.0982891172170639
Loss at iteration 290 : 0.08374261111021042
Loss at iteration 300 : 0.11271373927593231
Loss at iteration 310 : 0.12159295380115509
Loss at iteration 320 : 0.07255706936120987
Loss at iteration 330 : 0.0848911702632904
Loss at iteration 340 : 0.11076067388057709
Loss at iteration 350 : 0.07928026467561722
Loss at iteration 360 : 0.06728650629520416
Loss at iteration 370 : 0.08980365842580795
Loss at iteration 380 : 0.06372043490409851
Loss at iteration 390 : 0.11261485517024994
Loss at iteration 400 : 0.08048704266548157
Loss at iteration 410 : 0.08313985168933868
Loss at iteration 420 : 0.06441478431224823
Loss at iteration 430 : 0.08416929095983505
Loss at iteration 440 : 0.09787043184041977
Loss at iteration 450 : 0.09398625791072845
Loss at iteration 460 : 0.09017293155193329
Loss at iteration 470 : 0.0489407479763031
Loss at iteration 480 : 0.056272607296705246
Loss at iteration 490 : 0.06439942121505737
Loss at iteration 500 : 0.08897517621517181
Loss at iteration 510 : 0.1307229995727539
Loss at iteration 520 : 0.10223289579153061
Loss at iteration 530 : 0.09631578624248505
Loss at iteration 540 : 0.09364831447601318
Loss at iteration 550 : 0.08911874145269394
Loss at iteration 560 : 0.1090376004576683
Loss at iteration 570 : 0.10744653642177582
Loss at iteration 580 : 0.08541401475667953
Loss at iteration 590 : 0.07138314843177795
Loss at iteration 600 : 0.10963135957717896
Loss at iteration 610 : 0.09769146144390106
Loss at iteration 620 : 0.0858609601855278
Loss at iteration 630 : 0.08558164536952972
Loss at iteration 640 : 0.07341979444026947
Loss at iteration 650 : 0.08890681713819504
Loss at iteration 660 : 0.08659019321203232
Loss at iteration 670 : 0.09173598140478134
Loss at iteration 680 : 0.08638783544301987
Loss at iteration 690 : 0.08523194491863251
Loss at iteration 700 : 0.07324270904064178
Loss at iteration 710 : 0.057646244764328
Loss at iteration 720 : 0.08792947232723236
Loss at iteration 730 : 0.07734595984220505
Loss at iteration 740 : 0.08851733803749084
Loss at iteration 750 : 0.08387424796819687
Loss at iteration 760 : 0.07046829164028168
Loss at iteration 770 : 0.12057089805603027
Loss at iteration 780 : 0.06881078332662582
Loss at iteration 790 : 0.06348332017660141
Loss at iteration 800 : 0.06509148329496384
Loss at iteration 810 : 0.09993797540664673
Loss at iteration 820 : 0.12945806980133057
Loss at iteration 830 : 0.09996747225522995
Loss at iteration 840 : 0.06630000472068787
Loss at iteration 850 : 0.06138034164905548
Loss at iteration 860 : 0.11832257360219955
Loss at iteration 870 : 0.09567268937826157
Loss at iteration 880 : 0.0632941871881485
Loss at iteration 890 : 0.07992947101593018
Loss at iteration 900 : 0.08922508358955383
Loss at iteration 910 : 0.07652731984853745
Loss at iteration 920 : 0.08458112180233002
Loss at iteration 930 : 0.053505249321460724
Loss at iteration 940 : 0.06392146646976471
Loss at iteration 950 : 0.1022154688835144
Loss at iteration 960 : 0.09670615196228027
Loss at iteration 970 : 0.09151013195514679
Loss at iteration 980 : 0.10104604810476303
Loss at iteration 990 : 0.08145059645175934
Loss at iteration 1000 : 0.04615490883588791
Loss at iteration 1010 : 0.05639730021357536
Loss at iteration 1020 : 0.06599432975053787
Loss at iteration 1030 : 0.10408234596252441
Loss at iteration 1040 : 0.0843944400548935
Loss at iteration 1050 : 0.07847562432289124
Loss at iteration 1060 : 0.11796732991933823
Loss at iteration 1070 : 0.09354250133037567
Loss at iteration 1080 : 0.0805148035287857
Loss at iteration 1090 : 0.08087003231048584
Loss at iteration 1100 : 0.06353385001420975
Loss at iteration 1110 : 0.08746518939733505
Loss at iteration 1120 : 0.07047124207019806
Loss at iteration 1130 : 0.1241862028837204
Loss at iteration 1140 : 0.08472707122564316
Loss at iteration 1150 : 0.09327678382396698
Loss at iteration 1160 : 0.06660961359739304
Loss at iteration 1170 : 0.07539917528629303
Loss at iteration 1180 : 0.07474463433027267
Loss at iteration 1190 : 0.060567788779735565
Loss at iteration 1200 : 0.07729135453701019
Loss at iteration 1210 : 0.07348941266536713
The SSIM Value is: 0.712787801027298
The PSNR Value is: 21.205276743570963
the epoch is: 116
Loss at iteration 10 : 0.06390901654958725
Loss at iteration 20 : 0.08532866090536118
Loss at iteration 30 : 0.08632591366767883
Loss at iteration 40 : 0.09984159469604492
Loss at iteration 50 : 0.08881976455450058
Loss at iteration 60 : 0.09143213927745819
Loss at iteration 70 : 0.07903449982404709
Loss at iteration 80 : 0.10907822847366333
Loss at iteration 90 : 0.058682601898908615
Loss at iteration 100 : 0.08181214332580566
Loss at iteration 110 : 0.07983357459306717
Loss at iteration 120 : 0.08873307704925537
Loss at iteration 130 : 0.15215472877025604
Loss at iteration 140 : 0.07116454094648361
Loss at iteration 150 : 0.08772497624158859
Loss at iteration 160 : 0.08284489065408707
Loss at iteration 170 : 0.08254014700651169
Loss at iteration 180 : 0.12864550948143005
Loss at iteration 190 : 0.08924494683742523
Loss at iteration 200 : 0.09630834311246872
Loss at iteration 210 : 0.09886239469051361
Loss at iteration 220 : 0.07529018819332123
Loss at iteration 230 : 0.06400283426046371
Loss at iteration 240 : 0.06477683037519455
Loss at iteration 250 : 0.09276682138442993
Loss at iteration 260 : 0.13250818848609924
Loss at iteration 270 : 0.07030309736728668
Loss at iteration 280 : 0.11549215018749237
Loss at iteration 290 : 0.08569714426994324
Loss at iteration 300 : 0.11809630692005157
Loss at iteration 310 : 0.056518323719501495
Loss at iteration 320 : 0.08029582351446152
Loss at iteration 330 : 0.05593983456492424
Loss at iteration 340 : 0.04458571597933769
Loss at iteration 350 : 0.05511034280061722
Loss at iteration 360 : 0.08572477102279663
Loss at iteration 370 : 0.11033734679222107
Loss at iteration 380 : 0.10752160847187042
Loss at iteration 390 : 0.05391359329223633
Loss at iteration 400 : 0.09274786710739136
Loss at iteration 410 : 0.11915313452482224
Loss at iteration 420 : 0.058197151869535446
Loss at iteration 430 : 0.05014519393444061
Loss at iteration 440 : 0.08304230868816376
Loss at iteration 450 : 0.10045608133077621
Loss at iteration 460 : 0.12710218131542206
Loss at iteration 470 : 0.1165723204612732
Loss at iteration 480 : 0.09219610691070557
Loss at iteration 490 : 0.06919022649526596
Loss at iteration 500 : 0.06364931911230087
Loss at iteration 510 : 0.07876536250114441
Loss at iteration 520 : 0.05954655259847641
Loss at iteration 530 : 0.05468393489718437
Loss at iteration 540 : 0.09762930124998093
Loss at iteration 550 : 0.054853372275829315
Loss at iteration 560 : 0.039648354053497314
Loss at iteration 570 : 0.07095871865749359
Loss at iteration 580 : 0.08552541583776474
Loss at iteration 590 : 0.07333847880363464
Loss at iteration 600 : 0.07166589051485062
Loss at iteration 610 : 0.07512551546096802
Loss at iteration 620 : 0.05181850120425224
Loss at iteration 630 : 0.09660955518484116
Loss at iteration 640 : 0.10741418600082397
Loss at iteration 650 : 0.07977563142776489
Loss at iteration 660 : 0.0770334005355835
Loss at iteration 670 : 0.09344242513179779
Loss at iteration 680 : 0.060107119381427765
Loss at iteration 690 : 0.06933315843343735
Loss at iteration 700 : 0.080905482172966
Loss at iteration 710 : 0.05521390959620476
Loss at iteration 720 : 0.10093621909618378
Loss at iteration 730 : 0.0766095519065857
Loss at iteration 740 : 0.07933590561151505
Loss at iteration 750 : 0.06334187090396881
Loss at iteration 760 : 0.0921575278043747
Loss at iteration 770 : 0.07235062122344971
Loss at iteration 780 : 0.07721638679504395
Loss at iteration 790 : 0.10665647685527802
Loss at iteration 800 : 0.10338078439235687
Loss at iteration 810 : 0.045088451355695724
Loss at iteration 820 : 0.04519465193152428
Loss at iteration 830 : 0.08545154333114624
Loss at iteration 840 : 0.10114158689975739
Loss at iteration 850 : 0.08442328870296478
Loss at iteration 860 : 0.07602313160896301
Loss at iteration 870 : 0.10148444771766663
Loss at iteration 880 : 0.0970771312713623
Loss at iteration 890 : 0.1021590530872345
Loss at iteration 900 : 0.07319815456867218
Loss at iteration 910 : 0.07430246472358704
Loss at iteration 920 : 0.09555894136428833
Loss at iteration 930 : 0.11632470786571503
Loss at iteration 940 : 0.12720081210136414
Loss at iteration 950 : 0.07490163296461105
Loss at iteration 960 : 0.09789137542247772
Loss at iteration 970 : 0.06265509128570557
Loss at iteration 980 : 0.1184777021408081
Loss at iteration 990 : 0.13254791498184204
Loss at iteration 1000 : 0.10464327037334442
Loss at iteration 1010 : 0.10427799820899963
Loss at iteration 1020 : 0.05607708916068077
Loss at iteration 1030 : 0.07214681804180145
Loss at iteration 1040 : 0.08041378855705261
Loss at iteration 1050 : 0.0902179628610611
Loss at iteration 1060 : 0.07191784679889679
Loss at iteration 1070 : 0.07611390203237534
Loss at iteration 1080 : 0.09536893665790558
Loss at iteration 1090 : 0.0797332376241684
Loss at iteration 1100 : 0.07714693248271942
Loss at iteration 1110 : 0.0905812606215477
Loss at iteration 1120 : 0.0810445100069046
Loss at iteration 1130 : 0.07392625510692596
Loss at iteration 1140 : 0.0549868643283844
Loss at iteration 1150 : 0.10536583513021469
Loss at iteration 1160 : 0.05737311765551567
Loss at iteration 1170 : 0.07707034051418304
Loss at iteration 1180 : 0.0847364068031311
Loss at iteration 1190 : 0.11259619891643524
Loss at iteration 1200 : 0.101901113986969
Loss at iteration 1210 : 0.06455110758543015
The SSIM Value is: 0.7144929707050324
The PSNR Value is: 21.145821126302085
the epoch is: 117
Loss at iteration 10 : 0.0645337849855423
Loss at iteration 20 : 0.11076642572879791
Loss at iteration 30 : 0.0809914693236351
Loss at iteration 40 : 0.06078402325510979
Loss at iteration 50 : 0.062347352504730225
Loss at iteration 60 : 0.13968074321746826
Loss at iteration 70 : 0.08592744916677475
Loss at iteration 80 : 0.09404363483190536
Loss at iteration 90 : 0.07308144867420197
Loss at iteration 100 : 0.0820169746875763
Loss at iteration 110 : 0.11335055530071259
Loss at iteration 120 : 0.1077762246131897
Loss at iteration 130 : 0.13953684270381927
Loss at iteration 140 : 0.049819767475128174
Loss at iteration 150 : 0.10125400871038437
Loss at iteration 160 : 0.061479270458221436
Loss at iteration 170 : 0.0857759565114975
Loss at iteration 180 : 0.06671913713216782
Loss at iteration 190 : 0.082126185297966
Loss at iteration 200 : 0.09641660749912262
Loss at iteration 210 : 0.059048041701316833
Loss at iteration 220 : 0.06481713056564331
Loss at iteration 230 : 0.10681968927383423
Loss at iteration 240 : 0.11772888153791428
Loss at iteration 250 : 0.10703538358211517
Loss at iteration 260 : 0.10779464244842529
Loss at iteration 270 : 0.08061320334672928
Loss at iteration 280 : 0.07622496783733368
Loss at iteration 290 : 0.06669832020998001
Loss at iteration 300 : 0.06381291151046753
Loss at iteration 310 : 0.10549493879079819
Loss at iteration 320 : 0.11225670576095581
Loss at iteration 330 : 0.06855741143226624
Loss at iteration 340 : 0.11901157349348068
Loss at iteration 350 : 0.12037584185600281
Loss at iteration 360 : 0.09811681509017944
Loss at iteration 370 : 0.07427889853715897
Loss at iteration 380 : 0.11848887801170349
Loss at iteration 390 : 0.11639830470085144
Loss at iteration 400 : 0.06806398183107376
Loss at iteration 410 : 0.08387015759944916
Loss at iteration 420 : 0.08318721503019333
Loss at iteration 430 : 0.08054454624652863
Loss at iteration 440 : 0.07155957072973251
Loss at iteration 450 : 0.05413094162940979
Loss at iteration 460 : 0.1302235722541809
Loss at iteration 470 : 0.057029854506254196
Loss at iteration 480 : 0.08226247876882553
Loss at iteration 490 : 0.1413436383008957
Loss at iteration 500 : 0.09412678331136703
Loss at iteration 510 : 0.08199982345104218
Loss at iteration 520 : 0.07505933940410614
Loss at iteration 530 : 0.08008450269699097
Loss at iteration 540 : 0.1599271595478058
Loss at iteration 550 : 0.1048932820558548
Loss at iteration 560 : 0.054954320192337036
Loss at iteration 570 : 0.08564551174640656
Loss at iteration 580 : 0.08876526355743408
Loss at iteration 590 : 0.06817822903394699
Loss at iteration 600 : 0.09517012536525726
Loss at iteration 610 : 0.0916149765253067
Loss at iteration 620 : 0.06342858076095581
Loss at iteration 630 : 0.055680807679891586
Loss at iteration 640 : 0.09811868518590927
Loss at iteration 650 : 0.09360823035240173
Loss at iteration 660 : 0.054859813302755356
Loss at iteration 670 : 0.0820208266377449
Loss at iteration 680 : 0.057144440710544586
Loss at iteration 690 : 0.128631591796875
Loss at iteration 700 : 0.07554838061332703
Loss at iteration 710 : 0.10198589414358139
Loss at iteration 720 : 0.09963288903236389
Loss at iteration 730 : 0.10654410719871521
Loss at iteration 740 : 0.07485061138868332
Loss at iteration 750 : 0.10748016834259033
Loss at iteration 760 : 0.13032275438308716
Loss at iteration 770 : 0.07150755077600479
Loss at iteration 780 : 0.04879501089453697
Loss at iteration 790 : 0.07075272500514984
Loss at iteration 800 : 0.05729801580309868
Loss at iteration 810 : 0.10987292230129242
Loss at iteration 820 : 0.08251658082008362
Loss at iteration 830 : 0.08157303929328918
Loss at iteration 840 : 0.07074950635433197
Loss at iteration 850 : 0.15701526403427124
Loss at iteration 860 : 0.11644354462623596
Loss at iteration 870 : 0.12335322797298431
Loss at iteration 880 : 0.07528561353683472
Loss at iteration 890 : 0.12514811754226685
Loss at iteration 900 : 0.06836021691560745
Loss at iteration 910 : 0.06870988011360168
Loss at iteration 920 : 0.08243493735790253
Loss at iteration 930 : 0.07490342110395432
Loss at iteration 940 : 0.09391127526760101
Loss at iteration 950 : 0.09493865817785263
Loss at iteration 960 : 0.11589638888835907
Loss at iteration 970 : 0.056290265172719955
Loss at iteration 980 : 0.09326204657554626
Loss at iteration 990 : 0.03709855675697327
Loss at iteration 1000 : 0.06985801458358765
Loss at iteration 1010 : 0.05647767707705498
Loss at iteration 1020 : 0.12008900940418243
Loss at iteration 1030 : 0.143095463514328
Loss at iteration 1040 : 0.09975424408912659
Loss at iteration 1050 : 0.0874893069267273
Loss at iteration 1060 : 0.11223283410072327
Loss at iteration 1070 : 0.09075611084699631
Loss at iteration 1080 : 0.06116275116801262
Loss at iteration 1090 : 0.05210758373141289
Loss at iteration 1100 : 0.055390506982803345
Loss at iteration 1110 : 0.06975030153989792
Loss at iteration 1120 : 0.11804135143756866
Loss at iteration 1130 : 0.08411367982625961
Loss at iteration 1140 : 0.07395228743553162
Loss at iteration 1150 : 0.11983808130025864
Loss at iteration 1160 : 0.04015377536416054
Loss at iteration 1170 : 0.06688247621059418
Loss at iteration 1180 : 0.04341615363955498
Loss at iteration 1190 : 0.04905515909194946
Loss at iteration 1200 : 0.060820698738098145
Loss at iteration 1210 : 0.09684273600578308
The SSIM Value is: 0.7098787824312845
The PSNR Value is: 20.426862462361655
the epoch is: 118
Loss at iteration 10 : 0.07532413303852081
Loss at iteration 20 : 0.04038340598344803
Loss at iteration 30 : 0.06489300727844238
Loss at iteration 40 : 0.1013343334197998
Loss at iteration 50 : 0.10234122723340988
Loss at iteration 60 : 0.06605768203735352
Loss at iteration 70 : 0.1188545972108841
Loss at iteration 80 : 0.08298218995332718
Loss at iteration 90 : 0.099154993891716
Loss at iteration 100 : 0.06033548340201378
Loss at iteration 110 : 0.052876196801662445
Loss at iteration 120 : 0.07781641185283661
Loss at iteration 130 : 0.09481512010097504
Loss at iteration 140 : 0.06739088892936707
Loss at iteration 150 : 0.04824760556221008
Loss at iteration 160 : 0.0694524347782135
Loss at iteration 170 : 0.04344717785716057
Loss at iteration 180 : 0.06613411009311676
Loss at iteration 190 : 0.046440210193395615
Loss at iteration 200 : 0.06469551473855972
Loss at iteration 210 : 0.1537243127822876
Loss at iteration 220 : 0.05445520207285881
Loss at iteration 230 : 0.10500502586364746
Loss at iteration 240 : 0.0750405490398407
Loss at iteration 250 : 0.048513781279325485
Loss at iteration 260 : 0.0723717138171196
Loss at iteration 270 : 0.05746491998434067
Loss at iteration 280 : 0.11509902775287628
Loss at iteration 290 : 0.09995663166046143
Loss at iteration 300 : 0.06507815420627594
Loss at iteration 310 : 0.08931329101324081
Loss at iteration 320 : 0.0965183675289154
Loss at iteration 330 : 0.10775812715291977
Loss at iteration 340 : 0.14392906427383423
Loss at iteration 350 : 0.06660689413547516
Loss at iteration 360 : 0.06746629625558853
Loss at iteration 370 : 0.08179230988025665
Loss at iteration 380 : 0.1713910549879074
Loss at iteration 390 : 0.0726604014635086
Loss at iteration 400 : 0.08476786315441132
Loss at iteration 410 : 0.11220600455999374
Loss at iteration 420 : 0.06232469528913498
Loss at iteration 430 : 0.0884542390704155
Loss at iteration 440 : 0.09192462265491486
Loss at iteration 450 : 0.10016579926013947
Loss at iteration 460 : 0.07405129820108414
Loss at iteration 470 : 0.12487941235303879
Loss at iteration 480 : 0.08762902021408081
Loss at iteration 490 : 0.07744728028774261
Loss at iteration 500 : 0.06723099946975708
Loss at iteration 510 : 0.09363674372434616
Loss at iteration 520 : 0.10582538694143295
Loss at iteration 530 : 0.1048155426979065
Loss at iteration 540 : 0.07064010202884674
Loss at iteration 550 : 0.08757947385311127
Loss at iteration 560 : 0.08024083822965622
Loss at iteration 570 : 0.08918821066617966
Loss at iteration 580 : 0.0906522199511528
Loss at iteration 590 : 0.08196945488452911
Loss at iteration 600 : 0.09371618926525116
Loss at iteration 610 : 0.08408995717763901
Loss at iteration 620 : 0.08307226747274399
Loss at iteration 630 : 0.08130379021167755
Loss at iteration 640 : 0.11534442752599716
Loss at iteration 650 : 0.04967668280005455
Loss at iteration 660 : 0.05494921654462814
Loss at iteration 670 : 0.10916905850172043
Loss at iteration 680 : 0.05598471686244011
Loss at iteration 690 : 0.056129686534404755
Loss at iteration 700 : 0.07905728369951248
Loss at iteration 710 : 0.07899446785449982
Loss at iteration 720 : 0.07645043730735779
Loss at iteration 730 : 0.16259247064590454
Loss at iteration 740 : 0.06064118072390556
Loss at iteration 750 : 0.11661820858716965
Loss at iteration 760 : 0.05171456187963486
Loss at iteration 770 : 0.056301750242710114
Loss at iteration 780 : 0.07914131879806519
Loss at iteration 790 : 0.08962850272655487
Loss at iteration 800 : 0.06771504878997803
Loss at iteration 810 : 0.08663763105869293
Loss at iteration 820 : 0.11727368831634521
Loss at iteration 830 : 0.08908702433109283
Loss at iteration 840 : 0.08929284662008286
Loss at iteration 850 : 0.0727333351969719
Loss at iteration 860 : 0.07853799313306808
Loss at iteration 870 : 0.1042403131723404
Loss at iteration 880 : 0.09613877534866333
Loss at iteration 890 : 0.08815233409404755
Loss at iteration 900 : 0.10681869834661484
Loss at iteration 910 : 0.06691092252731323
Loss at iteration 920 : 0.09139557927846909
Loss at iteration 930 : 0.07675731182098389
Loss at iteration 940 : 0.11204300820827484
Loss at iteration 950 : 0.12318196147680283
Loss at iteration 960 : 0.058392614126205444
Loss at iteration 970 : 0.0730847492814064
Loss at iteration 980 : 0.10294576734304428
Loss at iteration 990 : 0.056615620851516724
Loss at iteration 1000 : 0.11633771657943726
Loss at iteration 1010 : 0.07464586943387985
Loss at iteration 1020 : 0.07544974982738495
Loss at iteration 1030 : 0.0782967060804367
Loss at iteration 1040 : 0.078057199716568
Loss at iteration 1050 : 0.07860517501831055
Loss at iteration 1060 : 0.12334059178829193
Loss at iteration 1070 : 0.07433399558067322
Loss at iteration 1080 : 0.06374764442443848
Loss at iteration 1090 : 0.06916895508766174
Loss at iteration 1100 : 0.1000121682882309
Loss at iteration 1110 : 0.09388063848018646
Loss at iteration 1120 : 0.07515349239110947
Loss at iteration 1130 : 0.08423881232738495
Loss at iteration 1140 : 0.07269411534070969
Loss at iteration 1150 : 0.10584541410207748
Loss at iteration 1160 : 0.06375592947006226
Loss at iteration 1170 : 0.07360532879829407
Loss at iteration 1180 : 0.07356202602386475
Loss at iteration 1190 : 0.1163807138800621
Loss at iteration 1200 : 0.08999103307723999
Loss at iteration 1210 : 0.09233768284320831
The SSIM Value is: 0.7131900191307068
The PSNR Value is: 21.23454106648763
the epoch is: 119
Loss at iteration 10 : 0.06083182618021965
Loss at iteration 20 : 0.07584653794765472
Loss at iteration 30 : 0.09713276475667953
Loss at iteration 40 : 0.07359881699085236
Loss at iteration 50 : 0.07064495980739594
Loss at iteration 60 : 0.06439065933227539
Loss at iteration 70 : 0.07632093131542206
Loss at iteration 80 : 0.08162059634923935
Loss at iteration 90 : 0.0563022717833519
Loss at iteration 100 : 0.10569553077220917
Loss at iteration 110 : 0.05693057179450989
Loss at iteration 120 : 0.05173368379473686
Loss at iteration 130 : 0.10049363970756531
Loss at iteration 140 : 0.11311080306768417
Loss at iteration 150 : 0.08565273135900497
Loss at iteration 160 : 0.07982140779495239
Loss at iteration 170 : 0.10892713069915771
Loss at iteration 180 : 0.06349530071020126
Loss at iteration 190 : 0.13001982867717743
Loss at iteration 200 : 0.08798132836818695
Loss at iteration 210 : 0.0680001750588417
Loss at iteration 220 : 0.058865737169981
Loss at iteration 230 : 0.06402363628149033
Loss at iteration 240 : 0.07468796521425247
Loss at iteration 250 : 0.07846035808324814
Loss at iteration 260 : 0.07180248945951462
Loss at iteration 270 : 0.052492331713438034
Loss at iteration 280 : 0.08523686975240707
Loss at iteration 290 : 0.0915021300315857
Loss at iteration 300 : 0.07770778238773346
Loss at iteration 310 : 0.10703015327453613
Loss at iteration 320 : 0.08835947513580322
Loss at iteration 330 : 0.08647461235523224
Loss at iteration 340 : 0.091908298432827
Loss at iteration 350 : 0.0765843614935875
Loss at iteration 360 : 0.06863176822662354
Loss at iteration 370 : 0.07227227836847305
Loss at iteration 380 : 0.07755690813064575
Loss at iteration 390 : 0.07858559489250183
Loss at iteration 400 : 0.05108507722616196
Loss at iteration 410 : 0.08967672288417816
Loss at iteration 420 : 0.05630304664373398
Loss at iteration 430 : 0.11421960592269897
Loss at iteration 440 : 0.1570844054222107
Loss at iteration 450 : 0.06249527260661125
Loss at iteration 460 : 0.09372642636299133
Loss at iteration 470 : 0.06968969851732254
Loss at iteration 480 : 0.10105109214782715
Loss at iteration 490 : 0.07869367301464081
Loss at iteration 500 : 0.06474363803863525
Loss at iteration 510 : 0.08538921922445297
Loss at iteration 520 : 0.0835714191198349
Loss at iteration 530 : 0.08292687684297562
Loss at iteration 540 : 0.08416419476270676
Loss at iteration 550 : 0.07878332585096359
Loss at iteration 560 : 0.05499113351106644
Loss at iteration 570 : 0.05346360802650452
Loss at iteration 580 : 0.06633317470550537
Loss at iteration 590 : 0.0990789532661438
Loss at iteration 600 : 0.09696006774902344
Loss at iteration 610 : 0.13109038770198822
Loss at iteration 620 : 0.12512022256851196
Loss at iteration 630 : 0.06703923642635345
Loss at iteration 640 : 0.07007724046707153
Loss at iteration 650 : 0.07474671304225922
Loss at iteration 660 : 0.11738637089729309
Loss at iteration 670 : 0.10020230710506439
Loss at iteration 680 : 0.09479912370443344
Loss at iteration 690 : 0.07820650190114975
Loss at iteration 700 : 0.15519091486930847
Loss at iteration 710 : 0.06650315225124359
Loss at iteration 720 : 0.04766411706805229
Loss at iteration 730 : 0.09042554348707199
Loss at iteration 740 : 0.07815854996442795
Loss at iteration 750 : 0.06601503491401672
Loss at iteration 760 : 0.0984906256198883
Loss at iteration 770 : 0.05841411277651787
Loss at iteration 780 : 0.11507116258144379
Loss at iteration 790 : 0.08519250154495239
Loss at iteration 800 : 0.09378620237112045
Loss at iteration 810 : 0.12533783912658691
Loss at iteration 820 : 0.0871904045343399
Loss at iteration 830 : 0.07451773434877396
Loss at iteration 840 : 0.04841381311416626
Loss at iteration 850 : 0.06626873463392258
Loss at iteration 860 : 0.08126024901866913
Loss at iteration 870 : 0.07994967699050903
Loss at iteration 880 : 0.10368941724300385
Loss at iteration 890 : 0.10148739814758301
Loss at iteration 900 : 0.06658228486776352
Loss at iteration 910 : 0.09965676069259644
Loss at iteration 920 : 0.0688614696264267
Loss at iteration 930 : 0.08798767626285553
Loss at iteration 940 : 0.12467694282531738
Loss at iteration 950 : 0.10974608361721039
Loss at iteration 960 : 0.09612607955932617
Loss at iteration 970 : 0.10954578965902328
Loss at iteration 980 : 0.06057044118642807
Loss at iteration 990 : 0.09168043732643127
Loss at iteration 1000 : 0.06133807823061943
Loss at iteration 1010 : 0.07547701895236969
Loss at iteration 1020 : 0.11292813718318939
Loss at iteration 1030 : 0.11438709497451782
Loss at iteration 1040 : 0.12325233221054077
Loss at iteration 1050 : 0.06784605979919434
Loss at iteration 1060 : 0.0645069032907486
Loss at iteration 1070 : 0.07546423375606537
Loss at iteration 1080 : 0.08622977137565613
Loss at iteration 1090 : 0.10750257968902588
Loss at iteration 1100 : 0.0807369202375412
Loss at iteration 1110 : 0.11863602697849274
Loss at iteration 1120 : 0.051499538123607635
Loss at iteration 1130 : 0.08837775886058807
Loss at iteration 1140 : 0.04369540140032768
Loss at iteration 1150 : 0.06677895784378052
Loss at iteration 1160 : 0.07611563056707382
Loss at iteration 1170 : 0.089668869972229
Loss at iteration 1180 : 0.0884973555803299
Loss at iteration 1190 : 0.07139351218938828
Loss at iteration 1200 : 0.0593586191534996
Loss at iteration 1210 : 0.098663330078125
The SSIM Value is: 0.709255596001943
The PSNR Value is: 20.71026611328125
the epoch is: 120
Loss at iteration 10 : 0.06149711459875107
Loss at iteration 20 : 0.07361279428005219
Loss at iteration 30 : 0.11569295823574066
Loss at iteration 40 : 0.13243736326694489
Loss at iteration 50 : 0.08976288884878159
Loss at iteration 60 : 0.07136434316635132
Loss at iteration 70 : 0.06939539313316345
Loss at iteration 80 : 0.06605848670005798
Loss at iteration 90 : 0.07605838775634766
Loss at iteration 100 : 0.0753069743514061
Loss at iteration 110 : 0.06316541135311127
Loss at iteration 120 : 0.09841947257518768
Loss at iteration 130 : 0.0648852214217186
Loss at iteration 140 : 0.1026490330696106
Loss at iteration 150 : 0.06049790605902672
Loss at iteration 160 : 0.07375124096870422
Loss at iteration 170 : 0.09902355074882507
Loss at iteration 180 : 0.06265158206224442
Loss at iteration 190 : 0.07650415599346161
Loss at iteration 200 : 0.06939774751663208
Loss at iteration 210 : 0.050449173897504807
Loss at iteration 220 : 0.11397679150104523
Loss at iteration 230 : 0.07713750004768372
Loss at iteration 240 : 0.08676326274871826
Loss at iteration 250 : 0.06890566647052765
Loss at iteration 260 : 0.06008128821849823
Loss at iteration 270 : 0.05968119204044342
Loss at iteration 280 : 0.07887279242277145
Loss at iteration 290 : 0.10012529790401459
Loss at iteration 300 : 0.0547424778342247
Loss at iteration 310 : 0.07534435391426086
Loss at iteration 320 : 0.05491962283849716
Loss at iteration 330 : 0.05679718777537346
Loss at iteration 340 : 0.07882418483495712
Loss at iteration 350 : 0.06390413641929626
Loss at iteration 360 : 0.09002213925123215
Loss at iteration 370 : 0.10381509363651276
Loss at iteration 380 : 0.08515767753124237
Loss at iteration 390 : 0.12898506224155426
Loss at iteration 400 : 0.07880684733390808
Loss at iteration 410 : 0.10055719316005707
Loss at iteration 420 : 0.057685524225234985
Loss at iteration 430 : 0.12136624753475189
Loss at iteration 440 : 0.1833268105983734
Loss at iteration 450 : 0.06954613327980042
Loss at iteration 460 : 0.08903523534536362
Loss at iteration 470 : 0.10855986177921295
Loss at iteration 480 : 0.0801657885313034
Loss at iteration 490 : 0.10777607560157776
Loss at iteration 500 : 0.073792964220047
Loss at iteration 510 : 0.13696497678756714
Loss at iteration 520 : 0.10081645846366882
Loss at iteration 530 : 0.07445110380649567
Loss at iteration 540 : 0.06947322189807892
Loss at iteration 550 : 0.11553780734539032
Loss at iteration 560 : 0.10809819400310516
Loss at iteration 570 : 0.0633498877286911
Loss at iteration 580 : 0.07294966280460358
Loss at iteration 590 : 0.1488317847251892
Loss at iteration 600 : 0.08269236236810684
Loss at iteration 610 : 0.10223951190710068
Loss at iteration 620 : 0.08512590825557709
Loss at iteration 630 : 0.07835549861192703
Loss at iteration 640 : 0.07738618552684784
Loss at iteration 650 : 0.06818841397762299
Loss at iteration 660 : 0.08241793513298035
Loss at iteration 670 : 0.1407715380191803
Loss at iteration 680 : 0.09575150907039642
Loss at iteration 690 : 0.09217896312475204
Loss at iteration 700 : 0.07002951949834824
Loss at iteration 710 : 0.0912570208311081
Loss at iteration 720 : 0.06518302857875824
Loss at iteration 730 : 0.05628317594528198
Loss at iteration 740 : 0.07716064155101776
Loss at iteration 750 : 0.10449443757534027
Loss at iteration 760 : 0.05882982909679413
Loss at iteration 770 : 0.0777462050318718
Loss at iteration 780 : 0.07591381669044495
Loss at iteration 790 : 0.061572588980197906
Loss at iteration 800 : 0.10393663495779037
Loss at iteration 810 : 0.13818010687828064
Loss at iteration 820 : 0.061783261597156525
Loss at iteration 830 : 0.07896044850349426
Loss at iteration 840 : 0.09869839996099472
Loss at iteration 850 : 0.08744415640830994
Loss at iteration 860 : 0.1071987897157669
Loss at iteration 870 : 0.10090713202953339
Loss at iteration 880 : 0.0522640161216259
Loss at iteration 890 : 0.05153841897845268
Loss at iteration 900 : 0.0832684263586998
Loss at iteration 910 : 0.06310833990573883
Loss at iteration 920 : 0.09101016819477081
Loss at iteration 930 : 0.06305801123380661
Loss at iteration 940 : 0.10019185394048691
Loss at iteration 950 : 0.0531824491918087
Loss at iteration 960 : 0.05760743468999863
Loss at iteration 970 : 0.05260913074016571
Loss at iteration 980 : 0.10541234910488129
Loss at iteration 990 : 0.07998515665531158
Loss at iteration 1000 : 0.10645245015621185
Loss at iteration 1010 : 0.10873328149318695
Loss at iteration 1020 : 0.07261167466640472
Loss at iteration 1030 : 0.08117201924324036
Loss at iteration 1040 : 0.06329543888568878
Loss at iteration 1050 : 0.08656272292137146
Loss at iteration 1060 : 0.06961087882518768
Loss at iteration 1070 : 0.05612283945083618
Loss at iteration 1080 : 0.08431067317724228
Loss at iteration 1090 : 0.06690233945846558
Loss at iteration 1100 : 0.049812667071819305
Loss at iteration 1110 : 0.10190054029226303
Loss at iteration 1120 : 0.07827417552471161
Loss at iteration 1130 : 0.05246686935424805
Loss at iteration 1140 : 0.0617913156747818
Loss at iteration 1150 : 0.07311109453439713
Loss at iteration 1160 : 0.0381622239947319
Loss at iteration 1170 : 0.10876503586769104
Loss at iteration 1180 : 0.10309556126594543
Loss at iteration 1190 : 0.08984019607305527
Loss at iteration 1200 : 0.06934111565351486
Loss at iteration 1210 : 0.07318919152021408
The SSIM Value is: 0.7154236773649851
The PSNR Value is: 20.92938372294108
the epoch is: 121
Loss at iteration 10 : 0.10073964297771454
Loss at iteration 20 : 0.10717721283435822
Loss at iteration 30 : 0.1274770200252533
Loss at iteration 40 : 0.12266606092453003
Loss at iteration 50 : 0.08211690187454224
Loss at iteration 60 : 0.06990118324756622
Loss at iteration 70 : 0.11225809901952744
Loss at iteration 80 : 0.06983568519353867
Loss at iteration 90 : 0.06159883737564087
Loss at iteration 100 : 0.046987831592559814
Loss at iteration 110 : 0.10412552952766418
Loss at iteration 120 : 0.11246957629919052
Loss at iteration 130 : 0.07745411247015
Loss at iteration 140 : 0.1152716875076294
Loss at iteration 150 : 0.09454578161239624
Loss at iteration 160 : 0.042525988072156906
Loss at iteration 170 : 0.0682922825217247
Loss at iteration 180 : 0.0663810595870018
Loss at iteration 190 : 0.08654843270778656
Loss at iteration 200 : 0.07814526557922363
Loss at iteration 210 : 0.09871086478233337
Loss at iteration 220 : 0.06008116900920868
Loss at iteration 230 : 0.07560917735099792
Loss at iteration 240 : 0.07031518220901489
Loss at iteration 250 : 0.12539103627204895
Loss at iteration 260 : 0.06485728919506073
Loss at iteration 270 : 0.11395331472158432
Loss at iteration 280 : 0.09914153814315796
Loss at iteration 290 : 0.0776946097612381
Loss at iteration 300 : 0.06541149318218231
Loss at iteration 310 : 0.04276071488857269
Loss at iteration 320 : 0.0686737596988678
Loss at iteration 330 : 0.09154844284057617
Loss at iteration 340 : 0.06010357290506363
Loss at iteration 350 : 0.08377903699874878
Loss at iteration 360 : 0.058218829333782196
Loss at iteration 370 : 0.04374852403998375
Loss at iteration 380 : 0.08920209854841232
Loss at iteration 390 : 0.08021559566259384
Loss at iteration 400 : 0.052145056426525116
Loss at iteration 410 : 0.10727240890264511
Loss at iteration 420 : 0.09283101558685303
Loss at iteration 430 : 0.06966736167669296
Loss at iteration 440 : 0.06300609558820724
Loss at iteration 450 : 0.06376826763153076
Loss at iteration 460 : 0.2525799572467804
Loss at iteration 470 : 0.0558483712375164
Loss at iteration 480 : 0.09713797271251678
Loss at iteration 490 : 0.06533336639404297
Loss at iteration 500 : 0.07199500501155853
Loss at iteration 510 : 0.07799073308706284
Loss at iteration 520 : 0.08761497586965561
Loss at iteration 530 : 0.0829019770026207
Loss at iteration 540 : 0.10647722333669662
Loss at iteration 550 : 0.08038213849067688
Loss at iteration 560 : 0.10909926891326904
Loss at iteration 570 : 0.0812835693359375
Loss at iteration 580 : 0.11974091827869415
Loss at iteration 590 : 0.0789271742105484
Loss at iteration 600 : 0.08366714417934418
Loss at iteration 610 : 0.09560485929250717
Loss at iteration 620 : 0.08104712516069412
Loss at iteration 630 : 0.08343039453029633
Loss at iteration 640 : 0.058061640709638596
Loss at iteration 650 : 0.0570962093770504
Loss at iteration 660 : 0.061136405915021896
Loss at iteration 670 : 0.0901985689997673
Loss at iteration 680 : 0.10944396257400513
Loss at iteration 690 : 0.08065846562385559
Loss at iteration 700 : 0.07014525681734085
Loss at iteration 710 : 0.0932445228099823
Loss at iteration 720 : 0.09558584541082382
Loss at iteration 730 : 0.06883038580417633
Loss at iteration 740 : 0.1503410041332245
Loss at iteration 750 : 0.11470995843410492
Loss at iteration 760 : 0.06175568327307701
Loss at iteration 770 : 0.09140682220458984
Loss at iteration 780 : 0.06993904709815979
Loss at iteration 790 : 0.10533475875854492
Loss at iteration 800 : 0.06629659980535507
Loss at iteration 810 : 0.09163153171539307
Loss at iteration 820 : 0.08474145829677582
Loss at iteration 830 : 0.09577278792858124
Loss at iteration 840 : 0.06910842657089233
Loss at iteration 850 : 0.08596721291542053
Loss at iteration 860 : 0.0496835932135582
Loss at iteration 870 : 0.10867836326360703
Loss at iteration 880 : 0.06196272373199463
Loss at iteration 890 : 0.09516280889511108
Loss at iteration 900 : 0.057628896087408066
Loss at iteration 910 : 0.1163458451628685
Loss at iteration 920 : 0.08808829635381699
Loss at iteration 930 : 0.1360703706741333
Loss at iteration 940 : 0.055673301219940186
Loss at iteration 950 : 0.09395607560873032
Loss at iteration 960 : 0.06279449909925461
Loss at iteration 970 : 0.0610809288918972
Loss at iteration 980 : 0.10694423317909241
Loss at iteration 990 : 0.09157806634902954
Loss at iteration 1000 : 0.06638212502002716
Loss at iteration 1010 : 0.09866597503423691
Loss at iteration 1020 : 0.07780236005783081
Loss at iteration 1030 : 0.0773487314581871
Loss at iteration 1040 : 0.05611791834235191
Loss at iteration 1050 : 0.07877586781978607
Loss at iteration 1060 : 0.05752059817314148
Loss at iteration 1070 : 0.0828796848654747
Loss at iteration 1080 : 0.1422707587480545
Loss at iteration 1090 : 0.041041646152734756
Loss at iteration 1100 : 0.04966924712061882
Loss at iteration 1110 : 0.07717639952898026
Loss at iteration 1120 : 0.08482136577367783
Loss at iteration 1130 : 0.0778375193476677
Loss at iteration 1140 : 0.1271423101425171
Loss at iteration 1150 : 0.11641934514045715
Loss at iteration 1160 : 0.04857175052165985
Loss at iteration 1170 : 0.08718231320381165
Loss at iteration 1180 : 0.08565045893192291
Loss at iteration 1190 : 0.07661746442317963
Loss at iteration 1200 : 0.0706089437007904
Loss at iteration 1210 : 0.06497414410114288
The SSIM Value is: 0.7130648533503214
The PSNR Value is: 20.9284423828125
the epoch is: 122
Loss at iteration 10 : 0.09698106348514557
Loss at iteration 20 : 0.0748383104801178
Loss at iteration 30 : 0.09096935391426086
Loss at iteration 40 : 0.13970281183719635
Loss at iteration 50 : 0.09901836514472961
Loss at iteration 60 : 0.12021900713443756
Loss at iteration 70 : 0.10839434713125229
Loss at iteration 80 : 0.10985156148672104
Loss at iteration 90 : 0.11449085175991058
Loss at iteration 100 : 0.07045434415340424
Loss at iteration 110 : 0.10533851385116577
Loss at iteration 120 : 0.08514086902141571
Loss at iteration 130 : 0.12914639711380005
Loss at iteration 140 : 0.0893944501876831
Loss at iteration 150 : 0.08888748288154602
Loss at iteration 160 : 0.08803393691778183
Loss at iteration 170 : 0.0750407800078392
Loss at iteration 180 : 0.08557205647230148
Loss at iteration 190 : 0.06024227291345596
Loss at iteration 200 : 0.12600475549697876
Loss at iteration 210 : 0.09161075204610825
Loss at iteration 220 : 0.060755111277103424
Loss at iteration 230 : 0.046372659504413605
Loss at iteration 240 : 0.08853360265493393
Loss at iteration 250 : 0.1133083626627922
Loss at iteration 260 : 0.09603746235370636
Loss at iteration 270 : 0.07463527470827103
Loss at iteration 280 : 0.09241442382335663
Loss at iteration 290 : 0.12409805506467819
Loss at iteration 300 : 0.08439308404922485
Loss at iteration 310 : 0.0847432017326355
Loss at iteration 320 : 0.09604914486408234
Loss at iteration 330 : 0.09436188638210297
Loss at iteration 340 : 0.10241448879241943
Loss at iteration 350 : 0.06325599551200867
Loss at iteration 360 : 0.06696277111768723
Loss at iteration 370 : 0.09111040085554123
Loss at iteration 380 : 0.09078358113765717
Loss at iteration 390 : 0.06845061480998993
Loss at iteration 400 : 0.1386285126209259
Loss at iteration 410 : 0.10133443772792816
Loss at iteration 420 : 0.07493165135383606
Loss at iteration 430 : 0.07695787400007248
Loss at iteration 440 : 0.10491503775119781
Loss at iteration 450 : 0.06684201210737228
Loss at iteration 460 : 0.09929899871349335
Loss at iteration 470 : 0.05612154304981232
Loss at iteration 480 : 0.06169435381889343
Loss at iteration 490 : 0.06920600682497025
Loss at iteration 500 : 0.07284238934516907
Loss at iteration 510 : 0.0860256776213646
Loss at iteration 520 : 0.06178654730319977
Loss at iteration 530 : 0.06747862696647644
Loss at iteration 540 : 0.09805917739868164
Loss at iteration 550 : 0.04961127042770386
Loss at iteration 560 : 0.08732329308986664
Loss at iteration 570 : 0.09840412437915802
Loss at iteration 580 : 0.07903759181499481
Loss at iteration 590 : 0.08553596585988998
Loss at iteration 600 : 0.06893572211265564
Loss at iteration 610 : 0.0805828720331192
Loss at iteration 620 : 0.07836315780878067
Loss at iteration 630 : 0.0659308135509491
Loss at iteration 640 : 0.04849701374769211
Loss at iteration 650 : 0.06922738999128342
Loss at iteration 660 : 0.08884774148464203
Loss at iteration 670 : 0.07787030935287476
Loss at iteration 680 : 0.06227058172225952
Loss at iteration 690 : 0.10581588745117188
Loss at iteration 700 : 0.10160331428050995
Loss at iteration 710 : 0.06854558736085892
Loss at iteration 720 : 0.09460310637950897
Loss at iteration 730 : 0.05821710079908371
Loss at iteration 740 : 0.09021145105361938
Loss at iteration 750 : 0.11040415614843369
Loss at iteration 760 : 0.09402242302894592
Loss at iteration 770 : 0.12600527703762054
Loss at iteration 780 : 0.05523878335952759
Loss at iteration 790 : 0.04489975795149803
Loss at iteration 800 : 0.11930501461029053
Loss at iteration 810 : 0.08294779062271118
Loss at iteration 820 : 0.08342339843511581
Loss at iteration 830 : 0.09112483263015747
Loss at iteration 840 : 0.1619528830051422
Loss at iteration 850 : 0.0910300463438034
Loss at iteration 860 : 0.06708911806344986
Loss at iteration 870 : 0.12117123603820801
Loss at iteration 880 : 0.07499035447835922
Loss at iteration 890 : 0.07333236187696457
Loss at iteration 900 : 0.04085073620080948
Loss at iteration 910 : 0.09980623424053192
Loss at iteration 920 : 0.07474766671657562
Loss at iteration 930 : 0.049539707601070404
Loss at iteration 940 : 0.050604552030563354
Loss at iteration 950 : 0.058841075748205185
Loss at iteration 960 : 0.07060945779085159
Loss at iteration 970 : 0.05776136368513107
Loss at iteration 980 : 0.07000737637281418
Loss at iteration 990 : 0.07654373347759247
Loss at iteration 1000 : 0.07402811199426651
Loss at iteration 1010 : 0.10820282995700836
Loss at iteration 1020 : 0.0683036595582962
Loss at iteration 1030 : 0.09118868410587311
Loss at iteration 1040 : 0.06703554838895798
Loss at iteration 1050 : 0.111289381980896
Loss at iteration 1060 : 0.07519754767417908
Loss at iteration 1070 : 0.09367525577545166
Loss at iteration 1080 : 0.11631488054990768
Loss at iteration 1090 : 0.1414332389831543
Loss at iteration 1100 : 0.06650669127702713
Loss at iteration 1110 : 0.0697425901889801
Loss at iteration 1120 : 0.062421880662441254
Loss at iteration 1130 : 0.089499831199646
Loss at iteration 1140 : 0.11489316076040268
Loss at iteration 1150 : 0.06272268295288086
Loss at iteration 1160 : 0.09429913759231567
Loss at iteration 1170 : 0.08385062217712402
Loss at iteration 1180 : 0.08953347057104111
Loss at iteration 1190 : 0.0775633156299591
Loss at iteration 1200 : 0.08015955984592438
Loss at iteration 1210 : 0.06173264980316162
The SSIM Value is: 0.7078288078308106
The PSNR Value is: 20.4804536819458
the epoch is: 123
Loss at iteration 10 : 0.06922496855258942
Loss at iteration 20 : 0.0962146520614624
Loss at iteration 30 : 0.15484409034252167
Loss at iteration 40 : 0.08397459238767624
Loss at iteration 50 : 0.0856514722108841
Loss at iteration 60 : 0.05062217637896538
Loss at iteration 70 : 0.07175695151090622
Loss at iteration 80 : 0.06874069571495056
Loss at iteration 90 : 0.059569768607616425
Loss at iteration 100 : 0.09207314252853394
Loss at iteration 110 : 0.09275901317596436
Loss at iteration 120 : 0.03797149285674095
Loss at iteration 130 : 0.15443941950798035
Loss at iteration 140 : 0.08989356458187103
Loss at iteration 150 : 0.05274378880858421
Loss at iteration 160 : 0.07396948337554932
Loss at iteration 170 : 0.06449174880981445
Loss at iteration 180 : 0.057545244693756104
Loss at iteration 190 : 0.09799777716398239
Loss at iteration 200 : 0.057878248393535614
Loss at iteration 210 : 0.10283428430557251
Loss at iteration 220 : 0.07172316312789917
Loss at iteration 230 : 0.08214570581912994
Loss at iteration 240 : 0.06455843895673752
Loss at iteration 250 : 0.07245996594429016
Loss at iteration 260 : 0.06548916548490524
Loss at iteration 270 : 0.054395705461502075
Loss at iteration 280 : 0.0702594518661499
Loss at iteration 290 : 0.06440983712673187
Loss at iteration 300 : 0.11632182449102402
Loss at iteration 310 : 0.07123225927352905
Loss at iteration 320 : 0.0609615258872509
Loss at iteration 330 : 0.11115235090255737
Loss at iteration 340 : 0.053689144551754
Loss at iteration 350 : 0.060742270201444626
Loss at iteration 360 : 0.09232775866985321
Loss at iteration 370 : 0.0849531888961792
Loss at iteration 380 : 0.0956355556845665
Loss at iteration 390 : 0.06916096806526184
Loss at iteration 400 : 0.09971654415130615
Loss at iteration 410 : 0.07983286678791046
Loss at iteration 420 : 0.08237342536449432
Loss at iteration 430 : 0.09330436587333679
Loss at iteration 440 : 0.07431134581565857
Loss at iteration 450 : 0.06616424024105072
Loss at iteration 460 : 0.07471673935651779
Loss at iteration 470 : 0.09575765579938889
Loss at iteration 480 : 0.052983224391937256
Loss at iteration 490 : 0.07094265520572662
Loss at iteration 500 : 0.10198947042226791
Loss at iteration 510 : 0.057091496884822845
Loss at iteration 520 : 0.07139873504638672
Loss at iteration 530 : 0.10005184262990952
Loss at iteration 540 : 0.07189545035362244
Loss at iteration 550 : 0.09573265165090561
Loss at iteration 560 : 0.06897535175085068
Loss at iteration 570 : 0.06413821876049042
Loss at iteration 580 : 0.12755000591278076
Loss at iteration 590 : 0.06769847124814987
Loss at iteration 600 : 0.052663080394268036
Loss at iteration 610 : 0.08072352409362793
Loss at iteration 620 : 0.0602869987487793
Loss at iteration 630 : 0.09069141000509262
Loss at iteration 640 : 0.09942375123500824
Loss at iteration 650 : 0.06776861101388931
Loss at iteration 660 : 0.13402578234672546
Loss at iteration 670 : 0.09086383879184723
Loss at iteration 680 : 0.04311845824122429
Loss at iteration 690 : 0.08329170197248459
Loss at iteration 700 : 0.10713442414999008
Loss at iteration 710 : 0.08054649829864502
Loss at iteration 720 : 0.06882432848215103
Loss at iteration 730 : 0.06751944124698639
Loss at iteration 740 : 0.06510771811008453
Loss at iteration 750 : 0.12089147418737411
Loss at iteration 760 : 0.07508880645036697
Loss at iteration 770 : 0.10184036940336227
Loss at iteration 780 : 0.06148010119795799
Loss at iteration 790 : 0.044308140873909
Loss at iteration 800 : 0.06955292820930481
Loss at iteration 810 : 0.08335044234991074
Loss at iteration 820 : 0.09523656964302063
Loss at iteration 830 : 0.06277848780155182
Loss at iteration 840 : 0.09587844461202621
Loss at iteration 850 : 0.10005636513233185
Loss at iteration 860 : 0.09260361641645432
Loss at iteration 870 : 0.1013619601726532
Loss at iteration 880 : 0.08040565997362137
Loss at iteration 890 : 0.10856235027313232
Loss at iteration 900 : 0.0418749675154686
Loss at iteration 910 : 0.05813685059547424
Loss at iteration 920 : 0.0915784239768982
Loss at iteration 930 : 0.05951472371816635
Loss at iteration 940 : 0.08314034342765808
Loss at iteration 950 : 0.0825754702091217
Loss at iteration 960 : 0.08572747558355331
Loss at iteration 970 : 0.0970393568277359
Loss at iteration 980 : 0.15672320127487183
Loss at iteration 990 : 0.09395813196897507
Loss at iteration 1000 : 0.07914192974567413
Loss at iteration 1010 : 0.09486021846532822
Loss at iteration 1020 : 0.09334328770637512
Loss at iteration 1030 : 0.10049288719892502
Loss at iteration 1040 : 0.0960729718208313
Loss at iteration 1050 : 0.06507282704114914
Loss at iteration 1060 : 0.1224222332239151
Loss at iteration 1070 : 0.08300457894802094
Loss at iteration 1080 : 0.07743914425373077
Loss at iteration 1090 : 0.11101607978343964
Loss at iteration 1100 : 0.13183891773223877
Loss at iteration 1110 : 0.1368316411972046
Loss at iteration 1120 : 0.09565050899982452
Loss at iteration 1130 : 0.06547888368368149
Loss at iteration 1140 : 0.10034941136837006
Loss at iteration 1150 : 0.06309808790683746
Loss at iteration 1160 : 0.0815059021115303
Loss at iteration 1170 : 0.11529386043548584
Loss at iteration 1180 : 0.06389163434505463
Loss at iteration 1190 : 0.09668518602848053
Loss at iteration 1200 : 0.1064702570438385
Loss at iteration 1210 : 0.07982446253299713
The SSIM Value is: 0.7176530639330546
The PSNR Value is: 21.109403228759767
the epoch is: 124
Loss at iteration 10 : 0.12227612733840942
Loss at iteration 20 : 0.08291871845722198
Loss at iteration 30 : 0.08010625094175339
Loss at iteration 40 : 0.06965425610542297
Loss at iteration 50 : 0.060409873723983765
Loss at iteration 60 : 0.07635845243930817
Loss at iteration 70 : 0.07926853001117706
Loss at iteration 80 : 0.0750146210193634
Loss at iteration 90 : 0.09248688071966171
Loss at iteration 100 : 0.09117783606052399
Loss at iteration 110 : 0.08875676244497299
Loss at iteration 120 : 0.053406935185194016
Loss at iteration 130 : 0.05430468171834946
Loss at iteration 140 : 0.10044322907924652
Loss at iteration 150 : 0.06240696832537651
Loss at iteration 160 : 0.11418112367391586
Loss at iteration 170 : 0.07570703327655792
Loss at iteration 180 : 0.08709385246038437
Loss at iteration 190 : 0.108244389295578
Loss at iteration 200 : 0.04269394278526306
Loss at iteration 210 : 0.1007138341665268
Loss at iteration 220 : 0.07112854719161987
Loss at iteration 230 : 0.06465363502502441
Loss at iteration 240 : 0.09587188810110092
Loss at iteration 250 : 0.09139488637447357
Loss at iteration 260 : 0.07116656005382538
Loss at iteration 270 : 0.07119989395141602
Loss at iteration 280 : 0.08723654597997665
Loss at iteration 290 : 0.08875342458486557
Loss at iteration 300 : 0.05649874731898308
Loss at iteration 310 : 0.09586714208126068
Loss at iteration 320 : 0.06346653401851654
Loss at iteration 330 : 0.07778739929199219
Loss at iteration 340 : 0.09567783772945404
Loss at iteration 350 : 0.15003380179405212
Loss at iteration 360 : 0.050992876291275024
Loss at iteration 370 : 0.07891914248466492
Loss at iteration 380 : 0.0463249534368515
Loss at iteration 390 : 0.10497959703207016
Loss at iteration 400 : 0.11019586026668549
Loss at iteration 410 : 0.09016862511634827
Loss at iteration 420 : 0.10750102996826172
Loss at iteration 430 : 0.0715065523982048
Loss at iteration 440 : 0.09495099633932114
Loss at iteration 450 : 0.08713513612747192
Loss at iteration 460 : 0.10186728090047836
Loss at iteration 470 : 0.06399048864841461
Loss at iteration 480 : 0.14858248829841614
Loss at iteration 490 : 0.08106818795204163
Loss at iteration 500 : 0.08794127404689789
Loss at iteration 510 : 0.0498630627989769
Loss at iteration 520 : 0.0662912055850029
Loss at iteration 530 : 0.049590613692998886
Loss at iteration 540 : 0.07429772615432739
Loss at iteration 550 : 0.06549793481826782
Loss at iteration 560 : 0.10539626330137253
Loss at iteration 570 : 0.06747211515903473
Loss at iteration 580 : 0.08207090944051743
Loss at iteration 590 : 0.08605939149856567
Loss at iteration 600 : 0.05462587624788284
Loss at iteration 610 : 0.10812598466873169
Loss at iteration 620 : 0.106881283223629
Loss at iteration 630 : 0.12988457083702087
Loss at iteration 640 : 0.07010733336210251
Loss at iteration 650 : 0.061571359634399414
Loss at iteration 660 : 0.10602401196956635
Loss at iteration 670 : 0.062129147350788116
Loss at iteration 680 : 0.07330629229545593
Loss at iteration 690 : 0.05488688498735428
Loss at iteration 700 : 0.05649735778570175
Loss at iteration 710 : 0.11023493856191635
Loss at iteration 720 : 0.10098138451576233
Loss at iteration 730 : 0.06768853962421417
Loss at iteration 740 : 0.08580078929662704
Loss at iteration 750 : 0.08550846576690674
Loss at iteration 760 : 0.08046908676624298
Loss at iteration 770 : 0.05376459285616875
Loss at iteration 780 : 0.08440513908863068
Loss at iteration 790 : 0.08585049957036972
Loss at iteration 800 : 0.05751334875822067
Loss at iteration 810 : 0.09282305091619492
Loss at iteration 820 : 0.12783733010292053
Loss at iteration 830 : 0.09055900573730469
Loss at iteration 840 : 0.08873125165700912
Loss at iteration 850 : 0.08098786324262619
Loss at iteration 860 : 0.07970067113637924
Loss at iteration 870 : 0.15287822484970093
Loss at iteration 880 : 0.058015376329422
Loss at iteration 890 : 0.05320022627711296
Loss at iteration 900 : 0.09673343598842621
Loss at iteration 910 : 0.09057238698005676
Loss at iteration 920 : 0.06495986878871918
Loss at iteration 930 : 0.11474296450614929
Loss at iteration 940 : 0.06803823262453079
Loss at iteration 950 : 0.12012531608343124
Loss at iteration 960 : 0.07293997704982758
Loss at iteration 970 : 0.06997867673635483
Loss at iteration 980 : 0.04647769778966904
Loss at iteration 990 : 0.10516701638698578
Loss at iteration 1000 : 0.08140607178211212
Loss at iteration 1010 : 0.07138437032699585
Loss at iteration 1020 : 0.056295912712812424
Loss at iteration 1030 : 0.06350946426391602
Loss at iteration 1040 : 0.0987730324268341
Loss at iteration 1050 : 0.05398301035165787
Loss at iteration 1060 : 0.09717150777578354
Loss at iteration 1070 : 0.07583512365818024
Loss at iteration 1080 : 0.0785394161939621
Loss at iteration 1090 : 0.059844594448804855
Loss at iteration 1100 : 0.08626145124435425
Loss at iteration 1110 : 0.09490828961133957
Loss at iteration 1120 : 0.0733068436384201
Loss at iteration 1130 : 0.10305912792682648
Loss at iteration 1140 : 0.09110663831233978
Loss at iteration 1150 : 0.07700159400701523
Loss at iteration 1160 : 0.06300278007984161
Loss at iteration 1170 : 0.0781160295009613
Loss at iteration 1180 : 0.10894754528999329
Loss at iteration 1190 : 0.07619471848011017
Loss at iteration 1200 : 0.08205833286046982
Loss at iteration 1210 : 0.11977094411849976
The SSIM Value is: 0.7090129236380259
The PSNR Value is: 20.810317039489746
the epoch is: 125
Loss at iteration 10 : 0.06877192854881287
Loss at iteration 20 : 0.08625876158475876
Loss at iteration 30 : 0.09106491506099701
Loss at iteration 40 : 0.08584457635879517
Loss at iteration 50 : 0.09229815006256104
Loss at iteration 60 : 0.09433802962303162
Loss at iteration 70 : 0.08453287184238434
Loss at iteration 80 : 0.09322138875722885
Loss at iteration 90 : 0.08025593310594559
Loss at iteration 100 : 0.09330543130636215
Loss at iteration 110 : 0.05528946965932846
Loss at iteration 120 : 0.06339608877897263
Loss at iteration 130 : 0.09782476723194122
Loss at iteration 140 : 0.09555377811193466
Loss at iteration 150 : 0.061152808368206024
Loss at iteration 160 : 0.0805373415350914
Loss at iteration 170 : 0.056209858506917953
Loss at iteration 180 : 0.07116350531578064
Loss at iteration 190 : 0.08328153938055038
Loss at iteration 200 : 0.04888361319899559
Loss at iteration 210 : 0.121776282787323
Loss at iteration 220 : 0.07943230867385864
Loss at iteration 230 : 0.06802751868963242
Loss at iteration 240 : 0.0646473616361618
Loss at iteration 250 : 0.10998840630054474
Loss at iteration 260 : 0.13094711303710938
Loss at iteration 270 : 0.0697154626250267
Loss at iteration 280 : 0.1295507848262787
Loss at iteration 290 : 0.04640916734933853
Loss at iteration 300 : 0.05931726098060608
Loss at iteration 310 : 0.12915396690368652
Loss at iteration 320 : 0.10865740478038788
Loss at iteration 330 : 0.06994664669036865
Loss at iteration 340 : 0.10042093694210052
Loss at iteration 350 : 0.10038667172193527
Loss at iteration 360 : 0.06603004038333893
Loss at iteration 370 : 0.06777997314929962
Loss at iteration 380 : 0.0947670191526413
Loss at iteration 390 : 0.08817785233259201
Loss at iteration 400 : 0.10152830928564072
Loss at iteration 410 : 0.1210712417960167
Loss at iteration 420 : 0.07134785503149033
Loss at iteration 430 : 0.06647670269012451
Loss at iteration 440 : 0.09043462574481964
Loss at iteration 450 : 0.08543284982442856
Loss at iteration 460 : 0.06370370090007782
Loss at iteration 470 : 0.06754867732524872
Loss at iteration 480 : 0.06439482420682907
Loss at iteration 490 : 0.05858733877539635
Loss at iteration 500 : 0.09928850829601288
Loss at iteration 510 : 0.0823470801115036
Loss at iteration 520 : 0.08226364105939865
Loss at iteration 530 : 0.09165690839290619
Loss at iteration 540 : 0.11156700551509857
Loss at iteration 550 : 0.06460072100162506
Loss at iteration 560 : 0.08028675615787506
Loss at iteration 570 : 0.07598407566547394
Loss at iteration 580 : 0.08188214898109436
Loss at iteration 590 : 0.09524530172348022
Loss at iteration 600 : 0.05234653875231743
Loss at iteration 610 : 0.07545214891433716
Loss at iteration 620 : 0.04063481092453003
Loss at iteration 630 : 0.03613375872373581
Loss at iteration 640 : 0.05675148963928223
Loss at iteration 650 : 0.09949075430631638
Loss at iteration 660 : 0.12380319833755493
Loss at iteration 670 : 0.06138753145933151
Loss at iteration 680 : 0.06165487319231033
Loss at iteration 690 : 0.08420783281326294
Loss at iteration 700 : 0.08403706550598145
Loss at iteration 710 : 0.054132722318172455
Loss at iteration 720 : 0.11752711236476898
Loss at iteration 730 : 0.0934554934501648
Loss at iteration 740 : 0.07864966243505478
Loss at iteration 750 : 0.07872682809829712
Loss at iteration 760 : 0.09484530985355377
Loss at iteration 770 : 0.1175396665930748
Loss at iteration 780 : 0.11079408973455429
Loss at iteration 790 : 0.0589810386300087
Loss at iteration 800 : 0.06156932935118675
Loss at iteration 810 : 0.10648223012685776
Loss at iteration 820 : 0.11088879406452179
Loss at iteration 830 : 0.046367891132831573
Loss at iteration 840 : 0.0688106119632721
Loss at iteration 850 : 0.09139853715896606
Loss at iteration 860 : 0.1090887188911438
Loss at iteration 870 : 0.08936667442321777
Loss at iteration 880 : 0.08096277713775635
Loss at iteration 890 : 0.06818485260009766
Loss at iteration 900 : 0.04789454862475395
Loss at iteration 910 : 0.1472470760345459
Loss at iteration 920 : 0.07379023730754852
Loss at iteration 930 : 0.12611597776412964
Loss at iteration 940 : 0.09901672601699829
Loss at iteration 950 : 0.08962889015674591
Loss at iteration 960 : 0.058040551841259
Loss at iteration 970 : 0.05345304310321808
Loss at iteration 980 : 0.08969142287969589
Loss at iteration 990 : 0.07821854948997498
Loss at iteration 1000 : 0.10357765853404999
Loss at iteration 1010 : 0.08330625295639038
Loss at iteration 1020 : 0.09548814594745636
Loss at iteration 1030 : 0.06316761672496796
Loss at iteration 1040 : 0.08358362317085266
Loss at iteration 1050 : 0.09829612821340561
Loss at iteration 1060 : 0.06367850303649902
Loss at iteration 1070 : 0.06852558255195618
Loss at iteration 1080 : 0.057075731456279755
Loss at iteration 1090 : 0.10449620336294174
Loss at iteration 1100 : 0.09621413052082062
Loss at iteration 1110 : 0.09968620538711548
Loss at iteration 1120 : 0.055047567933797836
Loss at iteration 1130 : 0.09983694553375244
Loss at iteration 1140 : 0.07555428147315979
Loss at iteration 1150 : 0.10170869529247284
Loss at iteration 1160 : 0.10335303843021393
Loss at iteration 1170 : 0.09897904098033905
Loss at iteration 1180 : 0.08297280967235565
Loss at iteration 1190 : 0.04791397973895073
Loss at iteration 1200 : 0.0841173380613327
Loss at iteration 1210 : 0.08121363818645477
The SSIM Value is: 0.7122751692930858
The PSNR Value is: 21.078036308288574
the epoch is: 126
Loss at iteration 10 : 0.10181628167629242
Loss at iteration 20 : 0.06509637832641602
Loss at iteration 30 : 0.06777828931808472
Loss at iteration 40 : 0.08324863016605377
Loss at iteration 50 : 0.11172887682914734
Loss at iteration 60 : 0.12232904136180878
Loss at iteration 70 : 0.07816334068775177
Loss at iteration 80 : 0.044139012694358826
Loss at iteration 90 : 0.0770585834980011
Loss at iteration 100 : 0.11842914670705795
Loss at iteration 110 : 0.07270751893520355
Loss at iteration 120 : 0.04440227150917053
Loss at iteration 130 : 0.0666150152683258
Loss at iteration 140 : 0.12375269830226898
Loss at iteration 150 : 0.07326894253492355
Loss at iteration 160 : 0.09641653299331665
Loss at iteration 170 : 0.05414896458387375
Loss at iteration 180 : 0.07057996839284897
Loss at iteration 190 : 0.09119939804077148
Loss at iteration 200 : 0.07664769887924194
Loss at iteration 210 : 0.10236706584692001
Loss at iteration 220 : 0.06992759555578232
Loss at iteration 230 : 0.09346368163824081
Loss at iteration 240 : 0.06829041242599487
Loss at iteration 250 : 0.11040930449962616
Loss at iteration 260 : 0.08752477169036865
Loss at iteration 270 : 0.09880213439464569
Loss at iteration 280 : 0.05213990435004234
Loss at iteration 290 : 0.09311293065547943
Loss at iteration 300 : 0.0988897904753685
Loss at iteration 310 : 0.11766325682401657
Loss at iteration 320 : 0.06219012290239334
Loss at iteration 330 : 0.07245534658432007
Loss at iteration 340 : 0.09833499789237976
Loss at iteration 350 : 0.06341477483510971
Loss at iteration 360 : 0.09012597799301147
Loss at iteration 370 : 0.11263172328472137
Loss at iteration 380 : 0.08038261532783508
Loss at iteration 390 : 0.09801334142684937
Loss at iteration 400 : 0.09042638540267944
Loss at iteration 410 : 0.09515216201543808
Loss at iteration 420 : 0.08808978646993637
Loss at iteration 430 : 0.07523427903652191
Loss at iteration 440 : 0.09102772176265717
Loss at iteration 450 : 0.09814111888408661
Loss at iteration 460 : 0.11701411008834839
Loss at iteration 470 : 0.1133417934179306
Loss at iteration 480 : 0.14589305222034454
Loss at iteration 490 : 0.060959309339523315
Loss at iteration 500 : 0.07525327801704407
Loss at iteration 510 : 0.14392030239105225
Loss at iteration 520 : 0.09451793134212494
Loss at iteration 530 : 0.05171988531947136
Loss at iteration 540 : 0.09266211837530136
Loss at iteration 550 : 0.10244518518447876
Loss at iteration 560 : 0.06986555457115173
Loss at iteration 570 : 0.11897405236959457
Loss at iteration 580 : 0.06886687874794006
Loss at iteration 590 : 0.11142699420452118
Loss at iteration 600 : 0.10016044229269028
Loss at iteration 610 : 0.05885237455368042
Loss at iteration 620 : 0.048845075070858
Loss at iteration 630 : 0.10725297033786774
Loss at iteration 640 : 0.08920943737030029
Loss at iteration 650 : 0.07369820028543472
Loss at iteration 660 : 0.07712036371231079
Loss at iteration 670 : 0.11305008828639984
Loss at iteration 680 : 0.08781695365905762
Loss at iteration 690 : 0.06214063614606857
Loss at iteration 700 : 0.07714013755321503
Loss at iteration 710 : 0.040509458631277084
Loss at iteration 720 : 0.07189139723777771
Loss at iteration 730 : 0.051189325749874115
Loss at iteration 740 : 0.0959089994430542
Loss at iteration 750 : 0.08483041822910309
Loss at iteration 760 : 0.07091544568538666
Loss at iteration 770 : 0.10489361733198166
Loss at iteration 780 : 0.08107338100671768
Loss at iteration 790 : 0.06724792718887329
Loss at iteration 800 : 0.06690441071987152
Loss at iteration 810 : 0.06208847463130951
Loss at iteration 820 : 0.0820111557841301
Loss at iteration 830 : 0.08278554677963257
Loss at iteration 840 : 0.11777442693710327
Loss at iteration 850 : 0.06770309805870056
Loss at iteration 860 : 0.06235840916633606
Loss at iteration 870 : 0.09110116958618164
Loss at iteration 880 : 0.09346266835927963
Loss at iteration 890 : 0.1006627231836319
Loss at iteration 900 : 0.08640124648809433
Loss at iteration 910 : 0.051660291850566864
Loss at iteration 920 : 0.0740019679069519
Loss at iteration 930 : 0.07904380559921265
Loss at iteration 940 : 0.13416768610477448
Loss at iteration 950 : 0.10347023606300354
Loss at iteration 960 : 0.08664574474096298
Loss at iteration 970 : 0.06515023857355118
Loss at iteration 980 : 0.10112170875072479
Loss at iteration 990 : 0.08075301349163055
Loss at iteration 1000 : 0.09633614867925644
Loss at iteration 1010 : 0.07842834293842316
Loss at iteration 1020 : 0.09108954668045044
Loss at iteration 1030 : 0.1407032608985901
Loss at iteration 1040 : 0.07843256741762161
Loss at iteration 1050 : 0.07458406686782837
Loss at iteration 1060 : 0.06632321327924728
Loss at iteration 1070 : 0.0750371664762497
Loss at iteration 1080 : 0.1369297355413437
Loss at iteration 1090 : 0.06708680838346481
Loss at iteration 1100 : 0.10147635638713837
Loss at iteration 1110 : 0.09881278872489929
Loss at iteration 1120 : 0.12997019290924072
Loss at iteration 1130 : 0.04164321348071098
Loss at iteration 1140 : 0.06311905384063721
Loss at iteration 1150 : 0.09513193368911743
Loss at iteration 1160 : 0.07826943695545197
Loss at iteration 1170 : 0.11313649266958237
Loss at iteration 1180 : 0.1037735864520073
Loss at iteration 1190 : 0.06257469952106476
Loss at iteration 1200 : 0.1075274795293808
Loss at iteration 1210 : 0.09445272386074066
The SSIM Value is: 0.7182484308878581
The PSNR Value is: 21.30040512084961
the epoch is: 127
Loss at iteration 10 : 0.07017014920711517
Loss at iteration 20 : 0.08429484069347382
Loss at iteration 30 : 0.10506050288677216
Loss at iteration 40 : 0.13480240106582642
Loss at iteration 50 : 0.07887846231460571
Loss at iteration 60 : 0.10190385580062866
Loss at iteration 70 : 0.06459839642047882
Loss at iteration 80 : 0.0849817618727684
Loss at iteration 90 : 0.06348584592342377
Loss at iteration 100 : 0.05657871812582016
Loss at iteration 110 : 0.07451849430799484
Loss at iteration 120 : 0.10574875771999359
Loss at iteration 130 : 0.058205537497997284
Loss at iteration 140 : 0.052261482924222946
Loss at iteration 150 : 0.06298679113388062
Loss at iteration 160 : 0.08199445903301239
Loss at iteration 170 : 0.12343611568212509
Loss at iteration 180 : 0.06559544801712036
Loss at iteration 190 : 0.11229901015758514
Loss at iteration 200 : 0.09057866036891937
Loss at iteration 210 : 0.08661042153835297
Loss at iteration 220 : 0.07071834802627563
Loss at iteration 230 : 0.049620620906353
Loss at iteration 240 : 0.05114644765853882
Loss at iteration 250 : 0.11894183605909348
Loss at iteration 260 : 0.09859083592891693
Loss at iteration 270 : 0.10239428281784058
Loss at iteration 280 : 0.04585399851202965
Loss at iteration 290 : 0.10762389749288559
Loss at iteration 300 : 0.05671314895153046
Loss at iteration 310 : 0.10213150829076767
Loss at iteration 320 : 0.06397058814764023
Loss at iteration 330 : 0.05368247628211975
Loss at iteration 340 : 0.11771389842033386
Loss at iteration 350 : 0.05841957777738571
Loss at iteration 360 : 0.07330521196126938
Loss at iteration 370 : 0.07064884155988693
Loss at iteration 380 : 0.08385652303695679
Loss at iteration 390 : 0.07535868883132935
Loss at iteration 400 : 0.04178428649902344
Loss at iteration 410 : 0.07663048803806305
Loss at iteration 420 : 0.11842995136976242
Loss at iteration 430 : 0.06835278868675232
Loss at iteration 440 : 0.048576515167951584
Loss at iteration 450 : 0.04911012575030327
Loss at iteration 460 : 0.072968490421772
Loss at iteration 470 : 0.09372278302907944
Loss at iteration 480 : 0.12060990184545517
Loss at iteration 490 : 0.056741636246442795
Loss at iteration 500 : 0.1043083593249321
Loss at iteration 510 : 0.07498466968536377
Loss at iteration 520 : 0.08100543171167374
Loss at iteration 530 : 0.0635838508605957
Loss at iteration 540 : 0.09198775142431259
Loss at iteration 550 : 0.11093195527791977
Loss at iteration 560 : 0.05254256725311279
Loss at iteration 570 : 0.06923821568489075
Loss at iteration 580 : 0.08913567662239075
Loss at iteration 590 : 0.10282643884420395
Loss at iteration 600 : 0.08663233369588852
Loss at iteration 610 : 0.053939446806907654
Loss at iteration 620 : 0.10097672790288925
Loss at iteration 630 : 0.08044940233230591
Loss at iteration 640 : 0.09005551785230637
Loss at iteration 650 : 0.086971715092659
Loss at iteration 660 : 0.07622753828763962
Loss at iteration 670 : 0.08661679923534393
Loss at iteration 680 : 0.07536846399307251
Loss at iteration 690 : 0.08837287127971649
Loss at iteration 700 : 0.10926970094442368
Loss at iteration 710 : 0.08933120965957642
Loss at iteration 720 : 0.049449555575847626
Loss at iteration 730 : 0.08615924417972565
Loss at iteration 740 : 0.07842176407575607
Loss at iteration 750 : 0.06767094880342484
Loss at iteration 760 : 0.09710242599248886
Loss at iteration 770 : 0.08731101453304291
Loss at iteration 780 : 0.07198924571275711
Loss at iteration 790 : 0.0468493215739727
Loss at iteration 800 : 0.10033070296049118
Loss at iteration 810 : 0.1148674339056015
Loss at iteration 820 : 0.0704214796423912
Loss at iteration 830 : 0.09738212823867798
Loss at iteration 840 : 0.08268263190984726
Loss at iteration 850 : 0.08565293252468109
Loss at iteration 860 : 0.09766058623790741
Loss at iteration 870 : 0.054759956896305084
Loss at iteration 880 : 0.10938596725463867
Loss at iteration 890 : 0.07519666105508804
Loss at iteration 900 : 0.0451999306678772
Loss at iteration 910 : 0.04389375448226929
Loss at iteration 920 : 0.05349602550268173
Loss at iteration 930 : 0.057834215462207794
Loss at iteration 940 : 0.07514485716819763
Loss at iteration 950 : 0.06906133145093918
Loss at iteration 960 : 0.10348902642726898
Loss at iteration 970 : 0.0758635401725769
Loss at iteration 980 : 0.0832781195640564
Loss at iteration 990 : 0.11652161926031113
Loss at iteration 1000 : 0.09452927112579346
Loss at iteration 1010 : 0.09493526071310043
Loss at iteration 1020 : 0.06730080395936966
Loss at iteration 1030 : 0.05700398609042168
Loss at iteration 1040 : 0.06661668419837952
Loss at iteration 1050 : 0.1070374995470047
Loss at iteration 1060 : 0.1147611066699028
Loss at iteration 1070 : 0.11383484303951263
Loss at iteration 1080 : 0.06858128309249878
Loss at iteration 1090 : 0.09184533357620239
Loss at iteration 1100 : 0.09186717122793198
Loss at iteration 1110 : 0.08820357918739319
Loss at iteration 1120 : 0.06706377863883972
Loss at iteration 1130 : 0.08382313698530197
Loss at iteration 1140 : 0.11139748990535736
Loss at iteration 1150 : 0.054126814007759094
Loss at iteration 1160 : 0.0925813540816307
Loss at iteration 1170 : 0.05773574113845825
Loss at iteration 1180 : 0.10198195278644562
Loss at iteration 1190 : 0.05277910828590393
Loss at iteration 1200 : 0.15414509177207947
Loss at iteration 1210 : 0.0639413446187973
The SSIM Value is: 0.7025031507015228
The PSNR Value is: 20.27522004445394
the epoch is: 128
Loss at iteration 10 : 0.10897701233625412
Loss at iteration 20 : 0.10702233016490936
Loss at iteration 30 : 0.11323212087154388
Loss at iteration 40 : 0.08790688961744308
Loss at iteration 50 : 0.10642832517623901
Loss at iteration 60 : 0.07426958531141281
Loss at iteration 70 : 0.0929051861166954
Loss at iteration 80 : 0.0674859881401062
Loss at iteration 90 : 0.10096482932567596
Loss at iteration 100 : 0.06067650765180588
Loss at iteration 110 : 0.07814690470695496
Loss at iteration 120 : 0.09132842719554901
Loss at iteration 130 : 0.06356114894151688
Loss at iteration 140 : 0.056374676525592804
Loss at iteration 150 : 0.04604212939739227
Loss at iteration 160 : 0.08421933650970459
Loss at iteration 170 : 0.08037543296813965
Loss at iteration 180 : 0.08060497790575027
Loss at iteration 190 : 0.07290396839380264
Loss at iteration 200 : 0.08684086799621582
Loss at iteration 210 : 0.11226850748062134
Loss at iteration 220 : 0.09517129510641098
Loss at iteration 230 : 0.059991877526044846
Loss at iteration 240 : 0.07235892117023468
Loss at iteration 250 : 0.07080959528684616
Loss at iteration 260 : 0.06867565214633942
Loss at iteration 270 : 0.07787346839904785
Loss at iteration 280 : 0.05460738763213158
Loss at iteration 290 : 0.1158791035413742
Loss at iteration 300 : 0.07873402535915375
Loss at iteration 310 : 0.09856629371643066
Loss at iteration 320 : 0.07831913232803345
Loss at iteration 330 : 0.04973564296960831
Loss at iteration 340 : 0.1001448854804039
Loss at iteration 350 : 0.04311556741595268
Loss at iteration 360 : 0.06677328050136566
Loss at iteration 370 : 0.06230415031313896
Loss at iteration 380 : 0.0662689357995987
Loss at iteration 390 : 0.06795516610145569
Loss at iteration 400 : 0.04545314237475395
Loss at iteration 410 : 0.09003311395645142
Loss at iteration 420 : 0.08134493231773376
Loss at iteration 430 : 0.06286419183015823
Loss at iteration 440 : 0.11252550780773163
Loss at iteration 450 : 0.08699768781661987
Loss at iteration 460 : 0.07998941838741302
Loss at iteration 470 : 0.1073627695441246
Loss at iteration 480 : 0.04998425766825676
Loss at iteration 490 : 0.0828566700220108
Loss at iteration 500 : 0.0862068459391594
Loss at iteration 510 : 0.10882014036178589
Loss at iteration 520 : 0.06505310535430908
Loss at iteration 530 : 0.10244598984718323
Loss at iteration 540 : 0.07013387233018875
Loss at iteration 550 : 0.08232918381690979
Loss at iteration 560 : 0.08179201185703278
Loss at iteration 570 : 0.10305242240428925
Loss at iteration 580 : 0.12010984122753143
Loss at iteration 590 : 0.07411587238311768
Loss at iteration 600 : 0.08679812401533127
Loss at iteration 610 : 0.06889796257019043
Loss at iteration 620 : 0.07954226434230804
Loss at iteration 630 : 0.08122926205396652
Loss at iteration 640 : 0.07830046117305756
Loss at iteration 650 : 0.09867781400680542
Loss at iteration 660 : 0.07670747488737106
Loss at iteration 670 : 0.04551976919174194
Loss at iteration 680 : 0.09490607678890228
Loss at iteration 690 : 0.09479734301567078
Loss at iteration 700 : 0.05938228219747543
Loss at iteration 710 : 0.07615557312965393
Loss at iteration 720 : 0.0486820749938488
Loss at iteration 730 : 0.03958939015865326
Loss at iteration 740 : 0.11442353576421738
Loss at iteration 750 : 0.09934870898723602
Loss at iteration 760 : 0.07185807824134827
Loss at iteration 770 : 0.05150468274950981
Loss at iteration 780 : 0.06459382176399231
Loss at iteration 790 : 0.07205025106668472
Loss at iteration 800 : 0.09930890798568726
Loss at iteration 810 : 0.07012319564819336
Loss at iteration 820 : 0.06381478160619736
Loss at iteration 830 : 0.11143790185451508
Loss at iteration 840 : 0.0741339772939682
Loss at iteration 850 : 0.049813129007816315
Loss at iteration 860 : 0.1047961562871933
Loss at iteration 870 : 0.05758313089609146
Loss at iteration 880 : 0.08271028846502304
Loss at iteration 890 : 0.10707032680511475
Loss at iteration 900 : 0.04596637934446335
Loss at iteration 910 : 0.07762277126312256
Loss at iteration 920 : 0.0386902317404747
Loss at iteration 930 : 0.12359137833118439
Loss at iteration 940 : 0.060487739741802216
Loss at iteration 950 : 0.08019973337650299
Loss at iteration 960 : 0.0480223149061203
Loss at iteration 970 : 0.09892141073942184
Loss at iteration 980 : 0.0755484402179718
Loss at iteration 990 : 0.04626940190792084
Loss at iteration 1000 : 0.057109128683805466
Loss at iteration 1010 : 0.07928171753883362
Loss at iteration 1020 : 0.0626160055398941
Loss at iteration 1030 : 0.08013027906417847
Loss at iteration 1040 : 0.08193191885948181
Loss at iteration 1050 : 0.09698250889778137
Loss at iteration 1060 : 0.07443227618932724
Loss at iteration 1070 : 0.09072236716747284
Loss at iteration 1080 : 0.08797574043273926
Loss at iteration 1090 : 0.11040600389242172
Loss at iteration 1100 : 0.08204653114080429
Loss at iteration 1110 : 0.06910590827465057
Loss at iteration 1120 : 0.07271343469619751
Loss at iteration 1130 : 0.1050606295466423
Loss at iteration 1140 : 0.08655479550361633
Loss at iteration 1150 : 0.07864396274089813
Loss at iteration 1160 : 0.076363705098629
Loss at iteration 1170 : 0.08427175134420395
Loss at iteration 1180 : 0.10778011381626129
Loss at iteration 1190 : 0.10998594760894775
Loss at iteration 1200 : 0.07330150902271271
Loss at iteration 1210 : 0.10263751447200775
The SSIM Value is: 0.7108704467614492
The PSNR Value is: 20.64089838663737
the epoch is: 129
Loss at iteration 10 : 0.11423718929290771
Loss at iteration 20 : 0.054119404405355453
Loss at iteration 30 : 0.13050013780593872
Loss at iteration 40 : 0.08673595637083054
Loss at iteration 50 : 0.10190578550100327
Loss at iteration 60 : 0.07033359259366989
Loss at iteration 70 : 0.06115340441465378
Loss at iteration 80 : 0.06870627403259277
Loss at iteration 90 : 0.07612523436546326
Loss at iteration 100 : 0.06791180372238159
Loss at iteration 110 : 0.12999016046524048
Loss at iteration 120 : 0.06494979560375214
Loss at iteration 130 : 0.09619887918233871
Loss at iteration 140 : 0.1369188129901886
Loss at iteration 150 : 0.07662338018417358
Loss at iteration 160 : 0.11487863957881927
Loss at iteration 170 : 0.06965595483779907
Loss at iteration 180 : 0.12405316531658173
Loss at iteration 190 : 0.06969430297613144
Loss at iteration 200 : 0.076295405626297
Loss at iteration 210 : 0.07343366742134094
Loss at iteration 220 : 0.06950515508651733
Loss at iteration 230 : 0.07410605251789093
Loss at iteration 240 : 0.06206512451171875
Loss at iteration 250 : 0.12418192625045776
Loss at iteration 260 : 0.061713915318250656
Loss at iteration 270 : 0.05842924863100052
Loss at iteration 280 : 0.06606005132198334
Loss at iteration 290 : 0.11599768698215485
Loss at iteration 300 : 0.10122287273406982
Loss at iteration 310 : 0.10721154510974884
Loss at iteration 320 : 0.14349199831485748
Loss at iteration 330 : 0.10142427682876587
Loss at iteration 340 : 0.07556822896003723
Loss at iteration 350 : 0.11055344343185425
Loss at iteration 360 : 0.06229680776596069
Loss at iteration 370 : 0.08870755136013031
Loss at iteration 380 : 0.090859055519104
Loss at iteration 390 : 0.07952553778886795
Loss at iteration 400 : 0.04153332859277725
Loss at iteration 410 : 0.09901010990142822
Loss at iteration 420 : 0.08732142299413681
Loss at iteration 430 : 0.11436998099088669
Loss at iteration 440 : 0.10969050973653793
Loss at iteration 450 : 0.07202980667352676
Loss at iteration 460 : 0.08736305683851242
Loss at iteration 470 : 0.09709277749061584
Loss at iteration 480 : 0.11788032948970795
Loss at iteration 490 : 0.06315448880195618
Loss at iteration 500 : 0.0715162605047226
Loss at iteration 510 : 0.09102971851825714
Loss at iteration 520 : 0.07213029265403748
Loss at iteration 530 : 0.0947219729423523
Loss at iteration 540 : 0.0887053832411766
Loss at iteration 550 : 0.14644138514995575
Loss at iteration 560 : 0.0883878692984581
Loss at iteration 570 : 0.07937950640916824
Loss at iteration 580 : 0.06183170527219772
Loss at iteration 590 : 0.08956220000982285
Loss at iteration 600 : 0.050443388521671295
Loss at iteration 610 : 0.07658548653125763
Loss at iteration 620 : 0.07394914329051971
Loss at iteration 630 : 0.07958248257637024
Loss at iteration 640 : 0.12142521142959595
Loss at iteration 650 : 0.057921670377254486
Loss at iteration 660 : 0.05408277362585068
Loss at iteration 670 : 0.07339103519916534
Loss at iteration 680 : 0.10173064470291138
Loss at iteration 690 : 0.09520082175731659
Loss at iteration 700 : 0.06394045054912567
Loss at iteration 710 : 0.08199939131736755
Loss at iteration 720 : 0.08649461716413498
Loss at iteration 730 : 0.07837015390396118
Loss at iteration 740 : 0.07093839347362518
Loss at iteration 750 : 0.06254412978887558
Loss at iteration 760 : 0.06061611697077751
Loss at iteration 770 : 0.0953141450881958
Loss at iteration 780 : 0.08322817832231522
Loss at iteration 790 : 0.0861053317785263
Loss at iteration 800 : 0.05681585520505905
Loss at iteration 810 : 0.05625641345977783
Loss at iteration 820 : 0.11090275645256042
Loss at iteration 830 : 0.06949332356452942
Loss at iteration 840 : 0.0666215568780899
Loss at iteration 850 : 0.11170648038387299
Loss at iteration 860 : 0.08922986686229706
Loss at iteration 870 : 0.08528744429349899
Loss at iteration 880 : 0.056963127106428146
Loss at iteration 890 : 0.08127336204051971
Loss at iteration 900 : 0.06982699036598206
Loss at iteration 910 : 0.06241079419851303
Loss at iteration 920 : 0.08680514991283417
Loss at iteration 930 : 0.08444752544164658
Loss at iteration 940 : 0.11417310684919357
Loss at iteration 950 : 0.07441606372594833
Loss at iteration 960 : 0.06965445727109909
Loss at iteration 970 : 0.055158354341983795
Loss at iteration 980 : 0.08499237149953842
Loss at iteration 990 : 0.12959852814674377
Loss at iteration 1000 : 0.05484588444232941
Loss at iteration 1010 : 0.07763465493917465
Loss at iteration 1020 : 0.05580989643931389
Loss at iteration 1030 : 0.05670399218797684
Loss at iteration 1040 : 0.12051856517791748
Loss at iteration 1050 : 0.04553775116801262
Loss at iteration 1060 : 0.09735517203807831
Loss at iteration 1070 : 0.06908362358808517
Loss at iteration 1080 : 0.08614607155323029
Loss at iteration 1090 : 0.11156028509140015
Loss at iteration 1100 : 0.07371675968170166
Loss at iteration 1110 : 0.08091045916080475
Loss at iteration 1120 : 0.055944181978702545
Loss at iteration 1130 : 0.062446966767311096
Loss at iteration 1140 : 0.08426046371459961
Loss at iteration 1150 : 0.11041828244924545
Loss at iteration 1160 : 0.06022879481315613
Loss at iteration 1170 : 0.1157887876033783
Loss at iteration 1180 : 0.11685516685247421
Loss at iteration 1190 : 0.04841767996549606
Loss at iteration 1200 : 0.126742884516716
Loss at iteration 1210 : 0.12333260476589203
The SSIM Value is: 0.7125445346037547
The PSNR Value is: 20.910469436645506
the epoch is: 130
Loss at iteration 10 : 0.04967458173632622
Loss at iteration 20 : 0.0495680570602417
Loss at iteration 30 : 0.10237159579992294
Loss at iteration 40 : 0.09126581251621246
Loss at iteration 50 : 0.12758976221084595
Loss at iteration 60 : 0.07966989278793335
Loss at iteration 70 : 0.07731619477272034
Loss at iteration 80 : 0.10272689908742905
Loss at iteration 90 : 0.09459768235683441
Loss at iteration 100 : 0.11729785799980164
Loss at iteration 110 : 0.08134069293737411
Loss at iteration 120 : 0.1110226958990097
Loss at iteration 130 : 0.09340324252843857
Loss at iteration 140 : 0.07200700044631958
Loss at iteration 150 : 0.09663838148117065
Loss at iteration 160 : 0.10022561997175217
Loss at iteration 170 : 0.06766761839389801
Loss at iteration 180 : 0.09957997500896454
Loss at iteration 190 : 0.08345729112625122
Loss at iteration 200 : 0.09250490367412567
Loss at iteration 210 : 0.0900903195142746
Loss at iteration 220 : 0.06552092730998993
Loss at iteration 230 : 0.07328029721975327
Loss at iteration 240 : 0.10333799570798874
Loss at iteration 250 : 0.08869700878858566
Loss at iteration 260 : 0.05397200584411621
Loss at iteration 270 : 0.09728065133094788
Loss at iteration 280 : 0.10220488160848618
Loss at iteration 290 : 0.11201886832714081
Loss at iteration 300 : 0.08132847398519516
Loss at iteration 310 : 0.043915972113609314
Loss at iteration 320 : 0.06172015890479088
Loss at iteration 330 : 0.06900341063737869
Loss at iteration 340 : 0.0984988808631897
Loss at iteration 350 : 0.11755895614624023
Loss at iteration 360 : 0.07002484798431396
Loss at iteration 370 : 0.09728460013866425
Loss at iteration 380 : 0.09550623595714569
Loss at iteration 390 : 0.07496877014636993
Loss at iteration 400 : 0.08244754374027252
Loss at iteration 410 : 0.13181689381599426
Loss at iteration 420 : 0.06186245009303093
Loss at iteration 430 : 0.06443586945533752
Loss at iteration 440 : 0.04434092342853546
Loss at iteration 450 : 0.05136409401893616
Loss at iteration 460 : 0.06783778220415115
Loss at iteration 470 : 0.055148571729660034
Loss at iteration 480 : 0.13201865553855896
Loss at iteration 490 : 0.07189296185970306
Loss at iteration 500 : 0.07353170961141586
Loss at iteration 510 : 0.06646332889795303
Loss at iteration 520 : 0.07335000485181808
Loss at iteration 530 : 0.07833506911993027
Loss at iteration 540 : 0.09331824630498886
Loss at iteration 550 : 0.07253794372081757
Loss at iteration 560 : 0.062129318714141846
Loss at iteration 570 : 0.11834590882062912
Loss at iteration 580 : 0.05169358476996422
Loss at iteration 590 : 0.07020732760429382
Loss at iteration 600 : 0.07351836562156677
Loss at iteration 610 : 0.07784584164619446
Loss at iteration 620 : 0.06560797989368439
Loss at iteration 630 : 0.06407936662435532
Loss at iteration 640 : 0.12136032432317734
Loss at iteration 650 : 0.07521659880876541
Loss at iteration 660 : 0.09648080915212631
Loss at iteration 670 : 0.08994579315185547
Loss at iteration 680 : 0.11277695745229721
Loss at iteration 690 : 0.08170881122350693
Loss at iteration 700 : 0.07877979427576065
Loss at iteration 710 : 0.05737706646323204
Loss at iteration 720 : 0.08199984580278397
Loss at iteration 730 : 0.07251409441232681
Loss at iteration 740 : 0.08945374190807343
Loss at iteration 750 : 0.09374165534973145
Loss at iteration 760 : 0.07454626262187958
Loss at iteration 770 : 0.06157975643873215
Loss at iteration 780 : 0.06263060867786407
Loss at iteration 790 : 0.11945834010839462
Loss at iteration 800 : 0.05323074012994766
Loss at iteration 810 : 0.0553259402513504
Loss at iteration 820 : 0.06382840126752853
Loss at iteration 830 : 0.04863101243972778
Loss at iteration 840 : 0.10844789445400238
Loss at iteration 850 : 0.06849691271781921
Loss at iteration 860 : 0.05515671893954277
Loss at iteration 870 : 0.07035784423351288
Loss at iteration 880 : 0.11447802186012268
Loss at iteration 890 : 0.0730888843536377
Loss at iteration 900 : 0.11784292757511139
Loss at iteration 910 : 0.05806039273738861
Loss at iteration 920 : 0.08915141224861145
Loss at iteration 930 : 0.08925961703062057
Loss at iteration 940 : 0.09083668887615204
Loss at iteration 950 : 0.07802696526050568
Loss at iteration 960 : 0.04556259512901306
Loss at iteration 970 : 0.08500039577484131
Loss at iteration 980 : 0.06126348674297333
Loss at iteration 990 : 0.07860121876001358
Loss at iteration 1000 : 0.08844470977783203
Loss at iteration 1010 : 0.06098516285419464
Loss at iteration 1020 : 0.10167956352233887
Loss at iteration 1030 : 0.0977698415517807
Loss at iteration 1040 : 0.09662522375583649
Loss at iteration 1050 : 0.09541039913892746
Loss at iteration 1060 : 0.04224025458097458
Loss at iteration 1070 : 0.1249963790178299
Loss at iteration 1080 : 0.0918157622218132
Loss at iteration 1090 : 0.0745021253824234
Loss at iteration 1100 : 0.09614431858062744
Loss at iteration 1110 : 0.07764305174350739
Loss at iteration 1120 : 0.09696116298437119
Loss at iteration 1130 : 0.08738430589437485
Loss at iteration 1140 : 0.08226576447486877
Loss at iteration 1150 : 0.09390725195407867
Loss at iteration 1160 : 0.10378074645996094
Loss at iteration 1170 : 0.15214690566062927
Loss at iteration 1180 : 0.047255419194698334
Loss at iteration 1190 : 0.1276516318321228
Loss at iteration 1200 : 0.0936087891459465
Loss at iteration 1210 : 0.05797130987048149
The SSIM Value is: 0.7179361442724864
The PSNR Value is: 21.06772435506185
the epoch is: 131
Loss at iteration 10 : 0.11014513671398163
Loss at iteration 20 : 0.10198938101530075
Loss at iteration 30 : 0.09154175221920013
Loss at iteration 40 : 0.10636976361274719
Loss at iteration 50 : 0.08002476394176483
Loss at iteration 60 : 0.08771674335002899
Loss at iteration 70 : 0.10992761701345444
Loss at iteration 80 : 0.07078107446432114
Loss at iteration 90 : 0.11248819530010223
Loss at iteration 100 : 0.051712702959775925
Loss at iteration 110 : 0.0631740540266037
Loss at iteration 120 : 0.08994251489639282
Loss at iteration 130 : 0.063895583152771
Loss at iteration 140 : 0.10684185475111008
Loss at iteration 150 : 0.10005825757980347
Loss at iteration 160 : 0.09284788370132446
Loss at iteration 170 : 0.0633167177438736
Loss at iteration 180 : 0.04828766733407974
Loss at iteration 190 : 0.0838233083486557
Loss at iteration 200 : 0.04851800203323364
Loss at iteration 210 : 0.04139998182654381
Loss at iteration 220 : 0.04245607182383537
Loss at iteration 230 : 0.09022809565067291
Loss at iteration 240 : 0.0773874968290329
Loss at iteration 250 : 0.09792622178792953
Loss at iteration 260 : 0.11931922286748886
Loss at iteration 270 : 0.09323225915431976
Loss at iteration 280 : 0.08232643455266953
Loss at iteration 290 : 0.06498389691114426
Loss at iteration 300 : 0.07679930329322815
Loss at iteration 310 : 0.09363164752721786
Loss at iteration 320 : 0.07696093618869781
Loss at iteration 330 : 0.07664632797241211
Loss at iteration 340 : 0.09611380100250244
Loss at iteration 350 : 0.06108963117003441
Loss at iteration 360 : 0.098191998898983
Loss at iteration 370 : 0.1234082505106926
Loss at iteration 380 : 0.06104514002799988
Loss at iteration 390 : 0.10883757472038269
Loss at iteration 400 : 0.06091434881091118
Loss at iteration 410 : 0.07981877774000168
Loss at iteration 420 : 0.0907328650355339
Loss at iteration 430 : 0.10977502167224884
Loss at iteration 440 : 0.12314771115779877
Loss at iteration 450 : 0.08871379494667053
Loss at iteration 460 : 0.05892936512827873
Loss at iteration 470 : 0.08297456800937653
Loss at iteration 480 : 0.08345457911491394
Loss at iteration 490 : 0.08546661585569382
Loss at iteration 500 : 0.07639827579259872
Loss at iteration 510 : 0.08799062669277191
Loss at iteration 520 : 0.06445347517728806
Loss at iteration 530 : 0.08494498580694199
Loss at iteration 540 : 0.09873415529727936
Loss at iteration 550 : 0.06960510462522507
Loss at iteration 560 : 0.05573321133852005
Loss at iteration 570 : 0.06549545377492905
Loss at iteration 580 : 0.11782502382993698
Loss at iteration 590 : 0.06653464585542679
Loss at iteration 600 : 0.07640966773033142
Loss at iteration 610 : 0.057945869863033295
Loss at iteration 620 : 0.14512017369270325
Loss at iteration 630 : 0.0725812166929245
Loss at iteration 640 : 0.14832648634910583
Loss at iteration 650 : 0.10612264275550842
Loss at iteration 660 : 0.1055619865655899
Loss at iteration 670 : 0.13991449773311615
Loss at iteration 680 : 0.09210221469402313
Loss at iteration 690 : 0.11354763805866241
Loss at iteration 700 : 0.08115169405937195
Loss at iteration 710 : 0.07692977786064148
Loss at iteration 720 : 0.08801029622554779
Loss at iteration 730 : 0.09525784105062485
Loss at iteration 740 : 0.110319584608078
Loss at iteration 750 : 0.05432778596878052
Loss at iteration 760 : 0.06610162556171417
Loss at iteration 770 : 0.04805117845535278
Loss at iteration 780 : 0.10476192831993103
Loss at iteration 790 : 0.04958784580230713
Loss at iteration 800 : 0.07122659683227539
Loss at iteration 810 : 0.04803842306137085
Loss at iteration 820 : 0.11595813930034637
Loss at iteration 830 : 0.11815591901540756
Loss at iteration 840 : 0.055731724947690964
Loss at iteration 850 : 0.09961031377315521
Loss at iteration 860 : 0.10815265029668808
Loss at iteration 870 : 0.08021056652069092
Loss at iteration 880 : 0.05882221460342407
Loss at iteration 890 : 0.08421455323696136
Loss at iteration 900 : 0.11314350366592407
Loss at iteration 910 : 0.128505676984787
Loss at iteration 920 : 0.08208124339580536
Loss at iteration 930 : 0.09582792222499847
Loss at iteration 940 : 0.1414121836423874
Loss at iteration 950 : 0.061617664992809296
Loss at iteration 960 : 0.0966026782989502
Loss at iteration 970 : 0.10899212211370468
Loss at iteration 980 : 0.08549849689006805
Loss at iteration 990 : 0.0805780440568924
Loss at iteration 1000 : 0.07662968337535858
Loss at iteration 1010 : 0.09481241554021835
Loss at iteration 1020 : 0.12330424785614014
Loss at iteration 1030 : 0.053141653537750244
Loss at iteration 1040 : 0.0971149429678917
Loss at iteration 1050 : 0.060896776616573334
Loss at iteration 1060 : 0.07785782217979431
Loss at iteration 1070 : 0.16759726405143738
Loss at iteration 1080 : 0.15292927622795105
Loss at iteration 1090 : 0.05099160596728325
Loss at iteration 1100 : 0.06916747987270355
Loss at iteration 1110 : 0.052271466702222824
Loss at iteration 1120 : 0.09268325567245483
Loss at iteration 1130 : 0.058485131710767746
Loss at iteration 1140 : 0.04682512208819389
Loss at iteration 1150 : 0.1158510372042656
Loss at iteration 1160 : 0.06654277443885803
Loss at iteration 1170 : 0.08807218074798584
Loss at iteration 1180 : 0.1147463321685791
Loss at iteration 1190 : 0.1154094934463501
Loss at iteration 1200 : 0.10110364854335785
Loss at iteration 1210 : 0.07109736651182175
The SSIM Value is: 0.7120515584945679
The PSNR Value is: 20.70368220011393
the epoch is: 132
Loss at iteration 10 : 0.11931133270263672
Loss at iteration 20 : 0.057295262813568115
Loss at iteration 30 : 0.08855879306793213
Loss at iteration 40 : 0.08717331290245056
Loss at iteration 50 : 0.08652283996343613
Loss at iteration 60 : 0.08081334829330444
Loss at iteration 70 : 0.07779974490404129
Loss at iteration 80 : 0.1314328908920288
Loss at iteration 90 : 0.10584041476249695
Loss at iteration 100 : 0.058892808854579926
Loss at iteration 110 : 0.059903644025325775
Loss at iteration 120 : 0.07634981721639633
Loss at iteration 130 : 0.05178108066320419
Loss at iteration 140 : 0.0572502426803112
Loss at iteration 150 : 0.0614333413541317
Loss at iteration 160 : 0.06049361452460289
Loss at iteration 170 : 0.07442322373390198
Loss at iteration 180 : 0.08835988491773605
Loss at iteration 190 : 0.10083457827568054
Loss at iteration 200 : 0.06939883530139923
Loss at iteration 210 : 0.08847242593765259
Loss at iteration 220 : 0.09254684299230576
Loss at iteration 230 : 0.08103271573781967
Loss at iteration 240 : 0.11314833164215088
Loss at iteration 250 : 0.07494200766086578
Loss at iteration 260 : 0.0678592324256897
Loss at iteration 270 : 0.06781765818595886
Loss at iteration 280 : 0.0979105532169342
Loss at iteration 290 : 0.055608391761779785
Loss at iteration 300 : 0.0890110582113266
Loss at iteration 310 : 0.08772227168083191
Loss at iteration 320 : 0.08134234696626663
Loss at iteration 330 : 0.054079484194517136
Loss at iteration 340 : 0.09405390918254852
Loss at iteration 350 : 0.07860571146011353
Loss at iteration 360 : 0.04697545990347862
Loss at iteration 370 : 0.1547759622335434
Loss at iteration 380 : 0.11871032416820526
Loss at iteration 390 : 0.05513762682676315
Loss at iteration 400 : 0.0891069769859314
Loss at iteration 410 : 0.09737574309110641
Loss at iteration 420 : 0.09122893214225769
Loss at iteration 430 : 0.050330750644207
Loss at iteration 440 : 0.0471343919634819
Loss at iteration 450 : 0.08151733130216599
Loss at iteration 460 : 0.056006066501140594
Loss at iteration 470 : 0.04857223853468895
Loss at iteration 480 : 0.10962113738059998
Loss at iteration 490 : 0.07429315149784088
Loss at iteration 500 : 0.0645189881324768
Loss at iteration 510 : 0.07882478833198547
Loss at iteration 520 : 0.09835370630025864
Loss at iteration 530 : 0.09628044068813324
Loss at iteration 540 : 0.06326191872358322
Loss at iteration 550 : 0.11908306181430817
Loss at iteration 560 : 0.09313827008008957
Loss at iteration 570 : 0.055285386741161346
Loss at iteration 580 : 0.074445441365242
Loss at iteration 590 : 0.0799141377210617
Loss at iteration 600 : 0.08330602198839188
Loss at iteration 610 : 0.10105518996715546
Loss at iteration 620 : 0.08066242188215256
Loss at iteration 630 : 0.0698515772819519
Loss at iteration 640 : 0.071915403008461
Loss at iteration 650 : 0.07971403002738953
Loss at iteration 660 : 0.04632549732923508
Loss at iteration 670 : 0.07384979724884033
Loss at iteration 680 : 0.046645816415548325
Loss at iteration 690 : 0.08614599704742432
Loss at iteration 700 : 0.07551758736371994
Loss at iteration 710 : 0.0666535422205925
Loss at iteration 720 : 0.09565632045269012
Loss at iteration 730 : 0.10534175485372543
Loss at iteration 740 : 0.08916943520307541
Loss at iteration 750 : 0.09796049445867538
Loss at iteration 760 : 0.051734767854213715
Loss at iteration 770 : 0.04160315915942192
Loss at iteration 780 : 0.14253617823123932
Loss at iteration 790 : 0.09767709672451019
Loss at iteration 800 : 0.04616033658385277
Loss at iteration 810 : 0.09343837201595306
Loss at iteration 820 : 0.07440043985843658
Loss at iteration 830 : 0.09272687882184982
Loss at iteration 840 : 0.06179199367761612
Loss at iteration 850 : 0.09145942330360413
Loss at iteration 860 : 0.09600727260112762
Loss at iteration 870 : 0.07914301753044128
Loss at iteration 880 : 0.06921768188476562
Loss at iteration 890 : 0.10356160998344421
Loss at iteration 900 : 0.0528654009103775
Loss at iteration 910 : 0.06480655074119568
Loss at iteration 920 : 0.10854695737361908
Loss at iteration 930 : 0.07940828055143356
Loss at iteration 940 : 0.10507895797491074
Loss at iteration 950 : 0.05810898542404175
Loss at iteration 960 : 0.061540089547634125
Loss at iteration 970 : 0.08065823465585709
Loss at iteration 980 : 0.08547290414571762
Loss at iteration 990 : 0.08104317635297775
Loss at iteration 1000 : 0.06383726000785828
Loss at iteration 1010 : 0.07423967123031616
Loss at iteration 1020 : 0.11534535884857178
Loss at iteration 1030 : 0.06600812077522278
Loss at iteration 1040 : 0.0885108932852745
Loss at iteration 1050 : 0.07217302918434143
Loss at iteration 1060 : 0.10111217200756073
Loss at iteration 1070 : 0.0895683690905571
Loss at iteration 1080 : 0.13456565141677856
Loss at iteration 1090 : 0.08863341808319092
Loss at iteration 1100 : 0.10674615204334259
Loss at iteration 1110 : 0.11421310901641846
Loss at iteration 1120 : 0.10061608254909515
Loss at iteration 1130 : 0.062168609350919724
Loss at iteration 1140 : 0.07870548218488693
Loss at iteration 1150 : 0.08071985095739365
Loss at iteration 1160 : 0.060284584760665894
Loss at iteration 1170 : 0.12862291932106018
Loss at iteration 1180 : 0.0710369274020195
Loss at iteration 1190 : 0.10268696397542953
Loss at iteration 1200 : 0.061317797750234604
Loss at iteration 1210 : 0.08153318613767624
The SSIM Value is: 0.7151561578114828
The PSNR Value is: 20.945748329162598
the epoch is: 133
Loss at iteration 10 : 0.08582138270139694
Loss at iteration 20 : 0.08678571879863739
Loss at iteration 30 : 0.08255157619714737
Loss at iteration 40 : 0.05523717403411865
Loss at iteration 50 : 0.09564006328582764
Loss at iteration 60 : 0.064329132437706
Loss at iteration 70 : 0.08335661143064499
Loss at iteration 80 : 0.0880860909819603
Loss at iteration 90 : 0.06663350760936737
Loss at iteration 100 : 0.06184351444244385
Loss at iteration 110 : 0.06461361050605774
Loss at iteration 120 : 0.1176455020904541
Loss at iteration 130 : 0.08076189458370209
Loss at iteration 140 : 0.06901894509792328
Loss at iteration 150 : 0.06568516790866852
Loss at iteration 160 : 0.1391354203224182
Loss at iteration 170 : 0.0657578781247139
Loss at iteration 180 : 0.12668640911579132
Loss at iteration 190 : 0.0662258118391037
Loss at iteration 200 : 0.05702430009841919
Loss at iteration 210 : 0.08033325523138046
Loss at iteration 220 : 0.08133295923471451
Loss at iteration 230 : 0.092546246945858
Loss at iteration 240 : 0.11434444785118103
Loss at iteration 250 : 0.06865307688713074
Loss at iteration 260 : 0.09423159807920456
Loss at iteration 270 : 0.07226835191249847
Loss at iteration 280 : 0.06229190155863762
Loss at iteration 290 : 0.10240455716848373
Loss at iteration 300 : 0.06237310171127319
Loss at iteration 310 : 0.07275709509849548
Loss at iteration 320 : 0.07813411951065063
Loss at iteration 330 : 0.08010590821504593
Loss at iteration 340 : 0.08640631288290024
Loss at iteration 350 : 0.13598237931728363
Loss at iteration 360 : 0.11987759917974472
Loss at iteration 370 : 0.07695555686950684
Loss at iteration 380 : 0.060780178755521774
Loss at iteration 390 : 0.0911199301481247
Loss at iteration 400 : 0.07860057055950165
Loss at iteration 410 : 0.10465168207883835
Loss at iteration 420 : 0.09371690452098846
Loss at iteration 430 : 0.06290566921234131
Loss at iteration 440 : 0.09325192868709564
Loss at iteration 450 : 0.08493158221244812
Loss at iteration 460 : 0.06291069835424423
Loss at iteration 470 : 0.0977538526058197
Loss at iteration 480 : 0.10454224795103073
Loss at iteration 490 : 0.12642167508602142
Loss at iteration 500 : 0.07360139489173889
Loss at iteration 510 : 0.12327658385038376
Loss at iteration 520 : 0.10981374979019165
Loss at iteration 530 : 0.05512905865907669
Loss at iteration 540 : 0.05952497571706772
Loss at iteration 550 : 0.0791231170296669
Loss at iteration 560 : 0.07949286699295044
Loss at iteration 570 : 0.11281567811965942
Loss at iteration 580 : 0.05365021154284477
Loss at iteration 590 : 0.09592810273170471
Loss at iteration 600 : 0.06887948513031006
Loss at iteration 610 : 0.14791002869606018
Loss at iteration 620 : 0.06045840308070183
Loss at iteration 630 : 0.08535134792327881
Loss at iteration 640 : 0.07895873486995697
Loss at iteration 650 : 0.10697788000106812
Loss at iteration 660 : 0.10107554495334625
Loss at iteration 670 : 0.08950237929821014
Loss at iteration 680 : 0.07403972744941711
Loss at iteration 690 : 0.07095286250114441
Loss at iteration 700 : 0.06489250063896179
Loss at iteration 710 : 0.09160786867141724
Loss at iteration 720 : 0.0837232694029808
Loss at iteration 730 : 0.0574905201792717
Loss at iteration 740 : 0.09242048114538193
Loss at iteration 750 : 0.09259244054555893
Loss at iteration 760 : 0.08709530532360077
Loss at iteration 770 : 0.07505076378583908
Loss at iteration 780 : 0.06822660565376282
Loss at iteration 790 : 0.08576172590255737
Loss at iteration 800 : 0.07480882108211517
Loss at iteration 810 : 0.07150882482528687
Loss at iteration 820 : 0.07112196832895279
Loss at iteration 830 : 0.07454273104667664
Loss at iteration 840 : 0.09332078695297241
Loss at iteration 850 : 0.11303266137838364
Loss at iteration 860 : 0.12939977645874023
Loss at iteration 870 : 0.10463596880435944
Loss at iteration 880 : 0.046201806515455246
Loss at iteration 890 : 0.07848363369703293
Loss at iteration 900 : 0.061205752193927765
Loss at iteration 910 : 0.09171982854604721
Loss at iteration 920 : 0.2034294605255127
Loss at iteration 930 : 0.09474490582942963
Loss at iteration 940 : 0.07306043058633804
Loss at iteration 950 : 0.048354826867580414
Loss at iteration 960 : 0.05727056786417961
Loss at iteration 970 : 0.056093793362379074
Loss at iteration 980 : 0.11920380592346191
Loss at iteration 990 : 0.06440839916467667
Loss at iteration 1000 : 0.09808071702718735
Loss at iteration 1010 : 0.11508284509181976
Loss at iteration 1020 : 0.06578598916530609
Loss at iteration 1030 : 0.06790880858898163
Loss at iteration 1040 : 0.05284104868769646
Loss at iteration 1050 : 0.08813975751399994
Loss at iteration 1060 : 0.09039583057165146
Loss at iteration 1070 : 0.05031512305140495
Loss at iteration 1080 : 0.0913897380232811
Loss at iteration 1090 : 0.05567176640033722
Loss at iteration 1100 : 0.06456073373556137
Loss at iteration 1110 : 0.08482703566551208
Loss at iteration 1120 : 0.058450616896152496
Loss at iteration 1130 : 0.06496024876832962
Loss at iteration 1140 : 0.08557884395122528
Loss at iteration 1150 : 0.07573221623897552
Loss at iteration 1160 : 0.08712965250015259
Loss at iteration 1170 : 0.09366631507873535
Loss at iteration 1180 : 0.09102022647857666
Loss at iteration 1190 : 0.06038804352283478
Loss at iteration 1200 : 0.06596697866916656
Loss at iteration 1210 : 0.09422790259122849
The SSIM Value is: 0.7147355834643047
The PSNR Value is: 21.054751586914062
the epoch is: 134
Loss at iteration 10 : 0.07686799019575119
Loss at iteration 20 : 0.0994441956281662
Loss at iteration 30 : 0.0587739571928978
Loss at iteration 40 : 0.07020339369773865
Loss at iteration 50 : 0.04728750139474869
Loss at iteration 60 : 0.05442250892519951
Loss at iteration 70 : 0.05375291034579277
Loss at iteration 80 : 0.06789744645357132
Loss at iteration 90 : 0.06930529326200485
Loss at iteration 100 : 0.06521566957235336
Loss at iteration 110 : 0.07931745052337646
Loss at iteration 120 : 0.08911796659231186
Loss at iteration 130 : 0.10457156598567963
Loss at iteration 140 : 0.048914581537246704
Loss at iteration 150 : 0.12958382070064545
Loss at iteration 160 : 0.07870959490537643
Loss at iteration 170 : 0.08645980060100555
Loss at iteration 180 : 0.10859706252813339
Loss at iteration 190 : 0.15304353833198547
Loss at iteration 200 : 0.06465117633342743
Loss at iteration 210 : 0.09649836272001266
Loss at iteration 220 : 0.10406827926635742
Loss at iteration 230 : 0.06559452414512634
Loss at iteration 240 : 0.07883847504854202
Loss at iteration 250 : 0.10916328430175781
Loss at iteration 260 : 0.16135212779045105
Loss at iteration 270 : 0.10117293894290924
Loss at iteration 280 : 0.07908851653337479
Loss at iteration 290 : 0.10000672936439514
Loss at iteration 300 : 0.10165278613567352
Loss at iteration 310 : 0.09542761743068695
Loss at iteration 320 : 0.05913371965289116
Loss at iteration 330 : 0.1022857055068016
Loss at iteration 340 : 0.08441436290740967
Loss at iteration 350 : 0.07696717977523804
Loss at iteration 360 : 0.09782774746417999
Loss at iteration 370 : 0.08626759052276611
Loss at iteration 380 : 0.06158295273780823
Loss at iteration 390 : 0.10024076700210571
Loss at iteration 400 : 0.04853864014148712
Loss at iteration 410 : 0.063783660531044
Loss at iteration 420 : 0.0824061781167984
Loss at iteration 430 : 0.1070331335067749
Loss at iteration 440 : 0.11192049086093903
Loss at iteration 450 : 0.1252770721912384
Loss at iteration 460 : 0.06623509526252747
Loss at iteration 470 : 0.065895676612854
Loss at iteration 480 : 0.10093657672405243
Loss at iteration 490 : 0.059055160731077194
Loss at iteration 500 : 0.13239280879497528
Loss at iteration 510 : 0.063819520175457
Loss at iteration 520 : 0.07704625278711319
Loss at iteration 530 : 0.08850594609975815
Loss at iteration 540 : 0.09634240716695786
Loss at iteration 550 : 0.12199561297893524
Loss at iteration 560 : 0.09780870378017426
Loss at iteration 570 : 0.068177729845047
Loss at iteration 580 : 0.06249634921550751
Loss at iteration 590 : 0.07331692427396774
Loss at iteration 600 : 0.046882182359695435
Loss at iteration 610 : 0.06621932238340378
Loss at iteration 620 : 0.04248660057783127
Loss at iteration 630 : 0.14251570403575897
Loss at iteration 640 : 0.11605654656887054
Loss at iteration 650 : 0.050568096339702606
Loss at iteration 660 : 0.11674913018941879
Loss at iteration 670 : 0.06644158065319061
Loss at iteration 680 : 0.0720413327217102
Loss at iteration 690 : 0.05538123846054077
Loss at iteration 700 : 0.09882569313049316
Loss at iteration 710 : 0.05671324580907822
Loss at iteration 720 : 0.06267472356557846
Loss at iteration 730 : 0.09517520666122437
Loss at iteration 740 : 0.05577439069747925
Loss at iteration 750 : 0.1530151069164276
Loss at iteration 760 : 0.0939347967505455
Loss at iteration 770 : 0.08332344889640808
Loss at iteration 780 : 0.05298071727156639
Loss at iteration 790 : 0.0870290994644165
Loss at iteration 800 : 0.06048319488763809
Loss at iteration 810 : 0.10599403083324432
Loss at iteration 820 : 0.08746488392353058
Loss at iteration 830 : 0.09213822335004807
Loss at iteration 840 : 0.0695594996213913
Loss at iteration 850 : 0.11728217452764511
Loss at iteration 860 : 0.10179203748703003
Loss at iteration 870 : 0.07230168581008911
Loss at iteration 880 : 0.06960557401180267
Loss at iteration 890 : 0.08303406089544296
Loss at iteration 900 : 0.07342249900102615
Loss at iteration 910 : 0.07951551675796509
Loss at iteration 920 : 0.08138956129550934
Loss at iteration 930 : 0.06959056109189987
Loss at iteration 940 : 0.09521150588989258
Loss at iteration 950 : 0.05479981750249863
Loss at iteration 960 : 0.0843384712934494
Loss at iteration 970 : 0.11562208831310272
Loss at iteration 980 : 0.10218989849090576
Loss at iteration 990 : 0.0561697781085968
Loss at iteration 1000 : 0.08957414329051971
Loss at iteration 1010 : 0.09547486156225204
Loss at iteration 1020 : 0.07514480501413345
Loss at iteration 1030 : 0.11647924035787582
Loss at iteration 1040 : 0.07733045518398285
Loss at iteration 1050 : 0.12240918725728989
Loss at iteration 1060 : 0.0740581601858139
Loss at iteration 1070 : 0.06742693483829498
Loss at iteration 1080 : 0.08954374492168427
Loss at iteration 1090 : 0.07283931970596313
Loss at iteration 1100 : 0.12220130860805511
Loss at iteration 1110 : 0.09206154197454453
Loss at iteration 1120 : 0.1412944793701172
Loss at iteration 1130 : 0.07288213074207306
Loss at iteration 1140 : 0.06428135931491852
Loss at iteration 1150 : 0.08705125749111176
Loss at iteration 1160 : 0.09377884864807129
Loss at iteration 1170 : 0.0896139144897461
Loss at iteration 1180 : 0.09918957948684692
Loss at iteration 1190 : 0.1511307954788208
Loss at iteration 1200 : 0.08262169361114502
Loss at iteration 1210 : 0.12780871987342834
The SSIM Value is: 0.7178031663099925
The PSNR Value is: 21.075448735555014
the epoch is: 135
Loss at iteration 10 : 0.10289158672094345
Loss at iteration 20 : 0.08947229385375977
Loss at iteration 30 : 0.060598984360694885
Loss at iteration 40 : 0.08411066979169846
Loss at iteration 50 : 0.09981340169906616
Loss at iteration 60 : 0.07950800657272339
Loss at iteration 70 : 0.09533872455358505
Loss at iteration 80 : 0.074623703956604
Loss at iteration 90 : 0.09216269105672836
Loss at iteration 100 : 0.04863321781158447
Loss at iteration 110 : 0.10381411015987396
Loss at iteration 120 : 0.0895303338766098
Loss at iteration 130 : 0.10859070718288422
Loss at iteration 140 : 0.08979260176420212
Loss at iteration 150 : 0.10681243240833282
Loss at iteration 160 : 0.09930931031703949
Loss at iteration 170 : 0.09201141446828842
Loss at iteration 180 : 0.12583550810813904
Loss at iteration 190 : 0.05616259574890137
Loss at iteration 200 : 0.14541533589363098
Loss at iteration 210 : 0.12116169929504395
Loss at iteration 220 : 0.10053154826164246
Loss at iteration 230 : 0.09685084223747253
Loss at iteration 240 : 0.0647704228758812
Loss at iteration 250 : 0.05806785449385643
Loss at iteration 260 : 0.07628071308135986
Loss at iteration 270 : 0.10049404203891754
Loss at iteration 280 : 0.0736321434378624
Loss at iteration 290 : 0.11358076333999634
Loss at iteration 300 : 0.10215868055820465
Loss at iteration 310 : 0.09335914254188538
Loss at iteration 320 : 0.05683582276105881
Loss at iteration 330 : 0.12272991240024567
Loss at iteration 340 : 0.07997908443212509
Loss at iteration 350 : 0.11622383445501328
Loss at iteration 360 : 0.12103940546512604
Loss at iteration 370 : 0.06433168798685074
Loss at iteration 380 : 0.06901004165410995
Loss at iteration 390 : 0.08887551724910736
Loss at iteration 400 : 0.07376950234174728
Loss at iteration 410 : 0.1303260177373886
Loss at iteration 420 : 0.0958113968372345
Loss at iteration 430 : 0.09460791200399399
Loss at iteration 440 : 0.07464315742254257
Loss at iteration 450 : 0.06477630138397217
Loss at iteration 460 : 0.06959204375743866
Loss at iteration 470 : 0.08127579092979431
Loss at iteration 480 : 0.07189374417066574
Loss at iteration 490 : 0.09956219047307968
Loss at iteration 500 : 0.06620966643095016
Loss at iteration 510 : 0.09734597057104111
Loss at iteration 520 : 0.05668898671865463
Loss at iteration 530 : 0.12022142112255096
Loss at iteration 540 : 0.09219105541706085
Loss at iteration 550 : 0.07014192640781403
Loss at iteration 560 : 0.09145480394363403
Loss at iteration 570 : 0.06468718498945236
Loss at iteration 580 : 0.10810919851064682
Loss at iteration 590 : 0.09193908423185349
Loss at iteration 600 : 0.07598390430212021
Loss at iteration 610 : 0.14339005947113037
Loss at iteration 620 : 0.05453309044241905
Loss at iteration 630 : 0.14707499742507935
Loss at iteration 640 : 0.08620689809322357
Loss at iteration 650 : 0.07511581480503082
Loss at iteration 660 : 0.054794684052467346
Loss at iteration 670 : 0.11264695972204208
Loss at iteration 680 : 0.05505910515785217
Loss at iteration 690 : 0.07057388871908188
Loss at iteration 700 : 0.09513299912214279
Loss at iteration 710 : 0.10583245754241943
Loss at iteration 720 : 0.12675677239894867
Loss at iteration 730 : 0.11032898724079132
Loss at iteration 740 : 0.061598360538482666
Loss at iteration 750 : 0.05472524091601372
Loss at iteration 760 : 0.10353051126003265
Loss at iteration 770 : 0.07415084540843964
Loss at iteration 780 : 0.07222694903612137
Loss at iteration 790 : 0.12283357977867126
Loss at iteration 800 : 0.08484592288732529
Loss at iteration 810 : 0.11583021283149719
Loss at iteration 820 : 0.09444761276245117
Loss at iteration 830 : 0.07988370209932327
Loss at iteration 840 : 0.09653519093990326
Loss at iteration 850 : 0.07891276478767395
Loss at iteration 860 : 0.12157535552978516
Loss at iteration 870 : 0.08713855594396591
Loss at iteration 880 : 0.07093174755573273
Loss at iteration 890 : 0.06996326148509979
Loss at iteration 900 : 0.07000431418418884
Loss at iteration 910 : 0.1298789381980896
Loss at iteration 920 : 0.0663699060678482
Loss at iteration 930 : 0.053815167397260666
Loss at iteration 940 : 0.05989376828074455
Loss at iteration 950 : 0.09065010398626328
Loss at iteration 960 : 0.06251457333564758
Loss at iteration 970 : 0.11007487773895264
Loss at iteration 980 : 0.1352759748697281
Loss at iteration 990 : 0.10527461022138596
Loss at iteration 1000 : 0.12029333412647247
Loss at iteration 1010 : 0.06876944750547409
Loss at iteration 1020 : 0.04940114542841911
Loss at iteration 1030 : 0.1375914067029953
Loss at iteration 1040 : 0.1383858025074005
Loss at iteration 1050 : 0.07234644889831543
Loss at iteration 1060 : 0.08654569089412689
Loss at iteration 1070 : 0.08862261474132538
Loss at iteration 1080 : 0.06393057852983475
Loss at iteration 1090 : 0.08787288516759872
Loss at iteration 1100 : 0.08450224250555038
Loss at iteration 1110 : 0.0711122378706932
Loss at iteration 1120 : 0.052462540566921234
Loss at iteration 1130 : 0.060304127633571625
Loss at iteration 1140 : 0.09118091315031052
Loss at iteration 1150 : 0.07848647236824036
Loss at iteration 1160 : 0.06466961652040482
Loss at iteration 1170 : 0.08503943681716919
Loss at iteration 1180 : 0.08553558588027954
Loss at iteration 1190 : 0.04822733253240585
Loss at iteration 1200 : 0.11720876395702362
Loss at iteration 1210 : 0.07240904867649078
The SSIM Value is: 0.7066446522871653
The PSNR Value is: 20.325532976786295
the epoch is: 136
Loss at iteration 10 : 0.06634170562028885
Loss at iteration 20 : 0.0676446408033371
Loss at iteration 30 : 0.05414411425590515
Loss at iteration 40 : 0.08788353204727173
Loss at iteration 50 : 0.04328408092260361
Loss at iteration 60 : 0.07325315475463867
Loss at iteration 70 : 0.12012499570846558
Loss at iteration 80 : 0.09630453586578369
Loss at iteration 90 : 0.09189119189977646
Loss at iteration 100 : 0.05228957533836365
Loss at iteration 110 : 0.09056150913238525
Loss at iteration 120 : 0.11549746990203857
Loss at iteration 130 : 0.13616672158241272
Loss at iteration 140 : 0.04906086251139641
Loss at iteration 150 : 0.07376959174871445
Loss at iteration 160 : 0.10163804143667221
Loss at iteration 170 : 0.09478475153446198
Loss at iteration 180 : 0.10320156067609787
Loss at iteration 190 : 0.15445077419281006
Loss at iteration 200 : 0.07552777975797653
Loss at iteration 210 : 0.05298195779323578
Loss at iteration 220 : 0.0892021507024765
Loss at iteration 230 : 0.037868112325668335
Loss at iteration 240 : 0.08943270891904831
Loss at iteration 250 : 0.032225921750068665
Loss at iteration 260 : 0.06415301561355591
Loss at iteration 270 : 0.09585121273994446
Loss at iteration 280 : 0.13915055990219116
Loss at iteration 290 : 0.1025814637541771
Loss at iteration 300 : 0.08591535687446594
Loss at iteration 310 : 0.09384014457464218
Loss at iteration 320 : 0.0870436355471611
Loss at iteration 330 : 0.14987337589263916
Loss at iteration 340 : 0.07430591434240341
Loss at iteration 350 : 0.06901072710752487
Loss at iteration 360 : 0.12195555865764618
Loss at iteration 370 : 0.10135136544704437
Loss at iteration 380 : 0.0811358094215393
Loss at iteration 390 : 0.06497575342655182
Loss at iteration 400 : 0.07651464641094208
Loss at iteration 410 : 0.12436442077159882
Loss at iteration 420 : 0.07570578902959824
Loss at iteration 430 : 0.09870894253253937
Loss at iteration 440 : 0.07523294538259506
Loss at iteration 450 : 0.08539538085460663
Loss at iteration 460 : 0.09843795001506805
Loss at iteration 470 : 0.09995200484991074
Loss at iteration 480 : 0.06255050748586655
Loss at iteration 490 : 0.08980552852153778
Loss at iteration 500 : 0.056613460183143616
Loss at iteration 510 : 0.051477573812007904
Loss at iteration 520 : 0.10635462403297424
Loss at iteration 530 : 0.097725510597229
Loss at iteration 540 : 0.0648433119058609
Loss at iteration 550 : 0.07978787273168564
Loss at iteration 560 : 0.04987003654241562
Loss at iteration 570 : 0.07699453085660934
Loss at iteration 580 : 0.08810517191886902
Loss at iteration 590 : 0.0671619400382042
Loss at iteration 600 : 0.05655037611722946
Loss at iteration 610 : 0.1310710310935974
Loss at iteration 620 : 0.06272785365581512
Loss at iteration 630 : 0.1209041178226471
Loss at iteration 640 : 0.08608680218458176
Loss at iteration 650 : 0.09233280271291733
Loss at iteration 660 : 0.06043208763003349
Loss at iteration 670 : 0.11507739871740341
Loss at iteration 680 : 0.08747445791959763
Loss at iteration 690 : 0.07213813066482544
Loss at iteration 700 : 0.09375032037496567
Loss at iteration 710 : 0.08573611825704575
Loss at iteration 720 : 0.08651509135961533
Loss at iteration 730 : 0.12603072822093964
Loss at iteration 740 : 0.07630231231451035
Loss at iteration 750 : 0.10865683853626251
Loss at iteration 760 : 0.04923384636640549
Loss at iteration 770 : 0.11033361405134201
Loss at iteration 780 : 0.051513656973838806
Loss at iteration 790 : 0.061850160360336304
Loss at iteration 800 : 0.11035748571157455
Loss at iteration 810 : 0.1215103268623352
Loss at iteration 820 : 0.0585559606552124
Loss at iteration 830 : 0.11052457988262177
Loss at iteration 840 : 0.09544388949871063
Loss at iteration 850 : 0.09031447768211365
Loss at iteration 860 : 0.09668510407209396
Loss at iteration 870 : 0.0897262841463089
Loss at iteration 880 : 0.09530939906835556
Loss at iteration 890 : 0.10156701505184174
Loss at iteration 900 : 0.06631386280059814
Loss at iteration 910 : 0.08778205513954163
Loss at iteration 920 : 0.07584486901760101
Loss at iteration 930 : 0.11408738791942596
Loss at iteration 940 : 0.12310069799423218
Loss at iteration 950 : 0.06059204787015915
Loss at iteration 960 : 0.06646323204040527
Loss at iteration 970 : 0.0855807289481163
Loss at iteration 980 : 0.11257364600896835
Loss at iteration 990 : 0.06533468514680862
Loss at iteration 1000 : 0.05453335493803024
Loss at iteration 1010 : 0.07018174976110458
Loss at iteration 1020 : 0.10893964767456055
Loss at iteration 1030 : 0.0831950455904007
Loss at iteration 1040 : 0.062408268451690674
Loss at iteration 1050 : 0.0926448255777359
Loss at iteration 1060 : 0.06100710853934288
Loss at iteration 1070 : 0.0663105770945549
Loss at iteration 1080 : 0.05719666928052902
Loss at iteration 1090 : 0.05857490748167038
Loss at iteration 1100 : 0.08719031512737274
Loss at iteration 1110 : 0.08485777676105499
Loss at iteration 1120 : 0.0615147203207016
Loss at iteration 1130 : 0.07833334058523178
Loss at iteration 1140 : 0.08352665603160858
Loss at iteration 1150 : 0.06314259767532349
Loss at iteration 1160 : 0.0689987987279892
Loss at iteration 1170 : 0.06573209166526794
Loss at iteration 1180 : 0.10285411775112152
Loss at iteration 1190 : 0.09465211629867554
Loss at iteration 1200 : 0.08622018992900848
Loss at iteration 1210 : 0.06988631188869476
The SSIM Value is: 0.710733582576116
The PSNR Value is: 20.409942563374837
the epoch is: 137
Loss at iteration 10 : 0.07108321785926819
Loss at iteration 20 : 0.06632398813962936
Loss at iteration 30 : 0.09386442601680756
Loss at iteration 40 : 0.057640980929136276
Loss at iteration 50 : 0.06599769741296768
Loss at iteration 60 : 0.07105419039726257
Loss at iteration 70 : 0.0722847729921341
Loss at iteration 80 : 0.11393391340970993
Loss at iteration 90 : 0.05894901603460312
Loss at iteration 100 : 0.050661709159612656
Loss at iteration 110 : 0.12324190139770508
Loss at iteration 120 : 0.0789792388677597
Loss at iteration 130 : 0.13575847446918488
Loss at iteration 140 : 0.06384950131177902
Loss at iteration 150 : 0.06771235167980194
Loss at iteration 160 : 0.06416227668523788
Loss at iteration 170 : 0.077328622341156
Loss at iteration 180 : 0.09714475274085999
Loss at iteration 190 : 0.0676550567150116
Loss at iteration 200 : 0.09157682955265045
Loss at iteration 210 : 0.08208359777927399
Loss at iteration 220 : 0.0724940150976181
Loss at iteration 230 : 0.13777858018875122
Loss at iteration 240 : 0.06959027051925659
Loss at iteration 250 : 0.0538032203912735
Loss at iteration 260 : 0.10809344053268433
Loss at iteration 270 : 0.05282626673579216
Loss at iteration 280 : 0.08580243587493896
Loss at iteration 290 : 0.10476166009902954
Loss at iteration 300 : 0.11028158664703369
Loss at iteration 310 : 0.07758059352636337
Loss at iteration 320 : 0.09289654344320297
Loss at iteration 330 : 0.0821613073348999
Loss at iteration 340 : 0.07004803419113159
Loss at iteration 350 : 0.09528189897537231
Loss at iteration 360 : 0.07063102722167969
Loss at iteration 370 : 0.0965656265616417
Loss at iteration 380 : 0.07032512873411179
Loss at iteration 390 : 0.07305646687746048
Loss at iteration 400 : 0.06911516934633255
Loss at iteration 410 : 0.10531328618526459
Loss at iteration 420 : 0.06798547506332397
Loss at iteration 430 : 0.09309776872396469
Loss at iteration 440 : 0.09107234328985214
Loss at iteration 450 : 0.06490650027990341
Loss at iteration 460 : 0.07932358980178833
Loss at iteration 470 : 0.091464102268219
Loss at iteration 480 : 0.08168687671422958
Loss at iteration 490 : 0.10092797875404358
Loss at iteration 500 : 0.07927350699901581
Loss at iteration 510 : 0.04842206835746765
Loss at iteration 520 : 0.08221028745174408
Loss at iteration 530 : 0.06776303052902222
Loss at iteration 540 : 0.053858187049627304
Loss at iteration 550 : 0.058371901512145996
Loss at iteration 560 : 0.04524596035480499
Loss at iteration 570 : 0.0672840103507042
Loss at iteration 580 : 0.1329822540283203
Loss at iteration 590 : 0.06514843553304672
Loss at iteration 600 : 0.06424430757761002
Loss at iteration 610 : 0.0955258458852768
Loss at iteration 620 : 0.07562839239835739
Loss at iteration 630 : 0.08767245709896088
Loss at iteration 640 : 0.07411908358335495
Loss at iteration 650 : 0.0894671231508255
Loss at iteration 660 : 0.1936754435300827
Loss at iteration 670 : 0.11206898093223572
Loss at iteration 680 : 0.11565715074539185
Loss at iteration 690 : 0.10194042325019836
Loss at iteration 700 : 0.06879903376102448
Loss at iteration 710 : 0.09792211651802063
Loss at iteration 720 : 0.07968728244304657
Loss at iteration 730 : 0.07932578027248383
Loss at iteration 740 : 0.09518343210220337
Loss at iteration 750 : 0.10681183636188507
Loss at iteration 760 : 0.08250876516103745
Loss at iteration 770 : 0.10054109990596771
Loss at iteration 780 : 0.11044891923666
Loss at iteration 790 : 0.05884667485952377
Loss at iteration 800 : 0.10465340316295624
Loss at iteration 810 : 0.0981522798538208
Loss at iteration 820 : 0.08842628449201584
Loss at iteration 830 : 0.08906082808971405
Loss at iteration 840 : 0.0906904861330986
Loss at iteration 850 : 0.11865156888961792
Loss at iteration 860 : 0.10950964689254761
Loss at iteration 870 : 0.07045242190361023
Loss at iteration 880 : 0.06542670726776123
Loss at iteration 890 : 0.08804939687252045
Loss at iteration 900 : 0.08729042857885361
Loss at iteration 910 : 0.10753434896469116
Loss at iteration 920 : 0.06486769765615463
Loss at iteration 930 : 0.08139718323945999
Loss at iteration 940 : 0.10871440172195435
Loss at iteration 950 : 0.10362245887517929
Loss at iteration 960 : 0.08138523995876312
Loss at iteration 970 : 0.11183115094900131
Loss at iteration 980 : 0.06325016915798187
Loss at iteration 990 : 0.12866666913032532
Loss at iteration 1000 : 0.06819667667150497
Loss at iteration 1010 : 0.11607277393341064
Loss at iteration 1020 : 0.05374626815319061
Loss at iteration 1030 : 0.06422676146030426
Loss at iteration 1040 : 0.10619323700666428
Loss at iteration 1050 : 0.06826477497816086
Loss at iteration 1060 : 0.10167329013347626
Loss at iteration 1070 : 0.046072326600551605
Loss at iteration 1080 : 0.06847076117992401
Loss at iteration 1090 : 0.07980997860431671
Loss at iteration 1100 : 0.08273939788341522
Loss at iteration 1110 : 0.07002823799848557
Loss at iteration 1120 : 0.0659591406583786
Loss at iteration 1130 : 0.06995587795972824
Loss at iteration 1140 : 0.10192187875509262
Loss at iteration 1150 : 0.06994250416755676
Loss at iteration 1160 : 0.07134559005498886
Loss at iteration 1170 : 0.08136024326086044
Loss at iteration 1180 : 0.041264645755290985
Loss at iteration 1190 : 0.08511722087860107
Loss at iteration 1200 : 0.06289342045783997
Loss at iteration 1210 : 0.06755130738019943
The SSIM Value is: 0.7156229138374328
The PSNR Value is: 21.032139333089194
the epoch is: 138
Loss at iteration 10 : 0.10013880580663681
Loss at iteration 20 : 0.09285295009613037
Loss at iteration 30 : 0.05471902713179588
Loss at iteration 40 : 0.08966827392578125
Loss at iteration 50 : 0.0780615583062172
Loss at iteration 60 : 0.132279634475708
Loss at iteration 70 : 0.06129245087504387
Loss at iteration 80 : 0.13794299960136414
Loss at iteration 90 : 0.05243075639009476
Loss at iteration 100 : 0.08483432233333588
Loss at iteration 110 : 0.06761031597852707
Loss at iteration 120 : 0.09447139501571655
Loss at iteration 130 : 0.06012089550495148
Loss at iteration 140 : 0.05641632527112961
Loss at iteration 150 : 0.05029353126883507
Loss at iteration 160 : 0.08812551945447922
Loss at iteration 170 : 0.06426717340946198
Loss at iteration 180 : 0.08768653124570847
Loss at iteration 190 : 0.06079613417387009
Loss at iteration 200 : 0.07760783284902573
Loss at iteration 210 : 0.081143319606781
Loss at iteration 220 : 0.0600062720477581
Loss at iteration 230 : 0.07054358720779419
Loss at iteration 240 : 0.07832686603069305
Loss at iteration 250 : 0.06490316241979599
Loss at iteration 260 : 0.08589690923690796
Loss at iteration 270 : 0.09856792539358139
Loss at iteration 280 : 0.09091072529554367
Loss at iteration 290 : 0.0902479737997055
Loss at iteration 300 : 0.0555298738181591
Loss at iteration 310 : 0.07550860941410065
Loss at iteration 320 : 0.06327055394649506
Loss at iteration 330 : 0.08609803766012192
Loss at iteration 340 : 0.07758123427629471
Loss at iteration 350 : 0.09670843929052353
Loss at iteration 360 : 0.0734429806470871
Loss at iteration 370 : 0.08740594238042831
Loss at iteration 380 : 0.08340054750442505
Loss at iteration 390 : 0.06299562752246857
Loss at iteration 400 : 0.13575203716754913
Loss at iteration 410 : 0.07226984202861786
Loss at iteration 420 : 0.06646230071783066
Loss at iteration 430 : 0.09962090104818344
Loss at iteration 440 : 0.11273287236690521
Loss at iteration 450 : 0.07472796738147736
Loss at iteration 460 : 0.08941390365362167
Loss at iteration 470 : 0.06513723731040955
Loss at iteration 480 : 0.10986139625310898
Loss at iteration 490 : 0.09241076558828354
Loss at iteration 500 : 0.07082471251487732
Loss at iteration 510 : 0.04000014066696167
Loss at iteration 520 : 0.09638531506061554
Loss at iteration 530 : 0.11471496522426605
Loss at iteration 540 : 0.06301131844520569
Loss at iteration 550 : 0.11526598036289215
Loss at iteration 560 : 0.07496758550405502
Loss at iteration 570 : 0.10122567415237427
Loss at iteration 580 : 0.08950108289718628
Loss at iteration 590 : 0.06561040878295898
Loss at iteration 600 : 0.15556077659130096
Loss at iteration 610 : 0.10162942111492157
Loss at iteration 620 : 0.04445207118988037
Loss at iteration 630 : 0.07722976803779602
Loss at iteration 640 : 0.09158091247081757
Loss at iteration 650 : 0.06588013470172882
Loss at iteration 660 : 0.08538593351840973
Loss at iteration 670 : 0.0751875638961792
Loss at iteration 680 : 0.05709415674209595
Loss at iteration 690 : 0.11112962663173676
Loss at iteration 700 : 0.10312186181545258
Loss at iteration 710 : 0.09225498139858246
Loss at iteration 720 : 0.11148812621831894
Loss at iteration 730 : 0.06374417245388031
Loss at iteration 740 : 0.0998222678899765
Loss at iteration 750 : 0.10227357596158981
Loss at iteration 760 : 0.07032416760921478
Loss at iteration 770 : 0.08332978934049606
Loss at iteration 780 : 0.08878660202026367
Loss at iteration 790 : 0.10459999740123749
Loss at iteration 800 : 0.08232016861438751
Loss at iteration 810 : 0.13480046391487122
Loss at iteration 820 : 0.0925314873456955
Loss at iteration 830 : 0.08892585337162018
Loss at iteration 840 : 0.07409824430942535
Loss at iteration 850 : 0.08856364339590073
Loss at iteration 860 : 0.082516148686409
Loss at iteration 870 : 0.10472311824560165
Loss at iteration 880 : 0.08951710909605026
Loss at iteration 890 : 0.08590756356716156
Loss at iteration 900 : 0.03149988502264023
Loss at iteration 910 : 0.13518887758255005
Loss at iteration 920 : 0.10021328181028366
Loss at iteration 930 : 0.09679921716451645
Loss at iteration 940 : 0.07276016473770142
Loss at iteration 950 : 0.07089532166719437
Loss at iteration 960 : 0.09447779506444931
Loss at iteration 970 : 0.12257736921310425
Loss at iteration 980 : 0.0629478394985199
Loss at iteration 990 : 0.0787779837846756
Loss at iteration 1000 : 0.04691316932439804
Loss at iteration 1010 : 0.1350477784872055
Loss at iteration 1020 : 0.08405285328626633
Loss at iteration 1030 : 0.07163004577159882
Loss at iteration 1040 : 0.09979201853275299
Loss at iteration 1050 : 0.07933713495731354
Loss at iteration 1060 : 0.1019294261932373
Loss at iteration 1070 : 0.08996488153934479
Loss at iteration 1080 : 0.06253122538328171
Loss at iteration 1090 : 0.09893393516540527
Loss at iteration 1100 : 0.0685054361820221
Loss at iteration 1110 : 0.05002012103796005
Loss at iteration 1120 : 0.06928939372301102
Loss at iteration 1130 : 0.11394102871417999
Loss at iteration 1140 : 0.10122306644916534
Loss at iteration 1150 : 0.06844821572303772
Loss at iteration 1160 : 0.11541937291622162
Loss at iteration 1170 : 0.06129248067736626
Loss at iteration 1180 : 0.07142194360494614
Loss at iteration 1190 : 0.10807149112224579
Loss at iteration 1200 : 0.04379267618060112
Loss at iteration 1210 : 0.07425417751073837
The SSIM Value is: 0.7139717936515808
The PSNR Value is: 20.733710098266602
the epoch is: 139
Loss at iteration 10 : 0.08865194767713547
Loss at iteration 20 : 0.0786283016204834
Loss at iteration 30 : 0.0738491341471672
Loss at iteration 40 : 0.1267814338207245
Loss at iteration 50 : 0.05626332014799118
Loss at iteration 60 : 0.11448495835065842
Loss at iteration 70 : 0.052722543478012085
Loss at iteration 80 : 0.1159086674451828
Loss at iteration 90 : 0.07812589406967163
Loss at iteration 100 : 0.0696869045495987
Loss at iteration 110 : 0.11955481767654419
Loss at iteration 120 : 0.0902855396270752
Loss at iteration 130 : 0.09498530626296997
Loss at iteration 140 : 0.08943461626768112
Loss at iteration 150 : 0.11484076082706451
Loss at iteration 160 : 0.11825843900442123
Loss at iteration 170 : 0.12902745604515076
Loss at iteration 180 : 0.06470788270235062
Loss at iteration 190 : 0.04526333510875702
Loss at iteration 200 : 0.09593328088521957
Loss at iteration 210 : 0.11824753135442734
Loss at iteration 220 : 0.10891836881637573
Loss at iteration 230 : 0.09562699496746063
Loss at iteration 240 : 0.08120646327733994
Loss at iteration 250 : 0.05399850010871887
Loss at iteration 260 : 0.07377563416957855
Loss at iteration 270 : 0.1036100834608078
Loss at iteration 280 : 0.15013128519058228
Loss at iteration 290 : 0.08830844610929489
Loss at iteration 300 : 0.09152811765670776
Loss at iteration 310 : 0.06580160558223724
Loss at iteration 320 : 0.09079589694738388
Loss at iteration 330 : 0.10526745021343231
Loss at iteration 340 : 0.10962962359189987
Loss at iteration 350 : 0.07681279629468918
Loss at iteration 360 : 0.0899534747004509
Loss at iteration 370 : 0.04595085233449936
Loss at iteration 380 : 0.052479811012744904
Loss at iteration 390 : 0.0753941461443901
Loss at iteration 400 : 0.08959108591079712
Loss at iteration 410 : 0.10894455760717392
Loss at iteration 420 : 0.07951756566762924
Loss at iteration 430 : 0.06487547606229782
Loss at iteration 440 : 0.10726084560155869
Loss at iteration 450 : 0.07792934775352478
Loss at iteration 460 : 0.12155409902334213
Loss at iteration 470 : 0.05592307075858116
Loss at iteration 480 : 0.08293696492910385
Loss at iteration 490 : 0.0799693688750267
Loss at iteration 500 : 0.049911048263311386
Loss at iteration 510 : 0.08555994927883148
Loss at iteration 520 : 0.095606230199337
Loss at iteration 530 : 0.05497187003493309
Loss at iteration 540 : 0.14465385675430298
Loss at iteration 550 : 0.0797145664691925
Loss at iteration 560 : 0.052232544869184494
Loss at iteration 570 : 0.08818226307630539
Loss at iteration 580 : 0.08741554617881775
Loss at iteration 590 : 0.09381809830665588
Loss at iteration 600 : 0.12484564632177353
Loss at iteration 610 : 0.05564912408590317
Loss at iteration 620 : 0.04312307760119438
Loss at iteration 630 : 0.07757066935300827
Loss at iteration 640 : 0.12332919239997864
Loss at iteration 650 : 0.11770768463611603
Loss at iteration 660 : 0.08179494738578796
Loss at iteration 670 : 0.0747518241405487
Loss at iteration 680 : 0.04468139261007309
Loss at iteration 690 : 0.10049091279506683
Loss at iteration 700 : 0.0908384770154953
Loss at iteration 710 : 0.08843296021223068
Loss at iteration 720 : 0.07651066035032272
Loss at iteration 730 : 0.04523871839046478
Loss at iteration 740 : 0.06206449866294861
Loss at iteration 750 : 0.11799194663763046
Loss at iteration 760 : 0.05021192878484726
Loss at iteration 770 : 0.06975500285625458
Loss at iteration 780 : 0.0984039157629013
Loss at iteration 790 : 0.07120928168296814
Loss at iteration 800 : 0.0973634347319603
Loss at iteration 810 : 0.07238491624593735
Loss at iteration 820 : 0.055264126509428024
Loss at iteration 830 : 0.07838740199804306
Loss at iteration 840 : 0.06889597326517105
Loss at iteration 850 : 0.09210880845785141
Loss at iteration 860 : 0.10320063680410385
Loss at iteration 870 : 0.07091014087200165
Loss at iteration 880 : 0.12268587201833725
Loss at iteration 890 : 0.09750303626060486
Loss at iteration 900 : 0.055217593908309937
Loss at iteration 910 : 0.0874251127243042
Loss at iteration 920 : 0.055167775601148605
Loss at iteration 930 : 0.09978479146957397
Loss at iteration 940 : 0.0714014545083046
Loss at iteration 950 : 0.14300832152366638
Loss at iteration 960 : 0.11426675319671631
Loss at iteration 970 : 0.10620637983083725
Loss at iteration 980 : 0.08026783913373947
Loss at iteration 990 : 0.06092534586787224
Loss at iteration 1000 : 0.13020877540111542
Loss at iteration 1010 : 0.07814037799835205
Loss at iteration 1020 : 0.08988573402166367
Loss at iteration 1030 : 0.08721210062503815
Loss at iteration 1040 : 0.0875086784362793
Loss at iteration 1050 : 0.08245237171649933
Loss at iteration 1060 : 0.12470579147338867
Loss at iteration 1070 : 0.09102395176887512
Loss at iteration 1080 : 0.06871096789836884
Loss at iteration 1090 : 0.07424098253250122
Loss at iteration 1100 : 0.04104681313037872
Loss at iteration 1110 : 0.13023583590984344
Loss at iteration 1120 : 0.09030161798000336
Loss at iteration 1130 : 0.11148037016391754
Loss at iteration 1140 : 0.05834861099720001
Loss at iteration 1150 : 0.08029170334339142
Loss at iteration 1160 : 0.06959801912307739
Loss at iteration 1170 : 0.08911478519439697
Loss at iteration 1180 : 0.06178048253059387
Loss at iteration 1190 : 0.07557092607021332
Loss at iteration 1200 : 0.05878432095050812
Loss at iteration 1210 : 0.07496222108602524
The SSIM Value is: 0.7207487344741821
The PSNR Value is: 21.202917989095052
the epoch is: 140
Loss at iteration 10 : 0.06664972007274628
Loss at iteration 20 : 0.08253389596939087
Loss at iteration 30 : 0.06846637278795242
Loss at iteration 40 : 0.1072523221373558
Loss at iteration 50 : 0.0771273523569107
Loss at iteration 60 : 0.10679920762777328
Loss at iteration 70 : 0.058057256042957306
Loss at iteration 80 : 0.04821430891752243
Loss at iteration 90 : 0.08474212884902954
Loss at iteration 100 : 0.049165379256010056
Loss at iteration 110 : 0.06884993612766266
Loss at iteration 120 : 0.10911296308040619
Loss at iteration 130 : 0.11084403842687607
Loss at iteration 140 : 0.12891094386577606
Loss at iteration 150 : 0.1155250072479248
Loss at iteration 160 : 0.09851646423339844
Loss at iteration 170 : 0.05550197884440422
Loss at iteration 180 : 0.1252790093421936
Loss at iteration 190 : 0.07139167934656143
Loss at iteration 200 : 0.09855709969997406
Loss at iteration 210 : 0.07196824252605438
Loss at iteration 220 : 0.06230737268924713
Loss at iteration 230 : 0.0927574560046196
Loss at iteration 240 : 0.05384695157408714
Loss at iteration 250 : 0.0776696428656578
Loss at iteration 260 : 0.06047993525862694
Loss at iteration 270 : 0.07591719925403595
Loss at iteration 280 : 0.06960762292146683
Loss at iteration 290 : 0.061352603137493134
Loss at iteration 300 : 0.10754355043172836
Loss at iteration 310 : 0.12161661684513092
Loss at iteration 320 : 0.10745935887098312
Loss at iteration 330 : 0.04372230917215347
Loss at iteration 340 : 0.07255315780639648
Loss at iteration 350 : 0.07288026064634323
Loss at iteration 360 : 0.11290352046489716
Loss at iteration 370 : 0.10093621909618378
Loss at iteration 380 : 0.0969705581665039
Loss at iteration 390 : 0.0685863122344017
Loss at iteration 400 : 0.06324505805969238
Loss at iteration 410 : 0.05255506932735443
Loss at iteration 420 : 0.11722949147224426
Loss at iteration 430 : 0.12369103729724884
Loss at iteration 440 : 0.06950105726718903
Loss at iteration 450 : 0.06515753269195557
Loss at iteration 460 : 0.06231781467795372
Loss at iteration 470 : 0.07214874774217606
Loss at iteration 480 : 0.056950896978378296
Loss at iteration 490 : 0.13014188408851624
Loss at iteration 500 : 0.07546267658472061
Loss at iteration 510 : 0.0863293930888176
Loss at iteration 520 : 0.12161394208669662
Loss at iteration 530 : 0.06757910549640656
Loss at iteration 540 : 0.08382966369390488
Loss at iteration 550 : 0.08809471130371094
Loss at iteration 560 : 0.086372971534729
Loss at iteration 570 : 0.11280399560928345
Loss at iteration 580 : 0.05248658359050751
Loss at iteration 590 : 0.06513142585754395
Loss at iteration 600 : 0.031517837196588516
Loss at iteration 610 : 0.15763244032859802
Loss at iteration 620 : 0.07647150754928589
Loss at iteration 630 : 0.048311568796634674
Loss at iteration 640 : 0.10542403161525726
Loss at iteration 650 : 0.1009647473692894
Loss at iteration 660 : 0.07375113666057587
Loss at iteration 670 : 0.074051633477211
Loss at iteration 680 : 0.05163104459643364
Loss at iteration 690 : 0.05114586278796196
Loss at iteration 700 : 0.1057644709944725
Loss at iteration 710 : 0.06951290369033813
Loss at iteration 720 : 0.06207893788814545
Loss at iteration 730 : 0.11958315223455429
Loss at iteration 740 : 0.06917011737823486
Loss at iteration 750 : 0.05428672954440117
Loss at iteration 760 : 0.05522433668375015
Loss at iteration 770 : 0.07241439074277878
Loss at iteration 780 : 0.08514019101858139
Loss at iteration 790 : 0.07824046909809113
Loss at iteration 800 : 0.05554486811161041
Loss at iteration 810 : 0.11906877160072327
Loss at iteration 820 : 0.09425222873687744
Loss at iteration 830 : 0.08442644774913788
Loss at iteration 840 : 0.08040551841259003
Loss at iteration 850 : 0.10703738033771515
Loss at iteration 860 : 0.13245578110218048
Loss at iteration 870 : 0.0754956379532814
Loss at iteration 880 : 0.06955446302890778
Loss at iteration 890 : 0.06961286813020706
Loss at iteration 900 : 0.08035560697317123
Loss at iteration 910 : 0.07229669392108917
Loss at iteration 920 : 0.06711079180240631
Loss at iteration 930 : 0.12510064244270325
Loss at iteration 940 : 0.07036089897155762
Loss at iteration 950 : 0.08646157383918762
Loss at iteration 960 : 0.050314269959926605
Loss at iteration 970 : 0.09439127147197723
Loss at iteration 980 : 0.10951900482177734
Loss at iteration 990 : 0.10945343226194382
Loss at iteration 1000 : 0.10128098726272583
Loss at iteration 1010 : 0.07110007852315903
Loss at iteration 1020 : 0.07729221880435944
Loss at iteration 1030 : 0.0713549256324768
Loss at iteration 1040 : 0.05408787727355957
Loss at iteration 1050 : 0.13208220899105072
Loss at iteration 1060 : 0.047646522521972656
Loss at iteration 1070 : 0.07772499322891235
Loss at iteration 1080 : 0.0860029086470604
Loss at iteration 1090 : 0.08299869298934937
Loss at iteration 1100 : 0.09553223848342896
Loss at iteration 1110 : 0.06166171282529831
Loss at iteration 1120 : 0.05367712303996086
Loss at iteration 1130 : 0.08286334574222565
Loss at iteration 1140 : 0.0917205661535263
Loss at iteration 1150 : 0.07513364404439926
Loss at iteration 1160 : 0.09993857145309448
Loss at iteration 1170 : 0.09347643703222275
Loss at iteration 1180 : 0.062060050666332245
Loss at iteration 1190 : 0.09042167663574219
Loss at iteration 1200 : 0.07573756575584412
Loss at iteration 1210 : 0.07684694230556488
The SSIM Value is: 0.7124841531117757
The PSNR Value is: 20.56994260152181
the epoch is: 141
Loss at iteration 10 : 0.05872762203216553
Loss at iteration 20 : 0.06324917823076248
Loss at iteration 30 : 0.09326106309890747
Loss at iteration 40 : 0.059552401304244995
Loss at iteration 50 : 0.10408583283424377
Loss at iteration 60 : 0.12269468605518341
Loss at iteration 70 : 0.07703284174203873
Loss at iteration 80 : 0.08583422750234604
Loss at iteration 90 : 0.08066240698099136
Loss at iteration 100 : 0.10183387994766235
Loss at iteration 110 : 0.06294536590576172
Loss at iteration 120 : 0.13393467664718628
Loss at iteration 130 : 0.06108662113547325
Loss at iteration 140 : 0.09559375047683716
Loss at iteration 150 : 0.1024288460612297
Loss at iteration 160 : 0.07499755918979645
Loss at iteration 170 : 0.10517126321792603
Loss at iteration 180 : 0.03967081010341644
Loss at iteration 190 : 0.07247421145439148
Loss at iteration 200 : 0.12537989020347595
Loss at iteration 210 : 0.063271164894104
Loss at iteration 220 : 0.06188582628965378
Loss at iteration 230 : 0.08739230036735535
Loss at iteration 240 : 0.053827863186597824
Loss at iteration 250 : 0.08158102631568909
Loss at iteration 260 : 0.07576914876699448
Loss at iteration 270 : 0.06621313840150833
Loss at iteration 280 : 0.12275084108114243
Loss at iteration 290 : 0.09223246574401855
Loss at iteration 300 : 0.1111254021525383
Loss at iteration 310 : 0.10983501374721527
Loss at iteration 320 : 0.0952976644039154
Loss at iteration 330 : 0.09261774271726608
Loss at iteration 340 : 0.06299405544996262
Loss at iteration 350 : 0.0808902159333229
Loss at iteration 360 : 0.08721890300512314
Loss at iteration 370 : 0.07459210604429245
Loss at iteration 380 : 0.07658819854259491
Loss at iteration 390 : 0.1031312569975853
Loss at iteration 400 : 0.06940176337957382
Loss at iteration 410 : 0.11580636352300644
Loss at iteration 420 : 0.06467609852552414
Loss at iteration 430 : 0.07808338105678558
Loss at iteration 440 : 0.10766704380512238
Loss at iteration 450 : 0.14748775959014893
Loss at iteration 460 : 0.0852801725268364
Loss at iteration 470 : 0.10022194683551788
Loss at iteration 480 : 0.08067629486322403
Loss at iteration 490 : 0.09004177898168564
Loss at iteration 500 : 0.08994284272193909
Loss at iteration 510 : 0.04736003279685974
Loss at iteration 520 : 0.06971654295921326
Loss at iteration 530 : 0.0647418349981308
Loss at iteration 540 : 0.15501275658607483
Loss at iteration 550 : 0.061980754137039185
Loss at iteration 560 : 0.06980349868535995
Loss at iteration 570 : 0.13415846228599548
Loss at iteration 580 : 0.04902496933937073
Loss at iteration 590 : 0.06809595227241516
Loss at iteration 600 : 0.09958144277334213
Loss at iteration 610 : 0.09063483774662018
Loss at iteration 620 : 0.06051402911543846
Loss at iteration 630 : 0.07575935125350952
Loss at iteration 640 : 0.1376114785671234
Loss at iteration 650 : 0.08025097846984863
Loss at iteration 660 : 0.0830335021018982
Loss at iteration 670 : 0.0665237158536911
Loss at iteration 680 : 0.14043448865413666
Loss at iteration 690 : 0.08715249598026276
Loss at iteration 700 : 0.11433079093694687
Loss at iteration 710 : 0.09766259789466858
Loss at iteration 720 : 0.06920851022005081
Loss at iteration 730 : 0.08762998878955841
Loss at iteration 740 : 0.07971252501010895
Loss at iteration 750 : 0.12503519654273987
Loss at iteration 760 : 0.09116842597723007
Loss at iteration 770 : 0.060355450958013535
Loss at iteration 780 : 0.08130884170532227
Loss at iteration 790 : 0.10149331390857697
Loss at iteration 800 : 0.07735297083854675
Loss at iteration 810 : 0.1144140362739563
Loss at iteration 820 : 0.05168537795543671
Loss at iteration 830 : 0.08018389344215393
Loss at iteration 840 : 0.08442682027816772
Loss at iteration 850 : 0.08302994817495346
Loss at iteration 860 : 0.08284282684326172
Loss at iteration 870 : 0.05435333773493767
Loss at iteration 880 : 0.09225109219551086
Loss at iteration 890 : 0.10917695611715317
Loss at iteration 900 : 0.07047341763973236
Loss at iteration 910 : 0.0794561430811882
Loss at iteration 920 : 0.07792773097753525
Loss at iteration 930 : 0.07997291535139084
Loss at iteration 940 : 0.06455758213996887
Loss at iteration 950 : 0.07656533271074295
Loss at iteration 960 : 0.08971937745809555
Loss at iteration 970 : 0.07126954197883606
Loss at iteration 980 : 0.085436150431633
Loss at iteration 990 : 0.11347189545631409
Loss at iteration 1000 : 0.058804430067539215
Loss at iteration 1010 : 0.07616503536701202
Loss at iteration 1020 : 0.05818357318639755
Loss at iteration 1030 : 0.07142922282218933
Loss at iteration 1040 : 0.06830423325300217
Loss at iteration 1050 : 0.08717917650938034
Loss at iteration 1060 : 0.09183523058891296
Loss at iteration 1070 : 0.0604911670088768
Loss at iteration 1080 : 0.0618736669421196
Loss at iteration 1090 : 0.1405797004699707
Loss at iteration 1100 : 0.1658654510974884
Loss at iteration 1110 : 0.03991427645087242
Loss at iteration 1120 : 0.09504429996013641
Loss at iteration 1130 : 0.09271299839019775
Loss at iteration 1140 : 0.09042858332395554
Loss at iteration 1150 : 0.08866460621356964
Loss at iteration 1160 : 0.16399487853050232
Loss at iteration 1170 : 0.10962571948766708
Loss at iteration 1180 : 0.06707052886486053
Loss at iteration 1190 : 0.09056943655014038
Loss at iteration 1200 : 0.08958923816680908
Loss at iteration 1210 : 0.07248198986053467
The SSIM Value is: 0.7201210955778757
The PSNR Value is: 21.51800956726074
the highest SSIM value is: 21.51800956726074
the epoch is: 142
Loss at iteration 10 : 0.06176390498876572
Loss at iteration 20 : 0.07903965562582016
Loss at iteration 30 : 0.12688755989074707
Loss at iteration 40 : 0.048152029514312744
Loss at iteration 50 : 0.10492981970310211
Loss at iteration 60 : 0.05421699583530426
Loss at iteration 70 : 0.06207399070262909
Loss at iteration 80 : 0.08401064574718475
Loss at iteration 90 : 0.1006631851196289
Loss at iteration 100 : 0.08015397191047668
Loss at iteration 110 : 0.07337723672389984
Loss at iteration 120 : 0.09175079315900803
Loss at iteration 130 : 0.09213946759700775
Loss at iteration 140 : 0.094146728515625
Loss at iteration 150 : 0.10365253686904907
Loss at iteration 160 : 0.06419495493173599
Loss at iteration 170 : 0.11044194549322128
Loss at iteration 180 : 0.09877314418554306
Loss at iteration 190 : 0.10186567157506943
Loss at iteration 200 : 0.0658244788646698
Loss at iteration 210 : 0.08293475955724716
Loss at iteration 220 : 0.10693137347698212
Loss at iteration 230 : 0.08210299164056778
Loss at iteration 240 : 0.10240577161312103
Loss at iteration 250 : 0.06987918168306351
Loss at iteration 260 : 0.08401499688625336
Loss at iteration 270 : 0.06921061128377914
Loss at iteration 280 : 0.10156025737524033
Loss at iteration 290 : 0.07850223034620285
Loss at iteration 300 : 0.08870353549718857
Loss at iteration 310 : 0.09034665673971176
Loss at iteration 320 : 0.09222843497991562
Loss at iteration 330 : 0.1013372391462326
Loss at iteration 340 : 0.05407083034515381
Loss at iteration 350 : 0.050327908247709274
Loss at iteration 360 : 0.09973238408565521
Loss at iteration 370 : 0.09141886234283447
Loss at iteration 380 : 0.07838086038827896
Loss at iteration 390 : 0.09454932063817978
Loss at iteration 400 : 0.11287720501422882
Loss at iteration 410 : 0.09922497719526291
Loss at iteration 420 : 0.05184923857450485
Loss at iteration 430 : 0.06821243464946747
Loss at iteration 440 : 0.09622520208358765
Loss at iteration 450 : 0.10360366106033325
Loss at iteration 460 : 0.09331464767456055
Loss at iteration 470 : 0.09762178361415863
Loss at iteration 480 : 0.0762336403131485
Loss at iteration 490 : 0.08590352535247803
Loss at iteration 500 : 0.05848875641822815
Loss at iteration 510 : 0.08760331571102142
Loss at iteration 520 : 0.10395890474319458
Loss at iteration 530 : 0.0654706284403801
Loss at iteration 540 : 0.06946393102407455
Loss at iteration 550 : 0.05656713247299194
Loss at iteration 560 : 0.0834091529250145
Loss at iteration 570 : 0.06555115431547165
Loss at iteration 580 : 0.10975003242492676
Loss at iteration 590 : 0.07360930740833282
Loss at iteration 600 : 0.06932938098907471
Loss at iteration 610 : 0.19083556532859802
Loss at iteration 620 : 0.08191168308258057
Loss at iteration 630 : 0.04580705985426903
Loss at iteration 640 : 0.06438996642827988
Loss at iteration 650 : 0.09620567411184311
Loss at iteration 660 : 0.07367534935474396
Loss at iteration 670 : 0.0981592983007431
Loss at iteration 680 : 0.08118144422769547
Loss at iteration 690 : 0.07216695696115494
Loss at iteration 700 : 0.09705738723278046
Loss at iteration 710 : 0.09287433326244354
Loss at iteration 720 : 0.10297857224941254
Loss at iteration 730 : 0.04766535013914108
Loss at iteration 740 : 0.07179498672485352
Loss at iteration 750 : 0.10939469188451767
Loss at iteration 760 : 0.06854473054409027
Loss at iteration 770 : 0.1089392900466919
Loss at iteration 780 : 0.061937641352415085
Loss at iteration 790 : 0.08776956796646118
Loss at iteration 800 : 0.08380593359470367
Loss at iteration 810 : 0.07192432880401611
Loss at iteration 820 : 0.07650351524353027
Loss at iteration 830 : 0.06752430647611618
Loss at iteration 840 : 0.06578045338392258
Loss at iteration 850 : 0.06411327421665192
Loss at iteration 860 : 0.05512738227844238
Loss at iteration 870 : 0.06081530451774597
Loss at iteration 880 : 0.07512366026639938
Loss at iteration 890 : 0.07094801962375641
Loss at iteration 900 : 0.08015909045934677
Loss at iteration 910 : 0.12905055284500122
Loss at iteration 920 : 0.09962379932403564
Loss at iteration 930 : 0.07887254655361176
Loss at iteration 940 : 0.05485959351062775
Loss at iteration 950 : 0.05124145746231079
Loss at iteration 960 : 0.08008354157209396
Loss at iteration 970 : 0.09945722669363022
Loss at iteration 980 : 0.1009390577673912
Loss at iteration 990 : 0.0772479772567749
Loss at iteration 1000 : 0.05242639780044556
Loss at iteration 1010 : 0.08773597329854965
Loss at iteration 1020 : 0.11091583967208862
Loss at iteration 1030 : 0.05452897399663925
Loss at iteration 1040 : 0.13414177298545837
Loss at iteration 1050 : 0.11061015725135803
Loss at iteration 1060 : 0.08911516517400742
Loss at iteration 1070 : 0.07547193765640259
Loss at iteration 1080 : 0.08215845376253128
Loss at iteration 1090 : 0.047518692910671234
Loss at iteration 1100 : 0.05832808464765549
Loss at iteration 1110 : 0.05713046342134476
Loss at iteration 1120 : 0.06971245259046555
Loss at iteration 1130 : 0.05786636471748352
Loss at iteration 1140 : 0.06196553632616997
Loss at iteration 1150 : 0.08012890815734863
Loss at iteration 1160 : 0.08038640022277832
Loss at iteration 1170 : 0.054075371474027634
Loss at iteration 1180 : 0.08216992765665054
Loss at iteration 1190 : 0.11894819140434265
Loss at iteration 1200 : 0.0594065859913826
Loss at iteration 1210 : 0.0779261514544487
The SSIM Value is: 0.7132951637109121
The PSNR Value is: 20.954742177327475
the epoch is: 143
Loss at iteration 10 : 0.09218494594097137
Loss at iteration 20 : 0.14068292081356049
Loss at iteration 30 : 0.11789997667074203
Loss at iteration 40 : 0.08808684349060059
Loss at iteration 50 : 0.05049476772546768
Loss at iteration 60 : 0.0794849619269371
Loss at iteration 70 : 0.1261608898639679
Loss at iteration 80 : 0.03652666509151459
Loss at iteration 90 : 0.07942505180835724
Loss at iteration 100 : 0.08507212996482849
Loss at iteration 110 : 0.059309735894203186
Loss at iteration 120 : 0.09901927411556244
Loss at iteration 130 : 0.10309064388275146
Loss at iteration 140 : 0.10062120854854584
Loss at iteration 150 : 0.09214379638433456
Loss at iteration 160 : 0.08848993480205536
Loss at iteration 170 : 0.08086134493350983
Loss at iteration 180 : 0.09827995300292969
Loss at iteration 190 : 0.061201147735118866
Loss at iteration 200 : 0.0980304628610611
Loss at iteration 210 : 0.10161097347736359
Loss at iteration 220 : 0.06776631623506546
Loss at iteration 230 : 0.05791398137807846
Loss at iteration 240 : 0.0732012614607811
Loss at iteration 250 : 0.06891493499279022
Loss at iteration 260 : 0.13410580158233643
Loss at iteration 270 : 0.06551903486251831
Loss at iteration 280 : 0.08450117707252502
Loss at iteration 290 : 0.06716922670602798
Loss at iteration 300 : 0.06321467459201813
Loss at iteration 310 : 0.10245852172374725
Loss at iteration 320 : 0.09120707958936691
Loss at iteration 330 : 0.09071795642375946
Loss at iteration 340 : 0.09960778802633286
Loss at iteration 350 : 0.07002998888492584
Loss at iteration 360 : 0.10983689874410629
Loss at iteration 370 : 0.095504991710186
Loss at iteration 380 : 0.08388043940067291
Loss at iteration 390 : 0.09638278931379318
Loss at iteration 400 : 0.08467255532741547
Loss at iteration 410 : 0.07575792819261551
Loss at iteration 420 : 0.09664952754974365
Loss at iteration 430 : 0.13415071368217468
Loss at iteration 440 : 0.07051940262317657
Loss at iteration 450 : 0.09343981742858887
Loss at iteration 460 : 0.0801384449005127
Loss at iteration 470 : 0.06707236170768738
Loss at iteration 480 : 0.05861331522464752
Loss at iteration 490 : 0.07793815433979034
Loss at iteration 500 : 0.07582530379295349
Loss at iteration 510 : 0.05060930550098419
Loss at iteration 520 : 0.080564484000206
Loss at iteration 530 : 0.10870969295501709
Loss at iteration 540 : 0.08586806058883667
Loss at iteration 550 : 0.060777902603149414
Loss at iteration 560 : 0.10760509967803955
Loss at iteration 570 : 0.06668906658887863
Loss at iteration 580 : 0.0957232266664505
Loss at iteration 590 : 0.056642137467861176
Loss at iteration 600 : 0.10530897229909897
Loss at iteration 610 : 0.10466453433036804
Loss at iteration 620 : 0.09436198323965073
Loss at iteration 630 : 0.09918081760406494
Loss at iteration 640 : 0.06408513337373734
Loss at iteration 650 : 0.08486523479223251
Loss at iteration 660 : 0.06713263690471649
Loss at iteration 670 : 0.048171114176511765
Loss at iteration 680 : 0.11037318408489227
Loss at iteration 690 : 0.06786152720451355
Loss at iteration 700 : 0.11879228055477142
Loss at iteration 710 : 0.06587716192007065
Loss at iteration 720 : 0.061195503920316696
Loss at iteration 730 : 0.10380183160305023
Loss at iteration 740 : 0.07486070692539215
Loss at iteration 750 : 0.10512162744998932
Loss at iteration 760 : 0.07991668581962585
Loss at iteration 770 : 0.07097271084785461
Loss at iteration 780 : 0.08413644134998322
Loss at iteration 790 : 0.0604003481566906
Loss at iteration 800 : 0.04674101993441582
Loss at iteration 810 : 0.09153658151626587
Loss at iteration 820 : 0.10910539329051971
Loss at iteration 830 : 0.08408185094594955
Loss at iteration 840 : 0.07002808153629303
Loss at iteration 850 : 0.15451934933662415
Loss at iteration 860 : 0.07742641866207123
Loss at iteration 870 : 0.07202639430761337
Loss at iteration 880 : 0.09709036350250244
Loss at iteration 890 : 0.07959441840648651
Loss at iteration 900 : 0.09357701241970062
Loss at iteration 910 : 0.09686755388975143
Loss at iteration 920 : 0.04547969624400139
Loss at iteration 930 : 0.06381630152463913
Loss at iteration 940 : 0.04254057630896568
Loss at iteration 950 : 0.11255975812673569
Loss at iteration 960 : 0.10169561952352524
Loss at iteration 970 : 0.05844103544950485
Loss at iteration 980 : 0.12134215235710144
Loss at iteration 990 : 0.09563837945461273
Loss at iteration 1000 : 0.06323979794979095
Loss at iteration 1010 : 0.1582746058702469
Loss at iteration 1020 : 0.1042896956205368
Loss at iteration 1030 : 0.060435011982917786
Loss at iteration 1040 : 0.17114713788032532
Loss at iteration 1050 : 0.07047151029109955
Loss at iteration 1060 : 0.07839317619800568
Loss at iteration 1070 : 0.08241145312786102
Loss at iteration 1080 : 0.09010770916938782
Loss at iteration 1090 : 0.07190567255020142
Loss at iteration 1100 : 0.05979686230421066
Loss at iteration 1110 : 0.09052228927612305
Loss at iteration 1120 : 0.08923803269863129
Loss at iteration 1130 : 0.05087500065565109
Loss at iteration 1140 : 0.0702936202287674
Loss at iteration 1150 : 0.065582275390625
Loss at iteration 1160 : 0.07370129972696304
Loss at iteration 1170 : 0.06427927315235138
Loss at iteration 1180 : 0.08258886635303497
Loss at iteration 1190 : 0.07453927397727966
Loss at iteration 1200 : 0.06619980931282043
Loss at iteration 1210 : 0.06091804429888725
The SSIM Value is: 0.7182285030682881
The PSNR Value is: 21.1030392964681
the epoch is: 144
Loss at iteration 10 : 0.058405399322509766
Loss at iteration 20 : 0.04347570985555649
Loss at iteration 30 : 0.0589851550757885
Loss at iteration 40 : 0.0808371901512146
Loss at iteration 50 : 0.05239594727754593
Loss at iteration 60 : 0.06074884533882141
Loss at iteration 70 : 0.06329238414764404
Loss at iteration 80 : 0.11514638364315033
Loss at iteration 90 : 0.08394937217235565
Loss at iteration 100 : 0.0989607572555542
Loss at iteration 110 : 0.05730731412768364
Loss at iteration 120 : 0.1051824688911438
Loss at iteration 130 : 0.0820130780339241
Loss at iteration 140 : 0.06096215173602104
Loss at iteration 150 : 0.07645374536514282
Loss at iteration 160 : 0.09995684027671814
Loss at iteration 170 : 0.09328188747167587
Loss at iteration 180 : 0.0644274652004242
Loss at iteration 190 : 0.05934714525938034
Loss at iteration 200 : 0.07653271406888962
Loss at iteration 210 : 0.09642805904150009
Loss at iteration 220 : 0.08649159967899323
Loss at iteration 230 : 0.10305619239807129
Loss at iteration 240 : 0.09468065947294235
Loss at iteration 250 : 0.09273982048034668
Loss at iteration 260 : 0.039771080017089844
Loss at iteration 270 : 0.07370686531066895
Loss at iteration 280 : 0.06905391812324524
Loss at iteration 290 : 0.07559921592473984
Loss at iteration 300 : 0.07949025928974152
Loss at iteration 310 : 0.06880101561546326
Loss at iteration 320 : 0.0829879567027092
Loss at iteration 330 : 0.12161581218242645
Loss at iteration 340 : 0.10082200169563293
Loss at iteration 350 : 0.056508101522922516
Loss at iteration 360 : 0.07284140586853027
Loss at iteration 370 : 0.05128544196486473
Loss at iteration 380 : 0.09023275226354599
Loss at iteration 390 : 0.1428573727607727
Loss at iteration 400 : 0.07692095637321472
Loss at iteration 410 : 0.10161907970905304
Loss at iteration 420 : 0.13433444499969482
Loss at iteration 430 : 0.11194896697998047
Loss at iteration 440 : 0.05720506235957146
Loss at iteration 450 : 0.0968523621559143
Loss at iteration 460 : 0.05491127818822861
Loss at iteration 470 : 0.10573955625295639
Loss at iteration 480 : 0.13645371794700623
Loss at iteration 490 : 0.062304820865392685
Loss at iteration 500 : 0.06587239354848862
Loss at iteration 510 : 0.10900120437145233
Loss at iteration 520 : 0.10065168142318726
Loss at iteration 530 : 0.11901500076055527
Loss at iteration 540 : 0.08670397102832794
Loss at iteration 550 : 0.11140252649784088
Loss at iteration 560 : 0.05538515746593475
Loss at iteration 570 : 0.06270624697208405
Loss at iteration 580 : 0.07064614444971085
Loss at iteration 590 : 0.07945516705513
Loss at iteration 600 : 0.0658557265996933
Loss at iteration 610 : 0.09843532741069794
Loss at iteration 620 : 0.06385032832622528
Loss at iteration 630 : 0.13213567435741425
Loss at iteration 640 : 0.14951160550117493
Loss at iteration 650 : 0.057263828814029694
Loss at iteration 660 : 0.07347392290830612
Loss at iteration 670 : 0.09181857109069824
Loss at iteration 680 : 0.12615491449832916
Loss at iteration 690 : 0.07795803248882294
Loss at iteration 700 : 0.10550105571746826
Loss at iteration 710 : 0.12764817476272583
Loss at iteration 720 : 0.08234141767024994
Loss at iteration 730 : 0.09231540560722351
Loss at iteration 740 : 0.09235788881778717
Loss at iteration 750 : 0.06194900721311569
Loss at iteration 760 : 0.0633959025144577
Loss at iteration 770 : 0.07108145952224731
Loss at iteration 780 : 0.060933321714401245
Loss at iteration 790 : 0.0942140519618988
Loss at iteration 800 : 0.06836839020252228
Loss at iteration 810 : 0.07139919698238373
Loss at iteration 820 : 0.1189219206571579
Loss at iteration 830 : 0.08537798374891281
Loss at iteration 840 : 0.09198813140392303
Loss at iteration 850 : 0.09551086276769638
Loss at iteration 860 : 0.09321176260709763
Loss at iteration 870 : 0.1400231570005417
Loss at iteration 880 : 0.07485091686248779
Loss at iteration 890 : 0.06913530826568604
Loss at iteration 900 : 0.0693318247795105
Loss at iteration 910 : 0.10083501785993576
Loss at iteration 920 : 0.0891314372420311
Loss at iteration 930 : 0.07059677690267563
Loss at iteration 940 : 0.09815892577171326
Loss at iteration 950 : 0.10613477230072021
Loss at iteration 960 : 0.09348276257514954
Loss at iteration 970 : 0.07049833238124847
Loss at iteration 980 : 0.10110457986593246
Loss at iteration 990 : 0.08440990746021271
Loss at iteration 1000 : 0.13486704230308533
Loss at iteration 1010 : 0.07740680873394012
Loss at iteration 1020 : 0.07290923595428467
Loss at iteration 1030 : 0.14104315638542175
Loss at iteration 1040 : 0.11606985330581665
Loss at iteration 1050 : 0.0977136418223381
Loss at iteration 1060 : 0.0628674179315567
Loss at iteration 1070 : 0.14304277300834656
Loss at iteration 1080 : 0.06329459697008133
Loss at iteration 1090 : 0.10067787021398544
Loss at iteration 1100 : 0.07717477530241013
Loss at iteration 1110 : 0.07497444748878479
Loss at iteration 1120 : 0.06577808409929276
Loss at iteration 1130 : 0.09390982240438461
Loss at iteration 1140 : 0.08266378939151764
Loss at iteration 1150 : 0.10522955656051636
Loss at iteration 1160 : 0.09715556353330612
Loss at iteration 1170 : 0.0753876268863678
Loss at iteration 1180 : 0.08802219480276108
Loss at iteration 1190 : 0.10750644654035568
Loss at iteration 1200 : 0.06811219453811646
Loss at iteration 1210 : 0.10523857176303864
The SSIM Value is: 0.7150346318880717
The PSNR Value is: 20.80325419108073
the epoch is: 145
Loss at iteration 10 : 0.11311481893062592
Loss at iteration 20 : 0.047300081700086594
Loss at iteration 30 : 0.07179676741361618
Loss at iteration 40 : 0.13291102647781372
Loss at iteration 50 : 0.11799751222133636
Loss at iteration 60 : 0.06062614172697067
Loss at iteration 70 : 0.07628457248210907
Loss at iteration 80 : 0.06985239684581757
Loss at iteration 90 : 0.05804325267672539
Loss at iteration 100 : 0.0353127047419548
Loss at iteration 110 : 0.07099799066781998
Loss at iteration 120 : 0.07509471476078033
Loss at iteration 130 : 0.05486943945288658
Loss at iteration 140 : 0.12211307138204575
Loss at iteration 150 : 0.11162742227315903
Loss at iteration 160 : 0.057442884892225266
Loss at iteration 170 : 0.07893016934394836
Loss at iteration 180 : 0.09622132778167725
Loss at iteration 190 : 0.06670121848583221
Loss at iteration 200 : 0.0945226401090622
Loss at iteration 210 : 0.1364956498146057
Loss at iteration 220 : 0.11863629519939423
Loss at iteration 230 : 0.09227268397808075
Loss at iteration 240 : 0.08529643714427948
Loss at iteration 250 : 0.09458604454994202
Loss at iteration 260 : 0.07504363358020782
Loss at iteration 270 : 0.07975029200315475
Loss at iteration 280 : 0.050402622669935226
Loss at iteration 290 : 0.07880677282810211
Loss at iteration 300 : 0.06109604239463806
Loss at iteration 310 : 0.0701780691742897
Loss at iteration 320 : 0.10838888585567474
Loss at iteration 330 : 0.06921457499265671
Loss at iteration 340 : 0.0974351167678833
Loss at iteration 350 : 0.0958511233329773
Loss at iteration 360 : 0.07818529009819031
Loss at iteration 370 : 0.08278264850378036
Loss at iteration 380 : 0.040799330919981
Loss at iteration 390 : 0.08492472022771835
Loss at iteration 400 : 0.0709507018327713
Loss at iteration 410 : 0.052215587347745895
Loss at iteration 420 : 0.07411521673202515
Loss at iteration 430 : 0.061286550015211105
Loss at iteration 440 : 0.06941084563732147
Loss at iteration 450 : 0.060012802481651306
Loss at iteration 460 : 0.09145192801952362
Loss at iteration 470 : 0.08504419028759003
Loss at iteration 480 : 0.11713238805532455
Loss at iteration 490 : 0.06095391511917114
Loss at iteration 500 : 0.08412058651447296
Loss at iteration 510 : 0.06985455751419067
Loss at iteration 520 : 0.07633504271507263
Loss at iteration 530 : 0.12317131459712982
Loss at iteration 540 : 0.07161583006381989
Loss at iteration 550 : 0.06533204019069672
Loss at iteration 560 : 0.07611429691314697
Loss at iteration 570 : 0.08483317494392395
Loss at iteration 580 : 0.09375300258398056
Loss at iteration 590 : 0.06564248353242874
Loss at iteration 600 : 0.04193063825368881
Loss at iteration 610 : 0.07332228869199753
Loss at iteration 620 : 0.06639416515827179
Loss at iteration 630 : 0.09123309701681137
Loss at iteration 640 : 0.12103470414876938
Loss at iteration 650 : 0.06536789983510971
Loss at iteration 660 : 0.11067026853561401
Loss at iteration 670 : 0.05424629896879196
Loss at iteration 680 : 0.09870865195989609
Loss at iteration 690 : 0.06838931143283844
Loss at iteration 700 : 0.09437644481658936
Loss at iteration 710 : 0.06879083067178726
Loss at iteration 720 : 0.07683171331882477
Loss at iteration 730 : 0.05266329646110535
Loss at iteration 740 : 0.10945278406143188
Loss at iteration 750 : 0.06507548689842224
Loss at iteration 760 : 0.08340738713741302
Loss at iteration 770 : 0.07367058098316193
Loss at iteration 780 : 0.06450803577899933
Loss at iteration 790 : 0.07112603634595871
Loss at iteration 800 : 0.10789413750171661
Loss at iteration 810 : 0.09806936234235764
Loss at iteration 820 : 0.08077828586101532
Loss at iteration 830 : 0.09441106021404266
Loss at iteration 840 : 0.08493725955486298
Loss at iteration 850 : 0.10361975431442261
Loss at iteration 860 : 0.094978928565979
Loss at iteration 870 : 0.060203712433576584
Loss at iteration 880 : 0.07900728285312653
Loss at iteration 890 : 0.08112438768148422
Loss at iteration 900 : 0.12516003847122192
Loss at iteration 910 : 0.11275361478328705
Loss at iteration 920 : 0.12326009571552277
Loss at iteration 930 : 0.10804973542690277
Loss at iteration 940 : 0.10920363664627075
Loss at iteration 950 : 0.09786993265151978
Loss at iteration 960 : 0.044665347784757614
Loss at iteration 970 : 0.08300697803497314
Loss at iteration 980 : 0.13259491324424744
Loss at iteration 990 : 0.10865044593811035
Loss at iteration 1000 : 0.08073176443576813
Loss at iteration 1010 : 0.10438716411590576
Loss at iteration 1020 : 0.05412972718477249
Loss at iteration 1030 : 0.10228721797466278
Loss at iteration 1040 : 0.08755895495414734
Loss at iteration 1050 : 0.0661819726228714
Loss at iteration 1060 : 0.0810500830411911
Loss at iteration 1070 : 0.05737411230802536
Loss at iteration 1080 : 0.1089305728673935
Loss at iteration 1090 : 0.0955323576927185
Loss at iteration 1100 : 0.06886221468448639
Loss at iteration 1110 : 0.06603477895259857
Loss at iteration 1120 : 0.05295775085687637
Loss at iteration 1130 : 0.07703092694282532
Loss at iteration 1140 : 0.09588274359703064
Loss at iteration 1150 : 0.09134795516729355
Loss at iteration 1160 : 0.08498796075582504
Loss at iteration 1170 : 0.08640609681606293
Loss at iteration 1180 : 0.06196404993534088
Loss at iteration 1190 : 0.1325889676809311
Loss at iteration 1200 : 0.083454929292202
Loss at iteration 1210 : 0.09503340721130371
The SSIM Value is: 0.7209702968597412
The PSNR Value is: 21.086492919921874
the epoch is: 146
Loss at iteration 10 : 0.0812615379691124
Loss at iteration 20 : 0.08240793645381927
Loss at iteration 30 : 0.09139280766248703
Loss at iteration 40 : 0.10825026035308838
Loss at iteration 50 : 0.140960693359375
Loss at iteration 60 : 0.14715033769607544
Loss at iteration 70 : 0.04928101226687431
Loss at iteration 80 : 0.08606237173080444
Loss at iteration 90 : 0.08698777854442596
Loss at iteration 100 : 0.09190419316291809
Loss at iteration 110 : 0.0821027010679245
Loss at iteration 120 : 0.09722602367401123
Loss at iteration 130 : 0.11469405144453049
Loss at iteration 140 : 0.052103277295827866
Loss at iteration 150 : 0.12307622283697128
Loss at iteration 160 : 0.10068671405315399
Loss at iteration 170 : 0.060725823044776917
Loss at iteration 180 : 0.06393346190452576
Loss at iteration 190 : 0.05924741178750992
Loss at iteration 200 : 0.07813866436481476
Loss at iteration 210 : 0.07297992706298828
Loss at iteration 220 : 0.06677984446287155
Loss at iteration 230 : 0.07378468662500381
Loss at iteration 240 : 0.0811605304479599
Loss at iteration 250 : 0.10987545549869537
Loss at iteration 260 : 0.08912753313779831
Loss at iteration 270 : 0.12581543624401093
Loss at iteration 280 : 0.08347296714782715
Loss at iteration 290 : 0.06967809051275253
Loss at iteration 300 : 0.061345215886831284
Loss at iteration 310 : 0.0822451114654541
Loss at iteration 320 : 0.06754745543003082
Loss at iteration 330 : 0.10833734273910522
Loss at iteration 340 : 0.0541774146258831
Loss at iteration 350 : 0.11386352777481079
Loss at iteration 360 : 0.09459563344717026
Loss at iteration 370 : 0.1056215688586235
Loss at iteration 380 : 0.0862807035446167
Loss at iteration 390 : 0.08931045234203339
Loss at iteration 400 : 0.14107465744018555
Loss at iteration 410 : 0.056509435176849365
Loss at iteration 420 : 0.05517345666885376
Loss at iteration 430 : 0.0873032808303833
Loss at iteration 440 : 0.1242336630821228
Loss at iteration 450 : 0.12477831542491913
Loss at iteration 460 : 0.051555335521698
Loss at iteration 470 : 0.07892273366451263
Loss at iteration 480 : 0.06301510334014893
Loss at iteration 490 : 0.05093654245138168
Loss at iteration 500 : 0.051712267100811005
Loss at iteration 510 : 0.11747555434703827
Loss at iteration 520 : 0.09797748923301697
Loss at iteration 530 : 0.06257537752389908
Loss at iteration 540 : 0.1258120983839035
Loss at iteration 550 : 0.15676349401474
Loss at iteration 560 : 0.12305919826030731
Loss at iteration 570 : 0.09607984870672226
Loss at iteration 580 : 0.07321831583976746
Loss at iteration 590 : 0.06517627835273743
Loss at iteration 600 : 0.05475084111094475
Loss at iteration 610 : 0.09951206296682358
Loss at iteration 620 : 0.06228616461157799
Loss at iteration 630 : 0.10752654075622559
Loss at iteration 640 : 0.09380041062831879
Loss at iteration 650 : 0.09750517457723618
Loss at iteration 660 : 0.11988361179828644
Loss at iteration 670 : 0.08829992264509201
Loss at iteration 680 : 0.09056176990270615
Loss at iteration 690 : 0.08391310274600983
Loss at iteration 700 : 0.04981973394751549
Loss at iteration 710 : 0.04712669551372528
Loss at iteration 720 : 0.06355534493923187
Loss at iteration 730 : 0.09569372981786728
Loss at iteration 740 : 0.08733298629522324
Loss at iteration 750 : 0.08574831485748291
Loss at iteration 760 : 0.09340866655111313
Loss at iteration 770 : 0.09267010539770126
Loss at iteration 780 : 0.08303110301494598
Loss at iteration 790 : 0.09220412373542786
Loss at iteration 800 : 0.07365669310092926
Loss at iteration 810 : 0.04771075397729874
Loss at iteration 820 : 0.07710902392864227
Loss at iteration 830 : 0.0951404869556427
Loss at iteration 840 : 0.10343142598867416
Loss at iteration 850 : 0.09716518968343735
Loss at iteration 860 : 0.08159913122653961
Loss at iteration 870 : 0.08771015703678131
Loss at iteration 880 : 0.06254781782627106
Loss at iteration 890 : 0.09762628376483917
Loss at iteration 900 : 0.07329392433166504
Loss at iteration 910 : 0.058125004172325134
Loss at iteration 920 : 0.07585464417934418
Loss at iteration 930 : 0.07554557174444199
Loss at iteration 940 : 0.06459169834852219
Loss at iteration 950 : 0.06849177181720734
Loss at iteration 960 : 0.10415460169315338
Loss at iteration 970 : 0.10839371383190155
Loss at iteration 980 : 0.06569197028875351
Loss at iteration 990 : 0.0770031213760376
Loss at iteration 1000 : 0.09446223080158234
Loss at iteration 1010 : 0.08547147363424301
Loss at iteration 1020 : 0.07459834218025208
Loss at iteration 1030 : 0.07725201547145844
Loss at iteration 1040 : 0.08027508854866028
Loss at iteration 1050 : 0.09256790578365326
Loss at iteration 1060 : 0.09661757946014404
Loss at iteration 1070 : 0.1344306617975235
Loss at iteration 1080 : 0.09383408725261688
Loss at iteration 1090 : 0.08331321179866791
Loss at iteration 1100 : 0.07283610105514526
Loss at iteration 1110 : 0.08375781029462814
Loss at iteration 1120 : 0.05567798763513565
Loss at iteration 1130 : 0.06718277186155319
Loss at iteration 1140 : 0.10072826594114304
Loss at iteration 1150 : 0.048750847578048706
Loss at iteration 1160 : 0.05579378455877304
Loss at iteration 1170 : 0.110385462641716
Loss at iteration 1180 : 0.13036707043647766
Loss at iteration 1190 : 0.08595836162567139
Loss at iteration 1200 : 0.08160246908664703
Loss at iteration 1210 : 0.061125218868255615
The SSIM Value is: 0.7165268699328105
The PSNR Value is: 21.29245045979818
the epoch is: 147
Loss at iteration 10 : 0.07715758681297302
Loss at iteration 20 : 0.05926365777850151
Loss at iteration 30 : 0.10972750186920166
Loss at iteration 40 : 0.11327263712882996
Loss at iteration 50 : 0.07693620771169662
Loss at iteration 60 : 0.0751759260892868
Loss at iteration 70 : 0.0962321013212204
Loss at iteration 80 : 0.12331943958997726
Loss at iteration 90 : 0.08381915092468262
Loss at iteration 100 : 0.06321264803409576
Loss at iteration 110 : 0.08190834522247314
Loss at iteration 120 : 0.08701187372207642
Loss at iteration 130 : 0.1084509938955307
Loss at iteration 140 : 0.10987644642591476
Loss at iteration 150 : 0.08920323848724365
Loss at iteration 160 : 0.10400839149951935
Loss at iteration 170 : 0.07509016990661621
Loss at iteration 180 : 0.06732644140720367
Loss at iteration 190 : 0.08166929334402084
Loss at iteration 200 : 0.09037899971008301
Loss at iteration 210 : 0.10681888461112976
Loss at iteration 220 : 0.07904887199401855
Loss at iteration 230 : 0.09145315736532211
Loss at iteration 240 : 0.07728531211614609
Loss at iteration 250 : 0.08517210930585861
Loss at iteration 260 : 0.06161130592226982
Loss at iteration 270 : 0.055074602365493774
Loss at iteration 280 : 0.07719551771879196
Loss at iteration 290 : 0.08142934739589691
Loss at iteration 300 : 0.09770248830318451
Loss at iteration 310 : 0.08823003619909286
Loss at iteration 320 : 0.07806678116321564
Loss at iteration 330 : 0.058615587651729584
Loss at iteration 340 : 0.06047090142965317
Loss at iteration 350 : 0.0754978358745575
Loss at iteration 360 : 0.08664971590042114
Loss at iteration 370 : 0.06523359566926956
Loss at iteration 380 : 0.10199454426765442
Loss at iteration 390 : 0.07697239518165588
Loss at iteration 400 : 0.09755223244428635
Loss at iteration 410 : 0.0744793564081192
Loss at iteration 420 : 0.05362109839916229
Loss at iteration 430 : 0.08782365918159485
Loss at iteration 440 : 0.11749541759490967
Loss at iteration 450 : 0.04413478821516037
Loss at iteration 460 : 0.07438602298498154
Loss at iteration 470 : 0.06217086315155029
Loss at iteration 480 : 0.14445236325263977
Loss at iteration 490 : 0.07328791916370392
Loss at iteration 500 : 0.05289658159017563
Loss at iteration 510 : 0.06954926252365112
Loss at iteration 520 : 0.09254339337348938
Loss at iteration 530 : 0.08425106853246689
Loss at iteration 540 : 0.08173374831676483
Loss at iteration 550 : 0.08582615107297897
Loss at iteration 560 : 0.05957949906587601
Loss at iteration 570 : 0.07110342383384705
Loss at iteration 580 : 0.08325192332267761
Loss at iteration 590 : 0.08494430035352707
Loss at iteration 600 : 0.1082439050078392
Loss at iteration 610 : 0.07930858433246613
Loss at iteration 620 : 0.0693001076579094
Loss at iteration 630 : 0.06852354109287262
Loss at iteration 640 : 0.11346423625946045
Loss at iteration 650 : 0.08797609806060791
Loss at iteration 660 : 0.06982442736625671
Loss at iteration 670 : 0.0630192831158638
Loss at iteration 680 : 0.0806196928024292
Loss at iteration 690 : 0.048360731452703476
Loss at iteration 700 : 0.09251796454191208
Loss at iteration 710 : 0.10503266751766205
Loss at iteration 720 : 0.08476781845092773
Loss at iteration 730 : 0.0803435891866684
Loss at iteration 740 : 0.0830773264169693
Loss at iteration 750 : 0.10198968648910522
Loss at iteration 760 : 0.06306533515453339
Loss at iteration 770 : 0.09735314548015594
Loss at iteration 780 : 0.05965863913297653
Loss at iteration 790 : 0.06333678215742111
Loss at iteration 800 : 0.1173342615365982
Loss at iteration 810 : 0.05875099450349808
Loss at iteration 820 : 0.0936526358127594
Loss at iteration 830 : 0.07883305847644806
Loss at iteration 840 : 0.06483897566795349
Loss at iteration 850 : 0.05082513019442558
Loss at iteration 860 : 0.10689420998096466
Loss at iteration 870 : 0.07797929644584656
Loss at iteration 880 : 0.10173752158880234
Loss at iteration 890 : 0.07793492078781128
Loss at iteration 900 : 0.043893005698919296
Loss at iteration 910 : 0.12113726139068604
Loss at iteration 920 : 0.0664968341588974
Loss at iteration 930 : 0.05879857391119003
Loss at iteration 940 : 0.08679607510566711
Loss at iteration 950 : 0.0771697387099266
Loss at iteration 960 : 0.08847560733556747
Loss at iteration 970 : 0.08668039739131927
Loss at iteration 980 : 0.07795130461454391
Loss at iteration 990 : 0.0790473148226738
Loss at iteration 1000 : 0.07720588147640228
Loss at iteration 1010 : 0.09348707646131516
Loss at iteration 1020 : 0.07141207158565521
Loss at iteration 1030 : 0.0718790590763092
Loss at iteration 1040 : 0.10212013125419617
Loss at iteration 1050 : 0.06759902834892273
Loss at iteration 1060 : 0.08415873348712921
Loss at iteration 1070 : 0.07062820345163345
Loss at iteration 1080 : 0.09141453355550766
Loss at iteration 1090 : 0.06622286140918732
Loss at iteration 1100 : 0.12303468585014343
Loss at iteration 1110 : 0.05226876959204674
Loss at iteration 1120 : 0.06389136612415314
Loss at iteration 1130 : 0.07949985563755035
Loss at iteration 1140 : 0.07940124720335007
Loss at iteration 1150 : 0.09240369498729706
Loss at iteration 1160 : 0.10292774438858032
Loss at iteration 1170 : 0.08114945888519287
Loss at iteration 1180 : 0.1018417477607727
Loss at iteration 1190 : 0.062294475734233856
Loss at iteration 1200 : 0.09171950817108154
Loss at iteration 1210 : 0.09599071741104126
The SSIM Value is: 0.715115495522817
The PSNR Value is: 20.958628590901693
the epoch is: 148
Loss at iteration 10 : 0.06935091316699982
Loss at iteration 20 : 0.0859636589884758
Loss at iteration 30 : 0.09189131110906601
Loss at iteration 40 : 0.0615570954978466
Loss at iteration 50 : 0.11700155586004257
Loss at iteration 60 : 0.09708428382873535
Loss at iteration 70 : 0.08817659318447113
Loss at iteration 80 : 0.06614605337381363
Loss at iteration 90 : 0.06619405001401901
Loss at iteration 100 : 0.12855887413024902
Loss at iteration 110 : 0.06985176354646683
Loss at iteration 120 : 0.04462794214487076
Loss at iteration 130 : 0.11134020984172821
Loss at iteration 140 : 0.08079876005649567
Loss at iteration 150 : 0.06499634683132172
Loss at iteration 160 : 0.04934978112578392
Loss at iteration 170 : 0.11066710948944092
Loss at iteration 180 : 0.08203934133052826
Loss at iteration 190 : 0.05450785160064697
Loss at iteration 200 : 0.06141229718923569
Loss at iteration 210 : 0.10339462757110596
Loss at iteration 220 : 0.05185002088546753
Loss at iteration 230 : 0.09768204391002655
Loss at iteration 240 : 0.11490142345428467
Loss at iteration 250 : 0.08322896808385849
Loss at iteration 260 : 0.08908802270889282
Loss at iteration 270 : 0.07974317669868469
Loss at iteration 280 : 0.05335621163249016
Loss at iteration 290 : 0.07445168495178223
Loss at iteration 300 : 0.08037146925926208
Loss at iteration 310 : 0.10332435369491577
Loss at iteration 320 : 0.04487317055463791
Loss at iteration 330 : 0.08823291957378387
Loss at iteration 340 : 0.08562454581260681
Loss at iteration 350 : 0.11997443437576294
Loss at iteration 360 : 0.09189675748348236
Loss at iteration 370 : 0.09009198844432831
Loss at iteration 380 : 0.1065656840801239
Loss at iteration 390 : 0.07784722745418549
Loss at iteration 400 : 0.07211034744977951
Loss at iteration 410 : 0.05327250808477402
Loss at iteration 420 : 0.08371540158987045
Loss at iteration 430 : 0.0984697937965393
Loss at iteration 440 : 0.06641335785388947
Loss at iteration 450 : 0.08648713678121567
Loss at iteration 460 : 0.07522916048765182
Loss at iteration 470 : 0.07263697683811188
Loss at iteration 480 : 0.10959571599960327
Loss at iteration 490 : 0.05492115020751953
Loss at iteration 500 : 0.08709033578634262
Loss at iteration 510 : 0.0942535325884819
Loss at iteration 520 : 0.10068552196025848
Loss at iteration 530 : 0.06665267795324326
Loss at iteration 540 : 0.048333510756492615
Loss at iteration 550 : 0.051272835582494736
Loss at iteration 560 : 0.08833472430706024
Loss at iteration 570 : 0.12659020721912384
Loss at iteration 580 : 0.117745041847229
Loss at iteration 590 : 0.05781827121973038
Loss at iteration 600 : 0.08389832079410553
Loss at iteration 610 : 0.09087231010198593
Loss at iteration 620 : 0.11766816675662994
Loss at iteration 630 : 0.0799708217382431
Loss at iteration 640 : 0.08107464015483856
Loss at iteration 650 : 0.08359336853027344
Loss at iteration 660 : 0.10353308171033859
Loss at iteration 670 : 0.05450240522623062
Loss at iteration 680 : 0.09552960097789764
Loss at iteration 690 : 0.11539791524410248
Loss at iteration 700 : 0.06351368129253387
Loss at iteration 710 : 0.09142874926328659
Loss at iteration 720 : 0.06210608035326004
Loss at iteration 730 : 0.0938364714384079
Loss at iteration 740 : 0.06670767068862915
Loss at iteration 750 : 0.07974900305271149
Loss at iteration 760 : 0.08656565845012665
Loss at iteration 770 : 0.1145915612578392
Loss at iteration 780 : 0.07085460424423218
Loss at iteration 790 : 0.109231136739254
Loss at iteration 800 : 0.14291350543498993
Loss at iteration 810 : 0.08508598804473877
Loss at iteration 820 : 0.09298735857009888
Loss at iteration 830 : 0.0827060341835022
Loss at iteration 840 : 0.08497395366430283
Loss at iteration 850 : 0.09014713764190674
Loss at iteration 860 : 0.06609433889389038
Loss at iteration 870 : 0.07014740258455276
Loss at iteration 880 : 0.0993707999587059
Loss at iteration 890 : 0.07973295450210571
Loss at iteration 900 : 0.07916446775197983
Loss at iteration 910 : 0.0961267352104187
Loss at iteration 920 : 0.08279329538345337
Loss at iteration 930 : 0.10407677292823792
Loss at iteration 940 : 0.04534084349870682
Loss at iteration 950 : 0.10489627718925476
Loss at iteration 960 : 0.0985206589102745
Loss at iteration 970 : 0.07678685337305069
Loss at iteration 980 : 0.0922139510512352
Loss at iteration 990 : 0.039644401520490646
Loss at iteration 1000 : 0.07644560188055038
Loss at iteration 1010 : 0.06123950332403183
Loss at iteration 1020 : 0.08328437060117722
Loss at iteration 1030 : 0.054011665284633636
Loss at iteration 1040 : 0.08528702706098557
Loss at iteration 1050 : 0.07168564200401306
Loss at iteration 1060 : 0.0668686032295227
Loss at iteration 1070 : 0.07998811453580856
Loss at iteration 1080 : 0.08022342622280121
Loss at iteration 1090 : 0.07628270983695984
Loss at iteration 1100 : 0.06208300590515137
Loss at iteration 1110 : 0.05401604622602463
Loss at iteration 1120 : 0.0807926282286644
Loss at iteration 1130 : 0.05879862233996391
Loss at iteration 1140 : 0.10450084507465363
Loss at iteration 1150 : 0.09741850942373276
Loss at iteration 1160 : 0.11235054582357407
Loss at iteration 1170 : 0.0568636879324913
Loss at iteration 1180 : 0.10668065398931503
Loss at iteration 1190 : 0.08262230455875397
Loss at iteration 1200 : 0.09190758317708969
Loss at iteration 1210 : 0.08236031234264374
The SSIM Value is: 0.7216180404027303
The PSNR Value is: 21.34201011657715
the epoch is: 149
Loss at iteration 10 : 0.07214419543743134
Loss at iteration 20 : 0.10922019183635712
Loss at iteration 30 : 0.06238753721117973
Loss at iteration 40 : 0.05581560730934143
Loss at iteration 50 : 0.14580479264259338
Loss at iteration 60 : 0.10757090896368027
Loss at iteration 70 : 0.0828675702214241
Loss at iteration 80 : 0.11928912252187729
Loss at iteration 90 : 0.06638070195913315
Loss at iteration 100 : 0.04589570313692093
Loss at iteration 110 : 0.07511576265096664
Loss at iteration 120 : 0.06017998978495598
Loss at iteration 130 : 0.06470318138599396
Loss at iteration 140 : 0.06320526450872421
Loss at iteration 150 : 0.05782574415206909
Loss at iteration 160 : 0.10534443706274033
Loss at iteration 170 : 0.09314560145139694
Loss at iteration 180 : 0.07797285914421082
Loss at iteration 190 : 0.061625927686691284
Loss at iteration 200 : 0.07620759308338165
Loss at iteration 210 : 0.0602521114051342
Loss at iteration 220 : 0.07345949113368988
Loss at iteration 230 : 0.0729011818766594
Loss at iteration 240 : 0.09687986224889755
Loss at iteration 250 : 0.05444399639964104
Loss at iteration 260 : 0.08196721225976944
Loss at iteration 270 : 0.08086331188678741
Loss at iteration 280 : 0.09586792439222336
Loss at iteration 290 : 0.08914075791835785
Loss at iteration 300 : 0.04354891926050186
Loss at iteration 310 : 0.09779131412506104
Loss at iteration 320 : 0.0737273320555687
Loss at iteration 330 : 0.07604848593473434
Loss at iteration 340 : 0.07896507531404495
Loss at iteration 350 : 0.10700920224189758
Loss at iteration 360 : 0.06632202118635178
Loss at iteration 370 : 0.11762627214193344
Loss at iteration 380 : 0.06155889481306076
Loss at iteration 390 : 0.07635974884033203
Loss at iteration 400 : 0.12413966655731201
Loss at iteration 410 : 0.09003166854381561
Loss at iteration 420 : 0.05670929700136185
Loss at iteration 430 : 0.08771954476833344
Loss at iteration 440 : 0.05079476162791252
Loss at iteration 450 : 0.053300462663173676
Loss at iteration 460 : 0.07343578338623047
Loss at iteration 470 : 0.10468187183141708
Loss at iteration 480 : 0.14295613765716553
Loss at iteration 490 : 0.14571848511695862
Loss at iteration 500 : 0.08558209240436554
Loss at iteration 510 : 0.11259689927101135
Loss at iteration 520 : 0.07163281738758087
Loss at iteration 530 : 0.0512203685939312
Loss at iteration 540 : 0.06292896717786789
Loss at iteration 550 : 0.037210188806056976
Loss at iteration 560 : 0.05748170241713524
Loss at iteration 570 : 0.07770916819572449
Loss at iteration 580 : 0.09545165300369263
Loss at iteration 590 : 0.13084523379802704
Loss at iteration 600 : 0.09199780225753784
Loss at iteration 610 : 0.10533084720373154
Loss at iteration 620 : 0.08680105209350586
Loss at iteration 630 : 0.07521750777959824
Loss at iteration 640 : 0.15180805325508118
Loss at iteration 650 : 0.0800822302699089
Loss at iteration 660 : 0.09824524819850922
Loss at iteration 670 : 0.10613958537578583
Loss at iteration 680 : 0.0716293454170227
Loss at iteration 690 : 0.04015355557203293
Loss at iteration 700 : 0.08320318162441254
Loss at iteration 710 : 0.1040823683142662
Loss at iteration 720 : 0.06498920917510986
Loss at iteration 730 : 0.049796830862760544
Loss at iteration 740 : 0.09022325277328491
Loss at iteration 750 : 0.06918524205684662
Loss at iteration 760 : 0.1208525151014328
Loss at iteration 770 : 0.07635316997766495
Loss at iteration 780 : 0.09619702398777008
Loss at iteration 790 : 0.07322828471660614
Loss at iteration 800 : 0.07326553761959076
Loss at iteration 810 : 0.076534204185009
Loss at iteration 820 : 0.1175404042005539
Loss at iteration 830 : 0.08263827115297318
Loss at iteration 840 : 0.09450165927410126
Loss at iteration 850 : 0.1182158812880516
Loss at iteration 860 : 0.0932205468416214
Loss at iteration 870 : 0.061037577688694
Loss at iteration 880 : 0.09262901544570923
Loss at iteration 890 : 0.06306284666061401
Loss at iteration 900 : 0.08019386231899261
Loss at iteration 910 : 0.0950503796339035
Loss at iteration 920 : 0.12949800491333008
Loss at iteration 930 : 0.13728660345077515
Loss at iteration 940 : 0.08376915752887726
Loss at iteration 950 : 0.05487921088933945
Loss at iteration 960 : 0.09443297982215881
Loss at iteration 970 : 0.09123349189758301
Loss at iteration 980 : 0.06635620445013046
Loss at iteration 990 : 0.07493932545185089
Loss at iteration 1000 : 0.07070821523666382
Loss at iteration 1010 : 0.08319584280252457
Loss at iteration 1020 : 0.0703357681632042
Loss at iteration 1030 : 0.05902869999408722
Loss at iteration 1040 : 0.06256318092346191
Loss at iteration 1050 : 0.06313569843769073
Loss at iteration 1060 : 0.07716576009988785
Loss at iteration 1070 : 0.08981331437826157
Loss at iteration 1080 : 0.05987672135233879
Loss at iteration 1090 : 0.10606592893600464
Loss at iteration 1100 : 0.06254560500383377
Loss at iteration 1110 : 0.07017027586698532
Loss at iteration 1120 : 0.06438177078962326
Loss at iteration 1130 : 0.08596479892730713
Loss at iteration 1140 : 0.06108199805021286
Loss at iteration 1150 : 0.1091349720954895
Loss at iteration 1160 : 0.09391093254089355
Loss at iteration 1170 : 0.07193897664546967
Loss at iteration 1180 : 0.0608162097632885
Loss at iteration 1190 : 0.11265220493078232
Loss at iteration 1200 : 0.07838299125432968
Loss at iteration 1210 : 0.07148948311805725
The SSIM Value is: 0.7177737673123677
The PSNR Value is: 20.85330187479655
the epoch is: 150
Loss at iteration 10 : 0.09220264852046967
Loss at iteration 20 : 0.049155525863170624
Loss at iteration 30 : 0.10235390067100525
Loss at iteration 40 : 0.09761660546064377
Loss at iteration 50 : 0.11007411777973175
Loss at iteration 60 : 0.09443800896406174
Loss at iteration 70 : 0.062182582914829254
Loss at iteration 80 : 0.10615723580121994
Loss at iteration 90 : 0.10820002853870392
Loss at iteration 100 : 0.041842781007289886
Loss at iteration 110 : 0.10665445029735565
Loss at iteration 120 : 0.06683723628520966
Loss at iteration 130 : 0.1737329661846161
Loss at iteration 140 : 0.11532412469387054
Loss at iteration 150 : 0.050176963210105896
Loss at iteration 160 : 0.0731894001364708
Loss at iteration 170 : 0.09625713527202606
Loss at iteration 180 : 0.09000249952077866
Loss at iteration 190 : 0.07098780572414398
Loss at iteration 200 : 0.10234054923057556
Loss at iteration 210 : 0.09143124520778656
Loss at iteration 220 : 0.11254464834928513
Loss at iteration 230 : 0.10225312411785126
Loss at iteration 240 : 0.08489077538251877
Loss at iteration 250 : 0.09072338044643402
Loss at iteration 260 : 0.08081626147031784
Loss at iteration 270 : 0.10596515983343124
Loss at iteration 280 : 0.07186654955148697
Loss at iteration 290 : 0.056324951350688934
Loss at iteration 300 : 0.08511602878570557
Loss at iteration 310 : 0.07483860850334167
Loss at iteration 320 : 0.09372960031032562
Loss at iteration 330 : 0.05737197399139404
Loss at iteration 340 : 0.0586279034614563
Loss at iteration 350 : 0.08737242221832275
Loss at iteration 360 : 0.09426429122686386
Loss at iteration 370 : 0.06777777522802353
Loss at iteration 380 : 0.08410005271434784
Loss at iteration 390 : 0.09108336269855499
Loss at iteration 400 : 0.05842825770378113
Loss at iteration 410 : 0.05267022177577019
Loss at iteration 420 : 0.060920845717191696
Loss at iteration 430 : 0.051819853484630585
Loss at iteration 440 : 0.07238063216209412
Loss at iteration 450 : 0.059485696256160736
Loss at iteration 460 : 0.07999362051486969
Loss at iteration 470 : 0.07612891495227814
Loss at iteration 480 : 0.11081601679325104
Loss at iteration 490 : 0.13010674715042114
Loss at iteration 500 : 0.10524523258209229
Loss at iteration 510 : 0.10065587610006332
Loss at iteration 520 : 0.053891610354185104
Loss at iteration 530 : 0.10336240381002426
Loss at iteration 540 : 0.10204889625310898
Loss at iteration 550 : 0.08653061091899872
Loss at iteration 560 : 0.09174370765686035
Loss at iteration 570 : 0.08142975717782974
Loss at iteration 580 : 0.095942422747612
Loss at iteration 590 : 0.06507070362567902
Loss at iteration 600 : 0.05801151320338249
Loss at iteration 610 : 0.08337192982435226
Loss at iteration 620 : 0.07423445582389832
Loss at iteration 630 : 0.11828073114156723
Loss at iteration 640 : 0.06250408291816711
Loss at iteration 650 : 0.05517362803220749
Loss at iteration 660 : 0.08480837196111679
Loss at iteration 670 : 0.07910370826721191
Loss at iteration 680 : 0.07985672354698181
Loss at iteration 690 : 0.11043448746204376
Loss at iteration 700 : 0.07926815748214722
Loss at iteration 710 : 0.1175704151391983
Loss at iteration 720 : 0.09398334473371506
Loss at iteration 730 : 0.0957670658826828
Loss at iteration 740 : 0.07075609266757965
Loss at iteration 750 : 0.10144519060850143
Loss at iteration 760 : 0.06820130348205566
Loss at iteration 770 : 0.07831113040447235
Loss at iteration 780 : 0.06703711301088333
Loss at iteration 790 : 0.06373295187950134
Loss at iteration 800 : 0.08764071762561798
Loss at iteration 810 : 0.06258460879325867
Loss at iteration 820 : 0.08988092839717865
Loss at iteration 830 : 0.04851352423429489
Loss at iteration 840 : 0.07322489470243454
Loss at iteration 850 : 0.1026228666305542
Loss at iteration 860 : 0.12116273492574692
Loss at iteration 870 : 0.07642784714698792
Loss at iteration 880 : 0.09125279635190964
Loss at iteration 890 : 0.09955704212188721
Loss at iteration 900 : 0.09347476810216904
Loss at iteration 910 : 0.057869911193847656
Loss at iteration 920 : 0.08106788247823715
Loss at iteration 930 : 0.053265780210494995
Loss at iteration 940 : 0.08169977366924286
Loss at iteration 950 : 0.11422507464885712
Loss at iteration 960 : 0.0947832316160202
Loss at iteration 970 : 0.10146261751651764
Loss at iteration 980 : 0.056148648262023926
Loss at iteration 990 : 0.08645886182785034
Loss at iteration 1000 : 0.08448654413223267
Loss at iteration 1010 : 0.054940320551395416
Loss at iteration 1020 : 0.12431354820728302
Loss at iteration 1030 : 0.09646926820278168
Loss at iteration 1040 : 0.08809536695480347
Loss at iteration 1050 : 0.08561454713344574
Loss at iteration 1060 : 0.13276945054531097
Loss at iteration 1070 : 0.19838246703147888
Loss at iteration 1080 : 0.07327365130186081
Loss at iteration 1090 : 0.13262496888637543
Loss at iteration 1100 : 0.06931160390377045
Loss at iteration 1110 : 0.09702835977077484
Loss at iteration 1120 : 0.07641113549470901
Loss at iteration 1130 : 0.05283496901392937
Loss at iteration 1140 : 0.08565695583820343
Loss at iteration 1150 : 0.052276741713285446
Loss at iteration 1160 : 0.05769965797662735
Loss at iteration 1170 : 0.0698847696185112
Loss at iteration 1180 : 0.11342911422252655
Loss at iteration 1190 : 0.05764096975326538
Loss at iteration 1200 : 0.06814558804035187
Loss at iteration 1210 : 0.12574803829193115
The SSIM Value is: 0.7159700632095337
The PSNR Value is: 20.859946378072102
the epoch is: 151
Loss at iteration 10 : 0.04949137195944786
Loss at iteration 20 : 0.0960368737578392
Loss at iteration 30 : 0.08426205813884735
Loss at iteration 40 : 0.09964185208082199
Loss at iteration 50 : 0.1301477998495102
Loss at iteration 60 : 0.07595506310462952
Loss at iteration 70 : 0.08521491289138794
Loss at iteration 80 : 0.13683265447616577
Loss at iteration 90 : 0.1306619644165039
Loss at iteration 100 : 0.060820501297712326
Loss at iteration 110 : 0.04775172472000122
Loss at iteration 120 : 0.08958996832370758
Loss at iteration 130 : 0.07136769592761993
Loss at iteration 140 : 0.0944875106215477
Loss at iteration 150 : 0.10484253615140915
Loss at iteration 160 : 0.0656973123550415
Loss at iteration 170 : 0.0841425284743309
Loss at iteration 180 : 0.042628854513168335
Loss at iteration 190 : 0.07235903292894363
Loss at iteration 200 : 0.10022946447134018
Loss at iteration 210 : 0.060588300228118896
Loss at iteration 220 : 0.06938573718070984
Loss at iteration 230 : 0.08279472589492798
Loss at iteration 240 : 0.06288585066795349
Loss at iteration 250 : 0.10619940608739853
Loss at iteration 260 : 0.084884874522686
Loss at iteration 270 : 0.06416718661785126
Loss at iteration 280 : 0.07472598552703857
Loss at iteration 290 : 0.0850071907043457
Loss at iteration 300 : 0.06454204767942429
Loss at iteration 310 : 0.12464618682861328
Loss at iteration 320 : 0.07199545204639435
Loss at iteration 330 : 0.06622080504894257
Loss at iteration 340 : 0.09629075229167938
Loss at iteration 350 : 0.1319972723722458
Loss at iteration 360 : 0.09531547874212265
Loss at iteration 370 : 0.06386891007423401
Loss at iteration 380 : 0.0596945658326149
Loss at iteration 390 : 0.04810941219329834
Loss at iteration 400 : 0.10744822025299072
Loss at iteration 410 : 0.13231536746025085
Loss at iteration 420 : 0.07849297672510147
Loss at iteration 430 : 0.10294473171234131
Loss at iteration 440 : 0.10011091828346252
Loss at iteration 450 : 0.12794294953346252
Loss at iteration 460 : 0.06977842003107071
Loss at iteration 470 : 0.07225151360034943
Loss at iteration 480 : 0.05667443200945854
Loss at iteration 490 : 0.08907274901866913
Loss at iteration 500 : 0.11668792366981506
Loss at iteration 510 : 0.08927106857299805
Loss at iteration 520 : 0.06968626379966736
Loss at iteration 530 : 0.08524855226278305
Loss at iteration 540 : 0.10226224362850189
Loss at iteration 550 : 0.10195519030094147
Loss at iteration 560 : 0.07821126282215118
Loss at iteration 570 : 0.08101199567317963
Loss at iteration 580 : 0.06018485873937607
Loss at iteration 590 : 0.06791889667510986
Loss at iteration 600 : 0.07072946429252625
Loss at iteration 610 : 0.06484925001859665
Loss at iteration 620 : 0.07444318383932114
Loss at iteration 630 : 0.06898708641529083
Loss at iteration 640 : 0.0982741117477417
Loss at iteration 650 : 0.06259170174598694
Loss at iteration 660 : 0.07895074784755707
Loss at iteration 670 : 0.05622705817222595
Loss at iteration 680 : 0.09483952820301056
Loss at iteration 690 : 0.08310120552778244
Loss at iteration 700 : 0.08737464994192123
Loss at iteration 710 : 0.059252701699733734
Loss at iteration 720 : 0.08069141209125519
Loss at iteration 730 : 0.1367635428905487
Loss at iteration 740 : 0.07610183209180832
Loss at iteration 750 : 0.0475437305867672
Loss at iteration 760 : 0.09635382890701294
Loss at iteration 770 : 0.04262115806341171
Loss at iteration 780 : 0.06845355033874512
Loss at iteration 790 : 0.13723433017730713
Loss at iteration 800 : 0.06273908913135529
Loss at iteration 810 : 0.08721625059843063
Loss at iteration 820 : 0.06214897334575653
Loss at iteration 830 : 0.06665917485952377
Loss at iteration 840 : 0.07355548441410065
Loss at iteration 850 : 0.0996270403265953
Loss at iteration 860 : 0.11020134389400482
Loss at iteration 870 : 0.08713215589523315
Loss at iteration 880 : 0.0788261890411377
Loss at iteration 890 : 0.08216127008199692
Loss at iteration 900 : 0.11254825443029404
Loss at iteration 910 : 0.07408855110406876
Loss at iteration 920 : 0.10150792449712753
Loss at iteration 930 : 0.04078412055969238
Loss at iteration 940 : 0.07873830199241638
Loss at iteration 950 : 0.05689854174852371
Loss at iteration 960 : 0.07185834646224976
Loss at iteration 970 : 0.14796201884746552
Loss at iteration 980 : 0.1106712818145752
Loss at iteration 990 : 0.07145833224058151
Loss at iteration 1000 : 0.11558493226766586
Loss at iteration 1010 : 0.08315303921699524
Loss at iteration 1020 : 0.12102209776639938
Loss at iteration 1030 : 0.10342156141996384
Loss at iteration 1040 : 0.11477955430746078
Loss at iteration 1050 : 0.0756826251745224
Loss at iteration 1060 : 0.090301513671875
Loss at iteration 1070 : 0.08586487174034119
Loss at iteration 1080 : 0.0556831881403923
Loss at iteration 1090 : 0.07197841256856918
Loss at iteration 1100 : 0.08037059754133224
Loss at iteration 1110 : 0.05113567039370537
Loss at iteration 1120 : 0.10415714979171753
Loss at iteration 1130 : 0.09343167394399643
Loss at iteration 1140 : 0.08720793575048447
Loss at iteration 1150 : 0.07645489275455475
Loss at iteration 1160 : 0.07985404878854752
Loss at iteration 1170 : 0.07846857607364655
Loss at iteration 1180 : 0.07863455265760422
Loss at iteration 1190 : 0.04237176477909088
Loss at iteration 1200 : 0.09205186367034912
Loss at iteration 1210 : 0.1382630318403244
The SSIM Value is: 0.7153031826019287
The PSNR Value is: 20.6641663869222
the epoch is: 152
Loss at iteration 10 : 0.08397191017866135
Loss at iteration 20 : 0.1004040539264679
Loss at iteration 30 : 0.14079904556274414
Loss at iteration 40 : 0.09846308082342148
Loss at iteration 50 : 0.08569019287824631
Loss at iteration 60 : 0.07893645018339157
Loss at iteration 70 : 0.08087922632694244
Loss at iteration 80 : 0.11286860704421997
Loss at iteration 90 : 0.06748178601264954
Loss at iteration 100 : 0.09500735253095627
Loss at iteration 110 : 0.0544007234275341
Loss at iteration 120 : 0.05940908193588257
Loss at iteration 130 : 0.06934051215648651
Loss at iteration 140 : 0.10850679874420166
Loss at iteration 150 : 0.10006169974803925
Loss at iteration 160 : 0.05589891970157623
Loss at iteration 170 : 0.10680487006902695
Loss at iteration 180 : 0.05852481722831726
Loss at iteration 190 : 0.07009224593639374
Loss at iteration 200 : 0.09540313482284546
Loss at iteration 210 : 0.09854632616043091
Loss at iteration 220 : 0.09572580456733704
Loss at iteration 230 : 0.0700768530368805
Loss at iteration 240 : 0.07520238310098648
Loss at iteration 250 : 0.09781437367200851
Loss at iteration 260 : 0.04881736636161804
Loss at iteration 270 : 0.12244170904159546
Loss at iteration 280 : 0.07211215794086456
Loss at iteration 290 : 0.07198689877986908
Loss at iteration 300 : 0.07191090285778046
Loss at iteration 310 : 0.0937253087759018
Loss at iteration 320 : 0.09288810193538666
Loss at iteration 330 : 0.04190066084265709
Loss at iteration 340 : 0.05948444455862045
Loss at iteration 350 : 0.13956929743289948
Loss at iteration 360 : 0.10861879587173462
Loss at iteration 370 : 0.1324734389781952
Loss at iteration 380 : 0.08552893251180649
Loss at iteration 390 : 0.086098812520504
Loss at iteration 400 : 0.09185498207807541
Loss at iteration 410 : 0.12068481743335724
Loss at iteration 420 : 0.11570209264755249
Loss at iteration 430 : 0.0845150351524353
Loss at iteration 440 : 0.10350058972835541
Loss at iteration 450 : 0.0769805759191513
Loss at iteration 460 : 0.08919360488653183
Loss at iteration 470 : 0.08071505278348923
Loss at iteration 480 : 0.05216966196894646
Loss at iteration 490 : 0.05547812581062317
Loss at iteration 500 : 0.1405251920223236
Loss at iteration 510 : 0.13598695397377014
Loss at iteration 520 : 0.06520869582891464
Loss at iteration 530 : 0.10678553581237793
Loss at iteration 540 : 0.11129774898290634
Loss at iteration 550 : 0.10997363179922104
Loss at iteration 560 : 0.0826314315199852
Loss at iteration 570 : 0.06785690784454346
Loss at iteration 580 : 0.12876564264297485
Loss at iteration 590 : 0.07383596152067184
Loss at iteration 600 : 0.08175855875015259
Loss at iteration 610 : 0.13144949078559875
Loss at iteration 620 : 0.11167408525943756
Loss at iteration 630 : 0.07057330012321472
Loss at iteration 640 : 0.11048680543899536
Loss at iteration 650 : 0.11313333362340927
Loss at iteration 660 : 0.06822164356708527
Loss at iteration 670 : 0.0868309736251831
Loss at iteration 680 : 0.1318604052066803
Loss at iteration 690 : 0.052202336490154266
Loss at iteration 700 : 0.1255936473608017
Loss at iteration 710 : 0.07297345995903015
Loss at iteration 720 : 0.10372426360845566
Loss at iteration 730 : 0.052507709711790085
Loss at iteration 740 : 0.10503628104925156
Loss at iteration 750 : 0.15840986371040344
Loss at iteration 760 : 0.06148231029510498
Loss at iteration 770 : 0.10201039165258408
Loss at iteration 780 : 0.054753344506025314
Loss at iteration 790 : 0.07138761878013611
Loss at iteration 800 : 0.06610961258411407
Loss at iteration 810 : 0.06923374533653259
Loss at iteration 820 : 0.11658407747745514
Loss at iteration 830 : 0.0449833869934082
Loss at iteration 840 : 0.1089911088347435
Loss at iteration 850 : 0.08341993391513824
Loss at iteration 860 : 0.11508996784687042
Loss at iteration 870 : 0.07365696132183075
Loss at iteration 880 : 0.09457626193761826
Loss at iteration 890 : 0.08209764212369919
Loss at iteration 900 : 0.09337334334850311
Loss at iteration 910 : 0.0833144411444664
Loss at iteration 920 : 0.09193305671215057
Loss at iteration 930 : 0.10354264825582504
Loss at iteration 940 : 0.06003895029425621
Loss at iteration 950 : 0.09803788363933563
Loss at iteration 960 : 0.0622195266187191
Loss at iteration 970 : 0.10336065292358398
Loss at iteration 980 : 0.07487541437149048
Loss at iteration 990 : 0.05391480773687363
Loss at iteration 1000 : 0.1359132081270218
Loss at iteration 1010 : 0.09125109016895294
Loss at iteration 1020 : 0.10667881369590759
Loss at iteration 1030 : 0.09126096963882446
Loss at iteration 1040 : 0.07589130103588104
Loss at iteration 1050 : 0.06624989956617355
Loss at iteration 1060 : 0.07235828042030334
Loss at iteration 1070 : 0.06487439572811127
Loss at iteration 1080 : 0.03579737991094589
Loss at iteration 1090 : 0.10044267028570175
Loss at iteration 1100 : 0.0628061518073082
Loss at iteration 1110 : 0.09535374492406845
Loss at iteration 1120 : 0.10883136838674545
Loss at iteration 1130 : 0.04708452522754669
Loss at iteration 1140 : 0.0664999708533287
Loss at iteration 1150 : 0.13106101751327515
Loss at iteration 1160 : 0.05440079793334007
Loss at iteration 1170 : 0.09657170623540878
Loss at iteration 1180 : 0.08099529892206192
Loss at iteration 1190 : 0.07744896411895752
Loss at iteration 1200 : 0.08020465075969696
Loss at iteration 1210 : 0.08178669214248657
The SSIM Value is: 0.7166003863016764
The PSNR Value is: 20.882806269327798
the epoch is: 153
Loss at iteration 10 : 0.09662541002035141
Loss at iteration 20 : 0.059178173542022705
Loss at iteration 30 : 0.12686195969581604
Loss at iteration 40 : 0.07157394289970398
Loss at iteration 50 : 0.08337622880935669
Loss at iteration 60 : 0.08664484322071075
Loss at iteration 70 : 0.0683736652135849
Loss at iteration 80 : 0.05912176892161369
Loss at iteration 90 : 0.13225899636745453
Loss at iteration 100 : 0.06075816601514816
Loss at iteration 110 : 0.07071346044540405
Loss at iteration 120 : 0.089476577937603
Loss at iteration 130 : 0.08231252431869507
Loss at iteration 140 : 0.09277787059545517
Loss at iteration 150 : 0.06572370231151581
Loss at iteration 160 : 0.07606694102287292
Loss at iteration 170 : 0.06164155527949333
Loss at iteration 180 : 0.0641147792339325
Loss at iteration 190 : 0.06771360337734222
Loss at iteration 200 : 0.0663624107837677
Loss at iteration 210 : 0.09910225868225098
Loss at iteration 220 : 0.1295309066772461
Loss at iteration 230 : 0.08696603775024414
Loss at iteration 240 : 0.05175758898258209
Loss at iteration 250 : 0.09214074909687042
Loss at iteration 260 : 0.08769994974136353
Loss at iteration 270 : 0.07430946081876755
Loss at iteration 280 : 0.056794047355651855
Loss at iteration 290 : 0.06932829320430756
Loss at iteration 300 : 0.09372727572917938
Loss at iteration 310 : 0.09250745177268982
Loss at iteration 320 : 0.09172402322292328
Loss at iteration 330 : 0.057236410677433014
Loss at iteration 340 : 0.09585150331258774
Loss at iteration 350 : 0.04909772798418999
Loss at iteration 360 : 0.0848843902349472
Loss at iteration 370 : 0.07982823997735977
Loss at iteration 380 : 0.07824917137622833
Loss at iteration 390 : 0.059948962181806564
Loss at iteration 400 : 0.1277196705341339
Loss at iteration 410 : 0.05412217974662781
Loss at iteration 420 : 0.10311941802501678
Loss at iteration 430 : 0.07134219259023666
Loss at iteration 440 : 0.05850480496883392
Loss at iteration 450 : 0.07070130109786987
Loss at iteration 460 : 0.05782300978899002
Loss at iteration 470 : 0.07396833598613739
Loss at iteration 480 : 0.07212720811367035
Loss at iteration 490 : 0.06254468113183975
Loss at iteration 500 : 0.08356679975986481
Loss at iteration 510 : 0.11670632660388947
Loss at iteration 520 : 0.09124531596899033
Loss at iteration 530 : 0.07940450310707092
Loss at iteration 540 : 0.11205163598060608
Loss at iteration 550 : 0.09818391501903534
Loss at iteration 560 : 0.05196920037269592
Loss at iteration 570 : 0.05754170939326286
Loss at iteration 580 : 0.09673094749450684
Loss at iteration 590 : 0.06636840105056763
Loss at iteration 600 : 0.08779119700193405
Loss at iteration 610 : 0.09608533978462219
Loss at iteration 620 : 0.03946400433778763
Loss at iteration 630 : 0.06374611705541611
Loss at iteration 640 : 0.060428693890571594
Loss at iteration 650 : 0.0646330714225769
Loss at iteration 660 : 0.10886814445257187
Loss at iteration 670 : 0.10066315531730652
Loss at iteration 680 : 0.12555640935897827
Loss at iteration 690 : 0.14543557167053223
Loss at iteration 700 : 0.09240492433309555
Loss at iteration 710 : 0.09169216454029083
Loss at iteration 720 : 0.057811856269836426
Loss at iteration 730 : 0.0790637880563736
Loss at iteration 740 : 0.10360771417617798
Loss at iteration 750 : 0.09697538614273071
Loss at iteration 760 : 0.06689479947090149
Loss at iteration 770 : 0.049541324377059937
Loss at iteration 780 : 0.08226298540830612
Loss at iteration 790 : 0.06763850152492523
Loss at iteration 800 : 0.07898744940757751
Loss at iteration 810 : 0.11698820441961288
Loss at iteration 820 : 0.08254089951515198
Loss at iteration 830 : 0.14872591197490692
Loss at iteration 840 : 0.06123402342200279
Loss at iteration 850 : 0.08632655441761017
Loss at iteration 860 : 0.07804327458143234
Loss at iteration 870 : 0.11806826293468475
Loss at iteration 880 : 0.07302166521549225
Loss at iteration 890 : 0.12113535404205322
Loss at iteration 900 : 0.07016882300376892
Loss at iteration 910 : 0.11079692840576172
Loss at iteration 920 : 0.07633264362812042
Loss at iteration 930 : 0.06744059175252914
Loss at iteration 940 : 0.07168221473693848
Loss at iteration 950 : 0.07864664494991302
Loss at iteration 960 : 0.09498310089111328
Loss at iteration 970 : 0.04385988414287567
Loss at iteration 980 : 0.09212958812713623
Loss at iteration 990 : 0.13279299437999725
Loss at iteration 1000 : 0.06284554302692413
Loss at iteration 1010 : 0.04400651901960373
Loss at iteration 1020 : 0.0784745141863823
Loss at iteration 1030 : 0.05777312442660332
Loss at iteration 1040 : 0.07598533481359482
Loss at iteration 1050 : 0.07958459854125977
Loss at iteration 1060 : 0.08236999064683914
Loss at iteration 1070 : 0.09658157825469971
Loss at iteration 1080 : 0.08100418001413345
Loss at iteration 1090 : 0.09138108789920807
Loss at iteration 1100 : 0.08653776347637177
Loss at iteration 1110 : 0.06896165013313293
Loss at iteration 1120 : 0.05362612009048462
Loss at iteration 1130 : 0.06718996167182922
Loss at iteration 1140 : 0.05483905225992203
Loss at iteration 1150 : 0.06757400184869766
Loss at iteration 1160 : 0.10640935599803925
Loss at iteration 1170 : 0.11448168754577637
Loss at iteration 1180 : 0.13710293173789978
Loss at iteration 1190 : 0.09443994611501694
Loss at iteration 1200 : 0.06730850785970688
Loss at iteration 1210 : 0.12341414391994476
The SSIM Value is: 0.7195497473080953
The PSNR Value is: 20.940780766805013
the epoch is: 154
Loss at iteration 10 : 0.06986075639724731
Loss at iteration 20 : 0.12126162648200989
Loss at iteration 30 : 0.12078744173049927
Loss at iteration 40 : 0.10519247502088547
Loss at iteration 50 : 0.08753027021884918
Loss at iteration 60 : 0.0801210105419159
Loss at iteration 70 : 0.10307147353887558
Loss at iteration 80 : 0.0792134702205658
Loss at iteration 90 : 0.10097851604223251
Loss at iteration 100 : 0.06355513632297516
Loss at iteration 110 : 0.08610557019710541
Loss at iteration 120 : 0.11729727685451508
Loss at iteration 130 : 0.10934856534004211
Loss at iteration 140 : 0.0761970579624176
Loss at iteration 150 : 0.09676619619131088
Loss at iteration 160 : 0.07001902908086777
Loss at iteration 170 : 0.06656230241060257
Loss at iteration 180 : 0.07910651713609695
Loss at iteration 190 : 0.07831079512834549
Loss at iteration 200 : 0.06845710426568985
Loss at iteration 210 : 0.06853752583265305
Loss at iteration 220 : 0.07477100193500519
Loss at iteration 230 : 0.11611291021108627
Loss at iteration 240 : 0.08686910569667816
Loss at iteration 250 : 0.08850421011447906
Loss at iteration 260 : 0.07393018901348114
Loss at iteration 270 : 0.06243911013007164
Loss at iteration 280 : 0.0632886067032814
Loss at iteration 290 : 0.09015616029500961
Loss at iteration 300 : 0.10112464427947998
Loss at iteration 310 : 0.08794292062520981
Loss at iteration 320 : 0.08366583287715912
Loss at iteration 330 : 0.09452041238546371
Loss at iteration 340 : 0.10125713795423508
Loss at iteration 350 : 0.07526713609695435
Loss at iteration 360 : 0.08409419655799866
Loss at iteration 370 : 0.0833885669708252
Loss at iteration 380 : 0.06878209859132767
Loss at iteration 390 : 0.06590306758880615
Loss at iteration 400 : 0.051315754652023315
Loss at iteration 410 : 0.07566924393177032
Loss at iteration 420 : 0.05090753361582756
Loss at iteration 430 : 0.11366504430770874
Loss at iteration 440 : 0.06278829276561737
Loss at iteration 450 : 0.11436252295970917
Loss at iteration 460 : 0.07876057922840118
Loss at iteration 470 : 0.09427790343761444
Loss at iteration 480 : 0.06834077090024948
Loss at iteration 490 : 0.07402749359607697
Loss at iteration 500 : 0.09383615106344223
Loss at iteration 510 : 0.0793023407459259
Loss at iteration 520 : 0.09747900068759918
Loss at iteration 530 : 0.09653802216053009
Loss at iteration 540 : 0.09017840027809143
Loss at iteration 550 : 0.07399722933769226
Loss at iteration 560 : 0.06259381026029587
Loss at iteration 570 : 0.06748195737600327
Loss at iteration 580 : 0.09573839604854584
Loss at iteration 590 : 0.09245450794696808
Loss at iteration 600 : 0.07839301973581314
Loss at iteration 610 : 0.11570243537425995
Loss at iteration 620 : 0.09471508115530014
Loss at iteration 630 : 0.10101940482854843
Loss at iteration 640 : 0.09586351364850998
Loss at iteration 650 : 0.08263842761516571
Loss at iteration 660 : 0.1065288633108139
Loss at iteration 670 : 0.10297238826751709
Loss at iteration 680 : 0.0849602222442627
Loss at iteration 690 : 0.11405974626541138
Loss at iteration 700 : 0.10456165671348572
Loss at iteration 710 : 0.046766187995672226
Loss at iteration 720 : 0.06495808809995651
Loss at iteration 730 : 0.06049427390098572
Loss at iteration 740 : 0.08933505415916443
Loss at iteration 750 : 0.11224415898323059
Loss at iteration 760 : 0.07489525526762009
Loss at iteration 770 : 0.06355033814907074
Loss at iteration 780 : 0.08365248143672943
Loss at iteration 790 : 0.0694701224565506
Loss at iteration 800 : 0.09139329195022583
Loss at iteration 810 : 0.11605449765920639
Loss at iteration 820 : 0.05351696535944939
Loss at iteration 830 : 0.08076678961515427
Loss at iteration 840 : 0.053001519292593
Loss at iteration 850 : 0.0733330026268959
Loss at iteration 860 : 0.0843411311507225
Loss at iteration 870 : 0.0826961025595665
Loss at iteration 880 : 0.08622237294912338
Loss at iteration 890 : 0.11953233182430267
Loss at iteration 900 : 0.07878138870000839
Loss at iteration 910 : 0.07332339882850647
Loss at iteration 920 : 0.10679666697978973
Loss at iteration 930 : 0.0841023325920105
Loss at iteration 940 : 0.13406354188919067
Loss at iteration 950 : 0.0875440463423729
Loss at iteration 960 : 0.08614331483840942
Loss at iteration 970 : 0.07259504497051239
Loss at iteration 980 : 0.13052430748939514
Loss at iteration 990 : 0.07071705162525177
Loss at iteration 1000 : 0.0843295082449913
Loss at iteration 1010 : 0.08590444177389145
Loss at iteration 1020 : 0.05868677422404289
Loss at iteration 1030 : 0.09911885112524033
Loss at iteration 1040 : 0.0763930231332779
Loss at iteration 1050 : 0.055493615567684174
Loss at iteration 1060 : 0.06505221128463745
Loss at iteration 1070 : 0.05292242020368576
Loss at iteration 1080 : 0.06551007926464081
Loss at iteration 1090 : 0.08705096691846848
Loss at iteration 1100 : 0.07517117261886597
Loss at iteration 1110 : 0.07177554816007614
Loss at iteration 1120 : 0.1242256760597229
Loss at iteration 1130 : 0.06300907582044601
Loss at iteration 1140 : 0.05809769034385681
Loss at iteration 1150 : 0.1067115068435669
Loss at iteration 1160 : 0.07893198728561401
Loss at iteration 1170 : 0.11501854658126831
Loss at iteration 1180 : 0.0905284583568573
Loss at iteration 1190 : 0.09394937753677368
Loss at iteration 1200 : 0.060829173773527145
Loss at iteration 1210 : 0.10187536478042603
The SSIM Value is: 0.7214700619379679
The PSNR Value is: 21.18170166015625
the epoch is: 155
Loss at iteration 10 : 0.062034375965595245
Loss at iteration 20 : 0.10860805958509445
Loss at iteration 30 : 0.11661066859960556
Loss at iteration 40 : 0.07765240222215652
Loss at iteration 50 : 0.06700912117958069
Loss at iteration 60 : 0.052436552941799164
Loss at iteration 70 : 0.09235088527202606
Loss at iteration 80 : 0.06687214225530624
Loss at iteration 90 : 0.07684621214866638
Loss at iteration 100 : 0.07727405428886414
Loss at iteration 110 : 0.08501312136650085
Loss at iteration 120 : 0.11140988767147064
Loss at iteration 130 : 0.05450528860092163
Loss at iteration 140 : 0.08716823160648346
Loss at iteration 150 : 0.08239902555942535
Loss at iteration 160 : 0.05658423528075218
Loss at iteration 170 : 0.058972131460905075
Loss at iteration 180 : 0.03989659249782562
Loss at iteration 190 : 0.06590060889720917
Loss at iteration 200 : 0.06582733988761902
Loss at iteration 210 : 0.07106249034404755
Loss at iteration 220 : 0.07807022333145142
Loss at iteration 230 : 0.10520569980144501
Loss at iteration 240 : 0.06466639041900635
Loss at iteration 250 : 0.07992428541183472
Loss at iteration 260 : 0.08354367315769196
Loss at iteration 270 : 0.09090620279312134
Loss at iteration 280 : 0.08967828750610352
Loss at iteration 290 : 0.06212639808654785
Loss at iteration 300 : 0.06499331444501877
Loss at iteration 310 : 0.04436275362968445
Loss at iteration 320 : 0.08139295876026154
Loss at iteration 330 : 0.09752511233091354
Loss at iteration 340 : 0.09792252629995346
Loss at iteration 350 : 0.07990817725658417
Loss at iteration 360 : 0.05074269697070122
Loss at iteration 370 : 0.10771845281124115
Loss at iteration 380 : 0.05938071012496948
Loss at iteration 390 : 0.06772790849208832
Loss at iteration 400 : 0.050547730177640915
Loss at iteration 410 : 0.07949887961149216
Loss at iteration 420 : 0.06951083987951279
Loss at iteration 430 : 0.10649355500936508
Loss at iteration 440 : 0.09559963643550873
Loss at iteration 450 : 0.1017586812376976
Loss at iteration 460 : 0.07457789778709412
Loss at iteration 470 : 0.0994119644165039
Loss at iteration 480 : 0.08753737062215805
Loss at iteration 490 : 0.06448937207460403
Loss at iteration 500 : 0.07455234229564667
Loss at iteration 510 : 0.10672180354595184
Loss at iteration 520 : 0.09062214195728302
Loss at iteration 530 : 0.09017329663038254
Loss at iteration 540 : 0.10472947359085083
Loss at iteration 550 : 0.08735080063343048
Loss at iteration 560 : 0.08946773409843445
Loss at iteration 570 : 0.08084529638290405
Loss at iteration 580 : 0.0530843548476696
Loss at iteration 590 : 0.06220438331365585
Loss at iteration 600 : 0.07351108640432358
Loss at iteration 610 : 0.05610613524913788
Loss at iteration 620 : 0.05610515922307968
Loss at iteration 630 : 0.06842143833637238
Loss at iteration 640 : 0.10594671964645386
Loss at iteration 650 : 0.05715831369161606
Loss at iteration 660 : 0.08425217866897583
Loss at iteration 670 : 0.06727275997400284
Loss at iteration 680 : 0.06390843540430069
Loss at iteration 690 : 0.060380689799785614
Loss at iteration 700 : 0.07510336488485336
Loss at iteration 710 : 0.07221779227256775
Loss at iteration 720 : 0.09029688686132431
Loss at iteration 730 : 0.10886478424072266
Loss at iteration 740 : 0.08846428245306015
Loss at iteration 750 : 0.0625293180346489
Loss at iteration 760 : 0.07910340279340744
Loss at iteration 770 : 0.08337460458278656
Loss at iteration 780 : 0.07559847086668015
Loss at iteration 790 : 0.10832659900188446
Loss at iteration 800 : 0.07371645420789719
Loss at iteration 810 : 0.05580917000770569
Loss at iteration 820 : 0.058810990303754807
Loss at iteration 830 : 0.10340134799480438
Loss at iteration 840 : 0.06849270313978195
Loss at iteration 850 : 0.07788898050785065
Loss at iteration 860 : 0.08117132633924484
Loss at iteration 870 : 0.08564319461584091
Loss at iteration 880 : 0.08364588022232056
Loss at iteration 890 : 0.07267910242080688
Loss at iteration 900 : 0.11243348568677902
Loss at iteration 910 : 0.10460567474365234
Loss at iteration 920 : 0.06902126967906952
Loss at iteration 930 : 0.09371980279684067
Loss at iteration 940 : 0.0941692590713501
Loss at iteration 950 : 0.07933560013771057
Loss at iteration 960 : 0.10891823470592499
Loss at iteration 970 : 0.061984311789274216
Loss at iteration 980 : 0.10269015282392502
Loss at iteration 990 : 0.08959250152111053
Loss at iteration 1000 : 0.07625452429056168
Loss at iteration 1010 : 0.052013956010341644
Loss at iteration 1020 : 0.05537066608667374
Loss at iteration 1030 : 0.08605311810970306
Loss at iteration 1040 : 0.057581450790166855
Loss at iteration 1050 : 0.059730060398578644
Loss at iteration 1060 : 0.11730708926916122
Loss at iteration 1070 : 0.07303424924612045
Loss at iteration 1080 : 0.05775092542171478
Loss at iteration 1090 : 0.11282818764448166
Loss at iteration 1100 : 0.14479795098304749
Loss at iteration 1110 : 0.07079249620437622
Loss at iteration 1120 : 0.08451367914676666
Loss at iteration 1130 : 0.08543583750724792
Loss at iteration 1140 : 0.06348372995853424
Loss at iteration 1150 : 0.09006696939468384
Loss at iteration 1160 : 0.1138693243265152
Loss at iteration 1170 : 0.07524199783802032
Loss at iteration 1180 : 0.10638053715229034
Loss at iteration 1190 : 0.07977482676506042
Loss at iteration 1200 : 0.0803099274635315
Loss at iteration 1210 : 0.07520037144422531
The SSIM Value is: 0.71529727379481
The PSNR Value is: 20.666039784749348
the epoch is: 156
Loss at iteration 10 : 0.08627985417842865
Loss at iteration 20 : 0.07370927184820175
Loss at iteration 30 : 0.06713618338108063
Loss at iteration 40 : 0.10713281482458115
Loss at iteration 50 : 0.0888243317604065
Loss at iteration 60 : 0.03866509720683098
Loss at iteration 70 : 0.11394841969013214
Loss at iteration 80 : 0.06532523036003113
Loss at iteration 90 : 0.055334679782390594
Loss at iteration 100 : 0.07628162205219269
Loss at iteration 110 : 0.06802154332399368
Loss at iteration 120 : 0.06632581353187561
Loss at iteration 130 : 0.08095212280750275
Loss at iteration 140 : 0.0712868794798851
Loss at iteration 150 : 0.11109839379787445
Loss at iteration 160 : 0.10017295926809311
Loss at iteration 170 : 0.07697740942239761
Loss at iteration 180 : 0.058535121381282806
Loss at iteration 190 : 0.07266999781131744
Loss at iteration 200 : 0.10000526160001755
Loss at iteration 210 : 0.09964119642972946
Loss at iteration 220 : 0.09425848722457886
Loss at iteration 230 : 0.04567652940750122
Loss at iteration 240 : 0.08694903552532196
Loss at iteration 250 : 0.08901246637105942
Loss at iteration 260 : 0.10450434684753418
Loss at iteration 270 : 0.08711036294698715
Loss at iteration 280 : 0.0737396702170372
Loss at iteration 290 : 0.11030745506286621
Loss at iteration 300 : 0.054046183824539185
Loss at iteration 310 : 0.0623091459274292
Loss at iteration 320 : 0.10334144532680511
Loss at iteration 330 : 0.04676252603530884
Loss at iteration 340 : 0.08773182332515717
Loss at iteration 350 : 0.08399495482444763
Loss at iteration 360 : 0.07788732647895813
Loss at iteration 370 : 0.13315922021865845
Loss at iteration 380 : 0.11791368573904037
Loss at iteration 390 : 0.0402403250336647
Loss at iteration 400 : 0.07775381207466125
Loss at iteration 410 : 0.08763782680034637
Loss at iteration 420 : 0.08064857125282288
Loss at iteration 430 : 0.06674645096063614
Loss at iteration 440 : 0.0710235983133316
Loss at iteration 450 : 0.06387363374233246
Loss at iteration 460 : 0.06146189570426941
Loss at iteration 470 : 0.07965372502803802
Loss at iteration 480 : 0.07711523771286011
Loss at iteration 490 : 0.08735471218824387
Loss at iteration 500 : 0.07713989913463593
Loss at iteration 510 : 0.08062076568603516
Loss at iteration 520 : 0.07999516278505325
Loss at iteration 530 : 0.06319992244243622
Loss at iteration 540 : 0.06922745704650879
Loss at iteration 550 : 0.07531297206878662
Loss at iteration 560 : 0.09893548488616943
Loss at iteration 570 : 0.06760375201702118
Loss at iteration 580 : 0.08200915157794952
Loss at iteration 590 : 0.04498256742954254
Loss at iteration 600 : 0.07962461560964584
Loss at iteration 610 : 0.06034903973340988
Loss at iteration 620 : 0.043646395206451416
Loss at iteration 630 : 0.08688949048519135
Loss at iteration 640 : 0.10730655491352081
Loss at iteration 650 : 0.08097818493843079
Loss at iteration 660 : 0.10629384964704514
Loss at iteration 670 : 0.08970305323600769
Loss at iteration 680 : 0.09537826478481293
Loss at iteration 690 : 0.09685938060283661
Loss at iteration 700 : 0.08593592047691345
Loss at iteration 710 : 0.07725915312767029
Loss at iteration 720 : 0.09340260922908783
Loss at iteration 730 : 0.07217346131801605
Loss at iteration 740 : 0.07464810460805893
Loss at iteration 750 : 0.07498049736022949
Loss at iteration 760 : 0.07450516521930695
Loss at iteration 770 : 0.09262025356292725
Loss at iteration 780 : 0.09354589879512787
Loss at iteration 790 : 0.07424339652061462
Loss at iteration 800 : 0.1057896837592125
Loss at iteration 810 : 0.10230904072523117
Loss at iteration 820 : 0.07870658487081528
Loss at iteration 830 : 0.08830421417951584
Loss at iteration 840 : 0.0681469514966011
Loss at iteration 850 : 0.05974589288234711
Loss at iteration 860 : 0.08917467296123505
Loss at iteration 870 : 0.08048846572637558
Loss at iteration 880 : 0.049872949719429016
Loss at iteration 890 : 0.06279468536376953
Loss at iteration 900 : 0.15983441472053528
Loss at iteration 910 : 0.11241905391216278
Loss at iteration 920 : 0.0948314294219017
Loss at iteration 930 : 0.0774163082242012
Loss at iteration 940 : 0.09224127233028412
Loss at iteration 950 : 0.09645894169807434
Loss at iteration 960 : 0.0699116587638855
Loss at iteration 970 : 0.08708494901657104
Loss at iteration 980 : 0.11414681375026703
Loss at iteration 990 : 0.056820474565029144
Loss at iteration 1000 : 0.046466656029224396
Loss at iteration 1010 : 0.08918788284063339
Loss at iteration 1020 : 0.059345658868551254
Loss at iteration 1030 : 0.11365509033203125
Loss at iteration 1040 : 0.10801302641630173
Loss at iteration 1050 : 0.09176793694496155
Loss at iteration 1060 : 0.07418812066316605
Loss at iteration 1070 : 0.11723782122135162
Loss at iteration 1080 : 0.10406897962093353
Loss at iteration 1090 : 0.07141759246587753
Loss at iteration 1100 : 0.08026351034641266
Loss at iteration 1110 : 0.07241886109113693
Loss at iteration 1120 : 0.11150156706571579
Loss at iteration 1130 : 0.07501620054244995
Loss at iteration 1140 : 0.1254415065050125
Loss at iteration 1150 : 0.05443425104022026
Loss at iteration 1160 : 0.10841554403305054
Loss at iteration 1170 : 0.11022371053695679
Loss at iteration 1180 : 0.04384113848209381
Loss at iteration 1190 : 0.06850995868444443
Loss at iteration 1200 : 0.0597698912024498
Loss at iteration 1210 : 0.07844901084899902
The SSIM Value is: 0.7152030785878499
The PSNR Value is: 20.935513750712076
the epoch is: 157
Loss at iteration 10 : 0.07515007257461548
Loss at iteration 20 : 0.053124260157346725
Loss at iteration 30 : 0.0646747574210167
Loss at iteration 40 : 0.10869179666042328
Loss at iteration 50 : 0.07260813564062119
Loss at iteration 60 : 0.0918637365102768
Loss at iteration 70 : 0.0965224951505661
Loss at iteration 80 : 0.08850239217281342
Loss at iteration 90 : 0.06935596466064453
Loss at iteration 100 : 0.08183805644512177
Loss at iteration 110 : 0.07971806079149246
Loss at iteration 120 : 0.09855598211288452
Loss at iteration 130 : 0.08477264642715454
Loss at iteration 140 : 0.09172257035970688
Loss at iteration 150 : 0.08399214595556259
Loss at iteration 160 : 0.07612813264131546
Loss at iteration 170 : 0.0639251321554184
Loss at iteration 180 : 0.05542416498064995
Loss at iteration 190 : 0.08609345555305481
Loss at iteration 200 : 0.08067908138036728
Loss at iteration 210 : 0.06307681649923325
Loss at iteration 220 : 0.11928682774305344
Loss at iteration 230 : 0.06048976629972458
Loss at iteration 240 : 0.05615890771150589
Loss at iteration 250 : 0.05737334489822388
Loss at iteration 260 : 0.09997962415218353
Loss at iteration 270 : 0.11918747425079346
Loss at iteration 280 : 0.0971587598323822
Loss at iteration 290 : 0.09824009984731674
Loss at iteration 300 : 0.05785735696554184
Loss at iteration 310 : 0.11008083820343018
Loss at iteration 320 : 0.1038743257522583
Loss at iteration 330 : 0.09326960891485214
Loss at iteration 340 : 0.05186157673597336
Loss at iteration 350 : 0.1375199854373932
Loss at iteration 360 : 0.059201158583164215
Loss at iteration 370 : 0.09354041516780853
Loss at iteration 380 : 0.06474582850933075
Loss at iteration 390 : 0.048303961753845215
Loss at iteration 400 : 0.08185594528913498
Loss at iteration 410 : 0.08111195266246796
Loss at iteration 420 : 0.09940659254789352
Loss at iteration 430 : 0.15677395462989807
Loss at iteration 440 : 0.06163613498210907
Loss at iteration 450 : 0.09020186960697174
Loss at iteration 460 : 0.10665306448936462
Loss at iteration 470 : 0.1064935252070427
Loss at iteration 480 : 0.06424477696418762
Loss at iteration 490 : 0.059360235929489136
Loss at iteration 500 : 0.1207689493894577
Loss at iteration 510 : 0.07800796627998352
Loss at iteration 520 : 0.06858670711517334
Loss at iteration 530 : 0.1106700450181961
Loss at iteration 540 : 0.12911957502365112
Loss at iteration 550 : 0.10093341767787933
Loss at iteration 560 : 0.09028635919094086
Loss at iteration 570 : 0.06983689963817596
Loss at iteration 580 : 0.0651678591966629
Loss at iteration 590 : 0.10102014243602753
Loss at iteration 600 : 0.10778903216123581
Loss at iteration 610 : 0.09326577186584473
Loss at iteration 620 : 0.06670334935188293
Loss at iteration 630 : 0.10656483471393585
Loss at iteration 640 : 0.053270891308784485
Loss at iteration 650 : 0.12029236555099487
Loss at iteration 660 : 0.1382327377796173
Loss at iteration 670 : 0.11054980754852295
Loss at iteration 680 : 0.09969266504049301
Loss at iteration 690 : 0.062315404415130615
Loss at iteration 700 : 0.1034872755408287
Loss at iteration 710 : 0.07722543925046921
Loss at iteration 720 : 0.04574386775493622
Loss at iteration 730 : 0.11589936912059784
Loss at iteration 740 : 0.09050452709197998
Loss at iteration 750 : 0.09592583030462265
Loss at iteration 760 : 0.08778278529644012
Loss at iteration 770 : 0.05083940550684929
Loss at iteration 780 : 0.08556383103132248
Loss at iteration 790 : 0.07871095836162567
Loss at iteration 800 : 0.10226338356733322
Loss at iteration 810 : 0.05657167732715607
Loss at iteration 820 : 0.08620022237300873
Loss at iteration 830 : 0.09708436578512192
Loss at iteration 840 : 0.06718893349170685
Loss at iteration 850 : 0.07226549834012985
Loss at iteration 860 : 0.08952842652797699
Loss at iteration 870 : 0.09669934213161469
Loss at iteration 880 : 0.05419158563017845
Loss at iteration 890 : 0.07370142638683319
Loss at iteration 900 : 0.12834720313549042
Loss at iteration 910 : 0.1334134340286255
Loss at iteration 920 : 0.0809267982840538
Loss at iteration 930 : 0.050726890563964844
Loss at iteration 940 : 0.09819480031728745
Loss at iteration 950 : 0.057094693183898926
Loss at iteration 960 : 0.06692446768283844
Loss at iteration 970 : 0.09586493670940399
Loss at iteration 980 : 0.0764816403388977
Loss at iteration 990 : 0.11405347287654877
Loss at iteration 1000 : 0.054926276206970215
Loss at iteration 1010 : 0.09939654171466827
Loss at iteration 1020 : 0.06055743619799614
Loss at iteration 1030 : 0.06846136599779129
Loss at iteration 1040 : 0.06981485337018967
Loss at iteration 1050 : 0.09857179969549179
Loss at iteration 1060 : 0.09397818893194199
Loss at iteration 1070 : 0.09640958905220032
Loss at iteration 1080 : 0.08513109385967255
Loss at iteration 1090 : 0.10030512511730194
Loss at iteration 1100 : 0.09007696807384491
Loss at iteration 1110 : 0.0682290717959404
Loss at iteration 1120 : 0.07891196757555008
Loss at iteration 1130 : 0.11339715123176575
Loss at iteration 1140 : 0.07346616685390472
Loss at iteration 1150 : 0.07398515939712524
Loss at iteration 1160 : 0.0966516062617302
Loss at iteration 1170 : 0.07309149205684662
Loss at iteration 1180 : 0.06127282977104187
Loss at iteration 1190 : 0.08881229162216187
Loss at iteration 1200 : 0.08343270421028137
Loss at iteration 1210 : 0.06948298960924149
The SSIM Value is: 0.7177539149920146
The PSNR Value is: 20.516842397054038
the epoch is: 158
Loss at iteration 10 : 0.0634414479136467
Loss at iteration 20 : 0.10341548919677734
Loss at iteration 30 : 0.09924166649580002
Loss at iteration 40 : 0.07408186793327332
Loss at iteration 50 : 0.07222822308540344
Loss at iteration 60 : 0.0944627970457077
Loss at iteration 70 : 0.09029144048690796
Loss at iteration 80 : 0.10395069420337677
Loss at iteration 90 : 0.06496688723564148
Loss at iteration 100 : 0.12010116875171661
Loss at iteration 110 : 0.0963011085987091
Loss at iteration 120 : 0.08494951575994492
Loss at iteration 130 : 0.1042410358786583
Loss at iteration 140 : 0.04077589511871338
Loss at iteration 150 : 0.10757746547460556
Loss at iteration 160 : 0.06143192574381828
Loss at iteration 170 : 0.05888676270842552
Loss at iteration 180 : 0.04868695139884949
Loss at iteration 190 : 0.04792351648211479
Loss at iteration 200 : 0.1015201210975647
Loss at iteration 210 : 0.06470619142055511
Loss at iteration 220 : 0.06273253262042999
Loss at iteration 230 : 0.06386196613311768
Loss at iteration 240 : 0.056779127568006516
Loss at iteration 250 : 0.06507185101509094
Loss at iteration 260 : 0.11210402101278305
Loss at iteration 270 : 0.10545745491981506
Loss at iteration 280 : 0.07103481143712997
Loss at iteration 290 : 0.0951208621263504
Loss at iteration 300 : 0.11909420788288116
Loss at iteration 310 : 0.07621870934963226
Loss at iteration 320 : 0.045827921479940414
Loss at iteration 330 : 0.06508171558380127
Loss at iteration 340 : 0.11467212438583374
Loss at iteration 350 : 0.08005966991186142
Loss at iteration 360 : 0.06731216609477997
Loss at iteration 370 : 0.08170220255851746
Loss at iteration 380 : 0.09496723860502243
Loss at iteration 390 : 0.12079273164272308
Loss at iteration 400 : 0.07200365513563156
Loss at iteration 410 : 0.0687960758805275
Loss at iteration 420 : 0.09209754317998886
Loss at iteration 430 : 0.08250527828931808
Loss at iteration 440 : 0.14669999480247498
Loss at iteration 450 : 0.07961301505565643
Loss at iteration 460 : 0.06968528032302856
Loss at iteration 470 : 0.0643201619386673
Loss at iteration 480 : 0.06699643284082413
Loss at iteration 490 : 0.06439124792814255
Loss at iteration 500 : 0.06590288132429123
Loss at iteration 510 : 0.06171460077166557
Loss at iteration 520 : 0.06838830560445786
Loss at iteration 530 : 0.07899665832519531
Loss at iteration 540 : 0.05197618529200554
Loss at iteration 550 : 0.05960256978869438
Loss at iteration 560 : 0.08690925687551498
Loss at iteration 570 : 0.05729467421770096
Loss at iteration 580 : 0.09750621020793915
Loss at iteration 590 : 0.12324389070272446
Loss at iteration 600 : 0.09138869494199753
Loss at iteration 610 : 0.0934564471244812
Loss at iteration 620 : 0.06977188587188721
Loss at iteration 630 : 0.06156481057405472
Loss at iteration 640 : 0.10454332828521729
Loss at iteration 650 : 0.06997624039649963
Loss at iteration 660 : 0.10160662978887558
Loss at iteration 670 : 0.0930342897772789
Loss at iteration 680 : 0.08109401166439056
Loss at iteration 690 : 0.05776352435350418
Loss at iteration 700 : 0.05324689671397209
Loss at iteration 710 : 0.07826446741819382
Loss at iteration 720 : 0.11343671381473541
Loss at iteration 730 : 0.05012185871601105
Loss at iteration 740 : 0.059188514947891235
Loss at iteration 750 : 0.06803204119205475
Loss at iteration 760 : 0.09125159680843353
Loss at iteration 770 : 0.04216185212135315
Loss at iteration 780 : 0.10542955249547958
Loss at iteration 790 : 0.08232621848583221
Loss at iteration 800 : 0.05197909474372864
Loss at iteration 810 : 0.07096898555755615
Loss at iteration 820 : 0.15180596709251404
Loss at iteration 830 : 0.09438036382198334
Loss at iteration 840 : 0.060985710471868515
Loss at iteration 850 : 0.06552307307720184
Loss at iteration 860 : 0.09663636982440948
Loss at iteration 870 : 0.0880391001701355
Loss at iteration 880 : 0.11715416610240936
Loss at iteration 890 : 0.06784065067768097
Loss at iteration 900 : 0.08168106526136398
Loss at iteration 910 : 0.06033746898174286
Loss at iteration 920 : 0.05660790205001831
Loss at iteration 930 : 0.11888580769300461
Loss at iteration 940 : 0.08842355012893677
Loss at iteration 950 : 0.13213148713111877
Loss at iteration 960 : 0.07263178378343582
Loss at iteration 970 : 0.047524720430374146
Loss at iteration 980 : 0.07470685243606567
Loss at iteration 990 : 0.05727824568748474
Loss at iteration 1000 : 0.08337390422821045
Loss at iteration 1010 : 0.13299721479415894
Loss at iteration 1020 : 0.07927559316158295
Loss at iteration 1030 : 0.07828795909881592
Loss at iteration 1040 : 0.06916987150907516
Loss at iteration 1050 : 0.08483968675136566
Loss at iteration 1060 : 0.12091611325740814
Loss at iteration 1070 : 0.10609860718250275
Loss at iteration 1080 : 0.05086836218833923
Loss at iteration 1090 : 0.07167720794677734
Loss at iteration 1100 : 0.07036951184272766
Loss at iteration 1110 : 0.1485472023487091
Loss at iteration 1120 : 0.05523058772087097
Loss at iteration 1130 : 0.08702178299427032
Loss at iteration 1140 : 0.07068195194005966
Loss at iteration 1150 : 0.11801191419363022
Loss at iteration 1160 : 0.07274100929498672
Loss at iteration 1170 : 0.06316433846950531
Loss at iteration 1180 : 0.06851503998041153
Loss at iteration 1190 : 0.07349568605422974
Loss at iteration 1200 : 0.08005594462156296
Loss at iteration 1210 : 0.09410524368286133
The SSIM Value is: 0.715941329797109
The PSNR Value is: 20.910317611694335
the epoch is: 159
Loss at iteration 10 : 0.06709407269954681
Loss at iteration 20 : 0.08273619413375854
Loss at iteration 30 : 0.09339204430580139
Loss at iteration 40 : 0.07086188346147537
Loss at iteration 50 : 0.0649770200252533
Loss at iteration 60 : 0.08669855445623398
Loss at iteration 70 : 0.07367496937513351
Loss at iteration 80 : 0.06973075866699219
Loss at iteration 90 : 0.08046109974384308
Loss at iteration 100 : 0.04798915237188339
Loss at iteration 110 : 0.09978612512350082
Loss at iteration 120 : 0.049289584159851074
Loss at iteration 130 : 0.08874049037694931
Loss at iteration 140 : 0.09062887728214264
Loss at iteration 150 : 0.08276478946208954
Loss at iteration 160 : 0.10206609219312668
Loss at iteration 170 : 0.05526164174079895
Loss at iteration 180 : 0.05519707500934601
Loss at iteration 190 : 0.06789002567529678
Loss at iteration 200 : 0.08784013986587524
Loss at iteration 210 : 0.07178889214992523
Loss at iteration 220 : 0.0935157760977745
Loss at iteration 230 : 0.10059517621994019
Loss at iteration 240 : 0.05860811471939087
Loss at iteration 250 : 0.13783201575279236
Loss at iteration 260 : 0.09268586337566376
Loss at iteration 270 : 0.08703559637069702
Loss at iteration 280 : 0.08020523190498352
Loss at iteration 290 : 0.1007600650191307
Loss at iteration 300 : 0.06271882355213165
Loss at iteration 310 : 0.08076377958059311
Loss at iteration 320 : 0.07677523046731949
Loss at iteration 330 : 0.06106424331665039
Loss at iteration 340 : 0.05492106080055237
Loss at iteration 350 : 0.08862458914518356
Loss at iteration 360 : 0.08110059797763824
Loss at iteration 370 : 0.07531580328941345
Loss at iteration 380 : 0.0806054174900055
Loss at iteration 390 : 0.06259003281593323
Loss at iteration 400 : 0.10644392669200897
Loss at iteration 410 : 0.055987399071455
Loss at iteration 420 : 0.097947858273983
Loss at iteration 430 : 0.11064030975103378
Loss at iteration 440 : 0.08366528153419495
Loss at iteration 450 : 0.07172522693872452
Loss at iteration 460 : 0.050004493445158005
Loss at iteration 470 : 0.1366560161113739
Loss at iteration 480 : 0.10954655706882477
Loss at iteration 490 : 0.07233239710330963
Loss at iteration 500 : 0.07277568429708481
Loss at iteration 510 : 0.06719055771827698
Loss at iteration 520 : 0.09687021374702454
Loss at iteration 530 : 0.12307743728160858
Loss at iteration 540 : 0.1012340784072876
Loss at iteration 550 : 0.13939276337623596
Loss at iteration 560 : 0.05370154231786728
Loss at iteration 570 : 0.04761694371700287
Loss at iteration 580 : 0.08893394470214844
Loss at iteration 590 : 0.09034252166748047
Loss at iteration 600 : 0.07356034219264984
Loss at iteration 610 : 0.051445372402668
Loss at iteration 620 : 0.0718865841627121
Loss at iteration 630 : 0.05314512178301811
Loss at iteration 640 : 0.07791677862405777
Loss at iteration 650 : 0.07672125101089478
Loss at iteration 660 : 0.05431051552295685
Loss at iteration 670 : 0.06603299081325531
Loss at iteration 680 : 0.09184335172176361
Loss at iteration 690 : 0.08391040563583374
Loss at iteration 700 : 0.08978774398565292
Loss at iteration 710 : 0.07064647972583771
Loss at iteration 720 : 0.06686310470104218
Loss at iteration 730 : 0.10492873191833496
Loss at iteration 740 : 0.07571875303983688
Loss at iteration 750 : 0.10780241340398788
Loss at iteration 760 : 0.1081090196967125
Loss at iteration 770 : 0.09306322783231735
Loss at iteration 780 : 0.07820303738117218
Loss at iteration 790 : 0.08169295638799667
Loss at iteration 800 : 0.1642051339149475
Loss at iteration 810 : 0.10280133783817291
Loss at iteration 820 : 0.055700093507766724
Loss at iteration 830 : 0.0820101797580719
Loss at iteration 840 : 0.0737469419836998
Loss at iteration 850 : 0.09035706520080566
Loss at iteration 860 : 0.08737322688102722
Loss at iteration 870 : 0.13740167021751404
Loss at iteration 880 : 0.09202806651592255
Loss at iteration 890 : 0.060325752943754196
Loss at iteration 900 : 0.09370248019695282
Loss at iteration 910 : 0.07745620608329773
Loss at iteration 920 : 0.09514927864074707
Loss at iteration 930 : 0.08760567009449005
Loss at iteration 940 : 0.10495477169752121
Loss at iteration 950 : 0.07887201756238937
Loss at iteration 960 : 0.10579933226108551
Loss at iteration 970 : 0.07790911197662354
Loss at iteration 980 : 0.08565717190504074
Loss at iteration 990 : 0.03453213721513748
Loss at iteration 1000 : 0.047802314162254333
Loss at iteration 1010 : 0.07867427915334702
Loss at iteration 1020 : 0.12104793637990952
Loss at iteration 1030 : 0.09665574878454208
Loss at iteration 1040 : 0.060429587960243225
Loss at iteration 1050 : 0.06186436861753464
Loss at iteration 1060 : 0.0659913644194603
Loss at iteration 1070 : 0.08942100405693054
Loss at iteration 1080 : 0.08542530983686447
Loss at iteration 1090 : 0.09110727161169052
Loss at iteration 1100 : 0.1005856841802597
Loss at iteration 1110 : 0.08378444612026215
Loss at iteration 1120 : 0.06641670316457748
Loss at iteration 1130 : 0.07995016872882843
Loss at iteration 1140 : 0.07489874958992004
Loss at iteration 1150 : 0.0695340558886528
Loss at iteration 1160 : 0.08666762709617615
Loss at iteration 1170 : 0.1304163783788681
Loss at iteration 1180 : 0.10074885189533234
Loss at iteration 1190 : 0.08768607676029205
Loss at iteration 1200 : 0.09771418571472168
Loss at iteration 1210 : 0.09799932688474655
The SSIM Value is: 0.7153228799502055
The PSNR Value is: 21.023402786254884
the epoch is: 160
Loss at iteration 10 : 0.13531005382537842
Loss at iteration 20 : 0.07573524862527847
Loss at iteration 30 : 0.09268349409103394
Loss at iteration 40 : 0.062123991549015045
Loss at iteration 50 : 0.05697759985923767
Loss at iteration 60 : 0.05324999615550041
Loss at iteration 70 : 0.10185890644788742
Loss at iteration 80 : 0.10669654607772827
Loss at iteration 90 : 0.09054537862539291
Loss at iteration 100 : 0.06620714068412781
Loss at iteration 110 : 0.0710315927863121
Loss at iteration 120 : 0.1191752552986145
Loss at iteration 130 : 0.1009446531534195
Loss at iteration 140 : 0.0560288242995739
Loss at iteration 150 : 0.11325599998235703
Loss at iteration 160 : 0.12180514633655548
Loss at iteration 170 : 0.11821635812520981
Loss at iteration 180 : 0.08354543149471283
Loss at iteration 190 : 0.09296618402004242
Loss at iteration 200 : 0.14338967204093933
Loss at iteration 210 : 0.10294924676418304
Loss at iteration 220 : 0.10322302579879761
Loss at iteration 230 : 0.09882356971502304
Loss at iteration 240 : 0.08318120241165161
Loss at iteration 250 : 0.09951901435852051
Loss at iteration 260 : 0.10504194349050522
Loss at iteration 270 : 0.06624865531921387
Loss at iteration 280 : 0.07956045120954514
Loss at iteration 290 : 0.09206550568342209
Loss at iteration 300 : 0.0834575742483139
Loss at iteration 310 : 0.07984210550785065
Loss at iteration 320 : 0.12173493206501007
Loss at iteration 330 : 0.10603997111320496
Loss at iteration 340 : 0.06811067461967468
Loss at iteration 350 : 0.07757404446601868
Loss at iteration 360 : 0.03974354267120361
Loss at iteration 370 : 0.05097305029630661
Loss at iteration 380 : 0.0808180570602417
Loss at iteration 390 : 0.05401789769530296
Loss at iteration 400 : 0.04789268970489502
Loss at iteration 410 : 0.09618636965751648
Loss at iteration 420 : 0.05452404171228409
Loss at iteration 430 : 0.08761336654424667
Loss at iteration 440 : 0.09002746641635895
Loss at iteration 450 : 0.11865818500518799
Loss at iteration 460 : 0.0979231745004654
Loss at iteration 470 : 0.06486256420612335
Loss at iteration 480 : 0.09070231765508652
Loss at iteration 490 : 0.06284035742282867
Loss at iteration 500 : 0.081937775015831
Loss at iteration 510 : 0.08188183605670929
Loss at iteration 520 : 0.06928631663322449
Loss at iteration 530 : 0.11684652417898178
Loss at iteration 540 : 0.08001722395420074
Loss at iteration 550 : 0.08763258904218674
Loss at iteration 560 : 0.07467082142829895
Loss at iteration 570 : 0.07604198157787323
Loss at iteration 580 : 0.038640521466732025
Loss at iteration 590 : 0.0927545577287674
Loss at iteration 600 : 0.09669201076030731
Loss at iteration 610 : 0.09083765745162964
Loss at iteration 620 : 0.057110127061605453
Loss at iteration 630 : 0.06279177218675613
Loss at iteration 640 : 0.11063103377819061
Loss at iteration 650 : 0.056027721613645554
Loss at iteration 660 : 0.07204402983188629
Loss at iteration 670 : 0.05928374454379082
Loss at iteration 680 : 0.061433009803295135
Loss at iteration 690 : 0.08791910111904144
Loss at iteration 700 : 0.06306525319814682
Loss at iteration 710 : 0.24531245231628418
Loss at iteration 720 : 0.11227099597454071
Loss at iteration 730 : 0.09108086675405502
Loss at iteration 740 : 0.05889343470335007
Loss at iteration 750 : 0.06673958152532578
Loss at iteration 760 : 0.0722169280052185
Loss at iteration 770 : 0.08466354012489319
Loss at iteration 780 : 0.08095700293779373
Loss at iteration 790 : 0.08122080564498901
Loss at iteration 800 : 0.08919932693243027
Loss at iteration 810 : 0.07645455002784729
Loss at iteration 820 : 0.10558216273784637
Loss at iteration 830 : 0.0785699188709259
Loss at iteration 840 : 0.08949369192123413
Loss at iteration 850 : 0.08023609220981598
Loss at iteration 860 : 0.13038036227226257
Loss at iteration 870 : 0.05623551458120346
Loss at iteration 880 : 0.08139054477214813
Loss at iteration 890 : 0.0685519278049469
Loss at iteration 900 : 0.08369342982769012
Loss at iteration 910 : 0.07201322913169861
Loss at iteration 920 : 0.12338124215602875
Loss at iteration 930 : 0.08904358744621277
Loss at iteration 940 : 0.05070044845342636
Loss at iteration 950 : 0.06623242795467377
Loss at iteration 960 : 0.1283896416425705
Loss at iteration 970 : 0.0926419124007225
Loss at iteration 980 : 0.08084003627300262
Loss at iteration 990 : 0.04747791215777397
Loss at iteration 1000 : 0.0619131438434124
Loss at iteration 1010 : 0.09665902704000473
Loss at iteration 1020 : 0.062220122665166855
Loss at iteration 1030 : 0.07497730851173401
Loss at iteration 1040 : 0.11050869524478912
Loss at iteration 1050 : 0.08815799653530121
Loss at iteration 1060 : 0.10284613072872162
Loss at iteration 1070 : 0.10811768472194672
Loss at iteration 1080 : 0.1137133240699768
Loss at iteration 1090 : 0.06110089272260666
Loss at iteration 1100 : 0.07920064777135849
Loss at iteration 1110 : 0.14050762355327606
Loss at iteration 1120 : 0.0771590918302536
Loss at iteration 1130 : 0.0757162794470787
Loss at iteration 1140 : 0.0933932363986969
Loss at iteration 1150 : 0.08528627455234528
Loss at iteration 1160 : 0.04628398269414902
Loss at iteration 1170 : 0.097428098320961
Loss at iteration 1180 : 0.08513244986534119
Loss at iteration 1190 : 0.11743693053722382
Loss at iteration 1200 : 0.09465780109167099
Loss at iteration 1210 : 0.05981936305761337
The SSIM Value is: 0.7261277834574381
The PSNR Value is: 21.609198633829752
the highest SSIM value is: 21.609198633829752
the epoch is: 161
Loss at iteration 10 : 0.0721551775932312
Loss at iteration 20 : 0.10090243816375732
Loss at iteration 30 : 0.07836676388978958
Loss at iteration 40 : 0.05679281800985336
Loss at iteration 50 : 0.16937002539634705
Loss at iteration 60 : 0.10371585935354233
Loss at iteration 70 : 0.08257143944501877
Loss at iteration 80 : 0.12322332710027695
Loss at iteration 90 : 0.06447833776473999
Loss at iteration 100 : 0.09797151386737823
Loss at iteration 110 : 0.0623912438750267
Loss at iteration 120 : 0.06326260417699814
Loss at iteration 130 : 0.09963536262512207
Loss at iteration 140 : 0.07414662837982178
Loss at iteration 150 : 0.03937801346182823
Loss at iteration 160 : 0.12296116352081299
Loss at iteration 170 : 0.06106644868850708
Loss at iteration 180 : 0.0774451345205307
Loss at iteration 190 : 0.1156596839427948
Loss at iteration 200 : 0.07445251941680908
Loss at iteration 210 : 0.06031119450926781
Loss at iteration 220 : 0.08131342381238937
Loss at iteration 230 : 0.07504576444625854
Loss at iteration 240 : 0.06746980547904968
Loss at iteration 250 : 0.08268244564533234
Loss at iteration 260 : 0.06546027213335037
Loss at iteration 270 : 0.07986121624708176
Loss at iteration 280 : 0.06633900105953217
Loss at iteration 290 : 0.08345432579517365
Loss at iteration 300 : 0.08216509968042374
Loss at iteration 310 : 0.041513387113809586
Loss at iteration 320 : 0.12199519574642181
Loss at iteration 330 : 0.09145832061767578
Loss at iteration 340 : 0.05724732205271721
Loss at iteration 350 : 0.07162533700466156
Loss at iteration 360 : 0.060078397393226624
Loss at iteration 370 : 0.10454913228750229
Loss at iteration 380 : 0.07001560926437378
Loss at iteration 390 : 0.11266543716192245
Loss at iteration 400 : 0.08825647830963135
Loss at iteration 410 : 0.0842963308095932
Loss at iteration 420 : 0.09509783983230591
Loss at iteration 430 : 0.08366676419973373
Loss at iteration 440 : 0.09035514295101166
Loss at iteration 450 : 0.09906884282827377
Loss at iteration 460 : 0.07588674128055573
Loss at iteration 470 : 0.043525636196136475
Loss at iteration 480 : 0.08108171820640564
Loss at iteration 490 : 0.09279441833496094
Loss at iteration 500 : 0.07057784497737885
Loss at iteration 510 : 0.08877643197774887
Loss at iteration 520 : 0.10914071649312973
Loss at iteration 530 : 0.10764440149068832
Loss at iteration 540 : 0.07370557636022568
Loss at iteration 550 : 0.05329728126525879
Loss at iteration 560 : 0.09951414912939072
Loss at iteration 570 : 0.09108658134937286
Loss at iteration 580 : 0.0721995085477829
Loss at iteration 590 : 0.054668527096509933
Loss at iteration 600 : 0.07554298639297485
Loss at iteration 610 : 0.07917623966932297
Loss at iteration 620 : 0.05229008197784424
Loss at iteration 630 : 0.07057123631238937
Loss at iteration 640 : 0.07149796932935715
Loss at iteration 650 : 0.09041621536016464
Loss at iteration 660 : 0.07031069695949554
Loss at iteration 670 : 0.10956032574176788
Loss at iteration 680 : 0.0918739065527916
Loss at iteration 690 : 0.09792931377887726
Loss at iteration 700 : 0.05898578092455864
Loss at iteration 710 : 0.06280957162380219
Loss at iteration 720 : 0.11100265383720398
Loss at iteration 730 : 0.05330139398574829
Loss at iteration 740 : 0.08038457483053207
Loss at iteration 750 : 0.06068547070026398
Loss at iteration 760 : 0.09645329415798187
Loss at iteration 770 : 0.06491416692733765
Loss at iteration 780 : 0.1284181922674179
Loss at iteration 790 : 0.08531223237514496
Loss at iteration 800 : 0.07786615192890167
Loss at iteration 810 : 0.05581358075141907
Loss at iteration 820 : 0.05973437428474426
Loss at iteration 830 : 0.07078221440315247
Loss at iteration 840 : 0.07021796703338623
Loss at iteration 850 : 0.06735628098249435
Loss at iteration 860 : 0.07056982815265656
Loss at iteration 870 : 0.10407388210296631
Loss at iteration 880 : 0.07580225169658661
Loss at iteration 890 : 0.09282980859279633
Loss at iteration 900 : 0.05644376575946808
Loss at iteration 910 : 0.04949065297842026
Loss at iteration 920 : 0.11033271253108978
Loss at iteration 930 : 0.05470840632915497
Loss at iteration 940 : 0.10709172487258911
Loss at iteration 950 : 0.0685204565525055
Loss at iteration 960 : 0.11656689643859863
Loss at iteration 970 : 0.07131873071193695
Loss at iteration 980 : 0.09686492383480072
Loss at iteration 990 : 0.06967905163764954
Loss at iteration 1000 : 0.06416605412960052
Loss at iteration 1010 : 0.07674558460712433
Loss at iteration 1020 : 0.07028050720691681
Loss at iteration 1030 : 0.07817967236042023
Loss at iteration 1040 : 0.09520193189382553
Loss at iteration 1050 : 0.14231345057487488
Loss at iteration 1060 : 0.05637435242533684
Loss at iteration 1070 : 0.07312864065170288
Loss at iteration 1080 : 0.09983132779598236
Loss at iteration 1090 : 0.10911667346954346
Loss at iteration 1100 : 0.09590857475996017
Loss at iteration 1110 : 0.09166824817657471
Loss at iteration 1120 : 0.07390358299016953
Loss at iteration 1130 : 0.056778352707624435
Loss at iteration 1140 : 0.10676980018615723
Loss at iteration 1150 : 0.07769884169101715
Loss at iteration 1160 : 0.04639303684234619
Loss at iteration 1170 : 0.09366299957036972
Loss at iteration 1180 : 0.1200314611196518
Loss at iteration 1190 : 0.09105681627988815
Loss at iteration 1200 : 0.12126182019710541
Loss at iteration 1210 : 0.09630044549703598
The SSIM Value is: 0.7157963156700134
The PSNR Value is: 20.865550740559897
the epoch is: 162
Loss at iteration 10 : 0.0693393275141716
Loss at iteration 20 : 0.10688032954931259
Loss at iteration 30 : 0.07984736561775208
Loss at iteration 40 : 0.06340181082487106
Loss at iteration 50 : 0.08007841557264328
Loss at iteration 60 : 0.07652579993009567
Loss at iteration 70 : 0.12618432939052582
Loss at iteration 80 : 0.0782044380903244
Loss at iteration 90 : 0.11231738328933716
Loss at iteration 100 : 0.08246343582868576
Loss at iteration 110 : 0.07132561504840851
Loss at iteration 120 : 0.06339573115110397
Loss at iteration 130 : 0.04752261936664581
Loss at iteration 140 : 0.08258651196956635
Loss at iteration 150 : 0.08649864047765732
Loss at iteration 160 : 0.12271465361118317
Loss at iteration 170 : 0.06029573082923889
Loss at iteration 180 : 0.12613292038440704
Loss at iteration 190 : 0.12792667746543884
Loss at iteration 200 : 0.046606987714767456
Loss at iteration 210 : 0.1003783792257309
Loss at iteration 220 : 0.09859393537044525
Loss at iteration 230 : 0.10637509822845459
Loss at iteration 240 : 0.04570354148745537
Loss at iteration 250 : 0.05627503618597984
Loss at iteration 260 : 0.09306052327156067
Loss at iteration 270 : 0.08144738525152206
Loss at iteration 280 : 0.08711963891983032
Loss at iteration 290 : 0.07887262105941772
Loss at iteration 300 : 0.12120547145605087
Loss at iteration 310 : 0.09796653687953949
Loss at iteration 320 : 0.07655196636915207
Loss at iteration 330 : 0.07903431355953217
Loss at iteration 340 : 0.08383643627166748
Loss at iteration 350 : 0.11171551793813705
Loss at iteration 360 : 0.08516683429479599
Loss at iteration 370 : 0.08458919823169708
Loss at iteration 380 : 0.08100061118602753
Loss at iteration 390 : 0.07144571840763092
Loss at iteration 400 : 0.06291621178388596
Loss at iteration 410 : 0.07063464820384979
Loss at iteration 420 : 0.059320926666259766
Loss at iteration 430 : 0.09950628131628036
Loss at iteration 440 : 0.06473325192928314
Loss at iteration 450 : 0.09942024946212769
Loss at iteration 460 : 0.06100001186132431
Loss at iteration 470 : 0.0977013111114502
Loss at iteration 480 : 0.08536544442176819
Loss at iteration 490 : 0.07382960617542267
Loss at iteration 500 : 0.05224701017141342
Loss at iteration 510 : 0.05431051179766655
Loss at iteration 520 : 0.05143044516444206
Loss at iteration 530 : 0.12161821126937866
Loss at iteration 540 : 0.11416326463222504
Loss at iteration 550 : 0.07535785436630249
Loss at iteration 560 : 0.08537932485342026
Loss at iteration 570 : 0.10560201853513718
Loss at iteration 580 : 0.10122418403625488
Loss at iteration 590 : 0.07073158025741577
Loss at iteration 600 : 0.06745132058858871
Loss at iteration 610 : 0.08053363114595413
Loss at iteration 620 : 0.09177699685096741
Loss at iteration 630 : 0.0927792638540268
Loss at iteration 640 : 0.05374317616224289
Loss at iteration 650 : 0.07112140208482742
Loss at iteration 660 : 0.06661072373390198
Loss at iteration 670 : 0.07784868776798248
Loss at iteration 680 : 0.1035495325922966
Loss at iteration 690 : 0.0779147669672966
Loss at iteration 700 : 0.08681418746709824
Loss at iteration 710 : 0.10008053481578827
Loss at iteration 720 : 0.07872514426708221
Loss at iteration 730 : 0.08173736184835434
Loss at iteration 740 : 0.08251088857650757
Loss at iteration 750 : 0.08500875532627106
Loss at iteration 760 : 0.05565149709582329
Loss at iteration 770 : 0.08137936890125275
Loss at iteration 780 : 0.061118435114622116
Loss at iteration 790 : 0.07463143765926361
Loss at iteration 800 : 0.09318313002586365
Loss at iteration 810 : 0.055863119661808014
Loss at iteration 820 : 0.04324597120285034
Loss at iteration 830 : 0.07382579147815704
Loss at iteration 840 : 0.08305805921554565
Loss at iteration 850 : 0.07073890417814255
Loss at iteration 860 : 0.06138698384165764
Loss at iteration 870 : 0.08183737844228745
Loss at iteration 880 : 0.09837858378887177
Loss at iteration 890 : 0.08055531978607178
Loss at iteration 900 : 0.09060033410787582
Loss at iteration 910 : 0.06766924262046814
Loss at iteration 920 : 0.09786161780357361
Loss at iteration 930 : 0.05968804657459259
Loss at iteration 940 : 0.10279979556798935
Loss at iteration 950 : 0.05435672402381897
Loss at iteration 960 : 0.10409180819988251
Loss at iteration 970 : 0.11170707643032074
Loss at iteration 980 : 0.10839861631393433
Loss at iteration 990 : 0.05837167054414749
Loss at iteration 1000 : 0.057500071823596954
Loss at iteration 1010 : 0.0940401703119278
Loss at iteration 1020 : 0.09413543343544006
Loss at iteration 1030 : 0.09596297144889832
Loss at iteration 1040 : 0.08423487842082977
Loss at iteration 1050 : 0.07479677349328995
Loss at iteration 1060 : 0.10220067203044891
Loss at iteration 1070 : 0.1029663011431694
Loss at iteration 1080 : 0.10978379100561142
Loss at iteration 1090 : 0.05781049653887749
Loss at iteration 1100 : 0.07899409532546997
Loss at iteration 1110 : 0.09305494278669357
Loss at iteration 1120 : 0.09302439540624619
Loss at iteration 1130 : 0.07772871851921082
Loss at iteration 1140 : 0.07473282516002655
Loss at iteration 1150 : 0.10461501777172089
Loss at iteration 1160 : 0.07704395055770874
Loss at iteration 1170 : 0.05074749514460564
Loss at iteration 1180 : 0.11953999847173691
Loss at iteration 1190 : 0.0810394138097763
Loss at iteration 1200 : 0.10898646712303162
Loss at iteration 1210 : 0.09716558456420898
The SSIM Value is: 0.7210568805535634
The PSNR Value is: 21.509494654337566
the epoch is: 163
Loss at iteration 10 : 0.058935701847076416
Loss at iteration 20 : 0.04830070585012436
Loss at iteration 30 : 0.0733700841665268
Loss at iteration 40 : 0.05293671786785126
Loss at iteration 50 : 0.0650334358215332
Loss at iteration 60 : 0.08483313769102097
Loss at iteration 70 : 0.06565985828638077
Loss at iteration 80 : 0.06700354069471359
Loss at iteration 90 : 0.13979953527450562
Loss at iteration 100 : 0.050309762358665466
Loss at iteration 110 : 0.09807991981506348
Loss at iteration 120 : 0.1019027978181839
Loss at iteration 130 : 0.07008864730596542
Loss at iteration 140 : 0.10785897821187973
Loss at iteration 150 : 0.0642043948173523
Loss at iteration 160 : 0.09598188102245331
Loss at iteration 170 : 0.07955734431743622
Loss at iteration 180 : 0.04696797952055931
Loss at iteration 190 : 0.08153821527957916
Loss at iteration 200 : 0.0855771154165268
Loss at iteration 210 : 0.12625202536582947
Loss at iteration 220 : 0.10746686160564423
Loss at iteration 230 : 0.08938276767730713
Loss at iteration 240 : 0.07223459333181381
Loss at iteration 250 : 0.1693153828382492
Loss at iteration 260 : 0.08296044915914536
Loss at iteration 270 : 0.06239631772041321
Loss at iteration 280 : 0.07397153973579407
Loss at iteration 290 : 0.075754314661026
Loss at iteration 300 : 0.10268175601959229
Loss at iteration 310 : 0.12202553451061249
Loss at iteration 320 : 0.08758842945098877
Loss at iteration 330 : 0.10766005516052246
Loss at iteration 340 : 0.08557511866092682
Loss at iteration 350 : 0.08748643100261688
Loss at iteration 360 : 0.04677244648337364
Loss at iteration 370 : 0.08310111612081528
Loss at iteration 380 : 0.07369360327720642
Loss at iteration 390 : 0.03408188372850418
Loss at iteration 400 : 0.08562102168798447
Loss at iteration 410 : 0.09815024584531784
Loss at iteration 420 : 0.13995569944381714
Loss at iteration 430 : 0.08659335970878601
Loss at iteration 440 : 0.11445271968841553
Loss at iteration 450 : 0.06424976140260696
Loss at iteration 460 : 0.062097012996673584
Loss at iteration 470 : 0.0946027934551239
Loss at iteration 480 : 0.09958305209875107
Loss at iteration 490 : 0.05667033791542053
Loss at iteration 500 : 0.11119924485683441
Loss at iteration 510 : 0.06404362618923187
Loss at iteration 520 : 0.05652811378240585
Loss at iteration 530 : 0.07267847657203674
Loss at iteration 540 : 0.0534338504076004
Loss at iteration 550 : 0.06408735364675522
Loss at iteration 560 : 0.09394719451665878
Loss at iteration 570 : 0.07164771109819412
Loss at iteration 580 : 0.07326138019561768
Loss at iteration 590 : 0.1093316376209259
Loss at iteration 600 : 0.1012154147028923
Loss at iteration 610 : 0.07592111825942993
Loss at iteration 620 : 0.1032504141330719
Loss at iteration 630 : 0.07362915575504303
Loss at iteration 640 : 0.12817150354385376
Loss at iteration 650 : 0.07285117357969284
Loss at iteration 660 : 0.0754917562007904
Loss at iteration 670 : 0.1097937524318695
Loss at iteration 680 : 0.040523283183574677
Loss at iteration 690 : 0.06513194739818573
Loss at iteration 700 : 0.07873838394880295
Loss at iteration 710 : 0.1125171035528183
Loss at iteration 720 : 0.06757523119449615
Loss at iteration 730 : 0.06494507193565369
Loss at iteration 740 : 0.08885234594345093
Loss at iteration 750 : 0.05187138915061951
Loss at iteration 760 : 0.09884874522686005
Loss at iteration 770 : 0.08630108833312988
Loss at iteration 780 : 0.1416689157485962
Loss at iteration 790 : 0.06951241195201874
Loss at iteration 800 : 0.07222330570220947
Loss at iteration 810 : 0.10172618925571442
Loss at iteration 820 : 0.08104704320430756
Loss at iteration 830 : 0.08838371932506561
Loss at iteration 840 : 0.04775368422269821
Loss at iteration 850 : 0.07575158774852753
Loss at iteration 860 : 0.09529237449169159
Loss at iteration 870 : 0.07641032338142395
Loss at iteration 880 : 0.06397373974323273
Loss at iteration 890 : 0.058501601219177246
Loss at iteration 900 : 0.08959033340215683
Loss at iteration 910 : 0.09196852147579193
Loss at iteration 920 : 0.09625077247619629
Loss at iteration 930 : 0.07047230005264282
Loss at iteration 940 : 0.05909765884280205
Loss at iteration 950 : 0.08759424090385437
Loss at iteration 960 : 0.06864285469055176
Loss at iteration 970 : 0.10872714221477509
Loss at iteration 980 : 0.08515208959579468
Loss at iteration 990 : 0.09176236391067505
Loss at iteration 1000 : 0.05558571219444275
Loss at iteration 1010 : 0.06884225457906723
Loss at iteration 1020 : 0.07977252453565598
Loss at iteration 1030 : 0.07789403200149536
Loss at iteration 1040 : 0.10680215060710907
Loss at iteration 1050 : 0.0807727575302124
Loss at iteration 1060 : 0.09599393606185913
Loss at iteration 1070 : 0.04246953874826431
Loss at iteration 1080 : 0.05931207537651062
Loss at iteration 1090 : 0.06994360685348511
Loss at iteration 1100 : 0.07326697558164597
Loss at iteration 1110 : 0.08209413290023804
Loss at iteration 1120 : 0.08393315970897675
Loss at iteration 1130 : 0.08068744093179703
Loss at iteration 1140 : 0.05726545304059982
Loss at iteration 1150 : 0.09378650039434433
Loss at iteration 1160 : 0.1613028645515442
Loss at iteration 1170 : 0.09332937002182007
Loss at iteration 1180 : 0.10239287465810776
Loss at iteration 1190 : 0.05753416568040848
Loss at iteration 1200 : 0.05993850156664848
Loss at iteration 1210 : 0.08840705454349518
The SSIM Value is: 0.7134382327397665
The PSNR Value is: 20.789702479044596
the epoch is: 164
Loss at iteration 10 : 0.055378325283527374
Loss at iteration 20 : 0.09535381197929382
Loss at iteration 30 : 0.09636320918798447
Loss at iteration 40 : 0.1171446293592453
Loss at iteration 50 : 0.10378512740135193
Loss at iteration 60 : 0.07943344861268997
Loss at iteration 70 : 0.07368350774049759
Loss at iteration 80 : 0.07247349619865417
Loss at iteration 90 : 0.048418544232845306
Loss at iteration 100 : 0.11374606937170029
Loss at iteration 110 : 0.07090204209089279
Loss at iteration 120 : 0.07685264199972153
Loss at iteration 130 : 0.12265994399785995
Loss at iteration 140 : 0.04788152500987053
Loss at iteration 150 : 0.13165444135665894
Loss at iteration 160 : 0.05688532814383507
Loss at iteration 170 : 0.10998763889074326
Loss at iteration 180 : 0.061613403260707855
Loss at iteration 190 : 0.11813287436962128
Loss at iteration 200 : 0.11396335065364838
Loss at iteration 210 : 0.08313732594251633
Loss at iteration 220 : 0.0570017546415329
Loss at iteration 230 : 0.10867544263601303
Loss at iteration 240 : 0.08936562389135361
Loss at iteration 250 : 0.08442217111587524
Loss at iteration 260 : 0.07067461311817169
Loss at iteration 270 : 0.09547515958547592
Loss at iteration 280 : 0.07082810997962952
Loss at iteration 290 : 0.07651060074567795
Loss at iteration 300 : 0.09456910938024521
Loss at iteration 310 : 0.05883947014808655
Loss at iteration 320 : 0.05998167023062706
Loss at iteration 330 : 0.09658294916152954
Loss at iteration 340 : 0.0680147260427475
Loss at iteration 350 : 0.089912548661232
Loss at iteration 360 : 0.07362940907478333
Loss at iteration 370 : 0.1245993822813034
Loss at iteration 380 : 0.08750583976507187
Loss at iteration 390 : 0.10218299925327301
Loss at iteration 400 : 0.09722768515348434
Loss at iteration 410 : 0.06371551007032394
Loss at iteration 420 : 0.05584370344877243
Loss at iteration 430 : 0.08696036040782928
Loss at iteration 440 : 0.08909537643194199
Loss at iteration 450 : 0.14279837906360626
Loss at iteration 460 : 0.08675392717123032
Loss at iteration 470 : 0.08771753311157227
Loss at iteration 480 : 0.07676291465759277
Loss at iteration 490 : 0.13587921857833862
Loss at iteration 500 : 0.06729848682880402
Loss at iteration 510 : 0.11581354588270187
Loss at iteration 520 : 0.053359255194664
Loss at iteration 530 : 0.14810426533222198
Loss at iteration 540 : 0.12196367979049683
Loss at iteration 550 : 0.08036896586418152
Loss at iteration 560 : 0.09533834457397461
Loss at iteration 570 : 0.096921905875206
Loss at iteration 580 : 0.05333113670349121
Loss at iteration 590 : 0.057155631482601166
Loss at iteration 600 : 0.08923546969890594
Loss at iteration 610 : 0.09389550983905792
Loss at iteration 620 : 0.06388705223798752
Loss at iteration 630 : 0.05449501797556877
Loss at iteration 640 : 0.11389128863811493
Loss at iteration 650 : 0.0633450448513031
Loss at iteration 660 : 0.09956580400466919
Loss at iteration 670 : 0.07291528582572937
Loss at iteration 680 : 0.07470433413982391
Loss at iteration 690 : 0.11323826760053635
Loss at iteration 700 : 0.09241610765457153
Loss at iteration 710 : 0.10358074307441711
Loss at iteration 720 : 0.0740702748298645
Loss at iteration 730 : 0.0905265212059021
Loss at iteration 740 : 0.07896454632282257
Loss at iteration 750 : 0.08743077516555786
Loss at iteration 760 : 0.0544622465968132
Loss at iteration 770 : 0.07646791636943817
Loss at iteration 780 : 0.052375175058841705
Loss at iteration 790 : 0.0768163800239563
Loss at iteration 800 : 0.06794856488704681
Loss at iteration 810 : 0.09736193716526031
Loss at iteration 820 : 0.0653330609202385
Loss at iteration 830 : 0.050193965435028076
Loss at iteration 840 : 0.08960306644439697
Loss at iteration 850 : 0.0706467479467392
Loss at iteration 860 : 0.06667383760213852
Loss at iteration 870 : 0.07893545180559158
Loss at iteration 880 : 0.08309604972600937
Loss at iteration 890 : 0.0554460808634758
Loss at iteration 900 : 0.08849364519119263
Loss at iteration 910 : 0.08071715384721756
Loss at iteration 920 : 0.06850646436214447
Loss at iteration 930 : 0.07625524699687958
Loss at iteration 940 : 0.060289572924375534
Loss at iteration 950 : 0.06785470247268677
Loss at iteration 960 : 0.08137945830821991
Loss at iteration 970 : 0.06179298460483551
Loss at iteration 980 : 0.08002234995365143
Loss at iteration 990 : 0.09359129518270493
Loss at iteration 1000 : 0.07195423543453217
Loss at iteration 1010 : 0.08458931744098663
Loss at iteration 1020 : 0.08557762205600739
Loss at iteration 1030 : 0.053498849272727966
Loss at iteration 1040 : 0.08673028647899628
Loss at iteration 1050 : 0.07103125751018524
Loss at iteration 1060 : 0.08722086250782013
Loss at iteration 1070 : 0.0564083456993103
Loss at iteration 1080 : 0.08319225907325745
Loss at iteration 1090 : 0.08736233413219452
Loss at iteration 1100 : 0.0922296866774559
Loss at iteration 1110 : 0.057686224579811096
Loss at iteration 1120 : 0.07582437992095947
Loss at iteration 1130 : 0.08732994645833969
Loss at iteration 1140 : 0.04672272503376007
Loss at iteration 1150 : 0.09093424677848816
Loss at iteration 1160 : 0.13149243593215942
Loss at iteration 1170 : 0.08245362341403961
Loss at iteration 1180 : 0.06260909140110016
Loss at iteration 1190 : 0.07544471323490143
Loss at iteration 1200 : 0.04436185956001282
Loss at iteration 1210 : 0.10357489436864853
The SSIM Value is: 0.712458747625351
The PSNR Value is: 20.434821128845215
the epoch is: 165
Loss at iteration 10 : 0.07638043165206909
Loss at iteration 20 : 0.07398954033851624
Loss at iteration 30 : 0.11448083817958832
Loss at iteration 40 : 0.0718841552734375
Loss at iteration 50 : 0.0654606744647026
Loss at iteration 60 : 0.054024823009967804
Loss at iteration 70 : 0.12317490577697754
Loss at iteration 80 : 0.09204472601413727
Loss at iteration 90 : 0.08870576322078705
Loss at iteration 100 : 0.10642208904027939
Loss at iteration 110 : 0.0641985833644867
Loss at iteration 120 : 0.06769748777151108
Loss at iteration 130 : 0.08659911155700684
Loss at iteration 140 : 0.0724525898694992
Loss at iteration 150 : 0.08346909284591675
Loss at iteration 160 : 0.0774540901184082
Loss at iteration 170 : 0.10294362902641296
Loss at iteration 180 : 0.05139633268117905
Loss at iteration 190 : 0.10860148072242737
Loss at iteration 200 : 0.05838437378406525
Loss at iteration 210 : 0.1147458553314209
Loss at iteration 220 : 0.11388280242681503
Loss at iteration 230 : 0.04789091274142265
Loss at iteration 240 : 0.1320740431547165
Loss at iteration 250 : 0.12002424895763397
Loss at iteration 260 : 0.13376539945602417
Loss at iteration 270 : 0.13246192038059235
Loss at iteration 280 : 0.08730530738830566
Loss at iteration 290 : 0.09454566240310669
Loss at iteration 300 : 0.06969553232192993
Loss at iteration 310 : 0.08055663108825684
Loss at iteration 320 : 0.09991540759801865
Loss at iteration 330 : 0.054631464183330536
Loss at iteration 340 : 0.10672815889120102
Loss at iteration 350 : 0.10780148208141327
Loss at iteration 360 : 0.09924488514661789
Loss at iteration 370 : 0.12588563561439514
Loss at iteration 380 : 0.0662521943449974
Loss at iteration 390 : 0.13066431879997253
Loss at iteration 400 : 0.07388491183519363
Loss at iteration 410 : 0.05500779673457146
Loss at iteration 420 : 0.06902314722537994
Loss at iteration 430 : 0.04564838111400604
Loss at iteration 440 : 0.07233379781246185
Loss at iteration 450 : 0.07361283898353577
Loss at iteration 460 : 0.07384200394153595
Loss at iteration 470 : 0.08850884437561035
Loss at iteration 480 : 0.12353622913360596
Loss at iteration 490 : 0.10091715306043625
Loss at iteration 500 : 0.0898350328207016
Loss at iteration 510 : 0.12931303679943085
Loss at iteration 520 : 0.13957169651985168
Loss at iteration 530 : 0.05816378444433212
Loss at iteration 540 : 0.061828505247831345
Loss at iteration 550 : 0.07073190808296204
Loss at iteration 560 : 0.10459728538990021
Loss at iteration 570 : 0.07803743332624435
Loss at iteration 580 : 0.06793548166751862
Loss at iteration 590 : 0.1384645253419876
Loss at iteration 600 : 0.05221335217356682
Loss at iteration 610 : 0.0723998099565506
Loss at iteration 620 : 0.08952420949935913
Loss at iteration 630 : 0.095279261469841
Loss at iteration 640 : 0.06359614431858063
Loss at iteration 650 : 0.05447494983673096
Loss at iteration 660 : 0.11379781365394592
Loss at iteration 670 : 0.07525176554918289
Loss at iteration 680 : 0.09333812445402145
Loss at iteration 690 : 0.10161183774471283
Loss at iteration 700 : 0.18470804393291473
Loss at iteration 710 : 0.09299221634864807
Loss at iteration 720 : 0.06777914613485336
Loss at iteration 730 : 0.08436140418052673
Loss at iteration 740 : 0.06608705222606659
Loss at iteration 750 : 0.07789015024900436
Loss at iteration 760 : 0.07170253992080688
Loss at iteration 770 : 0.07447057217359543
Loss at iteration 780 : 0.06051325052976608
Loss at iteration 790 : 0.09037415683269501
Loss at iteration 800 : 0.05247487127780914
Loss at iteration 810 : 0.06775371730327606
Loss at iteration 820 : 0.08877050131559372
Loss at iteration 830 : 0.06547318398952484
Loss at iteration 840 : 0.10911740362644196
Loss at iteration 850 : 0.09130456298589706
Loss at iteration 860 : 0.1432901918888092
Loss at iteration 870 : 0.060924023389816284
Loss at iteration 880 : 0.04866544529795647
Loss at iteration 890 : 0.12820319831371307
Loss at iteration 900 : 0.07579932361841202
Loss at iteration 910 : 0.08687405288219452
Loss at iteration 920 : 0.066710464656353
Loss at iteration 930 : 0.09084068238735199
Loss at iteration 940 : 0.09626851975917816
Loss at iteration 950 : 0.07789769768714905
Loss at iteration 960 : 0.08236631006002426
Loss at iteration 970 : 0.12434025853872299
Loss at iteration 980 : 0.0722443088889122
Loss at iteration 990 : 0.08010435849428177
Loss at iteration 1000 : 0.054427407681941986
Loss at iteration 1010 : 0.08685572445392609
Loss at iteration 1020 : 0.07830864191055298
Loss at iteration 1030 : 0.1036577969789505
Loss at iteration 1040 : 0.055132489651441574
Loss at iteration 1050 : 0.09710237383842468
Loss at iteration 1060 : 0.04064849019050598
Loss at iteration 1070 : 0.071278415620327
Loss at iteration 1080 : 0.05496963858604431
Loss at iteration 1090 : 0.09076403081417084
Loss at iteration 1100 : 0.1030450090765953
Loss at iteration 1110 : 0.12052381038665771
Loss at iteration 1120 : 0.10441368818283081
Loss at iteration 1130 : 0.07448583841323853
Loss at iteration 1140 : 0.05790066346526146
Loss at iteration 1150 : 0.10184572637081146
Loss at iteration 1160 : 0.11204054206609726
Loss at iteration 1170 : 0.07214075326919556
Loss at iteration 1180 : 0.12722641229629517
Loss at iteration 1190 : 0.05259577929973602
Loss at iteration 1200 : 0.08769282698631287
Loss at iteration 1210 : 0.10742133110761642
The SSIM Value is: 0.7175443410873413
The PSNR Value is: 20.903409703572592
the epoch is: 166
Loss at iteration 10 : 0.06373177468776703
Loss at iteration 20 : 0.0938400849699974
Loss at iteration 30 : 0.07972743362188339
Loss at iteration 40 : 0.13116519153118134
Loss at iteration 50 : 0.06802542507648468
Loss at iteration 60 : 0.07482846826314926
Loss at iteration 70 : 0.0702253058552742
Loss at iteration 80 : 0.08215828984975815
Loss at iteration 90 : 0.09757476300001144
Loss at iteration 100 : 0.05846988409757614
Loss at iteration 110 : 0.05993612855672836
Loss at iteration 120 : 0.0883919820189476
Loss at iteration 130 : 0.08426699042320251
Loss at iteration 140 : 0.09931036084890366
Loss at iteration 150 : 0.10580440610647202
Loss at iteration 160 : 0.09269605576992035
Loss at iteration 170 : 0.07551421970129013
Loss at iteration 180 : 0.048989634960889816
Loss at iteration 190 : 0.0648108720779419
Loss at iteration 200 : 0.08096832036972046
Loss at iteration 210 : 0.1170320212841034
Loss at iteration 220 : 0.08474259078502655
Loss at iteration 230 : 0.08574222773313522
Loss at iteration 240 : 0.09168040752410889
Loss at iteration 250 : 0.051501549780368805
Loss at iteration 260 : 0.0894710049033165
Loss at iteration 270 : 0.07633288204669952
Loss at iteration 280 : 0.04538147896528244
Loss at iteration 290 : 0.06953608244657516
Loss at iteration 300 : 0.12427149713039398
Loss at iteration 310 : 0.09806099534034729
Loss at iteration 320 : 0.09372278302907944
Loss at iteration 330 : 0.08503901958465576
Loss at iteration 340 : 0.041390638798475266
Loss at iteration 350 : 0.07709053158760071
Loss at iteration 360 : 0.061897944658994675
Loss at iteration 370 : 0.10990750789642334
Loss at iteration 380 : 0.07809919118881226
Loss at iteration 390 : 0.0675310418009758
Loss at iteration 400 : 0.05400731787085533
Loss at iteration 410 : 0.08192948997020721
Loss at iteration 420 : 0.0807366669178009
Loss at iteration 430 : 0.08024569600820541
Loss at iteration 440 : 0.07557709515094757
Loss at iteration 450 : 0.0906515121459961
Loss at iteration 460 : 0.046417705714702606
Loss at iteration 470 : 0.0710376650094986
Loss at iteration 480 : 0.04137271270155907
Loss at iteration 490 : 0.09423622488975525
Loss at iteration 500 : 0.0777827799320221
Loss at iteration 510 : 0.07079535722732544
Loss at iteration 520 : 0.08796639740467072
Loss at iteration 530 : 0.07016130536794662
Loss at iteration 540 : 0.10899677872657776
Loss at iteration 550 : 0.043978508561849594
Loss at iteration 560 : 0.1100342869758606
Loss at iteration 570 : 0.10505020618438721
Loss at iteration 580 : 0.05262812599539757
Loss at iteration 590 : 0.06415529549121857
Loss at iteration 600 : 0.05284024402499199
Loss at iteration 610 : 0.08212636411190033
Loss at iteration 620 : 0.12216001749038696
Loss at iteration 630 : 0.0745277851819992
Loss at iteration 640 : 0.08656221628189087
Loss at iteration 650 : 0.08461674302816391
Loss at iteration 660 : 0.09287557005882263
Loss at iteration 670 : 0.10450705140829086
Loss at iteration 680 : 0.0980079397559166
Loss at iteration 690 : 0.08273826539516449
Loss at iteration 700 : 0.061337944120168686
Loss at iteration 710 : 0.047667235136032104
Loss at iteration 720 : 0.07393953949213028
Loss at iteration 730 : 0.07865220308303833
Loss at iteration 740 : 0.06890708208084106
Loss at iteration 750 : 0.07966466248035431
Loss at iteration 760 : 0.08661776781082153
Loss at iteration 770 : 0.0689607560634613
Loss at iteration 780 : 0.11241708695888519
Loss at iteration 790 : 0.07014000415802002
Loss at iteration 800 : 0.08510857820510864
Loss at iteration 810 : 0.04614363610744476
Loss at iteration 820 : 0.07889336347579956
Loss at iteration 830 : 0.06310822069644928
Loss at iteration 840 : 0.1182207465171814
Loss at iteration 850 : 0.08046895265579224
Loss at iteration 860 : 0.08511273562908173
Loss at iteration 870 : 0.05342812091112137
Loss at iteration 880 : 0.07744946330785751
Loss at iteration 890 : 0.10703782737255096
Loss at iteration 900 : 0.07942402362823486
Loss at iteration 910 : 0.07180656492710114
Loss at iteration 920 : 0.07830916345119476
Loss at iteration 930 : 0.14550939202308655
Loss at iteration 940 : 0.07994534075260162
Loss at iteration 950 : 0.07479026168584824
Loss at iteration 960 : 0.06565804034471512
Loss at iteration 970 : 0.07491213828325272
Loss at iteration 980 : 0.14301246404647827
Loss at iteration 990 : 0.060136161744594574
Loss at iteration 1000 : 0.092073455452919
Loss at iteration 1010 : 0.07782092690467834
Loss at iteration 1020 : 0.1269664615392685
Loss at iteration 1030 : 0.10445459932088852
Loss at iteration 1040 : 0.16077524423599243
Loss at iteration 1050 : 0.07606615126132965
Loss at iteration 1060 : 0.10431957244873047
Loss at iteration 1070 : 0.10294841229915619
Loss at iteration 1080 : 0.06207863241434097
Loss at iteration 1090 : 0.09237780421972275
Loss at iteration 1100 : 0.06439721584320068
Loss at iteration 1110 : 0.09174388647079468
Loss at iteration 1120 : 0.06558683514595032
Loss at iteration 1130 : 0.12868434190750122
Loss at iteration 1140 : 0.0680759996175766
Loss at iteration 1150 : 0.07584952563047409
Loss at iteration 1160 : 0.0839376300573349
Loss at iteration 1170 : 0.04932377487421036
Loss at iteration 1180 : 0.11859828233718872
Loss at iteration 1190 : 0.11384356021881104
Loss at iteration 1200 : 0.07740504294633865
Loss at iteration 1210 : 0.07428442686796188
The SSIM Value is: 0.7173676331837971
The PSNR Value is: 21.09438877105713
the epoch is: 167
Loss at iteration 10 : 0.06529536098241806
Loss at iteration 20 : 0.06136864051222801
Loss at iteration 30 : 0.057467903941869736
Loss at iteration 40 : 0.08651439845561981
Loss at iteration 50 : 0.07992520183324814
Loss at iteration 60 : 0.06713559478521347
Loss at iteration 70 : 0.0729810893535614
Loss at iteration 80 : 0.1231447085738182
Loss at iteration 90 : 0.06607292592525482
Loss at iteration 100 : 0.08014687895774841
Loss at iteration 110 : 0.06095104664564133
Loss at iteration 120 : 0.08519347757101059
Loss at iteration 130 : 0.08954930305480957
Loss at iteration 140 : 0.09265336394309998
Loss at iteration 150 : 0.08621629327535629
Loss at iteration 160 : 0.10558339953422546
Loss at iteration 170 : 0.04771745204925537
Loss at iteration 180 : 0.14019477367401123
Loss at iteration 190 : 0.10420839488506317
Loss at iteration 200 : 0.09805013984441757
Loss at iteration 210 : 0.056108906865119934
Loss at iteration 220 : 0.10429390519857407
Loss at iteration 230 : 0.07338611036539078
Loss at iteration 240 : 0.07564102113246918
Loss at iteration 250 : 0.05318041890859604
Loss at iteration 260 : 0.11861177533864975
Loss at iteration 270 : 0.08919727802276611
Loss at iteration 280 : 0.04933779686689377
Loss at iteration 290 : 0.06971754878759384
Loss at iteration 300 : 0.0874556228518486
Loss at iteration 310 : 0.08858471363782883
Loss at iteration 320 : 0.10056986659765244
Loss at iteration 330 : 0.11930900812149048
Loss at iteration 340 : 0.08636705577373505
Loss at iteration 350 : 0.05476991459727287
Loss at iteration 360 : 0.09290086477994919
Loss at iteration 370 : 0.08255176991224289
Loss at iteration 380 : 0.09093189239501953
Loss at iteration 390 : 0.11085058748722076
Loss at iteration 400 : 0.11022316664457321
Loss at iteration 410 : 0.14519092440605164
Loss at iteration 420 : 0.056732505559921265
Loss at iteration 430 : 0.052744001150131226
Loss at iteration 440 : 0.06547155231237411
Loss at iteration 450 : 0.10664878785610199
Loss at iteration 460 : 0.06958162039518356
Loss at iteration 470 : 0.06611761450767517
Loss at iteration 480 : 0.060032106935977936
Loss at iteration 490 : 0.054362475872039795
Loss at iteration 500 : 0.05529024824500084
Loss at iteration 510 : 0.09275831282138824
Loss at iteration 520 : 0.08248670399188995
Loss at iteration 530 : 0.10114866495132446
Loss at iteration 540 : 0.09323225915431976
Loss at iteration 550 : 0.07589524239301682
Loss at iteration 560 : 0.0879109725356102
Loss at iteration 570 : 0.044225193560123444
Loss at iteration 580 : 0.09487494081258774
Loss at iteration 590 : 0.07071113586425781
Loss at iteration 600 : 0.11725929379463196
Loss at iteration 610 : 0.0704236552119255
Loss at iteration 620 : 0.06123372167348862
Loss at iteration 630 : 0.09000545740127563
Loss at iteration 640 : 0.08376690745353699
Loss at iteration 650 : 0.11390477418899536
Loss at iteration 660 : 0.07338611781597137
Loss at iteration 670 : 0.057699188590049744
Loss at iteration 680 : 0.07933913171291351
Loss at iteration 690 : 0.08708226680755615
Loss at iteration 700 : 0.09919606894254684
Loss at iteration 710 : 0.08044824004173279
Loss at iteration 720 : 0.05531769618391991
Loss at iteration 730 : 0.06961220502853394
Loss at iteration 740 : 0.1055704802274704
Loss at iteration 750 : 0.07571405172348022
Loss at iteration 760 : 0.13434474170207977
Loss at iteration 770 : 0.10899271816015244
Loss at iteration 780 : 0.07155445218086243
Loss at iteration 790 : 0.048620980232954025
Loss at iteration 800 : 0.0744936466217041
Loss at iteration 810 : 0.12668007612228394
Loss at iteration 820 : 0.08401551842689514
Loss at iteration 830 : 0.08363806456327438
Loss at iteration 840 : 0.0980762392282486
Loss at iteration 850 : 0.10720067471265793
Loss at iteration 860 : 0.07114733755588531
Loss at iteration 870 : 0.0972885712981224
Loss at iteration 880 : 0.05858106166124344
Loss at iteration 890 : 0.08950839936733246
Loss at iteration 900 : 0.08973255753517151
Loss at iteration 910 : 0.07217288017272949
Loss at iteration 920 : 0.09458965063095093
Loss at iteration 930 : 0.055031917989254
Loss at iteration 940 : 0.06929733604192734
Loss at iteration 950 : 0.08142227679491043
Loss at iteration 960 : 0.09592407196760178
Loss at iteration 970 : 0.1277761161327362
Loss at iteration 980 : 0.08810226619243622
Loss at iteration 990 : 0.11576172709465027
Loss at iteration 1000 : 0.08014041185379028
Loss at iteration 1010 : 0.08226262032985687
Loss at iteration 1020 : 0.07052315771579742
Loss at iteration 1030 : 0.10957664251327515
Loss at iteration 1040 : 0.07882095873355865
Loss at iteration 1050 : 0.07997404038906097
Loss at iteration 1060 : 0.13611343502998352
Loss at iteration 1070 : 0.07099040597677231
Loss at iteration 1080 : 0.057384781539440155
Loss at iteration 1090 : 0.08476974815130234
Loss at iteration 1100 : 0.06355462223291397
Loss at iteration 1110 : 0.125896155834198
Loss at iteration 1120 : 0.10101642459630966
Loss at iteration 1130 : 0.09877467155456543
Loss at iteration 1140 : 0.05202595144510269
Loss at iteration 1150 : 0.06667857617139816
Loss at iteration 1160 : 0.084503673017025
Loss at iteration 1170 : 0.09408137202262878
Loss at iteration 1180 : 0.0937567800283432
Loss at iteration 1190 : 0.10859780758619308
Loss at iteration 1200 : 0.11914625763893127
Loss at iteration 1210 : 0.07685346901416779
The SSIM Value is: 0.7200099945068359
The PSNR Value is: 21.113077799479168
the epoch is: 168
Loss at iteration 10 : 0.127483069896698
Loss at iteration 20 : 0.07104885578155518
Loss at iteration 30 : 0.07053574174642563
Loss at iteration 40 : 0.06055399775505066
Loss at iteration 50 : 0.1054837554693222
Loss at iteration 60 : 0.06107638031244278
Loss at iteration 70 : 0.10751711577177048
Loss at iteration 80 : 0.09685962647199631
Loss at iteration 90 : 0.08971887826919556
Loss at iteration 100 : 0.11718201637268066
Loss at iteration 110 : 0.07360012829303741
Loss at iteration 120 : 0.08815928548574448
Loss at iteration 130 : 0.06628583371639252
Loss at iteration 140 : 0.06328434497117996
Loss at iteration 150 : 0.0909833312034607
Loss at iteration 160 : 0.1356222927570343
Loss at iteration 170 : 0.09536576271057129
Loss at iteration 180 : 0.0651700347661972
Loss at iteration 190 : 0.06678307801485062
Loss at iteration 200 : 0.08859676122665405
Loss at iteration 210 : 0.050533294677734375
Loss at iteration 220 : 0.08961658179759979
Loss at iteration 230 : 0.0795588567852974
Loss at iteration 240 : 0.10233837366104126
Loss at iteration 250 : 0.11161452531814575
Loss at iteration 260 : 0.11072757095098495
Loss at iteration 270 : 0.08236317336559296
Loss at iteration 280 : 0.13825154304504395
Loss at iteration 290 : 0.051566772162914276
Loss at iteration 300 : 0.06849219650030136
Loss at iteration 310 : 0.11057361215353012
Loss at iteration 320 : 0.08122207969427109
Loss at iteration 330 : 0.09142772853374481
Loss at iteration 340 : 0.048723749816417694
Loss at iteration 350 : 0.08799751102924347
Loss at iteration 360 : 0.05230344459414482
Loss at iteration 370 : 0.11831482499837875
Loss at iteration 380 : 0.04664028435945511
Loss at iteration 390 : 0.08181893080472946
Loss at iteration 400 : 0.0697278305888176
Loss at iteration 410 : 0.06649311631917953
Loss at iteration 420 : 0.08169542253017426
Loss at iteration 430 : 0.09128303825855255
Loss at iteration 440 : 0.09946456551551819
Loss at iteration 450 : 0.04520168900489807
Loss at iteration 460 : 0.07864345610141754
Loss at iteration 470 : 0.08060365915298462
Loss at iteration 480 : 0.09072190523147583
Loss at iteration 490 : 0.05204273387789726
Loss at iteration 500 : 0.07289900630712509
Loss at iteration 510 : 0.061349209398031235
Loss at iteration 520 : 0.09667345881462097
Loss at iteration 530 : 0.0786394476890564
Loss at iteration 540 : 0.0725327730178833
Loss at iteration 550 : 0.06132354587316513
Loss at iteration 560 : 0.13000982999801636
Loss at iteration 570 : 0.08716078847646713
Loss at iteration 580 : 0.09502607583999634
Loss at iteration 590 : 0.0512576550245285
Loss at iteration 600 : 0.04916069656610489
Loss at iteration 610 : 0.045346226543188095
Loss at iteration 620 : 0.058475837111473083
Loss at iteration 630 : 0.06093285232782364
Loss at iteration 640 : 0.06530164927244186
Loss at iteration 650 : 0.07976185530424118
Loss at iteration 660 : 0.09435808658599854
Loss at iteration 670 : 0.06700707226991653
Loss at iteration 680 : 0.06709827482700348
Loss at iteration 690 : 0.07127349078655243
Loss at iteration 700 : 0.11723753064870834
Loss at iteration 710 : 0.09577690064907074
Loss at iteration 720 : 0.07766589522361755
Loss at iteration 730 : 0.08203504979610443
Loss at iteration 740 : 0.09669791907072067
Loss at iteration 750 : 0.0763920471072197
Loss at iteration 760 : 0.05880628153681755
Loss at iteration 770 : 0.08579256385564804
Loss at iteration 780 : 0.07544836401939392
Loss at iteration 790 : 0.07712798565626144
Loss at iteration 800 : 0.09361618757247925
Loss at iteration 810 : 0.09644447267055511
Loss at iteration 820 : 0.07190903276205063
Loss at iteration 830 : 0.09448390454053879
Loss at iteration 840 : 0.07857877016067505
Loss at iteration 850 : 0.09489083290100098
Loss at iteration 860 : 0.0957411527633667
Loss at iteration 870 : 0.05866130441427231
Loss at iteration 880 : 0.08378742635250092
Loss at iteration 890 : 0.11052677035331726
Loss at iteration 900 : 0.15276403725147247
Loss at iteration 910 : 0.05288435146212578
Loss at iteration 920 : 0.11116421222686768
Loss at iteration 930 : 0.11454176902770996
Loss at iteration 940 : 0.10115794837474823
Loss at iteration 950 : 0.11858692765235901
Loss at iteration 960 : 0.08648695796728134
Loss at iteration 970 : 0.09763248264789581
Loss at iteration 980 : 0.09903605282306671
Loss at iteration 990 : 0.09427811205387115
Loss at iteration 1000 : 0.12606579065322876
Loss at iteration 1010 : 0.06457539647817612
Loss at iteration 1020 : 0.1305752992630005
Loss at iteration 1030 : 0.07043072581291199
Loss at iteration 1040 : 0.0617544911801815
Loss at iteration 1050 : 0.06759330630302429
Loss at iteration 1060 : 0.10352574288845062
Loss at iteration 1070 : 0.058660611510276794
Loss at iteration 1080 : 0.07418083399534225
Loss at iteration 1090 : 0.06545525044202805
Loss at iteration 1100 : 0.05722327157855034
Loss at iteration 1110 : 0.08866038173437119
Loss at iteration 1120 : 0.08044971525669098
Loss at iteration 1130 : 0.11068829149007797
Loss at iteration 1140 : 0.10889126360416412
Loss at iteration 1150 : 0.06857211887836456
Loss at iteration 1160 : 0.08025223016738892
Loss at iteration 1170 : 0.05690809711813927
Loss at iteration 1180 : 0.09886054694652557
Loss at iteration 1190 : 0.10477147996425629
Loss at iteration 1200 : 0.06520433723926544
Loss at iteration 1210 : 0.09102382510900497
The SSIM Value is: 0.7210516174634297
The PSNR Value is: 21.083813095092772
the epoch is: 169
Loss at iteration 10 : 0.10452809929847717
Loss at iteration 20 : 0.07239696383476257
Loss at iteration 30 : 0.13721226155757904
Loss at iteration 40 : 0.07348231971263885
Loss at iteration 50 : 0.06749672442674637
Loss at iteration 60 : 0.09876187890768051
Loss at iteration 70 : 0.10773076862096786
Loss at iteration 80 : 0.06143665313720703
Loss at iteration 90 : 0.06864330917596817
Loss at iteration 100 : 0.09608475863933563
Loss at iteration 110 : 0.08095991611480713
Loss at iteration 120 : 0.07092690467834473
Loss at iteration 130 : 0.09202276170253754
Loss at iteration 140 : 0.08802437782287598
Loss at iteration 150 : 0.093263179063797
Loss at iteration 160 : 0.0744534432888031
Loss at iteration 170 : 0.08526474237442017
Loss at iteration 180 : 0.050779204815626144
Loss at iteration 190 : 0.060713570564985275
Loss at iteration 200 : 0.06770413368940353
Loss at iteration 210 : 0.0578971803188324
Loss at iteration 220 : 0.08718670904636383
Loss at iteration 230 : 0.11148922145366669
Loss at iteration 240 : 0.11355245113372803
Loss at iteration 250 : 0.13113978505134583
Loss at iteration 260 : 0.08648711442947388
Loss at iteration 270 : 0.04433441907167435
Loss at iteration 280 : 0.07429757714271545
Loss at iteration 290 : 0.07723638415336609
Loss at iteration 300 : 0.06367889791727066
Loss at iteration 310 : 0.15113244950771332
Loss at iteration 320 : 0.04811137914657593
Loss at iteration 330 : 0.059116970747709274
Loss at iteration 340 : 0.05734345316886902
Loss at iteration 350 : 0.07798086106777191
Loss at iteration 360 : 0.03649250790476799
Loss at iteration 370 : 0.08694710582494736
Loss at iteration 380 : 0.10157211869955063
Loss at iteration 390 : 0.08400683850049973
Loss at iteration 400 : 0.07265876233577728
Loss at iteration 410 : 0.07776045799255371
Loss at iteration 420 : 0.06721802055835724
Loss at iteration 430 : 0.07026172429323196
Loss at iteration 440 : 0.08183148503303528
Loss at iteration 450 : 0.09270020574331284
Loss at iteration 460 : 0.09736119210720062
Loss at iteration 470 : 0.08974437415599823
Loss at iteration 480 : 0.11482492089271545
Loss at iteration 490 : 0.0785917192697525
Loss at iteration 500 : 0.043251559138298035
Loss at iteration 510 : 0.08225220441818237
Loss at iteration 520 : 0.06889991462230682
Loss at iteration 530 : 0.12989558279514313
Loss at iteration 540 : 0.07060521841049194
Loss at iteration 550 : 0.06668972223997116
Loss at iteration 560 : 0.1020529493689537
Loss at iteration 570 : 0.09333594143390656
Loss at iteration 580 : 0.08077113330364227
Loss at iteration 590 : 0.09551849216222763
Loss at iteration 600 : 0.06418700516223907
Loss at iteration 610 : 0.07615268230438232
Loss at iteration 620 : 0.09846027940511703
Loss at iteration 630 : 0.08497627824544907
Loss at iteration 640 : 0.09320162236690521
Loss at iteration 650 : 0.1143060177564621
Loss at iteration 660 : 0.1160137876868248
Loss at iteration 670 : 0.06894110143184662
Loss at iteration 680 : 0.10044924914836884
Loss at iteration 690 : 0.05655483156442642
Loss at iteration 700 : 0.14677461981773376
Loss at iteration 710 : 0.10935764759778976
Loss at iteration 720 : 0.10991019010543823
Loss at iteration 730 : 0.06755131483078003
Loss at iteration 740 : 0.06390326470136642
Loss at iteration 750 : 0.08849462121725082
Loss at iteration 760 : 0.08357324451208115
Loss at iteration 770 : 0.07662308216094971
Loss at iteration 780 : 0.07389112561941147
Loss at iteration 790 : 0.09958555549383163
Loss at iteration 800 : 0.05550718307495117
Loss at iteration 810 : 0.047700099647045135
Loss at iteration 820 : 0.10780352354049683
Loss at iteration 830 : 0.06940116733312607
Loss at iteration 840 : 0.12801121175289154
Loss at iteration 850 : 0.09422337263822556
Loss at iteration 860 : 0.07706330716609955
Loss at iteration 870 : 0.0410783588886261
Loss at iteration 880 : 0.07066915184259415
Loss at iteration 890 : 0.08024786412715912
Loss at iteration 900 : 0.06757619976997375
Loss at iteration 910 : 0.10578835010528564
Loss at iteration 920 : 0.06864148378372192
Loss at iteration 930 : 0.09834308922290802
Loss at iteration 940 : 0.08302110433578491
Loss at iteration 950 : 0.07874543964862823
Loss at iteration 960 : 0.06890575587749481
Loss at iteration 970 : 0.09041699767112732
Loss at iteration 980 : 0.1171344667673111
Loss at iteration 990 : 0.0643492192029953
Loss at iteration 1000 : 0.07164239883422852
Loss at iteration 1010 : 0.11583664268255234
Loss at iteration 1020 : 0.07062871754169464
Loss at iteration 1030 : 0.10247864574193954
Loss at iteration 1040 : 0.08732609450817108
Loss at iteration 1050 : 0.1028546690940857
Loss at iteration 1060 : 0.09859520196914673
Loss at iteration 1070 : 0.0851968377828598
Loss at iteration 1080 : 0.10022195428609848
Loss at iteration 1090 : 0.1399211883544922
Loss at iteration 1100 : 0.08677077293395996
Loss at iteration 1110 : 0.09129074215888977
Loss at iteration 1120 : 0.046141739934682846
Loss at iteration 1130 : 0.09608461707830429
Loss at iteration 1140 : 0.10844751447439194
Loss at iteration 1150 : 0.07944311201572418
Loss at iteration 1160 : 0.08265051245689392
Loss at iteration 1170 : 0.09226477146148682
Loss at iteration 1180 : 0.09726963937282562
Loss at iteration 1190 : 0.09147986769676208
Loss at iteration 1200 : 0.10024213790893555
Loss at iteration 1210 : 0.09609423577785492
The SSIM Value is: 0.7151286403338114
The PSNR Value is: 20.814782333374023
the epoch is: 170
Loss at iteration 10 : 0.05657903105020523
Loss at iteration 20 : 0.15091587603092194
Loss at iteration 30 : 0.0684187114238739
Loss at iteration 40 : 0.081159807741642
Loss at iteration 50 : 0.07340238988399506
Loss at iteration 60 : 0.0899934396147728
Loss at iteration 70 : 0.10370562970638275
Loss at iteration 80 : 0.07617253065109253
Loss at iteration 90 : 0.10512398928403854
Loss at iteration 100 : 0.06832236796617508
Loss at iteration 110 : 0.10130207240581512
Loss at iteration 120 : 0.09460419416427612
Loss at iteration 130 : 0.051114924252033234
Loss at iteration 140 : 0.08086629956960678
Loss at iteration 150 : 0.13402336835861206
Loss at iteration 160 : 0.08635848015546799
Loss at iteration 170 : 0.09530489891767502
Loss at iteration 180 : 0.09786979854106903
Loss at iteration 190 : 0.0889752134680748
Loss at iteration 200 : 0.09886623919010162
Loss at iteration 210 : 0.09437583386898041
Loss at iteration 220 : 0.10103394836187363
Loss at iteration 230 : 0.12794773280620575
Loss at iteration 240 : 0.06802496314048767
Loss at iteration 250 : 0.09677824378013611
Loss at iteration 260 : 0.125263512134552
Loss at iteration 270 : 0.0659230649471283
Loss at iteration 280 : 0.11989131569862366
Loss at iteration 290 : 0.0852477103471756
Loss at iteration 300 : 0.08637871593236923
Loss at iteration 310 : 0.09009196609258652
Loss at iteration 320 : 0.06830668449401855
Loss at iteration 330 : 0.09633290022611618
Loss at iteration 340 : 0.07641580700874329
Loss at iteration 350 : 0.07229839265346527
Loss at iteration 360 : 0.08575265109539032
Loss at iteration 370 : 0.08495324850082397
Loss at iteration 380 : 0.0816558226943016
Loss at iteration 390 : 0.07403700798749924
Loss at iteration 400 : 0.08611365407705307
Loss at iteration 410 : 0.04503253847360611
Loss at iteration 420 : 0.07918649166822433
Loss at iteration 430 : 0.060013167560100555
Loss at iteration 440 : 0.06892834603786469
Loss at iteration 450 : 0.05704718455672264
Loss at iteration 460 : 0.11975252628326416
Loss at iteration 470 : 0.07868047058582306
Loss at iteration 480 : 0.09244029968976974
Loss at iteration 490 : 0.0816531702876091
Loss at iteration 500 : 0.11087163537740707
Loss at iteration 510 : 0.06341496109962463
Loss at iteration 520 : 0.08899890631437302
Loss at iteration 530 : 0.0784447118639946
Loss at iteration 540 : 0.10814859718084335
Loss at iteration 550 : 0.12023439258337021
Loss at iteration 560 : 0.07437039911746979
Loss at iteration 570 : 0.0958041176199913
Loss at iteration 580 : 0.048956673592329025
Loss at iteration 590 : 0.08503012359142303
Loss at iteration 600 : 0.06410974264144897
Loss at iteration 610 : 0.11836414039134979
Loss at iteration 620 : 0.06382344663143158
Loss at iteration 630 : 0.068044513463974
Loss at iteration 640 : 0.10063318908214569
Loss at iteration 650 : 0.05779910087585449
Loss at iteration 660 : 0.09594041109085083
Loss at iteration 670 : 0.07391094416379929
Loss at iteration 680 : 0.08893098682165146
Loss at iteration 690 : 0.07011952251195908
Loss at iteration 700 : 0.09518381208181381
Loss at iteration 710 : 0.08736829459667206
Loss at iteration 720 : 0.09638889133930206
Loss at iteration 730 : 0.11667634546756744
Loss at iteration 740 : 0.06108679622411728
Loss at iteration 750 : 0.08916743099689484
Loss at iteration 760 : 0.09090295433998108
Loss at iteration 770 : 0.04502470791339874
Loss at iteration 780 : 0.06613470613956451
Loss at iteration 790 : 0.12848979234695435
Loss at iteration 800 : 0.10297217220067978
Loss at iteration 810 : 0.08372899144887924
Loss at iteration 820 : 0.08580122888088226
Loss at iteration 830 : 0.10846270620822906
Loss at iteration 840 : 0.12196515500545502
Loss at iteration 850 : 0.0969403088092804
Loss at iteration 860 : 0.05989224836230278
Loss at iteration 870 : 0.08304814249277115
Loss at iteration 880 : 0.06121295690536499
Loss at iteration 890 : 0.04659666866064072
Loss at iteration 900 : 0.08879877626895905
Loss at iteration 910 : 0.07911967486143112
Loss at iteration 920 : 0.09623830765485764
Loss at iteration 930 : 0.08523109555244446
Loss at iteration 940 : 0.049757372587919235
Loss at iteration 950 : 0.09677146375179291
Loss at iteration 960 : 0.11895450204610825
Loss at iteration 970 : 0.08086152374744415
Loss at iteration 980 : 0.08241170644760132
Loss at iteration 990 : 0.08074840158224106
Loss at iteration 1000 : 0.10768915712833405
Loss at iteration 1010 : 0.0682106763124466
Loss at iteration 1020 : 0.05886996537446976
Loss at iteration 1030 : 0.07341821491718292
Loss at iteration 1040 : 0.05831519886851311
Loss at iteration 1050 : 0.05836871266365051
Loss at iteration 1060 : 0.08872090280056
Loss at iteration 1070 : 0.05706162378191948
Loss at iteration 1080 : 0.06997954845428467
Loss at iteration 1090 : 0.0711902379989624
Loss at iteration 1100 : 0.0811828076839447
Loss at iteration 1110 : 0.07349898666143417
Loss at iteration 1120 : 0.0683446079492569
Loss at iteration 1130 : 0.10349056124687195
Loss at iteration 1140 : 0.10207997262477875
Loss at iteration 1150 : 0.07299517095088959
Loss at iteration 1160 : 0.11348768323659897
Loss at iteration 1170 : 0.05149468779563904
Loss at iteration 1180 : 0.10735960304737091
Loss at iteration 1190 : 0.12107788026332855
Loss at iteration 1200 : 0.11845526099205017
Loss at iteration 1210 : 0.10553029179573059
The SSIM Value is: 0.7223315715789795
The PSNR Value is: 21.207498041788735
the epoch is: 171
Loss at iteration 10 : 0.10319006443023682
Loss at iteration 20 : 0.09061890840530396
Loss at iteration 30 : 0.07459475845098495
Loss at iteration 40 : 0.05859103798866272
Loss at iteration 50 : 0.07864207029342651
Loss at iteration 60 : 0.12393933534622192
Loss at iteration 70 : 0.04629618674516678
Loss at iteration 80 : 0.08816258609294891
Loss at iteration 90 : 0.07219094783067703
Loss at iteration 100 : 0.06420248746871948
Loss at iteration 110 : 0.0942491739988327
Loss at iteration 120 : 0.06951938569545746
Loss at iteration 130 : 0.06055397540330887
Loss at iteration 140 : 0.0632440596818924
Loss at iteration 150 : 0.0419173389673233
Loss at iteration 160 : 0.09546993672847748
Loss at iteration 170 : 0.0571151077747345
Loss at iteration 180 : 0.10409402847290039
Loss at iteration 190 : 0.0922108143568039
Loss at iteration 200 : 0.09787534177303314
Loss at iteration 210 : 0.09546802937984467
Loss at iteration 220 : 0.08406181633472443
Loss at iteration 230 : 0.062078528106212616
Loss at iteration 240 : 0.07168301939964294
Loss at iteration 250 : 0.07690802216529846
Loss at iteration 260 : 0.04582301899790764
Loss at iteration 270 : 0.08186071366071701
Loss at iteration 280 : 0.05851378291845322
Loss at iteration 290 : 0.08940105140209198
Loss at iteration 300 : 0.08050815761089325
Loss at iteration 310 : 0.12867528200149536
Loss at iteration 320 : 0.08354907482862473
Loss at iteration 330 : 0.10416877269744873
Loss at iteration 340 : 0.08165125548839569
Loss at iteration 350 : 0.07167360186576843
Loss at iteration 360 : 0.04577377438545227
Loss at iteration 370 : 0.07851828634738922
Loss at iteration 380 : 0.10132952779531479
Loss at iteration 390 : 0.04979537054896355
Loss at iteration 400 : 0.07808429002761841
Loss at iteration 410 : 0.04181868955492973
Loss at iteration 420 : 0.09497430920600891
Loss at iteration 430 : 0.09190794080495834
Loss at iteration 440 : 0.09019940346479416
Loss at iteration 450 : 0.10816730558872223
Loss at iteration 460 : 0.0742432102560997
Loss at iteration 470 : 0.06831350922584534
Loss at iteration 480 : 0.0627397745847702
Loss at iteration 490 : 0.06123322248458862
Loss at iteration 500 : 0.0953461155295372
Loss at iteration 510 : 0.09945686906576157
Loss at iteration 520 : 0.07702384889125824
Loss at iteration 530 : 0.06843309849500656
Loss at iteration 540 : 0.07613777369260788
Loss at iteration 550 : 0.05295313894748688
Loss at iteration 560 : 0.09324431419372559
Loss at iteration 570 : 0.10106899589300156
Loss at iteration 580 : 0.057137638330459595
Loss at iteration 590 : 0.08723978698253632
Loss at iteration 600 : 0.08041968941688538
Loss at iteration 610 : 0.07112789154052734
Loss at iteration 620 : 0.12423302978277206
Loss at iteration 630 : 0.07493676245212555
Loss at iteration 640 : 0.07280835509300232
Loss at iteration 650 : 0.08897042274475098
Loss at iteration 660 : 0.0955095887184143
Loss at iteration 670 : 0.06024898588657379
Loss at iteration 680 : 0.1433764547109604
Loss at iteration 690 : 0.15309971570968628
Loss at iteration 700 : 0.07187138497829437
Loss at iteration 710 : 0.08638870716094971
Loss at iteration 720 : 0.07056379318237305
Loss at iteration 730 : 0.0843849629163742
Loss at iteration 740 : 0.07375247776508331
Loss at iteration 750 : 0.10864626616239548
Loss at iteration 760 : 0.1063443273305893
Loss at iteration 770 : 0.06262300908565521
Loss at iteration 780 : 0.13212381303310394
Loss at iteration 790 : 0.07347828894853592
Loss at iteration 800 : 0.06869116425514221
Loss at iteration 810 : 0.10851030051708221
Loss at iteration 820 : 0.052419453859329224
Loss at iteration 830 : 0.10471417754888535
Loss at iteration 840 : 0.058123718947172165
Loss at iteration 850 : 0.10575565695762634
Loss at iteration 860 : 0.07675600051879883
Loss at iteration 870 : 0.06525824218988419
Loss at iteration 880 : 0.09654488414525986
Loss at iteration 890 : 0.0789605900645256
Loss at iteration 900 : 0.09095501899719238
Loss at iteration 910 : 0.06426213681697845
Loss at iteration 920 : 0.07808592915534973
Loss at iteration 930 : 0.12679019570350647
Loss at iteration 940 : 0.10970062017440796
Loss at iteration 950 : 0.07276686280965805
Loss at iteration 960 : 0.08543422073125839
Loss at iteration 970 : 0.10706594586372375
Loss at iteration 980 : 0.12441430985927582
Loss at iteration 990 : 0.09484122693538666
Loss at iteration 1000 : 0.05369284749031067
Loss at iteration 1010 : 0.10867184400558472
Loss at iteration 1020 : 0.07521966099739075
Loss at iteration 1030 : 0.09259964525699615
Loss at iteration 1040 : 0.07037882506847382
Loss at iteration 1050 : 0.04171764478087425
Loss at iteration 1060 : 0.08603738248348236
Loss at iteration 1070 : 0.07708688080310822
Loss at iteration 1080 : 0.09711344540119171
Loss at iteration 1090 : 0.05907605588436127
Loss at iteration 1100 : 0.11606283485889435
Loss at iteration 1110 : 0.10920143127441406
Loss at iteration 1120 : 0.07226145267486572
Loss at iteration 1130 : 0.09325294196605682
Loss at iteration 1140 : 0.08417989313602448
Loss at iteration 1150 : 0.06236110255122185
Loss at iteration 1160 : 0.05878961831331253
Loss at iteration 1170 : 0.08630017936229706
Loss at iteration 1180 : 0.08270176500082016
Loss at iteration 1190 : 0.06509826332330704
Loss at iteration 1200 : 0.09768298268318176
Loss at iteration 1210 : 0.08063389360904694
The SSIM Value is: 0.7206108291943868
The PSNR Value is: 20.911177317301433
the epoch is: 172
Loss at iteration 10 : 0.079264335334301
Loss at iteration 20 : 0.08295764029026031
Loss at iteration 30 : 0.10097888112068176
Loss at iteration 40 : 0.07203860580921173
Loss at iteration 50 : 0.09271564334630966
Loss at iteration 60 : 0.05395333096385002
Loss at iteration 70 : 0.08802291005849838
Loss at iteration 80 : 0.13583844900131226
Loss at iteration 90 : 0.08353155106306076
Loss at iteration 100 : 0.06767620146274567
Loss at iteration 110 : 0.09256908297538757
Loss at iteration 120 : 0.08325850963592529
Loss at iteration 130 : 0.10165505111217499
Loss at iteration 140 : 0.05838817358016968
Loss at iteration 150 : 0.05228229612112045
Loss at iteration 160 : 0.1413760781288147
Loss at iteration 170 : 0.06518980860710144
Loss at iteration 180 : 0.05816666781902313
Loss at iteration 190 : 0.046578627079725266
Loss at iteration 200 : 0.1129823550581932
Loss at iteration 210 : 0.177974671125412
Loss at iteration 220 : 0.07524944841861725
Loss at iteration 230 : 0.0757322609424591
Loss at iteration 240 : 0.12869983911514282
Loss at iteration 250 : 0.07103358209133148
Loss at iteration 260 : 0.08520954102277756
Loss at iteration 270 : 0.10661276429891586
Loss at iteration 280 : 0.129659503698349
Loss at iteration 290 : 0.09812454879283905
Loss at iteration 300 : 0.06805556267499924
Loss at iteration 310 : 0.06764554232358932
Loss at iteration 320 : 0.06612922251224518
Loss at iteration 330 : 0.061228569597005844
Loss at iteration 340 : 0.0788925290107727
Loss at iteration 350 : 0.0831843763589859
Loss at iteration 360 : 0.09200648963451385
Loss at iteration 370 : 0.06936145573854446
Loss at iteration 380 : 0.11683152616024017
Loss at iteration 390 : 0.05939449369907379
Loss at iteration 400 : 0.12432783097028732
Loss at iteration 410 : 0.07963228225708008
Loss at iteration 420 : 0.08804493397474289
Loss at iteration 430 : 0.11615758389234543
Loss at iteration 440 : 0.07791692018508911
Loss at iteration 450 : 0.07651285082101822
Loss at iteration 460 : 0.09203875064849854
Loss at iteration 470 : 0.05896400660276413
Loss at iteration 480 : 0.08164481818675995
Loss at iteration 490 : 0.07593206316232681
Loss at iteration 500 : 0.060348864644765854
Loss at iteration 510 : 0.11738734692335129
Loss at iteration 520 : 0.0387846976518631
Loss at iteration 530 : 0.09287667274475098
Loss at iteration 540 : 0.10937836021184921
Loss at iteration 550 : 0.08220330625772476
Loss at iteration 560 : 0.11063303798437119
Loss at iteration 570 : 0.09357768297195435
Loss at iteration 580 : 0.06780305504798889
Loss at iteration 590 : 0.06382808089256287
Loss at iteration 600 : 0.16585344076156616
Loss at iteration 610 : 0.062435582280159
Loss at iteration 620 : 0.08510589599609375
Loss at iteration 630 : 0.09121455252170563
Loss at iteration 640 : 0.06253412365913391
Loss at iteration 650 : 0.08036744594573975
Loss at iteration 660 : 0.09795289486646652
Loss at iteration 670 : 0.06721886992454529
Loss at iteration 680 : 0.06649573892354965
Loss at iteration 690 : 0.11218667030334473
Loss at iteration 700 : 0.14889204502105713
Loss at iteration 710 : 0.07519766688346863
Loss at iteration 720 : 0.14070358872413635
Loss at iteration 730 : 0.10846944153308868
Loss at iteration 740 : 0.07820585370063782
Loss at iteration 750 : 0.09170994162559509
Loss at iteration 760 : 0.10436370223760605
Loss at iteration 770 : 0.12580931186676025
Loss at iteration 780 : 0.07213190197944641
Loss at iteration 790 : 0.04381342977285385
Loss at iteration 800 : 0.12121069431304932
Loss at iteration 810 : 0.07904766499996185
Loss at iteration 820 : 0.0721169039607048
Loss at iteration 830 : 0.08156652748584747
Loss at iteration 840 : 0.06757733225822449
Loss at iteration 850 : 0.06854923814535141
Loss at iteration 860 : 0.07488279789686203
Loss at iteration 870 : 0.08365782350301743
Loss at iteration 880 : 0.07270902395248413
Loss at iteration 890 : 0.08498260378837585
Loss at iteration 900 : 0.08132557570934296
Loss at iteration 910 : 0.04661088436841965
Loss at iteration 920 : 0.07806768268346786
Loss at iteration 930 : 0.11315204948186874
Loss at iteration 940 : 0.07569919526576996
Loss at iteration 950 : 0.10786324739456177
Loss at iteration 960 : 0.0963546484708786
Loss at iteration 970 : 0.08808588981628418
Loss at iteration 980 : 0.06024843454360962
Loss at iteration 990 : 0.05721527338027954
Loss at iteration 1000 : 0.07647290080785751
Loss at iteration 1010 : 0.08087098598480225
Loss at iteration 1020 : 0.03859662264585495
Loss at iteration 1030 : 0.08379121124744415
Loss at iteration 1040 : 0.05006237328052521
Loss at iteration 1050 : 0.06826657056808472
Loss at iteration 1060 : 0.06635209918022156
Loss at iteration 1070 : 0.13230429589748383
Loss at iteration 1080 : 0.09228307008743286
Loss at iteration 1090 : 0.10222207009792328
Loss at iteration 1100 : 0.06718463450670242
Loss at iteration 1110 : 0.10394325107336044
Loss at iteration 1120 : 0.08812776207923889
Loss at iteration 1130 : 0.08412405103445053
Loss at iteration 1140 : 0.09861769527196884
Loss at iteration 1150 : 0.11018109321594238
Loss at iteration 1160 : 0.10541188716888428
Loss at iteration 1170 : 0.12885348498821259
Loss at iteration 1180 : 0.073880635201931
Loss at iteration 1190 : 0.11527078598737717
Loss at iteration 1200 : 0.05373294651508331
Loss at iteration 1210 : 0.05200909078121185
The SSIM Value is: 0.7187273462613424
The PSNR Value is: 20.844445037841798
the epoch is: 173
Loss at iteration 10 : 0.09094583243131638
Loss at iteration 20 : 0.08981017768383026
Loss at iteration 30 : 0.11410719156265259
Loss at iteration 40 : 0.05523739382624626
Loss at iteration 50 : 0.08201295137405396
Loss at iteration 60 : 0.11847418546676636
Loss at iteration 70 : 0.06935474276542664
Loss at iteration 80 : 0.08216157555580139
Loss at iteration 90 : 0.08072081953287125
Loss at iteration 100 : 0.06465904414653778
Loss at iteration 110 : 0.047348909080028534
Loss at iteration 120 : 0.05954392999410629
Loss at iteration 130 : 0.10767167806625366
Loss at iteration 140 : 0.07121528685092926
Loss at iteration 150 : 0.06775885820388794
Loss at iteration 160 : 0.0674990564584732
Loss at iteration 170 : 0.1400439441204071
Loss at iteration 180 : 0.1193259209394455
Loss at iteration 190 : 0.11160384118556976
Loss at iteration 200 : 0.04059199243783951
Loss at iteration 210 : 0.07448495924472809
Loss at iteration 220 : 0.08392167091369629
Loss at iteration 230 : 0.08627425134181976
Loss at iteration 240 : 0.11115570366382599
Loss at iteration 250 : 0.06505069881677628
Loss at iteration 260 : 0.059711091220378876
Loss at iteration 270 : 0.07347280532121658
Loss at iteration 280 : 0.08888816833496094
Loss at iteration 290 : 0.07964388281106949
Loss at iteration 300 : 0.06376617401838303
Loss at iteration 310 : 0.0771263837814331
Loss at iteration 320 : 0.056115083396434784
Loss at iteration 330 : 0.09476029127836227
Loss at iteration 340 : 0.07005316019058228
Loss at iteration 350 : 0.06643866002559662
Loss at iteration 360 : 0.07659775018692017
Loss at iteration 370 : 0.053029607981443405
Loss at iteration 380 : 0.10507228970527649
Loss at iteration 390 : 0.07337324321269989
Loss at iteration 400 : 0.10863711684942245
Loss at iteration 410 : 0.07516581565141678
Loss at iteration 420 : 0.11742854118347168
Loss at iteration 430 : 0.0784819945693016
Loss at iteration 440 : 0.1012343168258667
Loss at iteration 450 : 0.07397457212209702
Loss at iteration 460 : 0.06664999574422836
Loss at iteration 470 : 0.06688575446605682
Loss at iteration 480 : 0.0781414806842804
Loss at iteration 490 : 0.06663985550403595
Loss at iteration 500 : 0.048077236860990524
Loss at iteration 510 : 0.0887451022863388
Loss at iteration 520 : 0.10452574491500854
Loss at iteration 530 : 0.10151588916778564
Loss at iteration 540 : 0.13539950549602509
Loss at iteration 550 : 0.08473663032054901
Loss at iteration 560 : 0.07101963460445404
Loss at iteration 570 : 0.1130201667547226
Loss at iteration 580 : 0.08579453080892563
Loss at iteration 590 : 0.10109125077724457
Loss at iteration 600 : 0.08508789539337158
Loss at iteration 610 : 0.10104762017726898
Loss at iteration 620 : 0.12305637449026108
Loss at iteration 630 : 0.11205052584409714
Loss at iteration 640 : 0.06820668280124664
Loss at iteration 650 : 0.07026173174381256
Loss at iteration 660 : 0.08445383608341217
Loss at iteration 670 : 0.06863011419773102
Loss at iteration 680 : 0.05840417370200157
Loss at iteration 690 : 0.08033429086208344
Loss at iteration 700 : 0.07085941731929779
Loss at iteration 710 : 0.05493354797363281
Loss at iteration 720 : 0.09059791266918182
Loss at iteration 730 : 0.0947750136256218
Loss at iteration 740 : 0.08341595530509949
Loss at iteration 750 : 0.09050741046667099
Loss at iteration 760 : 0.11651550978422165
Loss at iteration 770 : 0.06739699095487595
Loss at iteration 780 : 0.11900155246257782
Loss at iteration 790 : 0.09974362701177597
Loss at iteration 800 : 0.07324454188346863
Loss at iteration 810 : 0.08174306899309158
Loss at iteration 820 : 0.06846802681684494
Loss at iteration 830 : 0.06656230986118317
Loss at iteration 840 : 0.12669198215007782
Loss at iteration 850 : 0.08541270345449448
Loss at iteration 860 : 0.06512024998664856
Loss at iteration 870 : 0.09252007305622101
Loss at iteration 880 : 0.047146715223789215
Loss at iteration 890 : 0.09121805429458618
Loss at iteration 900 : 0.10677536576986313
Loss at iteration 910 : 0.1077408641576767
Loss at iteration 920 : 0.1120716780424118
Loss at iteration 930 : 0.05180642008781433
Loss at iteration 940 : 0.059726666659116745
Loss at iteration 950 : 0.12427346408367157
Loss at iteration 960 : 0.0695028305053711
Loss at iteration 970 : 0.06907188147306442
Loss at iteration 980 : 0.09055503457784653
Loss at iteration 990 : 0.07278910279273987
Loss at iteration 1000 : 0.09021111577749252
Loss at iteration 1010 : 0.10754528641700745
Loss at iteration 1020 : 0.11045712232589722
Loss at iteration 1030 : 0.05227510258555412
Loss at iteration 1040 : 0.05950424075126648
Loss at iteration 1050 : 0.06435653567314148
Loss at iteration 1060 : 0.08435428142547607
Loss at iteration 1070 : 0.12059766054153442
Loss at iteration 1080 : 0.07103335857391357
Loss at iteration 1090 : 0.07097332924604416
Loss at iteration 1100 : 0.06961392611265182
Loss at iteration 1110 : 0.0753260925412178
Loss at iteration 1120 : 0.10492563247680664
Loss at iteration 1130 : 0.0976087898015976
Loss at iteration 1140 : 0.08395807445049286
Loss at iteration 1150 : 0.04288380593061447
Loss at iteration 1160 : 0.07114116847515106
Loss at iteration 1170 : 0.08871819823980331
Loss at iteration 1180 : 0.09206988662481308
Loss at iteration 1190 : 0.06841900944709778
Loss at iteration 1200 : 0.04975738376379013
Loss at iteration 1210 : 0.0536203607916832
The SSIM Value is: 0.7183659573396047
The PSNR Value is: 20.81743049621582
the epoch is: 174
Loss at iteration 10 : 0.07154029607772827
Loss at iteration 20 : 0.040403373539447784
Loss at iteration 30 : 0.05586156249046326
Loss at iteration 40 : 0.07124702632427216
Loss at iteration 50 : 0.06454956531524658
Loss at iteration 60 : 0.06268984079360962
Loss at iteration 70 : 0.10388992726802826
Loss at iteration 80 : 0.07250247150659561
Loss at iteration 90 : 0.07076701521873474
Loss at iteration 100 : 0.08806692808866501
Loss at iteration 110 : 0.09473242610692978
Loss at iteration 120 : 0.13071857392787933
Loss at iteration 130 : 0.10127320140600204
Loss at iteration 140 : 0.06874112039804459
Loss at iteration 150 : 0.13057133555412292
Loss at iteration 160 : 0.06868505477905273
Loss at iteration 170 : 0.04576372355222702
Loss at iteration 180 : 0.07506819069385529
Loss at iteration 190 : 0.08055543154478073
Loss at iteration 200 : 0.0781528577208519
Loss at iteration 210 : 0.08619894832372665
Loss at iteration 220 : 0.07612533867359161
Loss at iteration 230 : 0.07009809464216232
Loss at iteration 240 : 0.12657169997692108
Loss at iteration 250 : 0.11467862129211426
Loss at iteration 260 : 0.09733548015356064
Loss at iteration 270 : 0.07645940035581589
Loss at iteration 280 : 0.0721927285194397
Loss at iteration 290 : 0.07064250111579895
Loss at iteration 300 : 0.1370801329612732
Loss at iteration 310 : 0.05062885582447052
Loss at iteration 320 : 0.07750614732503891
Loss at iteration 330 : 0.07032576203346252
Loss at iteration 340 : 0.08022238314151764
Loss at iteration 350 : 0.09551545977592468
Loss at iteration 360 : 0.07490348815917969
Loss at iteration 370 : 0.056655339896678925
Loss at iteration 380 : 0.10677780956029892
Loss at iteration 390 : 0.07334122061729431
Loss at iteration 400 : 0.11187991499900818
Loss at iteration 410 : 0.09119188040494919
Loss at iteration 420 : 0.0837126076221466
Loss at iteration 430 : 0.08158838003873825
Loss at iteration 440 : 0.08946134895086288
Loss at iteration 450 : 0.07127410918474197
Loss at iteration 460 : 0.10558366775512695
Loss at iteration 470 : 0.10641278326511383
Loss at iteration 480 : 0.07595258951187134
Loss at iteration 490 : 0.052301447838544846
Loss at iteration 500 : 0.04315125197172165
Loss at iteration 510 : 0.10566176474094391
Loss at iteration 520 : 0.05145394429564476
Loss at iteration 530 : 0.07866773009300232
Loss at iteration 540 : 0.0812719389796257
Loss at iteration 550 : 0.07446514815092087
Loss at iteration 560 : 0.11006738245487213
Loss at iteration 570 : 0.07505477964878082
Loss at iteration 580 : 0.08904170989990234
Loss at iteration 590 : 0.09285487234592438
Loss at iteration 600 : 0.07900950312614441
Loss at iteration 610 : 0.06431081891059875
Loss at iteration 620 : 0.06065203994512558
Loss at iteration 630 : 0.06944051384925842
Loss at iteration 640 : 0.1082255095243454
Loss at iteration 650 : 0.124475859105587
Loss at iteration 660 : 0.0920533761382103
Loss at iteration 670 : 0.06933769583702087
Loss at iteration 680 : 0.06786882132291794
Loss at iteration 690 : 0.05810070037841797
Loss at iteration 700 : 0.07738500833511353
Loss at iteration 710 : 0.07898539304733276
Loss at iteration 720 : 0.06282031536102295
Loss at iteration 730 : 0.07472401112318039
Loss at iteration 740 : 0.049851130694150925
Loss at iteration 750 : 0.045729607343673706
Loss at iteration 760 : 0.06111627072095871
Loss at iteration 770 : 0.08703547716140747
Loss at iteration 780 : 0.08025407791137695
Loss at iteration 790 : 0.07437598705291748
Loss at iteration 800 : 0.12872329354286194
Loss at iteration 810 : 0.05116678401827812
Loss at iteration 820 : 0.09891142696142197
Loss at iteration 830 : 0.08714240044355392
Loss at iteration 840 : 0.09811228513717651
Loss at iteration 850 : 0.09963682293891907
Loss at iteration 860 : 0.11867513507604599
Loss at iteration 870 : 0.06545031070709229
Loss at iteration 880 : 0.07951425760984421
Loss at iteration 890 : 0.08395876735448837
Loss at iteration 900 : 0.05118691921234131
Loss at iteration 910 : 0.08085625618696213
Loss at iteration 920 : 0.08555948734283447
Loss at iteration 930 : 0.0822325348854065
Loss at iteration 940 : 0.06986918300390244
Loss at iteration 950 : 0.08006983250379562
Loss at iteration 960 : 0.06789952516555786
Loss at iteration 970 : 0.04890511557459831
Loss at iteration 980 : 0.05900469422340393
Loss at iteration 990 : 0.0980130061507225
Loss at iteration 1000 : 0.13176459074020386
Loss at iteration 1010 : 0.09391598403453827
Loss at iteration 1020 : 0.08094368875026703
Loss at iteration 1030 : 0.06563372910022736
Loss at iteration 1040 : 0.11814900487661362
Loss at iteration 1050 : 0.08865069597959518
Loss at iteration 1060 : 0.08208765089511871
Loss at iteration 1070 : 0.10059742629528046
Loss at iteration 1080 : 0.11463683843612671
Loss at iteration 1090 : 0.07685842365026474
Loss at iteration 1100 : 0.0721011608839035
Loss at iteration 1110 : 0.10622023046016693
Loss at iteration 1120 : 0.07804921269416809
Loss at iteration 1130 : 0.09995301067829132
Loss at iteration 1140 : 0.06710845232009888
Loss at iteration 1150 : 0.07778780162334442
Loss at iteration 1160 : 0.12087544798851013
Loss at iteration 1170 : 0.05669219419360161
Loss at iteration 1180 : 0.07673156261444092
Loss at iteration 1190 : 0.09140521287918091
Loss at iteration 1200 : 0.07034057378768921
Loss at iteration 1210 : 0.059449195861816406
The SSIM Value is: 0.7231645961602529
The PSNR Value is: 21.300984191894532
the epoch is: 175
Loss at iteration 10 : 0.11047439277172089
Loss at iteration 20 : 0.0822925791144371
Loss at iteration 30 : 0.06243392825126648
Loss at iteration 40 : 0.06374902278184891
Loss at iteration 50 : 0.06752331554889679
Loss at iteration 60 : 0.05260857939720154
Loss at iteration 70 : 0.10027459263801575
Loss at iteration 80 : 0.13256627321243286
Loss at iteration 90 : 0.06839831173419952
Loss at iteration 100 : 0.08462601900100708
Loss at iteration 110 : 0.09752382338047028
Loss at iteration 120 : 0.09383939951658249
Loss at iteration 130 : 0.060909971594810486
Loss at iteration 140 : 0.07816103100776672
Loss at iteration 150 : 0.11227963864803314
Loss at iteration 160 : 0.11517383903265
Loss at iteration 170 : 0.1213531494140625
Loss at iteration 180 : 0.09446603059768677
Loss at iteration 190 : 0.053563542664051056
Loss at iteration 200 : 0.08479981869459152
Loss at iteration 210 : 0.10847733914852142
Loss at iteration 220 : 0.0768572986125946
Loss at iteration 230 : 0.1350834220647812
Loss at iteration 240 : 0.08997592329978943
Loss at iteration 250 : 0.06024109572172165
Loss at iteration 260 : 0.09922485053539276
Loss at iteration 270 : 0.11044077575206757
Loss at iteration 280 : 0.045366182923316956
Loss at iteration 290 : 0.04886253923177719
Loss at iteration 300 : 0.08329158276319504
Loss at iteration 310 : 0.07875615358352661
Loss at iteration 320 : 0.042083680629730225
Loss at iteration 330 : 0.10405045002698898
Loss at iteration 340 : 0.0602242574095726
Loss at iteration 350 : 0.07849946618080139
Loss at iteration 360 : 0.059769533574581146
Loss at iteration 370 : 0.06143055483698845
Loss at iteration 380 : 0.06270847469568253
Loss at iteration 390 : 0.06599710881710052
Loss at iteration 400 : 0.07230222225189209
Loss at iteration 410 : 0.13171139359474182
Loss at iteration 420 : 0.09767135977745056
Loss at iteration 430 : 0.06749126315116882
Loss at iteration 440 : 0.10482301563024521
Loss at iteration 450 : 0.07414235919713974
Loss at iteration 460 : 0.0679265558719635
Loss at iteration 470 : 0.09011156857013702
Loss at iteration 480 : 0.09867614507675171
Loss at iteration 490 : 0.10386037826538086
Loss at iteration 500 : 0.06618392467498779
Loss at iteration 510 : 0.07814732939004898
Loss at iteration 520 : 0.06679900735616684
Loss at iteration 530 : 0.08996371924877167
Loss at iteration 540 : 0.10436408221721649
Loss at iteration 550 : 0.1074109673500061
Loss at iteration 560 : 0.07561036199331284
Loss at iteration 570 : 0.049621157348155975
Loss at iteration 580 : 0.07772193849086761
Loss at iteration 590 : 0.06581534445285797
Loss at iteration 600 : 0.08628666400909424
Loss at iteration 610 : 0.06624913215637207
Loss at iteration 620 : 0.07439979910850525
Loss at iteration 630 : 0.0824321061372757
Loss at iteration 640 : 0.08212941884994507
Loss at iteration 650 : 0.09679876267910004
Loss at iteration 660 : 0.14885884523391724
Loss at iteration 670 : 0.06426410377025604
Loss at iteration 680 : 0.048602163791656494
Loss at iteration 690 : 0.0710829347372055
Loss at iteration 700 : 0.10513969510793686
Loss at iteration 710 : 0.09297476708889008
Loss at iteration 720 : 0.07541445642709732
Loss at iteration 730 : 0.05556633695960045
Loss at iteration 740 : 0.05322709307074547
Loss at iteration 750 : 0.10312318056821823
Loss at iteration 760 : 0.1011844277381897
Loss at iteration 770 : 0.0949849933385849
Loss at iteration 780 : 0.07649672031402588
Loss at iteration 790 : 0.12147736549377441
Loss at iteration 800 : 0.08985960483551025
Loss at iteration 810 : 0.05674968659877777
Loss at iteration 820 : 0.09396326541900635
Loss at iteration 830 : 0.07179774343967438
Loss at iteration 840 : 0.07846479117870331
Loss at iteration 850 : 0.06209185719490051
Loss at iteration 860 : 0.08111237734556198
Loss at iteration 870 : 0.05823880061507225
Loss at iteration 880 : 0.06151564046740532
Loss at iteration 890 : 0.09520592540502548
Loss at iteration 900 : 0.050773363560438156
Loss at iteration 910 : 0.12598969042301178
Loss at iteration 920 : 0.07632745802402496
Loss at iteration 930 : 0.11562099307775497
Loss at iteration 940 : 0.0625329539179802
Loss at iteration 950 : 0.1421746164560318
Loss at iteration 960 : 0.09987761825323105
Loss at iteration 970 : 0.10656853765249252
Loss at iteration 980 : 0.09396170824766159
Loss at iteration 990 : 0.049089815467596054
Loss at iteration 1000 : 0.07066786289215088
Loss at iteration 1010 : 0.10484997928142548
Loss at iteration 1020 : 0.07476567476987839
Loss at iteration 1030 : 0.07819098979234695
Loss at iteration 1040 : 0.09075048565864563
Loss at iteration 1050 : 0.08342377841472626
Loss at iteration 1060 : 0.04360353946685791
Loss at iteration 1070 : 0.11359109729528427
Loss at iteration 1080 : 0.06732883304357529
Loss at iteration 1090 : 0.08294358104467392
Loss at iteration 1100 : 0.14024348556995392
Loss at iteration 1110 : 0.08680021017789841
Loss at iteration 1120 : 0.06049790978431702
Loss at iteration 1130 : 0.08976271003484726
Loss at iteration 1140 : 0.08393888175487518
Loss at iteration 1150 : 0.0885295495390892
Loss at iteration 1160 : 0.07023798674345016
Loss at iteration 1170 : 0.08060133457183838
Loss at iteration 1180 : 0.058204349130392075
Loss at iteration 1190 : 0.0950462743639946
Loss at iteration 1200 : 0.07555069774389267
Loss at iteration 1210 : 0.06717641651630402
The SSIM Value is: 0.7227931062380473
The PSNR Value is: 21.088897069295246
the epoch is: 176
Loss at iteration 10 : 0.08429156243801117
Loss at iteration 20 : 0.0990767702460289
Loss at iteration 30 : 0.047099143266677856
Loss at iteration 40 : 0.08270367980003357
Loss at iteration 50 : 0.09611143916845322
Loss at iteration 60 : 0.08403085172176361
Loss at iteration 70 : 0.06640308350324631
Loss at iteration 80 : 0.07437944412231445
Loss at iteration 90 : 0.09670117497444153
Loss at iteration 100 : 0.09422793984413147
Loss at iteration 110 : 0.0666845366358757
Loss at iteration 120 : 0.08736187219619751
Loss at iteration 130 : 0.10596954822540283
Loss at iteration 140 : 0.0840144157409668
Loss at iteration 150 : 0.06570795178413391
Loss at iteration 160 : 0.08053839206695557
Loss at iteration 170 : 0.05931371450424194
Loss at iteration 180 : 0.07699047029018402
Loss at iteration 190 : 0.06754279136657715
Loss at iteration 200 : 0.09712641686201096
Loss at iteration 210 : 0.12391526997089386
Loss at iteration 220 : 0.07629644125699997
Loss at iteration 230 : 0.08703367412090302
Loss at iteration 240 : 0.06515882909297943
Loss at iteration 250 : 0.136782705783844
Loss at iteration 260 : 0.11941700428724289
Loss at iteration 270 : 0.09338366985321045
Loss at iteration 280 : 0.048643529415130615
Loss at iteration 290 : 0.06314432621002197
Loss at iteration 300 : 0.10663490742444992
Loss at iteration 310 : 0.08206134289503098
Loss at iteration 320 : 0.1134679839015007
Loss at iteration 330 : 0.11014395207166672
Loss at iteration 340 : 0.06480860710144043
Loss at iteration 350 : 0.14260023832321167
Loss at iteration 360 : 0.08907881379127502
Loss at iteration 370 : 0.10186132788658142
Loss at iteration 380 : 0.0832444578409195
Loss at iteration 390 : 0.09016837924718857
Loss at iteration 400 : 0.06560283899307251
Loss at iteration 410 : 0.049680836498737335
Loss at iteration 420 : 0.08433561772108078
Loss at iteration 430 : 0.05283370614051819
Loss at iteration 440 : 0.06917025148868561
Loss at iteration 450 : 0.07201435416936874
Loss at iteration 460 : 0.12248478829860687
Loss at iteration 470 : 0.09046481549739838
Loss at iteration 480 : 0.08970293402671814
Loss at iteration 490 : 0.07046674191951752
Loss at iteration 500 : 0.06028212234377861
Loss at iteration 510 : 0.060980476438999176
Loss at iteration 520 : 0.0741727203130722
Loss at iteration 530 : 0.05101016163825989
Loss at iteration 540 : 0.08208680897951126
Loss at iteration 550 : 0.07022644579410553
Loss at iteration 560 : 0.09345948696136475
Loss at iteration 570 : 0.06190192699432373
Loss at iteration 580 : 0.04574689269065857
Loss at iteration 590 : 0.07145877182483673
Loss at iteration 600 : 0.10176118463277817
Loss at iteration 610 : 0.04946671426296234
Loss at iteration 620 : 0.08433893322944641
Loss at iteration 630 : 0.0716472789645195
Loss at iteration 640 : 0.09085121005773544
Loss at iteration 650 : 0.07204984873533249
Loss at iteration 660 : 0.05752864480018616
Loss at iteration 670 : 0.0935407504439354
Loss at iteration 680 : 0.05661603435873985
Loss at iteration 690 : 0.08524017035961151
Loss at iteration 700 : 0.05892656370997429
Loss at iteration 710 : 0.09540653228759766
Loss at iteration 720 : 0.0898469090461731
Loss at iteration 730 : 0.08688369393348694
Loss at iteration 740 : 0.07593953609466553
Loss at iteration 750 : 0.10212834924459457
Loss at iteration 760 : 0.05793910473585129
Loss at iteration 770 : 0.07077926397323608
Loss at iteration 780 : 0.07567223906517029
Loss at iteration 790 : 0.04757904261350632
Loss at iteration 800 : 0.10905876755714417
Loss at iteration 810 : 0.07844451069831848
Loss at iteration 820 : 0.05579277127981186
Loss at iteration 830 : 0.0792555958032608
Loss at iteration 840 : 0.11762591451406479
Loss at iteration 850 : 0.07371023297309875
Loss at iteration 860 : 0.09763805568218231
Loss at iteration 870 : 0.06865319609642029
Loss at iteration 880 : 0.07334792613983154
Loss at iteration 890 : 0.05380293354392052
Loss at iteration 900 : 0.08840113878250122
Loss at iteration 910 : 0.05879861116409302
Loss at iteration 920 : 0.07120431959629059
Loss at iteration 930 : 0.11746416985988617
Loss at iteration 940 : 0.05627720057964325
Loss at iteration 950 : 0.04536430165171623
Loss at iteration 960 : 0.05746753513813019
Loss at iteration 970 : 0.07770611345767975
Loss at iteration 980 : 0.09086110442876816
Loss at iteration 990 : 0.0912838727235794
Loss at iteration 1000 : 0.08065662533044815
Loss at iteration 1010 : 0.06746701151132584
Loss at iteration 1020 : 0.07602088153362274
Loss at iteration 1030 : 0.09192229807376862
Loss at iteration 1040 : 0.04993642121553421
Loss at iteration 1050 : 0.09189032018184662
Loss at iteration 1060 : 0.07588692754507065
Loss at iteration 1070 : 0.09732761234045029
Loss at iteration 1080 : 0.10184544324874878
Loss at iteration 1090 : 0.04357616603374481
Loss at iteration 1100 : 0.08462771773338318
Loss at iteration 1110 : 0.09632842987775803
Loss at iteration 1120 : 0.0893562063574791
Loss at iteration 1130 : 0.07186326384544373
Loss at iteration 1140 : 0.08659756183624268
Loss at iteration 1150 : 0.0422857403755188
Loss at iteration 1160 : 0.0604100227355957
Loss at iteration 1170 : 0.05996754765510559
Loss at iteration 1180 : 0.10853656381368637
Loss at iteration 1190 : 0.06356155872344971
Loss at iteration 1200 : 0.06749507784843445
Loss at iteration 1210 : 0.11088323593139648
The SSIM Value is: 0.7222926100095113
The PSNR Value is: 21.00815289815267
the epoch is: 177
Loss at iteration 10 : 0.05688857659697533
Loss at iteration 20 : 0.07469166815280914
Loss at iteration 30 : 0.08940478414297104
Loss at iteration 40 : 0.08124283701181412
Loss at iteration 50 : 0.06902733445167542
Loss at iteration 60 : 0.07445643842220306
Loss at iteration 70 : 0.06973791122436523
Loss at iteration 80 : 0.06140732765197754
Loss at iteration 90 : 0.07900901138782501
Loss at iteration 100 : 0.09897001832723618
Loss at iteration 110 : 0.05895780771970749
Loss at iteration 120 : 0.0704578310251236
Loss at iteration 130 : 0.07946264743804932
Loss at iteration 140 : 0.06425835192203522
Loss at iteration 150 : 0.08930039405822754
Loss at iteration 160 : 0.09644487500190735
Loss at iteration 170 : 0.08931388705968857
Loss at iteration 180 : 0.0890415757894516
Loss at iteration 190 : 0.07559153437614441
Loss at iteration 200 : 0.06037883087992668
Loss at iteration 210 : 0.05225745588541031
Loss at iteration 220 : 0.0779498741030693
Loss at iteration 230 : 0.06017770618200302
Loss at iteration 240 : 0.07133682072162628
Loss at iteration 250 : 0.057873498648405075
Loss at iteration 260 : 0.08928035944700241
Loss at iteration 270 : 0.0962403416633606
Loss at iteration 280 : 0.0427243709564209
Loss at iteration 290 : 0.07770844548940659
Loss at iteration 300 : 0.07878127694129944
Loss at iteration 310 : 0.08812158554792404
Loss at iteration 320 : 0.08191914111375809
Loss at iteration 330 : 0.08613555133342743
Loss at iteration 340 : 0.08377852290868759
Loss at iteration 350 : 0.07790205627679825
Loss at iteration 360 : 0.12819385528564453
Loss at iteration 370 : 0.07609224319458008
Loss at iteration 380 : 0.09056385606527328
Loss at iteration 390 : 0.0983877032995224
Loss at iteration 400 : 0.0989578366279602
Loss at iteration 410 : 0.11638383567333221
Loss at iteration 420 : 0.07222722470760345
Loss at iteration 430 : 0.05923166871070862
Loss at iteration 440 : 0.08853455632925034
Loss at iteration 450 : 0.1316017508506775
Loss at iteration 460 : 0.08514736592769623
Loss at iteration 470 : 0.07155101001262665
Loss at iteration 480 : 0.11217884719371796
Loss at iteration 490 : 0.12386276572942734
Loss at iteration 500 : 0.11861635744571686
Loss at iteration 510 : 0.06572137773036957
Loss at iteration 520 : 0.10359849780797958
Loss at iteration 530 : 0.12083268165588379
Loss at iteration 540 : 0.1031409204006195
Loss at iteration 550 : 0.04135286062955856
Loss at iteration 560 : 0.12446331977844238
Loss at iteration 570 : 0.08785437047481537
Loss at iteration 580 : 0.08673511445522308
Loss at iteration 590 : 0.10778125375509262
Loss at iteration 600 : 0.08573311567306519
Loss at iteration 610 : 0.11029015481472015
Loss at iteration 620 : 0.0584268644452095
Loss at iteration 630 : 0.058211252093315125
Loss at iteration 640 : 0.10786060988903046
Loss at iteration 650 : 0.11177682876586914
Loss at iteration 660 : 0.10242323577404022
Loss at iteration 670 : 0.07723798602819443
Loss at iteration 680 : 0.04955674707889557
Loss at iteration 690 : 0.05900469422340393
Loss at iteration 700 : 0.0950843021273613
Loss at iteration 710 : 0.06732822954654694
Loss at iteration 720 : 0.07141268253326416
Loss at iteration 730 : 0.06844878941774368
Loss at iteration 740 : 0.05107797682285309
Loss at iteration 750 : 0.06962591409683228
Loss at iteration 760 : 0.07078620791435242
Loss at iteration 770 : 0.07887229323387146
Loss at iteration 780 : 0.07477594166994095
Loss at iteration 790 : 0.07924851030111313
Loss at iteration 800 : 0.10674741864204407
Loss at iteration 810 : 0.0862584337592125
Loss at iteration 820 : 0.06034620851278305
Loss at iteration 830 : 0.07612389326095581
Loss at iteration 840 : 0.09085923433303833
Loss at iteration 850 : 0.10791969299316406
Loss at iteration 860 : 0.09972773492336273
Loss at iteration 870 : 0.1254996359348297
Loss at iteration 880 : 0.06163668632507324
Loss at iteration 890 : 0.0785038098692894
Loss at iteration 900 : 0.08956880867481232
Loss at iteration 910 : 0.09235513210296631
Loss at iteration 920 : 0.04468771070241928
Loss at iteration 930 : 0.05320096015930176
Loss at iteration 940 : 0.051121532917022705
Loss at iteration 950 : 0.08838924765586853
Loss at iteration 960 : 0.10459859669208527
Loss at iteration 970 : 0.09170683473348618
Loss at iteration 980 : 0.05792441964149475
Loss at iteration 990 : 0.10358528792858124
Loss at iteration 1000 : 0.07953310012817383
Loss at iteration 1010 : 0.08178475499153137
Loss at iteration 1020 : 0.13708741962909698
Loss at iteration 1030 : 0.0705660879611969
Loss at iteration 1040 : 0.06274574995040894
Loss at iteration 1050 : 0.08591600507497787
Loss at iteration 1060 : 0.09902554750442505
Loss at iteration 1070 : 0.06910380721092224
Loss at iteration 1080 : 0.08323289453983307
Loss at iteration 1090 : 0.07261738181114197
Loss at iteration 1100 : 0.06453511118888855
Loss at iteration 1110 : 0.08714011311531067
Loss at iteration 1120 : 0.060943178832530975
Loss at iteration 1130 : 0.08433827012777328
Loss at iteration 1140 : 0.061364661902189255
Loss at iteration 1150 : 0.07626649737358093
Loss at iteration 1160 : 0.075848288834095
Loss at iteration 1170 : 0.07030168175697327
Loss at iteration 1180 : 0.05087078735232353
Loss at iteration 1190 : 0.0865098237991333
Loss at iteration 1200 : 0.1056600958108902
Loss at iteration 1210 : 0.07058221101760864
The SSIM Value is: 0.7166483322779338
The PSNR Value is: 20.573127619425456
the epoch is: 178
Loss at iteration 10 : 0.11595695465803146
Loss at iteration 20 : 0.07645490765571594
Loss at iteration 30 : 0.09146767854690552
Loss at iteration 40 : 0.08387266099452972
Loss at iteration 50 : 0.15945789217948914
Loss at iteration 60 : 0.08809120953083038
Loss at iteration 70 : 0.07490372657775879
Loss at iteration 80 : 0.08895500749349594
Loss at iteration 90 : 0.05663600564002991
Loss at iteration 100 : 0.07032957673072815
Loss at iteration 110 : 0.11688636243343353
Loss at iteration 120 : 0.09048577398061752
Loss at iteration 130 : 0.11709891259670258
Loss at iteration 140 : 0.07961465418338776
Loss at iteration 150 : 0.06541022658348083
Loss at iteration 160 : 0.09017598628997803
Loss at iteration 170 : 0.08772232383489609
Loss at iteration 180 : 0.08119955658912659
Loss at iteration 190 : 0.08798261731863022
Loss at iteration 200 : 0.07455302774906158
Loss at iteration 210 : 0.09692053496837616
Loss at iteration 220 : 0.04560580849647522
Loss at iteration 230 : 0.060462161898612976
Loss at iteration 240 : 0.07737253606319427
Loss at iteration 250 : 0.06695613265037537
Loss at iteration 260 : 0.11658713221549988
Loss at iteration 270 : 0.07074304670095444
Loss at iteration 280 : 0.05779401957988739
Loss at iteration 290 : 0.08991150557994843
Loss at iteration 300 : 0.054807357490062714
Loss at iteration 310 : 0.07556094229221344
Loss at iteration 320 : 0.0727551206946373
Loss at iteration 330 : 0.07422897964715958
Loss at iteration 340 : 0.07455967366695404
Loss at iteration 350 : 0.0741758942604065
Loss at iteration 360 : 0.041943084448575974
Loss at iteration 370 : 0.10930114984512329
Loss at iteration 380 : 0.13210931420326233
Loss at iteration 390 : 0.06879715621471405
Loss at iteration 400 : 0.05349341034889221
Loss at iteration 410 : 0.1310504525899887
Loss at iteration 420 : 0.11068322509527206
Loss at iteration 430 : 0.0883278101682663
Loss at iteration 440 : 0.03598543256521225
Loss at iteration 450 : 0.056763581931591034
Loss at iteration 460 : 0.06394602358341217
Loss at iteration 470 : 0.0751660168170929
Loss at iteration 480 : 0.0497865155339241
Loss at iteration 490 : 0.06826841831207275
Loss at iteration 500 : 0.05418301373720169
Loss at iteration 510 : 0.0753055065870285
Loss at iteration 520 : 0.0824001282453537
Loss at iteration 530 : 0.08053761720657349
Loss at iteration 540 : 0.10339929908514023
Loss at iteration 550 : 0.08218510448932648
Loss at iteration 560 : 0.10783468931913376
Loss at iteration 570 : 0.07709373533725739
Loss at iteration 580 : 0.08048012852668762
Loss at iteration 590 : 0.10901381075382233
Loss at iteration 600 : 0.06671822816133499
Loss at iteration 610 : 0.08484312146902084
Loss at iteration 620 : 0.06848929822444916
Loss at iteration 630 : 0.0604533925652504
Loss at iteration 640 : 0.06995795667171478
Loss at iteration 650 : 0.08457905054092407
Loss at iteration 660 : 0.04155595228075981
Loss at iteration 670 : 0.08204277604818344
Loss at iteration 680 : 0.06990225613117218
Loss at iteration 690 : 0.0704440027475357
Loss at iteration 700 : 0.10494309663772583
Loss at iteration 710 : 0.10190512239933014
Loss at iteration 720 : 0.12238588184118271
Loss at iteration 730 : 0.0928511768579483
Loss at iteration 740 : 0.0803689956665039
Loss at iteration 750 : 0.08699208498001099
Loss at iteration 760 : 0.06108614802360535
Loss at iteration 770 : 0.08124329894781113
Loss at iteration 780 : 0.07914847135543823
Loss at iteration 790 : 0.08422589302062988
Loss at iteration 800 : 0.07368822395801544
Loss at iteration 810 : 0.05982179567217827
Loss at iteration 820 : 0.05424594134092331
Loss at iteration 830 : 0.08548767864704132
Loss at iteration 840 : 0.055231597274541855
Loss at iteration 850 : 0.13764074444770813
Loss at iteration 860 : 0.06290654838085175
Loss at iteration 870 : 0.06472112238407135
Loss at iteration 880 : 0.060961734503507614
Loss at iteration 890 : 0.08456101268529892
Loss at iteration 900 : 0.08875472843647003
Loss at iteration 910 : 0.10893213748931885
Loss at iteration 920 : 0.08416831493377686
Loss at iteration 930 : 0.09340911358594894
Loss at iteration 940 : 0.07564584910869598
Loss at iteration 950 : 0.07917729765176773
Loss at iteration 960 : 0.08856188505887985
Loss at iteration 970 : 0.07721282541751862
Loss at iteration 980 : 0.0757087990641594
Loss at iteration 990 : 0.09057684987783432
Loss at iteration 1000 : 0.05694125220179558
Loss at iteration 1010 : 0.13254544138908386
Loss at iteration 1020 : 0.11561219394207001
Loss at iteration 1030 : 0.059058092534542084
Loss at iteration 1040 : 0.07920115441083908
Loss at iteration 1050 : 0.086375892162323
Loss at iteration 1060 : 0.04459630697965622
Loss at iteration 1070 : 0.08647512644529343
Loss at iteration 1080 : 0.09145420044660568
Loss at iteration 1090 : 0.10924333333969116
Loss at iteration 1100 : 0.0889311283826828
Loss at iteration 1110 : 0.10788506269454956
Loss at iteration 1120 : 0.07766646146774292
Loss at iteration 1130 : 0.10032673925161362
Loss at iteration 1140 : 0.08942702412605286
Loss at iteration 1150 : 0.08071634918451309
Loss at iteration 1160 : 0.0939570963382721
Loss at iteration 1170 : 0.07961510121822357
Loss at iteration 1180 : 0.08081285655498505
Loss at iteration 1190 : 0.05534624680876732
Loss at iteration 1200 : 0.08173035085201263
Loss at iteration 1210 : 0.07094971090555191
The SSIM Value is: 0.7185382386048634
The PSNR Value is: 20.757713317871094
the epoch is: 179
Loss at iteration 10 : 0.07765163481235504
Loss at iteration 20 : 0.08763490617275238
Loss at iteration 30 : 0.06552548706531525
Loss at iteration 40 : 0.10231459140777588
Loss at iteration 50 : 0.08153264969587326
Loss at iteration 60 : 0.1489769071340561
Loss at iteration 70 : 0.07236239314079285
Loss at iteration 80 : 0.08659355342388153
Loss at iteration 90 : 0.06388692557811737
Loss at iteration 100 : 0.09136694669723511
Loss at iteration 110 : 0.06663809716701508
Loss at iteration 120 : 0.06196907162666321
Loss at iteration 130 : 0.056947194039821625
Loss at iteration 140 : 0.06496714055538177
Loss at iteration 150 : 0.07967755943536758
Loss at iteration 160 : 0.08252524584531784
Loss at iteration 170 : 0.11521147191524506
Loss at iteration 180 : 0.051179856061935425
Loss at iteration 190 : 0.06138233840465546
Loss at iteration 200 : 0.07315514981746674
Loss at iteration 210 : 0.06789322197437286
Loss at iteration 220 : 0.0896637886762619
Loss at iteration 230 : 0.12464262545108795
Loss at iteration 240 : 0.07313559204339981
Loss at iteration 250 : 0.09135092794895172
Loss at iteration 260 : 0.10438758134841919
Loss at iteration 270 : 0.050997085869312286
Loss at iteration 280 : 0.06495574116706848
Loss at iteration 290 : 0.04985019564628601
Loss at iteration 300 : 0.06249558925628662
Loss at iteration 310 : 0.08672617375850677
Loss at iteration 320 : 0.06318240612745285
Loss at iteration 330 : 0.09931714832782745
Loss at iteration 340 : 0.06744334101676941
Loss at iteration 350 : 0.09210789203643799
Loss at iteration 360 : 0.10997933149337769
Loss at iteration 370 : 0.07041828334331512
Loss at iteration 380 : 0.06593991816043854
Loss at iteration 390 : 0.07018186897039413
Loss at iteration 400 : 0.06848296523094177
Loss at iteration 410 : 0.09324498474597931
Loss at iteration 420 : 0.08905769884586334
Loss at iteration 430 : 0.08956737816333771
Loss at iteration 440 : 0.07752859592437744
Loss at iteration 450 : 0.085065558552742
Loss at iteration 460 : 0.09949049353599548
Loss at iteration 470 : 0.05781184881925583
Loss at iteration 480 : 0.0893634557723999
Loss at iteration 490 : 0.09741860628128052
Loss at iteration 500 : 0.11613915860652924
Loss at iteration 510 : 0.08597537875175476
Loss at iteration 520 : 0.07807186245918274
Loss at iteration 530 : 0.06205524131655693
Loss at iteration 540 : 0.0900590717792511
Loss at iteration 550 : 0.07969224452972412
Loss at iteration 560 : 0.07508008927106857
Loss at iteration 570 : 0.036194778978824615
Loss at iteration 580 : 0.0680997371673584
Loss at iteration 590 : 0.03817613422870636
Loss at iteration 600 : 0.06476549804210663
Loss at iteration 610 : 0.1051451787352562
Loss at iteration 620 : 0.09011861681938171
Loss at iteration 630 : 0.10417964309453964
Loss at iteration 640 : 0.08873872458934784
Loss at iteration 650 : 0.050827398896217346
Loss at iteration 660 : 0.06514368206262589
Loss at iteration 670 : 0.05224207043647766
Loss at iteration 680 : 0.09025522321462631
Loss at iteration 690 : 0.12125154584646225
Loss at iteration 700 : 0.07306608557701111
Loss at iteration 710 : 0.061838626861572266
Loss at iteration 720 : 0.08403127640485764
Loss at iteration 730 : 0.07550820708274841
Loss at iteration 740 : 0.11280183494091034
Loss at iteration 750 : 0.06447329372167587
Loss at iteration 760 : 0.09187155961990356
Loss at iteration 770 : 0.08404834568500519
Loss at iteration 780 : 0.06411333382129669
Loss at iteration 790 : 0.10454349219799042
Loss at iteration 800 : 0.06553246080875397
Loss at iteration 810 : 0.062056269496679306
Loss at iteration 820 : 0.07169593870639801
Loss at iteration 830 : 0.09049350023269653
Loss at iteration 840 : 0.07820568978786469
Loss at iteration 850 : 0.10725808143615723
Loss at iteration 860 : 0.07882597297430038
Loss at iteration 870 : 0.07091835141181946
Loss at iteration 880 : 0.08252601325511932
Loss at iteration 890 : 0.05518082529306412
Loss at iteration 900 : 0.09920813888311386
Loss at iteration 910 : 0.04407273977994919
Loss at iteration 920 : 0.08080394566059113
Loss at iteration 930 : 0.04845775291323662
Loss at iteration 940 : 0.07238450646400452
Loss at iteration 950 : 0.07893005758523941
Loss at iteration 960 : 0.05861883610486984
Loss at iteration 970 : 0.09092342853546143
Loss at iteration 980 : 0.059234630316495895
Loss at iteration 990 : 0.12421131134033203
Loss at iteration 1000 : 0.10718654841184616
Loss at iteration 1010 : 0.09762348979711533
Loss at iteration 1020 : 0.08930211514234543
Loss at iteration 1030 : 0.08490719646215439
Loss at iteration 1040 : 0.07077320665121078
Loss at iteration 1050 : 0.08069298416376114
Loss at iteration 1060 : 0.05790218710899353
Loss at iteration 1070 : 0.08004671335220337
Loss at iteration 1080 : 0.11624692380428314
Loss at iteration 1090 : 0.08219048380851746
Loss at iteration 1100 : 0.07466097921133041
Loss at iteration 1110 : 0.1673550307750702
Loss at iteration 1120 : 0.08608385920524597
Loss at iteration 1130 : 0.08697240054607391
Loss at iteration 1140 : 0.1318947821855545
Loss at iteration 1150 : 0.05641806125640869
Loss at iteration 1160 : 0.09116761386394501
Loss at iteration 1170 : 0.07342789322137833
Loss at iteration 1180 : 0.07231716811656952
Loss at iteration 1190 : 0.05562528595328331
Loss at iteration 1200 : 0.06564649939537048
Loss at iteration 1210 : 0.09351897239685059
The SSIM Value is: 0.7161462207635244
The PSNR Value is: 20.7942995707194
the epoch is: 180
Loss at iteration 10 : 0.08810299634933472
Loss at iteration 20 : 0.08678555488586426
Loss at iteration 30 : 0.06902788579463959
Loss at iteration 40 : 0.11917787045240402
Loss at iteration 50 : 0.09104746580123901
Loss at iteration 60 : 0.04757387563586235
Loss at iteration 70 : 0.06583565473556519
Loss at iteration 80 : 0.06652180105447769
Loss at iteration 90 : 0.07805526256561279
Loss at iteration 100 : 0.08066567778587341
Loss at iteration 110 : 0.0887308344244957
Loss at iteration 120 : 0.06676632165908813
Loss at iteration 130 : 0.09700724482536316
Loss at iteration 140 : 0.06623247265815735
Loss at iteration 150 : 0.06561680138111115
Loss at iteration 160 : 0.07169170677661896
Loss at iteration 170 : 0.07324473559856415
Loss at iteration 180 : 0.048856839537620544
Loss at iteration 190 : 0.0550350658595562
Loss at iteration 200 : 0.0850362777709961
Loss at iteration 210 : 0.12372993677854538
Loss at iteration 220 : 0.08989112824201584
Loss at iteration 230 : 0.06015735864639282
Loss at iteration 240 : 0.08441158384084702
Loss at iteration 250 : 0.06711088865995407
Loss at iteration 260 : 0.07102291285991669
Loss at iteration 270 : 0.06978647410869598
Loss at iteration 280 : 0.07421696186065674
Loss at iteration 290 : 0.06855203211307526
Loss at iteration 300 : 0.07192696630954742
Loss at iteration 310 : 0.1127641424536705
Loss at iteration 320 : 0.09748286008834839
Loss at iteration 330 : 0.09561491012573242
Loss at iteration 340 : 0.05543295666575432
Loss at iteration 350 : 0.11752429604530334
Loss at iteration 360 : 0.1149456724524498
Loss at iteration 370 : 0.03401679918169975
Loss at iteration 380 : 0.08577235788106918
Loss at iteration 390 : 0.0708790272474289
Loss at iteration 400 : 0.06742245703935623
Loss at iteration 410 : 0.044987887144088745
Loss at iteration 420 : 0.07753320038318634
Loss at iteration 430 : 0.0620148740708828
Loss at iteration 440 : 0.09173383563756943
Loss at iteration 450 : 0.07465775310993195
Loss at iteration 460 : 0.07280629128217697
Loss at iteration 470 : 0.09189703315496445
Loss at iteration 480 : 0.12587757408618927
Loss at iteration 490 : 0.09958039969205856
Loss at iteration 500 : 0.07081644237041473
Loss at iteration 510 : 0.04498710855841637
Loss at iteration 520 : 0.08898290246725082
Loss at iteration 530 : 0.08570128679275513
Loss at iteration 540 : 0.06234998255968094
Loss at iteration 550 : 0.06713201105594635
Loss at iteration 560 : 0.09244580566883087
Loss at iteration 570 : 0.08112018555402756
Loss at iteration 580 : 0.12639787793159485
Loss at iteration 590 : 0.07514526695013046
Loss at iteration 600 : 0.03626289963722229
Loss at iteration 610 : 0.0896586924791336
Loss at iteration 620 : 0.12909464538097382
Loss at iteration 630 : 0.09111075848340988
Loss at iteration 640 : 0.11496510356664658
Loss at iteration 650 : 0.08189015090465546
Loss at iteration 660 : 0.08360995352268219
Loss at iteration 670 : 0.1105673760175705
Loss at iteration 680 : 0.06679272651672363
Loss at iteration 690 : 0.06839179992675781
Loss at iteration 700 : 0.05874636769294739
Loss at iteration 710 : 0.05631909891963005
Loss at iteration 720 : 0.09933941066265106
Loss at iteration 730 : 0.08871696889400482
Loss at iteration 740 : 0.06857191026210785
Loss at iteration 750 : 0.08258853852748871
Loss at iteration 760 : 0.09070400893688202
Loss at iteration 770 : 0.08688750863075256
Loss at iteration 780 : 0.08025815337896347
Loss at iteration 790 : 0.09816863387823105
Loss at iteration 800 : 0.06377489864826202
Loss at iteration 810 : 0.09994564950466156
Loss at iteration 820 : 0.06730632483959198
Loss at iteration 830 : 0.08351316303014755
Loss at iteration 840 : 0.05059304088354111
Loss at iteration 850 : 0.07826001942157745
Loss at iteration 860 : 0.07756967097520828
Loss at iteration 870 : 0.11987569183111191
Loss at iteration 880 : 0.06604249775409698
Loss at iteration 890 : 0.06081915646791458
Loss at iteration 900 : 0.0745338648557663
Loss at iteration 910 : 0.06460461020469666
Loss at iteration 920 : 0.061364658176898956
Loss at iteration 930 : 0.09096066653728485
Loss at iteration 940 : 0.11526497453451157
Loss at iteration 950 : 0.06256251782178879
Loss at iteration 960 : 0.09959661215543747
Loss at iteration 970 : 0.0798293799161911
Loss at iteration 980 : 0.087058886885643
Loss at iteration 990 : 0.0612289160490036
Loss at iteration 1000 : 0.098300039768219
Loss at iteration 1010 : 0.0843784436583519
Loss at iteration 1020 : 0.06424480676651001
Loss at iteration 1030 : 0.06909337639808655
Loss at iteration 1040 : 0.05148143321275711
Loss at iteration 1050 : 0.08089089393615723
Loss at iteration 1060 : 0.081081323325634
Loss at iteration 1070 : 0.06861597299575806
Loss at iteration 1080 : 0.11505046486854553
Loss at iteration 1090 : 0.07603716850280762
Loss at iteration 1100 : 0.06378494948148727
Loss at iteration 1110 : 0.04408104345202446
Loss at iteration 1120 : 0.09655316919088364
Loss at iteration 1130 : 0.1507006734609604
Loss at iteration 1140 : 0.08779193460941315
Loss at iteration 1150 : 0.056582532823085785
Loss at iteration 1160 : 0.06401628255844116
Loss at iteration 1170 : 0.08117391169071198
Loss at iteration 1180 : 0.051370762288570404
Loss at iteration 1190 : 0.056931398808956146
Loss at iteration 1200 : 0.09534603357315063
Loss at iteration 1210 : 0.12616072595119476
The SSIM Value is: 0.7218763868014018
The PSNR Value is: 21.112140909830728
the epoch is: 181
Loss at iteration 10 : 0.09038849920034409
Loss at iteration 20 : 0.06180116534233093
Loss at iteration 30 : 0.0747150406241417
Loss at iteration 40 : 0.1159997433423996
Loss at iteration 50 : 0.11396282911300659
Loss at iteration 60 : 0.10517017543315887
Loss at iteration 70 : 0.06238943338394165
Loss at iteration 80 : 0.07515724003314972
Loss at iteration 90 : 0.09580905735492706
Loss at iteration 100 : 0.09375863522291183
Loss at iteration 110 : 0.07019910216331482
Loss at iteration 120 : 0.09405176341533661
Loss at iteration 130 : 0.08622321486473083
Loss at iteration 140 : 0.08289051055908203
Loss at iteration 150 : 0.0675622746348381
Loss at iteration 160 : 0.05106305330991745
Loss at iteration 170 : 0.0871686339378357
Loss at iteration 180 : 0.05119830369949341
Loss at iteration 190 : 0.08466054499149323
Loss at iteration 200 : 0.10015535354614258
Loss at iteration 210 : 0.07025657594203949
Loss at iteration 220 : 0.056245893239974976
Loss at iteration 230 : 0.13590477406978607
Loss at iteration 240 : 0.08627434819936752
Loss at iteration 250 : 0.11973594129085541
Loss at iteration 260 : 0.09154784679412842
Loss at iteration 270 : 0.09488187730312347
Loss at iteration 280 : 0.11856614053249359
Loss at iteration 290 : 0.08041458576917648
Loss at iteration 300 : 0.06772442907094955
Loss at iteration 310 : 0.10891023278236389
Loss at iteration 320 : 0.09607832133769989
Loss at iteration 330 : 0.05317196995019913
Loss at iteration 340 : 0.0542774423956871
Loss at iteration 350 : 0.08865868300199509
Loss at iteration 360 : 0.05359652638435364
Loss at iteration 370 : 0.08244039118289948
Loss at iteration 380 : 0.09100840985774994
Loss at iteration 390 : 0.07749086618423462
Loss at iteration 400 : 0.10603338479995728
Loss at iteration 410 : 0.05562847852706909
Loss at iteration 420 : 0.06667459011077881
Loss at iteration 430 : 0.0870596170425415
Loss at iteration 440 : 0.07947909086942673
Loss at iteration 450 : 0.10160791128873825
Loss at iteration 460 : 0.09233474731445312
Loss at iteration 470 : 0.10108917951583862
Loss at iteration 480 : 0.0705803856253624
Loss at iteration 490 : 0.08916932344436646
Loss at iteration 500 : 0.06978341937065125
Loss at iteration 510 : 0.07430806010961533
Loss at iteration 520 : 0.08089841902256012
Loss at iteration 530 : 0.0486258827149868
Loss at iteration 540 : 0.06184527277946472
Loss at iteration 550 : 0.09572752565145493
Loss at iteration 560 : 0.08302982151508331
Loss at iteration 570 : 0.09316631406545639
Loss at iteration 580 : 0.08371205627918243
Loss at iteration 590 : 0.0953998863697052
Loss at iteration 600 : 0.03958458453416824
Loss at iteration 610 : 0.07600240409374237
Loss at iteration 620 : 0.07199297845363617
Loss at iteration 630 : 0.11022909730672836
Loss at iteration 640 : 0.04519883915781975
Loss at iteration 650 : 0.07105719298124313
Loss at iteration 660 : 0.07273256778717041
Loss at iteration 670 : 0.12340705096721649
Loss at iteration 680 : 0.09420490264892578
Loss at iteration 690 : 0.09218444675207138
Loss at iteration 700 : 0.07103952765464783
Loss at iteration 710 : 0.07591873407363892
Loss at iteration 720 : 0.0689452737569809
Loss at iteration 730 : 0.06177043169736862
Loss at iteration 740 : 0.07902192324399948
Loss at iteration 750 : 0.07524927705526352
Loss at iteration 760 : 0.09582289308309555
Loss at iteration 770 : 0.08167777955532074
Loss at iteration 780 : 0.09326590597629547
Loss at iteration 790 : 0.06992574036121368
Loss at iteration 800 : 0.09388896077871323
Loss at iteration 810 : 0.04387247562408447
Loss at iteration 820 : 0.06366625428199768
Loss at iteration 830 : 0.0819244459271431
Loss at iteration 840 : 0.06942614912986755
Loss at iteration 850 : 0.08868356794118881
Loss at iteration 860 : 0.0705157145857811
Loss at iteration 870 : 0.14625424146652222
Loss at iteration 880 : 0.08497276902198792
Loss at iteration 890 : 0.06676937639713287
Loss at iteration 900 : 0.09064148366451263
Loss at iteration 910 : 0.10035060346126556
Loss at iteration 920 : 0.07176648825407028
Loss at iteration 930 : 0.08108410239219666
Loss at iteration 940 : 0.09058486670255661
Loss at iteration 950 : 0.09054769575595856
Loss at iteration 960 : 0.08838526904582977
Loss at iteration 970 : 0.05475287884473801
Loss at iteration 980 : 0.1121746152639389
Loss at iteration 990 : 0.07099451124668121
Loss at iteration 1000 : 0.07536406815052032
Loss at iteration 1010 : 0.10307835042476654
Loss at iteration 1020 : 0.08207689225673676
Loss at iteration 1030 : 0.06538994610309601
Loss at iteration 1040 : 0.08824329823255539
Loss at iteration 1050 : 0.05499023199081421
Loss at iteration 1060 : 0.04826097935438156
Loss at iteration 1070 : 0.09205840528011322
Loss at iteration 1080 : 0.06178751215338707
Loss at iteration 1090 : 0.0818943902850151
Loss at iteration 1100 : 0.07409997284412384
Loss at iteration 1110 : 0.10571354627609253
Loss at iteration 1120 : 0.06622698158025742
Loss at iteration 1130 : 0.10369393229484558
Loss at iteration 1140 : 0.08024929463863373
Loss at iteration 1150 : 0.07832782715559006
Loss at iteration 1160 : 0.08549228310585022
Loss at iteration 1170 : 0.10032936930656433
Loss at iteration 1180 : 0.09148932993412018
Loss at iteration 1190 : 0.10048924386501312
Loss at iteration 1200 : 0.11017696559429169
Loss at iteration 1210 : 0.06371937692165375
The SSIM Value is: 0.7154941856861115
The PSNR Value is: 20.52380765279134
the epoch is: 182
Loss at iteration 10 : 0.0690593272447586
Loss at iteration 20 : 0.11547248065471649
Loss at iteration 30 : 0.06813643872737885
Loss at iteration 40 : 0.09867449849843979
Loss at iteration 50 : 0.08285678923130035
Loss at iteration 60 : 0.06478593498468399
Loss at iteration 70 : 0.07673609256744385
Loss at iteration 80 : 0.06131213903427124
Loss at iteration 90 : 0.10560685396194458
Loss at iteration 100 : 0.11919452250003815
Loss at iteration 110 : 0.05168277025222778
Loss at iteration 120 : 0.056521784514188766
Loss at iteration 130 : 0.08305748552083969
Loss at iteration 140 : 0.05449749529361725
Loss at iteration 150 : 0.07972405850887299
Loss at iteration 160 : 0.07763176411390305
Loss at iteration 170 : 0.09124340116977692
Loss at iteration 180 : 0.04975959658622742
Loss at iteration 190 : 0.12200886011123657
Loss at iteration 200 : 0.13075688481330872
Loss at iteration 210 : 0.08901314437389374
Loss at iteration 220 : 0.0689685270190239
Loss at iteration 230 : 0.1024356335401535
Loss at iteration 240 : 0.08733503520488739
Loss at iteration 250 : 0.09062601625919342
Loss at iteration 260 : 0.09534955024719238
Loss at iteration 270 : 0.07125797867774963
Loss at iteration 280 : 0.15010994672775269
Loss at iteration 290 : 0.06275278329849243
Loss at iteration 300 : 0.044803425669670105
Loss at iteration 310 : 0.06942436099052429
Loss at iteration 320 : 0.0694437026977539
Loss at iteration 330 : 0.09083925932645798
Loss at iteration 340 : 0.11854887008666992
Loss at iteration 350 : 0.07421702891588211
Loss at iteration 360 : 0.05123719573020935
Loss at iteration 370 : 0.06551316380500793
Loss at iteration 380 : 0.062384095042943954
Loss at iteration 390 : 0.1044163927435875
Loss at iteration 400 : 0.09411275386810303
Loss at iteration 410 : 0.12189851701259613
Loss at iteration 420 : 0.08348115533590317
Loss at iteration 430 : 0.05945646017789841
Loss at iteration 440 : 0.10929358005523682
Loss at iteration 450 : 0.046165745705366135
Loss at iteration 460 : 0.08238689601421356
Loss at iteration 470 : 0.09645923227071762
Loss at iteration 480 : 0.0881604254245758
Loss at iteration 490 : 0.13431791961193085
Loss at iteration 500 : 0.07909329235553741
Loss at iteration 510 : 0.0786447674036026
Loss at iteration 520 : 0.06939026713371277
Loss at iteration 530 : 0.0769844576716423
Loss at iteration 540 : 0.10255391895771027
Loss at iteration 550 : 0.07722266763448715
Loss at iteration 560 : 0.1180526614189148
Loss at iteration 570 : 0.06717678904533386
Loss at iteration 580 : 0.05471042916178703
Loss at iteration 590 : 0.11937326192855835
Loss at iteration 600 : 0.12435732781887054
Loss at iteration 610 : 0.09707924723625183
Loss at iteration 620 : 0.12460823357105255
Loss at iteration 630 : 0.1416345089673996
Loss at iteration 640 : 0.060845695436000824
Loss at iteration 650 : 0.053892381489276886
Loss at iteration 660 : 0.11079585552215576
Loss at iteration 670 : 0.05168217048048973
Loss at iteration 680 : 0.08704696595668793
Loss at iteration 690 : 0.08692489564418793
Loss at iteration 700 : 0.060916390269994736
Loss at iteration 710 : 0.059433721005916595
Loss at iteration 720 : 0.062085650861263275
Loss at iteration 730 : 0.12744349241256714
Loss at iteration 740 : 0.05201536417007446
Loss at iteration 750 : 0.0798138976097107
Loss at iteration 760 : 0.06968508660793304
Loss at iteration 770 : 0.06797732412815094
Loss at iteration 780 : 0.07272626459598541
Loss at iteration 790 : 0.07820875197649002
Loss at iteration 800 : 0.07291610538959503
Loss at iteration 810 : 0.04356185719370842
Loss at iteration 820 : 0.0967203825712204
Loss at iteration 830 : 0.08835665881633759
Loss at iteration 840 : 0.1067524179816246
Loss at iteration 850 : 0.1260463297367096
Loss at iteration 860 : 0.07719986140727997
Loss at iteration 870 : 0.0973576158285141
Loss at iteration 880 : 0.069072425365448
Loss at iteration 890 : 0.09048816561698914
Loss at iteration 900 : 0.07522889971733093
Loss at iteration 910 : 0.09641411900520325
Loss at iteration 920 : 0.09085512161254883
Loss at iteration 930 : 0.1128615289926529
Loss at iteration 940 : 0.04193683713674545
Loss at iteration 950 : 0.09583763778209686
Loss at iteration 960 : 0.07876738905906677
Loss at iteration 970 : 0.06095866858959198
Loss at iteration 980 : 0.0924471840262413
Loss at iteration 990 : 0.06570001691579819
Loss at iteration 1000 : 0.0663808286190033
Loss at iteration 1010 : 0.10654713213443756
Loss at iteration 1020 : 0.11360625922679901
Loss at iteration 1030 : 0.05287828668951988
Loss at iteration 1040 : 0.07623878121376038
Loss at iteration 1050 : 0.07615941017866135
Loss at iteration 1060 : 0.06536989659070969
Loss at iteration 1070 : 0.06650783121585846
Loss at iteration 1080 : 0.061868809163570404
Loss at iteration 1090 : 0.11336344480514526
Loss at iteration 1100 : 0.062076568603515625
Loss at iteration 1110 : 0.06643693894147873
Loss at iteration 1120 : 0.06352781504392624
Loss at iteration 1130 : 0.08843770623207092
Loss at iteration 1140 : 0.06564392149448395
Loss at iteration 1150 : 0.08996974676847458
Loss at iteration 1160 : 0.07758328318595886
Loss at iteration 1170 : 0.07685892283916473
Loss at iteration 1180 : 0.0846402496099472
Loss at iteration 1190 : 0.1078115925192833
Loss at iteration 1200 : 0.08026908338069916
Loss at iteration 1210 : 0.05249089002609253
The SSIM Value is: 0.7133467892805735
The PSNR Value is: 20.4089994430542
the epoch is: 183
Loss at iteration 10 : 0.07767562568187714
Loss at iteration 20 : 0.08281885832548141
Loss at iteration 30 : 0.07679927349090576
Loss at iteration 40 : 0.0848659873008728
Loss at iteration 50 : 0.08366825431585312
Loss at iteration 60 : 0.12421149015426636
Loss at iteration 70 : 0.06628631055355072
Loss at iteration 80 : 0.08677038550376892
Loss at iteration 90 : 0.08189094811677933
Loss at iteration 100 : 0.09759218990802765
Loss at iteration 110 : 0.06374908983707428
Loss at iteration 120 : 0.08300008624792099
Loss at iteration 130 : 0.05732615292072296
Loss at iteration 140 : 0.09545616805553436
Loss at iteration 150 : 0.09448178857564926
Loss at iteration 160 : 0.08160580694675446
Loss at iteration 170 : 0.08138030022382736
Loss at iteration 180 : 0.1136855036020279
Loss at iteration 190 : 0.06972860544919968
Loss at iteration 200 : 0.09652423858642578
Loss at iteration 210 : 0.11056196689605713
Loss at iteration 220 : 0.05504366010427475
Loss at iteration 230 : 0.05028226971626282
Loss at iteration 240 : 0.06193489953875542
Loss at iteration 250 : 0.09135865420103073
Loss at iteration 260 : 0.1085241287946701
Loss at iteration 270 : 0.06130843609571457
Loss at iteration 280 : 0.09511571377515793
Loss at iteration 290 : 0.08382099866867065
Loss at iteration 300 : 0.08653166145086288
Loss at iteration 310 : 0.05537787824869156
Loss at iteration 320 : 0.07679462432861328
Loss at iteration 330 : 0.08508621901273727
Loss at iteration 340 : 0.08109791576862335
Loss at iteration 350 : 0.06704966723918915
Loss at iteration 360 : 0.06655161827802658
Loss at iteration 370 : 0.06990348547697067
Loss at iteration 380 : 0.08132314682006836
Loss at iteration 390 : 0.07562966644763947
Loss at iteration 400 : 0.1237436830997467
Loss at iteration 410 : 0.07756602019071579
Loss at iteration 420 : 0.12459384649991989
Loss at iteration 430 : 0.08765914291143417
Loss at iteration 440 : 0.11823809146881104
Loss at iteration 450 : 0.11125248670578003
Loss at iteration 460 : 0.06697916239500046
Loss at iteration 470 : 0.08667724579572678
Loss at iteration 480 : 0.09032326936721802
Loss at iteration 490 : 0.08166896551847458
Loss at iteration 500 : 0.06541681289672852
Loss at iteration 510 : 0.08631499856710434
Loss at iteration 520 : 0.09139546006917953
Loss at iteration 530 : 0.07636314630508423
Loss at iteration 540 : 0.06726404279470444
Loss at iteration 550 : 0.08857183903455734
Loss at iteration 560 : 0.06731747835874557
Loss at iteration 570 : 0.07228311896324158
Loss at iteration 580 : 0.1162010133266449
Loss at iteration 590 : 0.08033637702465057
Loss at iteration 600 : 0.10485639423131943
Loss at iteration 610 : 0.09181012958288193
Loss at iteration 620 : 0.08759264647960663
Loss at iteration 630 : 0.07924720644950867
Loss at iteration 640 : 0.10446535795927048
Loss at iteration 650 : 0.1122695654630661
Loss at iteration 660 : 0.09733843803405762
Loss at iteration 670 : 0.08427141606807709
Loss at iteration 680 : 0.07223857194185257
Loss at iteration 690 : 0.05150241777300835
Loss at iteration 700 : 0.1052061915397644
Loss at iteration 710 : 0.08271589875221252
Loss at iteration 720 : 0.05216425657272339
Loss at iteration 730 : 0.08750762790441513
Loss at iteration 740 : 0.06790024042129517
Loss at iteration 750 : 0.0945812463760376
Loss at iteration 760 : 0.06243624538183212
Loss at iteration 770 : 0.1369692087173462
Loss at iteration 780 : 0.0610642284154892
Loss at iteration 790 : 0.1049632877111435
Loss at iteration 800 : 0.0550374835729599
Loss at iteration 810 : 0.0887073278427124
Loss at iteration 820 : 0.06691036373376846
Loss at iteration 830 : 0.12644916772842407
Loss at iteration 840 : 0.09443694353103638
Loss at iteration 850 : 0.10279585421085358
Loss at iteration 860 : 0.09191803634166718
Loss at iteration 870 : 0.054767489433288574
Loss at iteration 880 : 0.09474527090787888
Loss at iteration 890 : 0.10169710963964462
Loss at iteration 900 : 0.08281653374433517
Loss at iteration 910 : 0.11370231956243515
Loss at iteration 920 : 0.10166119039058685
Loss at iteration 930 : 0.06378307938575745
Loss at iteration 940 : 0.07427108287811279
Loss at iteration 950 : 0.10075579583644867
Loss at iteration 960 : 0.09950592368841171
Loss at iteration 970 : 0.06247377395629883
Loss at iteration 980 : 0.10479897260665894
Loss at iteration 990 : 0.11751168966293335
Loss at iteration 1000 : 0.09973761439323425
Loss at iteration 1010 : 0.09923164546489716
Loss at iteration 1020 : 0.08176868408918381
Loss at iteration 1030 : 0.0680389329791069
Loss at iteration 1040 : 0.06403003633022308
Loss at iteration 1050 : 0.04752068966627121
Loss at iteration 1060 : 0.06727789342403412
Loss at iteration 1070 : 0.10567444562911987
Loss at iteration 1080 : 0.07654567807912827
Loss at iteration 1090 : 0.13290899991989136
Loss at iteration 1100 : 0.08509733527898788
Loss at iteration 1110 : 0.08584451675415039
Loss at iteration 1120 : 0.09054474532604218
Loss at iteration 1130 : 0.05554578825831413
Loss at iteration 1140 : 0.07013329863548279
Loss at iteration 1150 : 0.05476534366607666
Loss at iteration 1160 : 0.07653262466192245
Loss at iteration 1170 : 0.07710835337638855
Loss at iteration 1180 : 0.11945490539073944
Loss at iteration 1190 : 0.09945043176412582
Loss at iteration 1200 : 0.09854668378829956
Loss at iteration 1210 : 0.09808491915464401
The SSIM Value is: 0.7229575157165528
The PSNR Value is: 20.65467274983724
the epoch is: 184
Loss at iteration 10 : 0.08928510546684265
Loss at iteration 20 : 0.11100602149963379
Loss at iteration 30 : 0.11414188146591187
Loss at iteration 40 : 0.07552716135978699
Loss at iteration 50 : 0.11638180911540985
Loss at iteration 60 : 0.1039169654250145
Loss at iteration 70 : 0.08133058249950409
Loss at iteration 80 : 0.1590479016304016
Loss at iteration 90 : 0.10780660808086395
Loss at iteration 100 : 0.06795039772987366
Loss at iteration 110 : 0.09833838790655136
Loss at iteration 120 : 0.0749632939696312
Loss at iteration 130 : 0.057357095181941986
Loss at iteration 140 : 0.08741454780101776
Loss at iteration 150 : 0.07134480774402618
Loss at iteration 160 : 0.08777787536382675
Loss at iteration 170 : 0.09388720989227295
Loss at iteration 180 : 0.1370936781167984
Loss at iteration 190 : 0.11376240849494934
Loss at iteration 200 : 0.10334724187850952
Loss at iteration 210 : 0.07962879538536072
Loss at iteration 220 : 0.0829969048500061
Loss at iteration 230 : 0.04381326958537102
Loss at iteration 240 : 0.0906054824590683
Loss at iteration 250 : 0.07067114859819412
Loss at iteration 260 : 0.09366364777088165
Loss at iteration 270 : 0.08890923112630844
Loss at iteration 280 : 0.10023709386587143
Loss at iteration 290 : 0.14200934767723083
Loss at iteration 300 : 0.047241806983947754
Loss at iteration 310 : 0.05320372432470322
Loss at iteration 320 : 0.05021221563220024
Loss at iteration 330 : 0.10587368905544281
Loss at iteration 340 : 0.11732121556997299
Loss at iteration 350 : 0.0875779315829277
Loss at iteration 360 : 0.09359493106603622
Loss at iteration 370 : 0.08929326385259628
Loss at iteration 380 : 0.06630011647939682
Loss at iteration 390 : 0.06881637871265411
Loss at iteration 400 : 0.11761584132909775
Loss at iteration 410 : 0.13123445212841034
Loss at iteration 420 : 0.05311542749404907
Loss at iteration 430 : 0.10897382348775864
Loss at iteration 440 : 0.05304757505655289
Loss at iteration 450 : 0.1264490932226181
Loss at iteration 460 : 0.10114803910255432
Loss at iteration 470 : 0.041795432567596436
Loss at iteration 480 : 0.11044245213270187
Loss at iteration 490 : 0.067117840051651
Loss at iteration 500 : 0.05468834936618805
Loss at iteration 510 : 0.07763265073299408
Loss at iteration 520 : 0.08444308489561081
Loss at iteration 530 : 0.07836829125881195
Loss at iteration 540 : 0.059503551572561264
Loss at iteration 550 : 0.0367555096745491
Loss at iteration 560 : 0.0749654620885849
Loss at iteration 570 : 0.08156692236661911
Loss at iteration 580 : 0.04631081223487854
Loss at iteration 590 : 0.07586865872144699
Loss at iteration 600 : 0.10607817023992538
Loss at iteration 610 : 0.07530251890420914
Loss at iteration 620 : 0.08426427096128464
Loss at iteration 630 : 0.12800584733486176
Loss at iteration 640 : 0.08140869438648224
Loss at iteration 650 : 0.08628340065479279
Loss at iteration 660 : 0.08162879943847656
Loss at iteration 670 : 0.08796161413192749
Loss at iteration 680 : 0.10177715867757797
Loss at iteration 690 : 0.07689179480075836
Loss at iteration 700 : 0.12180230021476746
Loss at iteration 710 : 0.07831291854381561
Loss at iteration 720 : 0.11204344034194946
Loss at iteration 730 : 0.132106214761734
Loss at iteration 740 : 0.07284706830978394
Loss at iteration 750 : 0.050218772143125534
Loss at iteration 760 : 0.13288718461990356
Loss at iteration 770 : 0.08477084338665009
Loss at iteration 780 : 0.09618737548589706
Loss at iteration 790 : 0.07789847999811172
Loss at iteration 800 : 0.07797324657440186
Loss at iteration 810 : 0.07201182842254639
Loss at iteration 820 : 0.08375098556280136
Loss at iteration 830 : 0.1265062540769577
Loss at iteration 840 : 0.08142763376235962
Loss at iteration 850 : 0.05928530544042587
Loss at iteration 860 : 0.07404318451881409
Loss at iteration 870 : 0.07634513825178146
Loss at iteration 880 : 0.0773659273982048
Loss at iteration 890 : 0.07177352905273438
Loss at iteration 900 : 0.08290117979049683
Loss at iteration 910 : 0.036413803696632385
Loss at iteration 920 : 0.07857620716094971
Loss at iteration 930 : 0.06533784419298172
Loss at iteration 940 : 0.08355511724948883
Loss at iteration 950 : 0.0680600255727768
Loss at iteration 960 : 0.09140186011791229
Loss at iteration 970 : 0.07067616283893585
Loss at iteration 980 : 0.08475551009178162
Loss at iteration 990 : 0.05292719230055809
Loss at iteration 1000 : 0.09105126559734344
Loss at iteration 1010 : 0.09572353214025497
Loss at iteration 1020 : 0.05862096697092056
Loss at iteration 1030 : 0.12617449462413788
Loss at iteration 1040 : 0.09969006478786469
Loss at iteration 1050 : 0.11084583401679993
Loss at iteration 1060 : 0.08575962483882904
Loss at iteration 1070 : 0.06109323352575302
Loss at iteration 1080 : 0.06710776686668396
Loss at iteration 1090 : 0.04532536119222641
Loss at iteration 1100 : 0.05935049057006836
Loss at iteration 1110 : 0.11806156486272812
Loss at iteration 1120 : 0.08793836832046509
Loss at iteration 1130 : 0.07110419869422913
Loss at iteration 1140 : 0.09661425650119781
Loss at iteration 1150 : 0.05450192093849182
Loss at iteration 1160 : 0.1551820933818817
Loss at iteration 1170 : 0.0968606099486351
Loss at iteration 1180 : 0.07172441482543945
Loss at iteration 1190 : 0.09552901238203049
Loss at iteration 1200 : 0.0835375189781189
Loss at iteration 1210 : 0.07016505300998688
The SSIM Value is: 0.7211563110351562
The PSNR Value is: 20.89429702758789
the epoch is: 185
Loss at iteration 10 : 0.1090729758143425
Loss at iteration 20 : 0.10461956262588501
Loss at iteration 30 : 0.08169616758823395
Loss at iteration 40 : 0.053832441568374634
Loss at iteration 50 : 0.08251361548900604
Loss at iteration 60 : 0.09105449169874191
Loss at iteration 70 : 0.07319412380456924
Loss at iteration 80 : 0.09725522249937057
Loss at iteration 90 : 0.06651323288679123
Loss at iteration 100 : 0.053286295384168625
Loss at iteration 110 : 0.08196349442005157
Loss at iteration 120 : 0.06808723509311676
Loss at iteration 130 : 0.0802585631608963
Loss at iteration 140 : 0.09676648676395416
Loss at iteration 150 : 0.10599856823682785
Loss at iteration 160 : 0.07992903888225555
Loss at iteration 170 : 0.09185302257537842
Loss at iteration 180 : 0.09801849722862244
Loss at iteration 190 : 0.07841687649488449
Loss at iteration 200 : 0.09300677478313446
Loss at iteration 210 : 0.053947072476148605
Loss at iteration 220 : 0.0728643387556076
Loss at iteration 230 : 0.11249949783086777
Loss at iteration 240 : 0.08872149884700775
Loss at iteration 250 : 0.07627412676811218
Loss at iteration 260 : 0.05886712670326233
Loss at iteration 270 : 0.08530037850141525
Loss at iteration 280 : 0.08592282235622406
Loss at iteration 290 : 0.07677938044071198
Loss at iteration 300 : 0.049254320561885834
Loss at iteration 310 : 0.06231766939163208
Loss at iteration 320 : 0.05828256160020828
Loss at iteration 330 : 0.10491602122783661
Loss at iteration 340 : 0.07016526162624359
Loss at iteration 350 : 0.0855800211429596
Loss at iteration 360 : 0.055069439113140106
Loss at iteration 370 : 0.07102476805448532
Loss at iteration 380 : 0.09508346021175385
Loss at iteration 390 : 0.09134967625141144
Loss at iteration 400 : 0.06493416428565979
Loss at iteration 410 : 0.07423745095729828
Loss at iteration 420 : 0.09559386223554611
Loss at iteration 430 : 0.08050550520420074
Loss at iteration 440 : 0.09364351630210876
Loss at iteration 450 : 0.13191887736320496
Loss at iteration 460 : 0.0832163468003273
Loss at iteration 470 : 0.11069836467504501
Loss at iteration 480 : 0.08433934301137924
Loss at iteration 490 : 0.12894439697265625
Loss at iteration 500 : 0.0609511062502861
Loss at iteration 510 : 0.08541519939899445
Loss at iteration 520 : 0.06703982502222061
Loss at iteration 530 : 0.10077430307865143
Loss at iteration 540 : 0.08970030397176743
Loss at iteration 550 : 0.0943998321890831
Loss at iteration 560 : 0.07520681619644165
Loss at iteration 570 : 0.07089963555335999
Loss at iteration 580 : 0.08574270457029343
Loss at iteration 590 : 0.05467410013079643
Loss at iteration 600 : 0.0501348078250885
Loss at iteration 610 : 0.07134753465652466
Loss at iteration 620 : 0.11896049976348877
Loss at iteration 630 : 0.043397873640060425
Loss at iteration 640 : 0.07799047231674194
Loss at iteration 650 : 0.05943265184760094
Loss at iteration 660 : 0.0678890198469162
Loss at iteration 670 : 0.0570206344127655
Loss at iteration 680 : 0.08833278715610504
Loss at iteration 690 : 0.05065126344561577
Loss at iteration 700 : 0.06496363878250122
Loss at iteration 710 : 0.07581478357315063
Loss at iteration 720 : 0.05775018781423569
Loss at iteration 730 : 0.05512865632772446
Loss at iteration 740 : 0.06387391686439514
Loss at iteration 750 : 0.08996140956878662
Loss at iteration 760 : 0.04299570247530937
Loss at iteration 770 : 0.07762181013822556
Loss at iteration 780 : 0.11662157624959946
Loss at iteration 790 : 0.08054934442043304
Loss at iteration 800 : 0.11798512190580368
Loss at iteration 810 : 0.0569448284804821
Loss at iteration 820 : 0.07823163270950317
Loss at iteration 830 : 0.09847943484783173
Loss at iteration 840 : 0.10220909118652344
Loss at iteration 850 : 0.09573005139827728
Loss at iteration 860 : 0.07516886293888092
Loss at iteration 870 : 0.07369616627693176
Loss at iteration 880 : 0.12435978651046753
Loss at iteration 890 : 0.09399253129959106
Loss at iteration 900 : 0.043540336191654205
Loss at iteration 910 : 0.0742064118385315
Loss at iteration 920 : 0.05918557569384575
Loss at iteration 930 : 0.06813099980354309
Loss at iteration 940 : 0.056985072791576385
Loss at iteration 950 : 0.09708523750305176
Loss at iteration 960 : 0.0654965490102768
Loss at iteration 970 : 0.12215858697891235
Loss at iteration 980 : 0.04898642748594284
Loss at iteration 990 : 0.06342150270938873
Loss at iteration 1000 : 0.04339950531721115
Loss at iteration 1010 : 0.0769471526145935
Loss at iteration 1020 : 0.07564108818769455
Loss at iteration 1030 : 0.08498211950063705
Loss at iteration 1040 : 0.050651371479034424
Loss at iteration 1050 : 0.10327732563018799
Loss at iteration 1060 : 0.07671434432268143
Loss at iteration 1070 : 0.09463205188512802
Loss at iteration 1080 : 0.11206266283988953
Loss at iteration 1090 : 0.07912179082632065
Loss at iteration 1100 : 0.0903206467628479
Loss at iteration 1110 : 0.06134135648608208
Loss at iteration 1120 : 0.09009242057800293
Loss at iteration 1130 : 0.07966901361942291
Loss at iteration 1140 : 0.0738048404455185
Loss at iteration 1150 : 0.08161285519599915
Loss at iteration 1160 : 0.06693082302808762
Loss at iteration 1170 : 0.09860074520111084
Loss at iteration 1180 : 0.06872326880693436
Loss at iteration 1190 : 0.08181469887495041
Loss at iteration 1200 : 0.06596346199512482
Loss at iteration 1210 : 0.08565942943096161
The SSIM Value is: 0.7248538533846537
The PSNR Value is: 20.91645876566569
the epoch is: 186
Loss at iteration 10 : 0.09363925457000732
Loss at iteration 20 : 0.0612863264977932
Loss at iteration 30 : 0.08846861124038696
Loss at iteration 40 : 0.06618401408195496
Loss at iteration 50 : 0.07416902482509613
Loss at iteration 60 : 0.08390063047409058
Loss at iteration 70 : 0.09107716381549835
Loss at iteration 80 : 0.08092179894447327
Loss at iteration 90 : 0.05024559795856476
Loss at iteration 100 : 0.04948046803474426
Loss at iteration 110 : 0.10251587629318237
Loss at iteration 120 : 0.03727627545595169
Loss at iteration 130 : 0.07586638629436493
Loss at iteration 140 : 0.08704440295696259
Loss at iteration 150 : 0.08466508239507675
Loss at iteration 160 : 0.056037984788417816
Loss at iteration 170 : 0.10426586121320724
Loss at iteration 180 : 0.06787985563278198
Loss at iteration 190 : 0.07163763791322708
Loss at iteration 200 : 0.08381134271621704
Loss at iteration 210 : 0.08147379755973816
Loss at iteration 220 : 0.12218174338340759
Loss at iteration 230 : 0.0489824116230011
Loss at iteration 240 : 0.06616911292076111
Loss at iteration 250 : 0.06525862216949463
Loss at iteration 260 : 0.11786527931690216
Loss at iteration 270 : 0.07131614536046982
Loss at iteration 280 : 0.10486367344856262
Loss at iteration 290 : 0.13308686017990112
Loss at iteration 300 : 0.12094360589981079
Loss at iteration 310 : 0.06619281321763992
Loss at iteration 320 : 0.07138833403587341
Loss at iteration 330 : 0.10110773891210556
Loss at iteration 340 : 0.06959033757448196
Loss at iteration 350 : 0.07028929889202118
Loss at iteration 360 : 0.05239700525999069
Loss at iteration 370 : 0.11580073088407516
Loss at iteration 380 : 0.10439901798963547
Loss at iteration 390 : 0.08355730772018433
Loss at iteration 400 : 0.06376028060913086
Loss at iteration 410 : 0.0791240781545639
Loss at iteration 420 : 0.1272251307964325
Loss at iteration 430 : 0.09677001088857651
Loss at iteration 440 : 0.09048660844564438
Loss at iteration 450 : 0.09675370156764984
Loss at iteration 460 : 0.08531628549098969
Loss at iteration 470 : 0.05439937114715576
Loss at iteration 480 : 0.08932610601186752
Loss at iteration 490 : 0.08609656989574432
Loss at iteration 500 : 0.06652015447616577
Loss at iteration 510 : 0.10073095560073853
Loss at iteration 520 : 0.06408178061246872
Loss at iteration 530 : 0.07607168704271317
Loss at iteration 540 : 0.08822678029537201
Loss at iteration 550 : 0.09047429263591766
Loss at iteration 560 : 0.05129578709602356
Loss at iteration 570 : 0.08181913197040558
Loss at iteration 580 : 0.10486989468336105
Loss at iteration 590 : 0.09335897862911224
Loss at iteration 600 : 0.0934935212135315
Loss at iteration 610 : 0.15867480635643005
Loss at iteration 620 : 0.04966649040579796
Loss at iteration 630 : 0.07017019391059875
Loss at iteration 640 : 0.06518204510211945
Loss at iteration 650 : 0.07403916120529175
Loss at iteration 660 : 0.07062601298093796
Loss at iteration 670 : 0.04218523949384689
Loss at iteration 680 : 0.11889618635177612
Loss at iteration 690 : 0.10376610606908798
Loss at iteration 700 : 0.052788786590099335
Loss at iteration 710 : 0.10197021812200546
Loss at iteration 720 : 0.05507349222898483
Loss at iteration 730 : 0.09076762199401855
Loss at iteration 740 : 0.0689862072467804
Loss at iteration 750 : 0.06324058771133423
Loss at iteration 760 : 0.08210982382297516
Loss at iteration 770 : 0.09140896797180176
Loss at iteration 780 : 0.08544515073299408
Loss at iteration 790 : 0.13340513408184052
Loss at iteration 800 : 0.13033698499202728
Loss at iteration 810 : 0.11112450063228607
Loss at iteration 820 : 0.062121883034706116
Loss at iteration 830 : 0.09085335582494736
Loss at iteration 840 : 0.09289561212062836
Loss at iteration 850 : 0.09378288686275482
Loss at iteration 860 : 0.08307737112045288
Loss at iteration 870 : 0.06914232671260834
Loss at iteration 880 : 0.08676449954509735
Loss at iteration 890 : 0.06054788455367088
Loss at iteration 900 : 0.11829879134893417
Loss at iteration 910 : 0.07715727388858795
Loss at iteration 920 : 0.07004614919424057
Loss at iteration 930 : 0.09370215982198715
Loss at iteration 940 : 0.06872592866420746
Loss at iteration 950 : 0.05824781581759453
Loss at iteration 960 : 0.08962821960449219
Loss at iteration 970 : 0.04891052097082138
Loss at iteration 980 : 0.09871990978717804
Loss at iteration 990 : 0.10286691039800644
Loss at iteration 1000 : 0.07263205200433731
Loss at iteration 1010 : 0.11591821163892746
Loss at iteration 1020 : 0.06697826087474823
Loss at iteration 1030 : 0.07980188727378845
Loss at iteration 1040 : 0.06755364686250687
Loss at iteration 1050 : 0.08111356198787689
Loss at iteration 1060 : 0.11141405999660492
Loss at iteration 1070 : 0.10150361061096191
Loss at iteration 1080 : 0.1303527057170868
Loss at iteration 1090 : 0.053979650139808655
Loss at iteration 1100 : 0.07017309963703156
Loss at iteration 1110 : 0.07285469770431519
Loss at iteration 1120 : 0.09819385409355164
Loss at iteration 1130 : 0.04552999138832092
Loss at iteration 1140 : 0.08706408739089966
Loss at iteration 1150 : 0.11926226317882538
Loss at iteration 1160 : 0.06370250880718231
Loss at iteration 1170 : 0.042957719415426254
Loss at iteration 1180 : 0.07434216141700745
Loss at iteration 1190 : 0.14572671055793762
Loss at iteration 1200 : 0.12577126920223236
Loss at iteration 1210 : 0.08367510885000229
The SSIM Value is: 0.7080745081106822
The PSNR Value is: 20.025642903645835
the epoch is: 187
Loss at iteration 10 : 0.08753232657909393
Loss at iteration 20 : 0.07855819165706635
Loss at iteration 30 : 0.07371827960014343
Loss at iteration 40 : 0.07846381515264511
Loss at iteration 50 : 0.10318105667829514
Loss at iteration 60 : 0.05569690465927124
Loss at iteration 70 : 0.05408073589205742
Loss at iteration 80 : 0.08502991497516632
Loss at iteration 90 : 0.09009389579296112
Loss at iteration 100 : 0.10615461319684982
Loss at iteration 110 : 0.07828088104724884
Loss at iteration 120 : 0.08821724355220795
Loss at iteration 130 : 0.14296984672546387
Loss at iteration 140 : 0.05919446796178818
Loss at iteration 150 : 0.03553897887468338
Loss at iteration 160 : 0.08864472806453705
Loss at iteration 170 : 0.11456184089183807
Loss at iteration 180 : 0.08677099645137787
Loss at iteration 190 : 0.08291088044643402
Loss at iteration 200 : 0.04366278275847435
Loss at iteration 210 : 0.0686294436454773
Loss at iteration 220 : 0.061055999249219894
Loss at iteration 230 : 0.07519728690385818
Loss at iteration 240 : 0.07593755424022675
Loss at iteration 250 : 0.08590148389339447
Loss at iteration 260 : 0.07376071810722351
Loss at iteration 270 : 0.09832960367202759
Loss at iteration 280 : 0.10482043027877808
Loss at iteration 290 : 0.06838910281658173
Loss at iteration 300 : 0.08947490155696869
Loss at iteration 310 : 0.10881732404232025
Loss at iteration 320 : 0.10987672209739685
Loss at iteration 330 : 0.1067386120557785
Loss at iteration 340 : 0.14867475628852844
Loss at iteration 350 : 0.1191163882613182
Loss at iteration 360 : 0.1083907037973404
Loss at iteration 370 : 0.09726126492023468
Loss at iteration 380 : 0.07677935063838959
Loss at iteration 390 : 0.07579603791236877
Loss at iteration 400 : 0.08092834055423737
Loss at iteration 410 : 0.08618910610675812
Loss at iteration 420 : 0.0792069062590599
Loss at iteration 430 : 0.11235947906970978
Loss at iteration 440 : 0.08920818567276001
Loss at iteration 450 : 0.0847635492682457
Loss at iteration 460 : 0.10222353786230087
Loss at iteration 470 : 0.10549255460500717
Loss at iteration 480 : 0.07258665561676025
Loss at iteration 490 : 0.11295561492443085
Loss at iteration 500 : 0.11720746755599976
Loss at iteration 510 : 0.12687265872955322
Loss at iteration 520 : 0.10959480702877045
Loss at iteration 530 : 0.07247981429100037
Loss at iteration 540 : 0.05536144971847534
Loss at iteration 550 : 0.05902384966611862
Loss at iteration 560 : 0.04168624430894852
Loss at iteration 570 : 0.051983729004859924
Loss at iteration 580 : 0.0863262265920639
Loss at iteration 590 : 0.08021239936351776
Loss at iteration 600 : 0.12658372521400452
Loss at iteration 610 : 0.09305928647518158
Loss at iteration 620 : 0.07443125545978546
Loss at iteration 630 : 0.10076795518398285
Loss at iteration 640 : 0.1015423834323883
Loss at iteration 650 : 0.07316701114177704
Loss at iteration 660 : 0.08155175298452377
Loss at iteration 670 : 0.05752568691968918
Loss at iteration 680 : 0.0970495194196701
Loss at iteration 690 : 0.07251228392124176
Loss at iteration 700 : 0.09561102092266083
Loss at iteration 710 : 0.06045827269554138
Loss at iteration 720 : 0.09375669807195663
Loss at iteration 730 : 0.050729505717754364
Loss at iteration 740 : 0.07181713730096817
Loss at iteration 750 : 0.07118381559848785
Loss at iteration 760 : 0.06464846432209015
Loss at iteration 770 : 0.10152363777160645
Loss at iteration 780 : 0.08769102394580841
Loss at iteration 790 : 0.06003248691558838
Loss at iteration 800 : 0.06291788816452026
Loss at iteration 810 : 0.05570325255393982
Loss at iteration 820 : 0.07772070169448853
Loss at iteration 830 : 0.07939858734607697
Loss at iteration 840 : 0.07742241770029068
Loss at iteration 850 : 0.05949508771300316
Loss at iteration 860 : 0.09016354382038116
Loss at iteration 870 : 0.08685211837291718
Loss at iteration 880 : 0.08324916660785675
Loss at iteration 890 : 0.07736272364854813
Loss at iteration 900 : 0.053822554647922516
Loss at iteration 910 : 0.06523938477039337
Loss at iteration 920 : 0.08194757997989655
Loss at iteration 930 : 0.056734077632427216
Loss at iteration 940 : 0.07728751003742218
Loss at iteration 950 : 0.09123723208904266
Loss at iteration 960 : 0.07326921075582504
Loss at iteration 970 : 0.1368284672498703
Loss at iteration 980 : 0.07367591559886932
Loss at iteration 990 : 0.04332829639315605
Loss at iteration 1000 : 0.099088653922081
Loss at iteration 1010 : 0.07400491833686829
Loss at iteration 1020 : 0.09709804505109787
Loss at iteration 1030 : 0.06287131458520889
Loss at iteration 1040 : 0.07711618393659592
Loss at iteration 1050 : 0.056979965418577194
Loss at iteration 1060 : 0.08052564412355423
Loss at iteration 1070 : 0.10062156617641449
Loss at iteration 1080 : 0.14579924941062927
Loss at iteration 1090 : 0.05197494477033615
Loss at iteration 1100 : 0.05022226274013519
Loss at iteration 1110 : 0.10189392417669296
Loss at iteration 1120 : 0.08832399547100067
Loss at iteration 1130 : 0.0517725944519043
Loss at iteration 1140 : 0.08324132114648819
Loss at iteration 1150 : 0.12834936380386353
Loss at iteration 1160 : 0.05223226174712181
Loss at iteration 1170 : 0.09536236524581909
Loss at iteration 1180 : 0.12225057929754257
Loss at iteration 1190 : 0.08915182203054428
Loss at iteration 1200 : 0.0669860914349556
Loss at iteration 1210 : 0.10576124489307404
The SSIM Value is: 0.7149839480717977
The PSNR Value is: 20.25213197072347
the epoch is: 188
Loss at iteration 10 : 0.09532210230827332
Loss at iteration 20 : 0.06298069655895233
Loss at iteration 30 : 0.0773567333817482
Loss at iteration 40 : 0.0624537393450737
Loss at iteration 50 : 0.11805801093578339
Loss at iteration 60 : 0.08823393285274506
Loss at iteration 70 : 0.055651724338531494
Loss at iteration 80 : 0.058479975908994675
Loss at iteration 90 : 0.1476794332265854
Loss at iteration 100 : 0.06690733134746552
Loss at iteration 110 : 0.08400338888168335
Loss at iteration 120 : 0.06548525393009186
Loss at iteration 130 : 0.11188393086194992
Loss at iteration 140 : 0.08898614346981049
Loss at iteration 150 : 0.07366976886987686
Loss at iteration 160 : 0.0855773538351059
Loss at iteration 170 : 0.06382419168949127
Loss at iteration 180 : 0.0641665980219841
Loss at iteration 190 : 0.06738905608654022
Loss at iteration 200 : 0.08816421031951904
Loss at iteration 210 : 0.07263137400150299
Loss at iteration 220 : 0.13221055269241333
Loss at iteration 230 : 0.08322954922914505
Loss at iteration 240 : 0.07730092108249664
Loss at iteration 250 : 0.047256968915462494
Loss at iteration 260 : 0.061856746673583984
Loss at iteration 270 : 0.054948657751083374
Loss at iteration 280 : 0.09982546418905258
Loss at iteration 290 : 0.07330282032489777
Loss at iteration 300 : 0.1215585321187973
Loss at iteration 310 : 0.10556656122207642
Loss at iteration 320 : 0.06777605414390564
Loss at iteration 330 : 0.074076808989048
Loss at iteration 340 : 0.10784747451543808
Loss at iteration 350 : 0.05722913146018982
Loss at iteration 360 : 0.07387664914131165
Loss at iteration 370 : 0.07018110156059265
Loss at iteration 380 : 0.10506972670555115
Loss at iteration 390 : 0.08278869092464447
Loss at iteration 400 : 0.07371446490287781
Loss at iteration 410 : 0.06696993112564087
Loss at iteration 420 : 0.10909511893987656
Loss at iteration 430 : 0.08803653717041016
Loss at iteration 440 : 0.09061658382415771
Loss at iteration 450 : 0.09295020997524261
Loss at iteration 460 : 0.10382246226072311
Loss at iteration 470 : 0.10081305354833603
Loss at iteration 480 : 0.08437060564756393
Loss at iteration 490 : 0.060477711260318756
Loss at iteration 500 : 0.06724031269550323
Loss at iteration 510 : 0.07734265923500061
Loss at iteration 520 : 0.07414749264717102
Loss at iteration 530 : 0.09627658128738403
Loss at iteration 540 : 0.07541077584028244
Loss at iteration 550 : 0.08535200357437134
Loss at iteration 560 : 0.05672687292098999
Loss at iteration 570 : 0.056953877210617065
Loss at iteration 580 : 0.0856972336769104
Loss at iteration 590 : 0.08289355039596558
Loss at iteration 600 : 0.05796352028846741
Loss at iteration 610 : 0.09746105968952179
Loss at iteration 620 : 0.06426355242729187
Loss at iteration 630 : 0.08942978084087372
Loss at iteration 640 : 0.09750693291425705
Loss at iteration 650 : 0.08598985522985458
Loss at iteration 660 : 0.0978270173072815
Loss at iteration 670 : 0.08388813585042953
Loss at iteration 680 : 0.12145785987377167
Loss at iteration 690 : 0.08938594162464142
Loss at iteration 700 : 0.08458711951971054
Loss at iteration 710 : 0.10451217740774155
Loss at iteration 720 : 0.08636987209320068
Loss at iteration 730 : 0.06289064884185791
Loss at iteration 740 : 0.07469968497753143
Loss at iteration 750 : 0.05063736066222191
Loss at iteration 760 : 0.08488108962774277
Loss at iteration 770 : 0.07478593289852142
Loss at iteration 780 : 0.0614594891667366
Loss at iteration 790 : 0.09846532344818115
Loss at iteration 800 : 0.10986731946468353
Loss at iteration 810 : 0.08373570442199707
Loss at iteration 820 : 0.09747347235679626
Loss at iteration 830 : 0.09951759874820709
Loss at iteration 840 : 0.08133413642644882
Loss at iteration 850 : 0.11448079347610474
Loss at iteration 860 : 0.08801069110631943
Loss at iteration 870 : 0.07458007335662842
Loss at iteration 880 : 0.14376041293144226
Loss at iteration 890 : 0.0843365490436554
Loss at iteration 900 : 0.14237993955612183
Loss at iteration 910 : 0.09753283113241196
Loss at iteration 920 : 0.07988382130861282
Loss at iteration 930 : 0.08360783755779266
Loss at iteration 940 : 0.09349921345710754
Loss at iteration 950 : 0.09772568941116333
Loss at iteration 960 : 0.05874235928058624
Loss at iteration 970 : 0.06692568957805634
Loss at iteration 980 : 0.0590689480304718
Loss at iteration 990 : 0.13932949304580688
Loss at iteration 1000 : 0.08598040044307709
Loss at iteration 1010 : 0.052913062274456024
Loss at iteration 1020 : 0.11033918708562851
Loss at iteration 1030 : 0.060212671756744385
Loss at iteration 1040 : 0.08737076818943024
Loss at iteration 1050 : 0.07320267707109451
Loss at iteration 1060 : 0.06219249218702316
Loss at iteration 1070 : 0.05754319950938225
Loss at iteration 1080 : 0.07378523796796799
Loss at iteration 1090 : 0.07451226562261581
Loss at iteration 1100 : 0.10830153524875641
Loss at iteration 1110 : 0.1146278828382492
Loss at iteration 1120 : 0.04618138074874878
Loss at iteration 1130 : 0.1280306577682495
Loss at iteration 1140 : 0.08252449333667755
Loss at iteration 1150 : 0.06150846928358078
Loss at iteration 1160 : 0.06500550359487534
Loss at iteration 1170 : 0.09659924358129501
Loss at iteration 1180 : 0.10285532474517822
Loss at iteration 1190 : 0.07241024821996689
Loss at iteration 1200 : 0.053886234760284424
Loss at iteration 1210 : 0.11789785325527191
The SSIM Value is: 0.717667039235433
The PSNR Value is: 20.804248046875
the epoch is: 189
Loss at iteration 10 : 0.08526943624019623
Loss at iteration 20 : 0.09383225440979004
Loss at iteration 30 : 0.06931636482477188
Loss at iteration 40 : 0.060316070914268494
Loss at iteration 50 : 0.04522107169032097
Loss at iteration 60 : 0.11092448234558105
Loss at iteration 70 : 0.0677269920706749
Loss at iteration 80 : 0.047878287732601166
Loss at iteration 90 : 0.043448060750961304
Loss at iteration 100 : 0.06640224903821945
Loss at iteration 110 : 0.08665087819099426
Loss at iteration 120 : 0.07364209741353989
Loss at iteration 130 : 0.046836934983730316
Loss at iteration 140 : 0.09409021586179733
Loss at iteration 150 : 0.10496368259191513
Loss at iteration 160 : 0.05505649745464325
Loss at iteration 170 : 0.07359547913074493
Loss at iteration 180 : 0.07954271137714386
Loss at iteration 190 : 0.07492648810148239
Loss at iteration 200 : 0.049640439450740814
Loss at iteration 210 : 0.08049742877483368
Loss at iteration 220 : 0.05339794605970383
Loss at iteration 230 : 0.06124214455485344
Loss at iteration 240 : 0.051817625761032104
Loss at iteration 250 : 0.0729893371462822
Loss at iteration 260 : 0.11459486186504364
Loss at iteration 270 : 0.11798160523176193
Loss at iteration 280 : 0.07872790843248367
Loss at iteration 290 : 0.0892052948474884
Loss at iteration 300 : 0.07575580477714539
Loss at iteration 310 : 0.07879380881786346
Loss at iteration 320 : 0.0614156536757946
Loss at iteration 330 : 0.07359852641820908
Loss at iteration 340 : 0.09563110768795013
Loss at iteration 350 : 0.08372749388217926
Loss at iteration 360 : 0.11800714582204819
Loss at iteration 370 : 0.0691455528140068
Loss at iteration 380 : 0.05840069055557251
Loss at iteration 390 : 0.07544855773448944
Loss at iteration 400 : 0.1102161705493927
Loss at iteration 410 : 0.09980075806379318
Loss at iteration 420 : 0.06612259149551392
Loss at iteration 430 : 0.1245553195476532
Loss at iteration 440 : 0.09142181277275085
Loss at iteration 450 : 0.05563577637076378
Loss at iteration 460 : 0.08094858378171921
Loss at iteration 470 : 0.09946303069591522
Loss at iteration 480 : 0.06408088654279709
Loss at iteration 490 : 0.09012234210968018
Loss at iteration 500 : 0.07179698348045349
Loss at iteration 510 : 0.06379131972789764
Loss at iteration 520 : 0.06259915232658386
Loss at iteration 530 : 0.08411240577697754
Loss at iteration 540 : 0.11035432666540146
Loss at iteration 550 : 0.1011182889342308
Loss at iteration 560 : 0.04775308817625046
Loss at iteration 570 : 0.040218427777290344
Loss at iteration 580 : 0.06967514753341675
Loss at iteration 590 : 0.0541573129594326
Loss at iteration 600 : 0.07384441792964935
Loss at iteration 610 : 0.09241336584091187
Loss at iteration 620 : 0.09245271980762482
Loss at iteration 630 : 0.08855848014354706
Loss at iteration 640 : 0.079790398478508
Loss at iteration 650 : 0.0426734983921051
Loss at iteration 660 : 0.07394153624773026
Loss at iteration 670 : 0.0428716316819191
Loss at iteration 680 : 0.07247257232666016
Loss at iteration 690 : 0.10552432388067245
Loss at iteration 700 : 0.07934115827083588
Loss at iteration 710 : 0.08599730581045151
Loss at iteration 720 : 0.09235914796590805
Loss at iteration 730 : 0.09405597299337387
Loss at iteration 740 : 0.12484538555145264
Loss at iteration 750 : 0.11377035826444626
Loss at iteration 760 : 0.04565158113837242
Loss at iteration 770 : 0.09125866740942001
Loss at iteration 780 : 0.08745568245649338
Loss at iteration 790 : 0.1153310239315033
Loss at iteration 800 : 0.05564902350306511
Loss at iteration 810 : 0.08221514523029327
Loss at iteration 820 : 0.10343074798583984
Loss at iteration 830 : 0.11467917263507843
Loss at iteration 840 : 0.10487638413906097
Loss at iteration 850 : 0.05224286764860153
Loss at iteration 860 : 0.08871962875127792
Loss at iteration 870 : 0.060731466859579086
Loss at iteration 880 : 0.10668959468603134
Loss at iteration 890 : 0.06480827927589417
Loss at iteration 900 : 0.0998382568359375
Loss at iteration 910 : 0.12418633699417114
Loss at iteration 920 : 0.10733017325401306
Loss at iteration 930 : 0.04627610370516777
Loss at iteration 940 : 0.04795246943831444
Loss at iteration 950 : 0.04649980366230011
Loss at iteration 960 : 0.07316450774669647
Loss at iteration 970 : 0.05876871943473816
Loss at iteration 980 : 0.07744309306144714
Loss at iteration 990 : 0.08609277009963989
Loss at iteration 1000 : 0.06812646239995956
Loss at iteration 1010 : 0.102496437728405
Loss at iteration 1020 : 0.0667172446846962
Loss at iteration 1030 : 0.07020917534828186
Loss at iteration 1040 : 0.09012573957443237
Loss at iteration 1050 : 0.09202392399311066
Loss at iteration 1060 : 0.0505882129073143
Loss at iteration 1070 : 0.07256235927343369
Loss at iteration 1080 : 0.10094426572322845
Loss at iteration 1090 : 0.08656641095876694
Loss at iteration 1100 : 0.08149339258670807
Loss at iteration 1110 : 0.07359704375267029
Loss at iteration 1120 : 0.08510341495275497
Loss at iteration 1130 : 0.07876978814601898
Loss at iteration 1140 : 0.0849381685256958
Loss at iteration 1150 : 0.08577197790145874
Loss at iteration 1160 : 0.09365306794643402
Loss at iteration 1170 : 0.10683409869670868
Loss at iteration 1180 : 0.07512372732162476
Loss at iteration 1190 : 0.10073836892843246
Loss at iteration 1200 : 0.04965425282716751
Loss at iteration 1210 : 0.0817817971110344
The SSIM Value is: 0.727037521203359
The PSNR Value is: 21.279065704345705
the epoch is: 190
Loss at iteration 10 : 0.06513213366270065
Loss at iteration 20 : 0.08173549175262451
Loss at iteration 30 : 0.07721012085676193
Loss at iteration 40 : 0.09166436642408371
Loss at iteration 50 : 0.08530241250991821
Loss at iteration 60 : 0.0788125991821289
Loss at iteration 70 : 0.06635286659002304
Loss at iteration 80 : 0.0755021721124649
Loss at iteration 90 : 0.05345715209841728
Loss at iteration 100 : 0.07585366070270538
Loss at iteration 110 : 0.08671405166387558
Loss at iteration 120 : 0.06920628249645233
Loss at iteration 130 : 0.09128989279270172
Loss at iteration 140 : 0.06172614544630051
Loss at iteration 150 : 0.09722284972667694
Loss at iteration 160 : 0.16002342104911804
Loss at iteration 170 : 0.07546667754650116
Loss at iteration 180 : 0.09149694442749023
Loss at iteration 190 : 0.07531853765249252
Loss at iteration 200 : 0.06989984214305878
Loss at iteration 210 : 0.13021647930145264
Loss at iteration 220 : 0.07793858647346497
Loss at iteration 230 : 0.049503110349178314
Loss at iteration 240 : 0.06294836848974228
Loss at iteration 250 : 0.09566442668437958
Loss at iteration 260 : 0.058493517339229584
Loss at iteration 270 : 0.12838004529476166
Loss at iteration 280 : 0.09123514592647552
Loss at iteration 290 : 0.12948176264762878
Loss at iteration 300 : 0.060078494250774384
Loss at iteration 310 : 0.13151800632476807
Loss at iteration 320 : 0.05417231470346451
Loss at iteration 330 : 0.0953693613409996
Loss at iteration 340 : 0.11226817220449448
Loss at iteration 350 : 0.09886288642883301
Loss at iteration 360 : 0.10103203356266022
Loss at iteration 370 : 0.08557973802089691
Loss at iteration 380 : 0.07543478906154633
Loss at iteration 390 : 0.061035946011543274
Loss at iteration 400 : 0.0601118803024292
Loss at iteration 410 : 0.08038849383592606
Loss at iteration 420 : 0.05596480891108513
Loss at iteration 430 : 0.08434092998504639
Loss at iteration 440 : 0.0605267733335495
Loss at iteration 450 : 0.07696975767612457
Loss at iteration 460 : 0.06072613224387169
Loss at iteration 470 : 0.0703820213675499
Loss at iteration 480 : 0.08166521042585373
Loss at iteration 490 : 0.06638925522565842
Loss at iteration 500 : 0.0710265040397644
Loss at iteration 510 : 0.13018399477005005
Loss at iteration 520 : 0.06589189171791077
Loss at iteration 530 : 0.06159288436174393
Loss at iteration 540 : 0.08952057361602783
Loss at iteration 550 : 0.0777241587638855
Loss at iteration 560 : 0.11154311895370483
Loss at iteration 570 : 0.06500713527202606
Loss at iteration 580 : 0.06624194979667664
Loss at iteration 590 : 0.08284929394721985
Loss at iteration 600 : 0.04657410830259323
Loss at iteration 610 : 0.08927714824676514
Loss at iteration 620 : 0.06647976487874985
Loss at iteration 630 : 0.1076221689581871
Loss at iteration 640 : 0.08269452303647995
Loss at iteration 650 : 0.09283554553985596
Loss at iteration 660 : 0.12249629199504852
Loss at iteration 670 : 0.07115709781646729
Loss at iteration 680 : 0.08591268956661224
Loss at iteration 690 : 0.07092515379190445
Loss at iteration 700 : 0.09544884413480759
Loss at iteration 710 : 0.08737841248512268
Loss at iteration 720 : 0.12354060262441635
Loss at iteration 730 : 0.0695667713880539
Loss at iteration 740 : 0.06368058919906616
Loss at iteration 750 : 0.08678673207759857
Loss at iteration 760 : 0.0441863052546978
Loss at iteration 770 : 0.12923721969127655
Loss at iteration 780 : 0.08731818199157715
Loss at iteration 790 : 0.07240274548530579
Loss at iteration 800 : 0.055382441729307175
Loss at iteration 810 : 0.07798831909894943
Loss at iteration 820 : 0.10780763626098633
Loss at iteration 830 : 0.09464484453201294
Loss at iteration 840 : 0.05605059117078781
Loss at iteration 850 : 0.13184097409248352
Loss at iteration 860 : 0.059861958026885986
Loss at iteration 870 : 0.07852929830551147
Loss at iteration 880 : 0.07174690812826157
Loss at iteration 890 : 0.06667430698871613
Loss at iteration 900 : 0.07718055695295334
Loss at iteration 910 : 0.0745818242430687
Loss at iteration 920 : 0.09283451735973358
Loss at iteration 930 : 0.0867622122168541
Loss at iteration 940 : 0.15617084503173828
Loss at iteration 950 : 0.07093495875597
Loss at iteration 960 : 0.08584468066692352
Loss at iteration 970 : 0.10123973339796066
Loss at iteration 980 : 0.10665019601583481
Loss at iteration 990 : 0.09549860656261444
Loss at iteration 1000 : 0.08821504563093185
Loss at iteration 1010 : 0.07868267595767975
Loss at iteration 1020 : 0.08546151220798492
Loss at iteration 1030 : 0.0736507773399353
Loss at iteration 1040 : 0.06521862745285034
Loss at iteration 1050 : 0.08721804618835449
Loss at iteration 1060 : 0.0626550242304802
Loss at iteration 1070 : 0.05312705039978027
Loss at iteration 1080 : 0.13826459646224976
Loss at iteration 1090 : 0.08293086290359497
Loss at iteration 1100 : 0.06275248527526855
Loss at iteration 1110 : 0.060755617916584015
Loss at iteration 1120 : 0.09138166904449463
Loss at iteration 1130 : 0.09796267747879028
Loss at iteration 1140 : 0.07663284987211227
Loss at iteration 1150 : 0.07672005146741867
Loss at iteration 1160 : 0.0942235216498375
Loss at iteration 1170 : 0.09526632726192474
Loss at iteration 1180 : 0.08610256761312485
Loss at iteration 1190 : 0.056280385702848434
Loss at iteration 1200 : 0.0759933739900589
Loss at iteration 1210 : 0.09507310390472412
The SSIM Value is: 0.7205279032389323
The PSNR Value is: 20.74290262858073
the epoch is: 191
Loss at iteration 10 : 0.05923512205481529
Loss at iteration 20 : 0.05632328987121582
Loss at iteration 30 : 0.08711731433868408
Loss at iteration 40 : 0.1411609649658203
Loss at iteration 50 : 0.08340154588222504
Loss at iteration 60 : 0.07006802409887314
Loss at iteration 70 : 0.07468760013580322
Loss at iteration 80 : 0.05407536029815674
Loss at iteration 90 : 0.049637001007795334
Loss at iteration 100 : 0.08259034156799316
Loss at iteration 110 : 0.06364168226718903
Loss at iteration 120 : 0.09970590472221375
Loss at iteration 130 : 0.07715077698230743
Loss at iteration 140 : 0.07198770344257355
Loss at iteration 150 : 0.09278756380081177
Loss at iteration 160 : 0.11554938554763794
Loss at iteration 170 : 0.07082521170377731
Loss at iteration 180 : 0.10259472578763962
Loss at iteration 190 : 0.1166120395064354
Loss at iteration 200 : 0.07298735529184341
Loss at iteration 210 : 0.0601864829659462
Loss at iteration 220 : 0.06687598675489426
Loss at iteration 230 : 0.07220248878002167
Loss at iteration 240 : 0.08143667876720428
Loss at iteration 250 : 0.08741647005081177
Loss at iteration 260 : 0.09699257463216782
Loss at iteration 270 : 0.08933588117361069
Loss at iteration 280 : 0.07238885015249252
Loss at iteration 290 : 0.05399743840098381
Loss at iteration 300 : 0.10333485901355743
Loss at iteration 310 : 0.11461576819419861
Loss at iteration 320 : 0.09134843945503235
Loss at iteration 330 : 0.08362433314323425
Loss at iteration 340 : 0.0816861093044281
Loss at iteration 350 : 0.06931034475564957
Loss at iteration 360 : 0.06555191427469254
Loss at iteration 370 : 0.09175503253936768
Loss at iteration 380 : 0.11927376687526703
Loss at iteration 390 : 0.10280128568410873
Loss at iteration 400 : 0.14151866734027863
Loss at iteration 410 : 0.07982796430587769
Loss at iteration 420 : 0.08407062292098999
Loss at iteration 430 : 0.12225016951560974
Loss at iteration 440 : 0.08651136606931686
Loss at iteration 450 : 0.10370352119207382
Loss at iteration 460 : 0.107765793800354
Loss at iteration 470 : 0.0713522732257843
Loss at iteration 480 : 0.08597509562969208
Loss at iteration 490 : 0.05470990389585495
Loss at iteration 500 : 0.09101538360118866
Loss at iteration 510 : 0.06814737617969513
Loss at iteration 520 : 0.059399183839559555
Loss at iteration 530 : 0.06606163084506989
Loss at iteration 540 : 0.043581314384937286
Loss at iteration 550 : 0.06366180628538132
Loss at iteration 560 : 0.04998939856886864
Loss at iteration 570 : 0.11499696969985962
Loss at iteration 580 : 0.12459425628185272
Loss at iteration 590 : 0.08918774127960205
Loss at iteration 600 : 0.0679866299033165
Loss at iteration 610 : 0.08447396755218506
Loss at iteration 620 : 0.06764346361160278
Loss at iteration 630 : 0.05169277638196945
Loss at iteration 640 : 0.08115056157112122
Loss at iteration 650 : 0.08471020311117172
Loss at iteration 660 : 0.0835156962275505
Loss at iteration 670 : 0.044715724885463715
Loss at iteration 680 : 0.09101518243551254
Loss at iteration 690 : 0.06717731803655624
Loss at iteration 700 : 0.05147543177008629
Loss at iteration 710 : 0.06196175143122673
Loss at iteration 720 : 0.07190253585577011
Loss at iteration 730 : 0.0780768021941185
Loss at iteration 740 : 0.0728188157081604
Loss at iteration 750 : 0.07269086688756943
Loss at iteration 760 : 0.08273445069789886
Loss at iteration 770 : 0.07878841459751129
Loss at iteration 780 : 0.08852629363536835
Loss at iteration 790 : 0.07865622639656067
Loss at iteration 800 : 0.09961794316768646
Loss at iteration 810 : 0.046019405126571655
Loss at iteration 820 : 0.08365653455257416
Loss at iteration 830 : 0.09154655784368515
Loss at iteration 840 : 0.0644373893737793
Loss at iteration 850 : 0.12289601564407349
Loss at iteration 860 : 0.0880897268652916
Loss at iteration 870 : 0.06220019981265068
Loss at iteration 880 : 0.07658382505178452
Loss at iteration 890 : 0.06907098740339279
Loss at iteration 900 : 0.0797562301158905
Loss at iteration 910 : 0.06608524173498154
Loss at iteration 920 : 0.09259753674268723
Loss at iteration 930 : 0.0915178507566452
Loss at iteration 940 : 0.05544527992606163
Loss at iteration 950 : 0.06835514307022095
Loss at iteration 960 : 0.08741723746061325
Loss at iteration 970 : 0.05618491396307945
Loss at iteration 980 : 0.0790092945098877
Loss at iteration 990 : 0.10006790608167648
Loss at iteration 1000 : 0.06715019047260284
Loss at iteration 1010 : 0.07210962474346161
Loss at iteration 1020 : 0.14796796441078186
Loss at iteration 1030 : 0.06623069941997528
Loss at iteration 1040 : 0.10463651269674301
Loss at iteration 1050 : 0.08548928797245026
Loss at iteration 1060 : 0.07634354382753372
Loss at iteration 1070 : 0.05341072380542755
Loss at iteration 1080 : 0.10195031017065048
Loss at iteration 1090 : 0.0889555811882019
Loss at iteration 1100 : 0.10381319373846054
Loss at iteration 1110 : 0.10534721612930298
Loss at iteration 1120 : 0.06942226737737656
Loss at iteration 1130 : 0.08266180753707886
Loss at iteration 1140 : 0.06410565972328186
Loss at iteration 1150 : 0.06494074314832687
Loss at iteration 1160 : 0.11197959631681442
Loss at iteration 1170 : 0.06762991845607758
Loss at iteration 1180 : 0.09279559552669525
Loss at iteration 1190 : 0.09503789246082306
Loss at iteration 1200 : 0.06295102834701538
Loss at iteration 1210 : 0.10006897151470184
The SSIM Value is: 0.7223100503285725
The PSNR Value is: 20.4698637008667
the epoch is: 192
Loss at iteration 10 : 0.07561080157756805
Loss at iteration 20 : 0.060181405395269394
Loss at iteration 30 : 0.06698304414749146
Loss at iteration 40 : 0.0989387035369873
Loss at iteration 50 : 0.10489340126514435
Loss at iteration 60 : 0.08650024235248566
Loss at iteration 70 : 0.10984648764133453
Loss at iteration 80 : 0.10406184196472168
Loss at iteration 90 : 0.07497885078191757
Loss at iteration 100 : 0.09481686353683472
Loss at iteration 110 : 0.07470707595348358
Loss at iteration 120 : 0.11242946982383728
Loss at iteration 130 : 0.11866860091686249
Loss at iteration 140 : 0.06748385727405548
Loss at iteration 150 : 0.047032635658979416
Loss at iteration 160 : 0.07916143536567688
Loss at iteration 170 : 0.04947163909673691
Loss at iteration 180 : 0.08778803050518036
Loss at iteration 190 : 0.06299380958080292
Loss at iteration 200 : 0.06259819865226746
Loss at iteration 210 : 0.05231494829058647
Loss at iteration 220 : 0.06591985374689102
Loss at iteration 230 : 0.08672241866588593
Loss at iteration 240 : 0.07836979627609253
Loss at iteration 250 : 0.095020592212677
Loss at iteration 260 : 0.06578537076711655
Loss at iteration 270 : 0.10015635192394257
Loss at iteration 280 : 0.10018857568502426
Loss at iteration 290 : 0.057586949318647385
Loss at iteration 300 : 0.0900215357542038
Loss at iteration 310 : 0.07583186030387878
Loss at iteration 320 : 0.10521934926509857
Loss at iteration 330 : 0.07524517178535461
Loss at iteration 340 : 0.08495916426181793
Loss at iteration 350 : 0.09060879051685333
Loss at iteration 360 : 0.11131905019283295
Loss at iteration 370 : 0.07359389960765839
Loss at iteration 380 : 0.1086915135383606
Loss at iteration 390 : 0.09876983612775803
Loss at iteration 400 : 0.04784686863422394
Loss at iteration 410 : 0.07521466910839081
Loss at iteration 420 : 0.09148383140563965
Loss at iteration 430 : 0.08118489384651184
Loss at iteration 440 : 0.09238617867231369
Loss at iteration 450 : 0.05295727774500847
Loss at iteration 460 : 0.07616163790225983
Loss at iteration 470 : 0.07184319198131561
Loss at iteration 480 : 0.04631149768829346
Loss at iteration 490 : 0.09474882483482361
Loss at iteration 500 : 0.06479455530643463
Loss at iteration 510 : 0.10687048733234406
Loss at iteration 520 : 0.06799076497554779
Loss at iteration 530 : 0.051504381000995636
Loss at iteration 540 : 0.06403006613254547
Loss at iteration 550 : 0.0839269608259201
Loss at iteration 560 : 0.05940253287553787
Loss at iteration 570 : 0.07472708076238632
Loss at iteration 580 : 0.09653522819280624
Loss at iteration 590 : 0.08753551542758942
Loss at iteration 600 : 0.08307724446058273
Loss at iteration 610 : 0.049875520169734955
Loss at iteration 620 : 0.10851170867681503
Loss at iteration 630 : 0.0561327263712883
Loss at iteration 640 : 0.09120039641857147
Loss at iteration 650 : 0.09844259917736053
Loss at iteration 660 : 0.07174083590507507
Loss at iteration 670 : 0.08169758319854736
Loss at iteration 680 : 0.10111632198095322
Loss at iteration 690 : 0.08848118782043457
Loss at iteration 700 : 0.12770096957683563
Loss at iteration 710 : 0.05713927745819092
Loss at iteration 720 : 0.08073210716247559
Loss at iteration 730 : 0.056058257818222046
Loss at iteration 740 : 0.07266485691070557
Loss at iteration 750 : 0.09670712798833847
Loss at iteration 760 : 0.08114305138587952
Loss at iteration 770 : 0.0770166665315628
Loss at iteration 780 : 0.08003510534763336
Loss at iteration 790 : 0.05536065623164177
Loss at iteration 800 : 0.045957088470458984
Loss at iteration 810 : 0.06898555159568787
Loss at iteration 820 : 0.08645351976156235
Loss at iteration 830 : 0.06450581550598145
Loss at iteration 840 : 0.059785448014736176
Loss at iteration 850 : 0.08674709498882294
Loss at iteration 860 : 0.0614122673869133
Loss at iteration 870 : 0.10423370450735092
Loss at iteration 880 : 0.07559467107057571
Loss at iteration 890 : 0.05339153856039047
Loss at iteration 900 : 0.09563981741666794
Loss at iteration 910 : 0.10031735897064209
Loss at iteration 920 : 0.07337258011102676
Loss at iteration 930 : 0.08146364241838455
Loss at iteration 940 : 0.10476355999708176
Loss at iteration 950 : 0.05526796728372574
Loss at iteration 960 : 0.0842372477054596
Loss at iteration 970 : 0.074176125228405
Loss at iteration 980 : 0.05036845803260803
Loss at iteration 990 : 0.10219395160675049
Loss at iteration 1000 : 0.07575889676809311
Loss at iteration 1010 : 0.07084742188453674
Loss at iteration 1020 : 0.08410309255123138
Loss at iteration 1030 : 0.09257492423057556
Loss at iteration 1040 : 0.0800366923213005
Loss at iteration 1050 : 0.07638111710548401
Loss at iteration 1060 : 0.06281296908855438
Loss at iteration 1070 : 0.1090342104434967
Loss at iteration 1080 : 0.1059245690703392
Loss at iteration 1090 : 0.10099998116493225
Loss at iteration 1100 : 0.06873714923858643
Loss at iteration 1110 : 0.06214464455842972
Loss at iteration 1120 : 0.11067898571491241
Loss at iteration 1130 : 0.08265126496553421
Loss at iteration 1140 : 0.07618115842342377
Loss at iteration 1150 : 0.12075857818126678
Loss at iteration 1160 : 0.06851112842559814
Loss at iteration 1170 : 0.09702329337596893
Loss at iteration 1180 : 0.07762885093688965
Loss at iteration 1190 : 0.08030635863542557
Loss at iteration 1200 : 0.12911206483840942
Loss at iteration 1210 : 0.08681803196668625
The SSIM Value is: 0.7199353496233623
The PSNR Value is: 20.54288012186686
the epoch is: 193
Loss at iteration 10 : 0.057614218443632126
Loss at iteration 20 : 0.05737105756998062
Loss at iteration 30 : 0.12560299038887024
Loss at iteration 40 : 0.1373848021030426
Loss at iteration 50 : 0.09141914546489716
Loss at iteration 60 : 0.06581812351942062
Loss at iteration 70 : 0.09169811010360718
Loss at iteration 80 : 0.06726226210594177
Loss at iteration 90 : 0.10065984725952148
Loss at iteration 100 : 0.05927467346191406
Loss at iteration 110 : 0.04462289810180664
Loss at iteration 120 : 0.05539318546652794
Loss at iteration 130 : 0.04099133983254433
Loss at iteration 140 : 0.11702080816030502
Loss at iteration 150 : 0.07544595003128052
Loss at iteration 160 : 0.0635206401348114
Loss at iteration 170 : 0.08239167183637619
Loss at iteration 180 : 0.09422477334737778
Loss at iteration 190 : 0.05177795886993408
Loss at iteration 200 : 0.05955743044614792
Loss at iteration 210 : 0.06872312724590302
Loss at iteration 220 : 0.06659843772649765
Loss at iteration 230 : 0.11959479749202728
Loss at iteration 240 : 0.09523095935583115
Loss at iteration 250 : 0.11404073238372803
Loss at iteration 260 : 0.11335474252700806
Loss at iteration 270 : 0.08702510595321655
Loss at iteration 280 : 0.08412402868270874
Loss at iteration 290 : 0.10174774378538132
Loss at iteration 300 : 0.087115578353405
Loss at iteration 310 : 0.0624377578496933
Loss at iteration 320 : 0.1018093079328537
Loss at iteration 330 : 0.05686311051249504
Loss at iteration 340 : 0.06343011558055878
Loss at iteration 350 : 0.11373615264892578
Loss at iteration 360 : 0.1143202930688858
Loss at iteration 370 : 0.06565292924642563
Loss at iteration 380 : 0.09553895890712738
Loss at iteration 390 : 0.06551742553710938
Loss at iteration 400 : 0.06574989855289459
Loss at iteration 410 : 0.07176589965820312
Loss at iteration 420 : 0.12620922923088074
Loss at iteration 430 : 0.10515817254781723
Loss at iteration 440 : 0.08644457906484604
Loss at iteration 450 : 0.053943388164043427
Loss at iteration 460 : 0.08994116634130478
Loss at iteration 470 : 0.05797424539923668
Loss at iteration 480 : 0.06575146317481995
Loss at iteration 490 : 0.07919642329216003
Loss at iteration 500 : 0.1054590567946434
Loss at iteration 510 : 0.07750188559293747
Loss at iteration 520 : 0.08279038220643997
Loss at iteration 530 : 0.1145332083106041
Loss at iteration 540 : 0.06825576722621918
Loss at iteration 550 : 0.09388270229101181
Loss at iteration 560 : 0.07968208193778992
Loss at iteration 570 : 0.09356524795293808
Loss at iteration 580 : 0.10041660070419312
Loss at iteration 590 : 0.09784895181655884
Loss at iteration 600 : 0.05764869600534439
Loss at iteration 610 : 0.04923134297132492
Loss at iteration 620 : 0.07845106720924377
Loss at iteration 630 : 0.1280606985092163
Loss at iteration 640 : 0.04801131412386894
Loss at iteration 650 : 0.10154740512371063
Loss at iteration 660 : 0.06651021540164948
Loss at iteration 670 : 0.07819730788469315
Loss at iteration 680 : 0.08032423257827759
Loss at iteration 690 : 0.08928053826093674
Loss at iteration 700 : 0.07259450852870941
Loss at iteration 710 : 0.0546872615814209
Loss at iteration 720 : 0.10926179587841034
Loss at iteration 730 : 0.04527289792895317
Loss at iteration 740 : 0.12346161901950836
Loss at iteration 750 : 0.07379251718521118
Loss at iteration 760 : 0.0974927768111229
Loss at iteration 770 : 0.09225581586360931
Loss at iteration 780 : 0.10204821825027466
Loss at iteration 790 : 0.055857956409454346
Loss at iteration 800 : 0.06937246024608612
Loss at iteration 810 : 0.11053724586963654
Loss at iteration 820 : 0.08719676733016968
Loss at iteration 830 : 0.07603135704994202
Loss at iteration 840 : 0.06611266732215881
Loss at iteration 850 : 0.05242784321308136
Loss at iteration 860 : 0.07403692603111267
Loss at iteration 870 : 0.04983413219451904
Loss at iteration 880 : 0.07703831046819687
Loss at iteration 890 : 0.05667182430624962
Loss at iteration 900 : 0.12705546617507935
Loss at iteration 910 : 0.09357219934463501
Loss at iteration 920 : 0.09648575633764267
Loss at iteration 930 : 0.07004746049642563
Loss at iteration 940 : 0.11613701283931732
Loss at iteration 950 : 0.09207066148519516
Loss at iteration 960 : 0.055285826325416565
Loss at iteration 970 : 0.12929539382457733
Loss at iteration 980 : 0.10167072713375092
Loss at iteration 990 : 0.06997096538543701
Loss at iteration 1000 : 0.09150108695030212
Loss at iteration 1010 : 0.06759878247976303
Loss at iteration 1020 : 0.0898493155837059
Loss at iteration 1030 : 0.06214892119169235
Loss at iteration 1040 : 0.0720290094614029
Loss at iteration 1050 : 0.07810129970312119
Loss at iteration 1060 : 0.069495290517807
Loss at iteration 1070 : 0.09292574226856232
Loss at iteration 1080 : 0.10688269138336182
Loss at iteration 1090 : 0.07777605950832367
Loss at iteration 1100 : 0.08634578436613083
Loss at iteration 1110 : 0.0707002580165863
Loss at iteration 1120 : 0.08634620904922485
Loss at iteration 1130 : 0.10646717250347137
Loss at iteration 1140 : 0.04986188933253288
Loss at iteration 1150 : 0.08922427892684937
Loss at iteration 1160 : 0.061788856983184814
Loss at iteration 1170 : 0.06916847079992294
Loss at iteration 1180 : 0.11001227796077728
Loss at iteration 1190 : 0.09073465317487717
Loss at iteration 1200 : 0.060848113149404526
Loss at iteration 1210 : 0.06505201756954193
The SSIM Value is: 0.7238611817359925
The PSNR Value is: 20.82887331644694
the epoch is: 194
Loss at iteration 10 : 0.1101817786693573
Loss at iteration 20 : 0.07621952146291733
Loss at iteration 30 : 0.06466361880302429
Loss at iteration 40 : 0.08743292093276978
Loss at iteration 50 : 0.0673256367444992
Loss at iteration 60 : 0.06080269813537598
Loss at iteration 70 : 0.0901830792427063
Loss at iteration 80 : 0.15216794610023499
Loss at iteration 90 : 0.07018960267305374
Loss at iteration 100 : 0.11764045804738998
Loss at iteration 110 : 0.07648725807666779
Loss at iteration 120 : 0.06757818162441254
Loss at iteration 130 : 0.07480257749557495
Loss at iteration 140 : 0.05936501547694206
Loss at iteration 150 : 0.05904620140790939
Loss at iteration 160 : 0.059249334037303925
Loss at iteration 170 : 0.11741581559181213
Loss at iteration 180 : 0.07102518528699875
Loss at iteration 190 : 0.09558115899562836
Loss at iteration 200 : 0.09623244404792786
Loss at iteration 210 : 0.07082732766866684
Loss at iteration 220 : 0.12388959527015686
Loss at iteration 230 : 0.040716104209423065
Loss at iteration 240 : 0.056631993502378464
Loss at iteration 250 : 0.06052698940038681
Loss at iteration 260 : 0.05737907811999321
Loss at iteration 270 : 0.057269539684057236
Loss at iteration 280 : 0.09505818784236908
Loss at iteration 290 : 0.08273349702358246
Loss at iteration 300 : 0.1283639371395111
Loss at iteration 310 : 0.07297059148550034
Loss at iteration 320 : 0.09258271753787994
Loss at iteration 330 : 0.10749771445989609
Loss at iteration 340 : 0.04568149894475937
Loss at iteration 350 : 0.09981601685285568
Loss at iteration 360 : 0.10365615785121918
Loss at iteration 370 : 0.08558639883995056
Loss at iteration 380 : 0.07952040433883667
Loss at iteration 390 : 0.085507333278656
Loss at iteration 400 : 0.07131534814834595
Loss at iteration 410 : 0.08570854365825653
Loss at iteration 420 : 0.07999183237552643
Loss at iteration 430 : 0.09692534059286118
Loss at iteration 440 : 0.09677672386169434
Loss at iteration 450 : 0.08822737634181976
Loss at iteration 460 : 0.06997072696685791
Loss at iteration 470 : 0.10242819786071777
Loss at iteration 480 : 0.050043851137161255
Loss at iteration 490 : 0.12452150881290436
Loss at iteration 500 : 0.10641037672758102
Loss at iteration 510 : 0.07477357238531113
Loss at iteration 520 : 0.08534833043813705
Loss at iteration 530 : 0.06127452105283737
Loss at iteration 540 : 0.07127659767866135
Loss at iteration 550 : 0.04488246887922287
Loss at iteration 560 : 0.05728728324174881
Loss at iteration 570 : 0.075201615691185
Loss at iteration 580 : 0.07703837752342224
Loss at iteration 590 : 0.06178027391433716
Loss at iteration 600 : 0.06946342438459396
Loss at iteration 610 : 0.11449681222438812
Loss at iteration 620 : 0.10561611503362656
Loss at iteration 630 : 0.07579708844423294
Loss at iteration 640 : 0.056567274034023285
Loss at iteration 650 : 0.08823084831237793
Loss at iteration 660 : 0.09385814517736435
Loss at iteration 670 : 0.1253376305103302
Loss at iteration 680 : 0.08933495730161667
Loss at iteration 690 : 0.09399765729904175
Loss at iteration 700 : 0.06482716649770737
Loss at iteration 710 : 0.07190993428230286
Loss at iteration 720 : 0.07128138840198517
Loss at iteration 730 : 0.07563101500272751
Loss at iteration 740 : 0.0453663170337677
Loss at iteration 750 : 0.061793893575668335
Loss at iteration 760 : 0.043408870697021484
Loss at iteration 770 : 0.12865358591079712
Loss at iteration 780 : 0.13055351376533508
Loss at iteration 790 : 0.06992896646261215
Loss at iteration 800 : 0.06800787150859833
Loss at iteration 810 : 0.06873312592506409
Loss at iteration 820 : 0.052116863429546356
Loss at iteration 830 : 0.09773703664541245
Loss at iteration 840 : 0.09332697838544846
Loss at iteration 850 : 0.11244382709264755
Loss at iteration 860 : 0.06642942130565643
Loss at iteration 870 : 0.12049566209316254
Loss at iteration 880 : 0.09416436403989792
Loss at iteration 890 : 0.07842755317687988
Loss at iteration 900 : 0.07167667150497437
Loss at iteration 910 : 0.05228018760681152
Loss at iteration 920 : 0.07804437726736069
Loss at iteration 930 : 0.06748487055301666
Loss at iteration 940 : 0.07107682526111603
Loss at iteration 950 : 0.10928291082382202
Loss at iteration 960 : 0.06624415516853333
Loss at iteration 970 : 0.061428096145391464
Loss at iteration 980 : 0.11274876445531845
Loss at iteration 990 : 0.06357883661985397
Loss at iteration 1000 : 0.06306244432926178
Loss at iteration 1010 : 0.07374043762683868
Loss at iteration 1020 : 0.07177819311618805
Loss at iteration 1030 : 0.051394857466220856
Loss at iteration 1040 : 0.07252398133277893
Loss at iteration 1050 : 0.1123351901769638
Loss at iteration 1060 : 0.059746455401182175
Loss at iteration 1070 : 0.10339213907718658
Loss at iteration 1080 : 0.07613879442214966
Loss at iteration 1090 : 0.08538760244846344
Loss at iteration 1100 : 0.11925166845321655
Loss at iteration 1110 : 0.11435084789991379
Loss at iteration 1120 : 0.05005432292819023
Loss at iteration 1130 : 0.08378587663173676
Loss at iteration 1140 : 0.11642780900001526
Loss at iteration 1150 : 0.12198707461357117
Loss at iteration 1160 : 0.09009477496147156
Loss at iteration 1170 : 0.06664769351482391
Loss at iteration 1180 : 0.10092969983816147
Loss at iteration 1190 : 0.06815728545188904
Loss at iteration 1200 : 0.08304431289434433
Loss at iteration 1210 : 0.11734165251255035
The SSIM Value is: 0.719412624835968
The PSNR Value is: 20.823496754964193
the epoch is: 195
Loss at iteration 10 : 0.06045796722173691
Loss at iteration 20 : 0.09694628417491913
Loss at iteration 30 : 0.11475285142660141
Loss at iteration 40 : 0.06281646341085434
Loss at iteration 50 : 0.09095145761966705
Loss at iteration 60 : 0.08431744575500488
Loss at iteration 70 : 0.07926627993583679
Loss at iteration 80 : 0.0779775008559227
Loss at iteration 90 : 0.05944354832172394
Loss at iteration 100 : 0.06939904391765594
Loss at iteration 110 : 0.10125595331192017
Loss at iteration 120 : 0.07703347504138947
Loss at iteration 130 : 0.05931076407432556
Loss at iteration 140 : 0.04603840038180351
Loss at iteration 150 : 0.07819119840860367
Loss at iteration 160 : 0.05636921525001526
Loss at iteration 170 : 0.11670796573162079
Loss at iteration 180 : 0.13719744980335236
Loss at iteration 190 : 0.03812285512685776
Loss at iteration 200 : 0.08566688001155853
Loss at iteration 210 : 0.08826330304145813
Loss at iteration 220 : 0.07544168084859848
Loss at iteration 230 : 0.10396718233823776
Loss at iteration 240 : 0.1300174593925476
Loss at iteration 250 : 0.1078069657087326
Loss at iteration 260 : 0.043070316314697266
Loss at iteration 270 : 0.10510276257991791
Loss at iteration 280 : 0.0671551525592804
Loss at iteration 290 : 0.07356777787208557
Loss at iteration 300 : 0.09036269038915634
Loss at iteration 310 : 0.09619311988353729
Loss at iteration 320 : 0.06309285014867783
Loss at iteration 330 : 0.04999711364507675
Loss at iteration 340 : 0.03916102647781372
Loss at iteration 350 : 0.1170155256986618
Loss at iteration 360 : 0.05421210452914238
Loss at iteration 370 : 0.06833378970623016
Loss at iteration 380 : 0.06636454910039902
Loss at iteration 390 : 0.0799407958984375
Loss at iteration 400 : 0.08906251192092896
Loss at iteration 410 : 0.07154593616724014
Loss at iteration 420 : 0.05988004058599472
Loss at iteration 430 : 0.050529446452856064
Loss at iteration 440 : 0.05142030864953995
Loss at iteration 450 : 0.09972556680440903
Loss at iteration 460 : 0.0920981764793396
Loss at iteration 470 : 0.0558449849486351
Loss at iteration 480 : 0.04019150137901306
Loss at iteration 490 : 0.10390877723693848
Loss at iteration 500 : 0.05242785066366196
Loss at iteration 510 : 0.10435017198324203
Loss at iteration 520 : 0.1155814453959465
Loss at iteration 530 : 0.08308839052915573
Loss at iteration 540 : 0.061889227479696274
Loss at iteration 550 : 0.0947522521018982
Loss at iteration 560 : 0.1102885752916336
Loss at iteration 570 : 0.06199733540415764
Loss at iteration 580 : 0.0775638148188591
Loss at iteration 590 : 0.0873788595199585
Loss at iteration 600 : 0.11834196746349335
Loss at iteration 610 : 0.06347998976707458
Loss at iteration 620 : 0.11149132251739502
Loss at iteration 630 : 0.07073920220136642
Loss at iteration 640 : 0.07848435640335083
Loss at iteration 650 : 0.05371860787272453
Loss at iteration 660 : 0.12174910306930542
Loss at iteration 670 : 0.05531098693609238
Loss at iteration 680 : 0.0864056870341301
Loss at iteration 690 : 0.07902087271213531
Loss at iteration 700 : 0.08341263234615326
Loss at iteration 710 : 0.16082057356834412
Loss at iteration 720 : 0.09419877827167511
Loss at iteration 730 : 0.07699119299650192
Loss at iteration 740 : 0.10362713038921356
Loss at iteration 750 : 0.057378314435482025
Loss at iteration 760 : 0.06773236393928528
Loss at iteration 770 : 0.10828489065170288
Loss at iteration 780 : 0.05160977691411972
Loss at iteration 790 : 0.06320370733737946
Loss at iteration 800 : 0.11454406380653381
Loss at iteration 810 : 0.06875784695148468
Loss at iteration 820 : 0.09801309555768967
Loss at iteration 830 : 0.08481280505657196
Loss at iteration 840 : 0.051751554012298584
Loss at iteration 850 : 0.11031334102153778
Loss at iteration 860 : 0.08759903907775879
Loss at iteration 870 : 0.06758619844913483
Loss at iteration 880 : 0.06537207961082458
Loss at iteration 890 : 0.06585662066936493
Loss at iteration 900 : 0.047888122498989105
Loss at iteration 910 : 0.09113448858261108
Loss at iteration 920 : 0.0743960589170456
Loss at iteration 930 : 0.12226252257823944
Loss at iteration 940 : 0.06391189992427826
Loss at iteration 950 : 0.097702756524086
Loss at iteration 960 : 0.0999533161520958
Loss at iteration 970 : 0.08016744256019592
Loss at iteration 980 : 0.09206163138151169
Loss at iteration 990 : 0.058665931224823
Loss at iteration 1000 : 0.076390340924263
Loss at iteration 1010 : 0.10422062873840332
Loss at iteration 1020 : 0.10870395600795746
Loss at iteration 1030 : 0.05511332303285599
Loss at iteration 1040 : 0.07403327524662018
Loss at iteration 1050 : 0.0692962184548378
Loss at iteration 1060 : 0.043311476707458496
Loss at iteration 1070 : 0.07260994613170624
Loss at iteration 1080 : 0.08003409951925278
Loss at iteration 1090 : 0.0930284857749939
Loss at iteration 1100 : 0.10034944117069244
Loss at iteration 1110 : 0.08280055224895477
Loss at iteration 1120 : 0.07109449058771133
Loss at iteration 1130 : 0.06908372044563293
Loss at iteration 1140 : 0.05597082898020744
Loss at iteration 1150 : 0.05277585983276367
Loss at iteration 1160 : 0.08490701019763947
Loss at iteration 1170 : 0.0851396918296814
Loss at iteration 1180 : 0.06965596973896027
Loss at iteration 1190 : 0.08835899829864502
Loss at iteration 1200 : 0.06727283447980881
Loss at iteration 1210 : 0.08173395693302155
The SSIM Value is: 0.7201602856318156
The PSNR Value is: 20.648898442586262
the epoch is: 196
Loss at iteration 10 : 0.07535465061664581
Loss at iteration 20 : 0.0836702287197113
Loss at iteration 30 : 0.11815772950649261
Loss at iteration 40 : 0.10139596462249756
Loss at iteration 50 : 0.07600873708724976
Loss at iteration 60 : 0.09359318017959595
Loss at iteration 70 : 0.06605984270572662
Loss at iteration 80 : 0.1286090612411499
Loss at iteration 90 : 0.0443972572684288
Loss at iteration 100 : 0.07836748659610748
Loss at iteration 110 : 0.062334172427654266
Loss at iteration 120 : 0.09468298405408859
Loss at iteration 130 : 0.057452425360679626
Loss at iteration 140 : 0.06613506376743317
Loss at iteration 150 : 0.11328504979610443
Loss at iteration 160 : 0.11348282545804977
Loss at iteration 170 : 0.08120439946651459
Loss at iteration 180 : 0.06752040982246399
Loss at iteration 190 : 0.07702131569385529
Loss at iteration 200 : 0.08160051703453064
Loss at iteration 210 : 0.10952906310558319
Loss at iteration 220 : 0.09166215360164642
Loss at iteration 230 : 0.07066185772418976
Loss at iteration 240 : 0.0631103590130806
Loss at iteration 250 : 0.10312120616436005
Loss at iteration 260 : 0.11191427707672119
Loss at iteration 270 : 0.11697477102279663
Loss at iteration 280 : 0.08232098072767258
Loss at iteration 290 : 0.12548188865184784
Loss at iteration 300 : 0.10280661284923553
Loss at iteration 310 : 0.0736791342496872
Loss at iteration 320 : 0.08046011626720428
Loss at iteration 330 : 0.06880176067352295
Loss at iteration 340 : 0.07691778987646103
Loss at iteration 350 : 0.04917933791875839
Loss at iteration 360 : 0.04424121975898743
Loss at iteration 370 : 0.05228102207183838
Loss at iteration 380 : 0.08242256939411163
Loss at iteration 390 : 0.0815696269273758
Loss at iteration 400 : 0.07655169814825058
Loss at iteration 410 : 0.06809481978416443
Loss at iteration 420 : 0.08597823232412338
Loss at iteration 430 : 0.07010053098201752
Loss at iteration 440 : 0.0623803436756134
Loss at iteration 450 : 0.06459049880504608
Loss at iteration 460 : 0.07852722704410553
Loss at iteration 470 : 0.07386133074760437
Loss at iteration 480 : 0.0949663445353508
Loss at iteration 490 : 0.0821528434753418
Loss at iteration 500 : 0.07996203005313873
Loss at iteration 510 : 0.07660618424415588
Loss at iteration 520 : 0.17282792925834656
Loss at iteration 530 : 0.06402202695608139
Loss at iteration 540 : 0.1113213300704956
Loss at iteration 550 : 0.06654922664165497
Loss at iteration 560 : 0.07991620153188705
Loss at iteration 570 : 0.08925195038318634
Loss at iteration 580 : 0.06721135228872299
Loss at iteration 590 : 0.10439540445804596
Loss at iteration 600 : 0.09300099313259125
Loss at iteration 610 : 0.09033813327550888
Loss at iteration 620 : 0.0911375880241394
Loss at iteration 630 : 0.05704271048307419
Loss at iteration 640 : 0.05781354755163193
Loss at iteration 650 : 0.12723152339458466
Loss at iteration 660 : 0.14570891857147217
Loss at iteration 670 : 0.06963574886322021
Loss at iteration 680 : 0.09427918493747711
Loss at iteration 690 : 0.0962718278169632
Loss at iteration 700 : 0.09841656684875488
Loss at iteration 710 : 0.04369378834962845
Loss at iteration 720 : 0.08341781795024872
Loss at iteration 730 : 0.08475353568792343
Loss at iteration 740 : 0.09105759859085083
Loss at iteration 750 : 0.07180394232273102
Loss at iteration 760 : 0.09028343856334686
Loss at iteration 770 : 0.07541017234325409
Loss at iteration 780 : 0.11443015187978745
Loss at iteration 790 : 0.08421671390533447
Loss at iteration 800 : 0.06936365365982056
Loss at iteration 810 : 0.09640708565711975
Loss at iteration 820 : 0.06062586233019829
Loss at iteration 830 : 0.06064441055059433
Loss at iteration 840 : 0.07774519175291061
Loss at iteration 850 : 0.09863146394491196
Loss at iteration 860 : 0.10264792293310165
Loss at iteration 870 : 0.08628851175308228
Loss at iteration 880 : 0.06451520323753357
Loss at iteration 890 : 0.10992966592311859
Loss at iteration 900 : 0.108917735517025
Loss at iteration 910 : 0.11846613883972168
Loss at iteration 920 : 0.05981878936290741
Loss at iteration 930 : 0.07771097123622894
Loss at iteration 940 : 0.07549390196800232
Loss at iteration 950 : 0.0843842625617981
Loss at iteration 960 : 0.05285419151186943
Loss at iteration 970 : 0.07309387624263763
Loss at iteration 980 : 0.08823172748088837
Loss at iteration 990 : 0.0735611543059349
Loss at iteration 1000 : 0.06807821989059448
Loss at iteration 1010 : 0.09168093651533127
Loss at iteration 1020 : 0.05082971975207329
Loss at iteration 1030 : 0.11273255199193954
Loss at iteration 1040 : 0.15854933857917786
Loss at iteration 1050 : 0.08842800557613373
Loss at iteration 1060 : 0.0706196129322052
Loss at iteration 1070 : 0.12080030143260956
Loss at iteration 1080 : 0.13366737961769104
Loss at iteration 1090 : 0.1065959557890892
Loss at iteration 1100 : 0.08906715363264084
Loss at iteration 1110 : 0.08579881489276886
Loss at iteration 1120 : 0.07008093595504761
Loss at iteration 1130 : 0.09668945521116257
Loss at iteration 1140 : 0.10486006736755371
Loss at iteration 1150 : 0.0826093852519989
Loss at iteration 1160 : 0.06555019319057465
Loss at iteration 1170 : 0.0814238116145134
Loss at iteration 1180 : 0.11957164853811264
Loss at iteration 1190 : 0.06664858758449554
Loss at iteration 1200 : 0.08049191534519196
Loss at iteration 1210 : 0.11093981564044952
The SSIM Value is: 0.7184677640597026
The PSNR Value is: 20.384822209676106
the epoch is: 197
Loss at iteration 10 : 0.07526946067810059
Loss at iteration 20 : 0.12468630075454712
Loss at iteration 30 : 0.07521995157003403
Loss at iteration 40 : 0.10074525326490402
Loss at iteration 50 : 0.07354710251092911
Loss at iteration 60 : 0.08472554385662079
Loss at iteration 70 : 0.09227822721004486
Loss at iteration 80 : 0.06949122250080109
Loss at iteration 90 : 0.11662354320287704
Loss at iteration 100 : 0.07246309518814087
Loss at iteration 110 : 0.0852893516421318
Loss at iteration 120 : 0.05623486638069153
Loss at iteration 130 : 0.11517907679080963
Loss at iteration 140 : 0.06009746715426445
Loss at iteration 150 : 0.13838209211826324
Loss at iteration 160 : 0.06304117292165756
Loss at iteration 170 : 0.07693317532539368
Loss at iteration 180 : 0.07768581062555313
Loss at iteration 190 : 0.14342837035655975
Loss at iteration 200 : 0.09155148267745972
Loss at iteration 210 : 0.0701872929930687
Loss at iteration 220 : 0.0665624588727951
Loss at iteration 230 : 0.10570834577083588
Loss at iteration 240 : 0.04828494042158127
Loss at iteration 250 : 0.06534755229949951
Loss at iteration 260 : 0.0847410187125206
Loss at iteration 270 : 0.08734387159347534
Loss at iteration 280 : 0.06280738860368729
Loss at iteration 290 : 0.11439694464206696
Loss at iteration 300 : 0.11208104342222214
Loss at iteration 310 : 0.10258394479751587
Loss at iteration 320 : 0.10972549766302109
Loss at iteration 330 : 0.07599969208240509
Loss at iteration 340 : 0.08722424507141113
Loss at iteration 350 : 0.12384651601314545
Loss at iteration 360 : 0.056540340185165405
Loss at iteration 370 : 0.08726568520069122
Loss at iteration 380 : 0.10016938298940659
Loss at iteration 390 : 0.12134357541799545
Loss at iteration 400 : 0.14668160676956177
Loss at iteration 410 : 0.13489864766597748
Loss at iteration 420 : 0.06551873683929443
Loss at iteration 430 : 0.050965674221515656
Loss at iteration 440 : 0.045009858906269073
Loss at iteration 450 : 0.10697021335363388
Loss at iteration 460 : 0.05673912912607193
Loss at iteration 470 : 0.089322030544281
Loss at iteration 480 : 0.07150083780288696
Loss at iteration 490 : 0.09156553447246552
Loss at iteration 500 : 0.05686698108911514
Loss at iteration 510 : 0.08087752759456635
Loss at iteration 520 : 0.12370793521404266
Loss at iteration 530 : 0.08537930250167847
Loss at iteration 540 : 0.06232406198978424
Loss at iteration 550 : 0.07099582999944687
Loss at iteration 560 : 0.0732952281832695
Loss at iteration 570 : 0.13640505075454712
Loss at iteration 580 : 0.06239659711718559
Loss at iteration 590 : 0.08201682567596436
Loss at iteration 600 : 0.04979005455970764
Loss at iteration 610 : 0.08854975551366806
Loss at iteration 620 : 0.08527718484401703
Loss at iteration 630 : 0.060163408517837524
Loss at iteration 640 : 0.07100684940814972
Loss at iteration 650 : 0.07728137820959091
Loss at iteration 660 : 0.043504923582077026
Loss at iteration 670 : 0.10471007227897644
Loss at iteration 680 : 0.11747714132070541
Loss at iteration 690 : 0.07556070387363434
Loss at iteration 700 : 0.059492915868759155
Loss at iteration 710 : 0.0898771584033966
Loss at iteration 720 : 0.07341072708368301
Loss at iteration 730 : 0.12626288831233978
Loss at iteration 740 : 0.10919978469610214
Loss at iteration 750 : 0.08191373944282532
Loss at iteration 760 : 0.1357518434524536
Loss at iteration 770 : 0.05989938974380493
Loss at iteration 780 : 0.10545804351568222
Loss at iteration 790 : 0.06451472640037537
Loss at iteration 800 : 0.06091272085905075
Loss at iteration 810 : 0.08964121341705322
Loss at iteration 820 : 0.10368625819683075
Loss at iteration 830 : 0.08986295759677887
Loss at iteration 840 : 0.08897028863430023
Loss at iteration 850 : 0.07360365986824036
Loss at iteration 860 : 0.06465302407741547
Loss at iteration 870 : 0.0644134134054184
Loss at iteration 880 : 0.06828957796096802
Loss at iteration 890 : 0.07214261591434479
Loss at iteration 900 : 0.09724919497966766
Loss at iteration 910 : 0.09367229044437408
Loss at iteration 920 : 0.06399703025817871
Loss at iteration 930 : 0.0729583352804184
Loss at iteration 940 : 0.08224503695964813
Loss at iteration 950 : 0.08339562267065048
Loss at iteration 960 : 0.07104223221540451
Loss at iteration 970 : 0.11802934110164642
Loss at iteration 980 : 0.0681648775935173
Loss at iteration 990 : 0.056960176676511765
Loss at iteration 1000 : 0.08665170520544052
Loss at iteration 1010 : 0.04196189343929291
Loss at iteration 1020 : 0.06754690408706665
Loss at iteration 1030 : 0.06931248307228088
Loss at iteration 1040 : 0.07925665378570557
Loss at iteration 1050 : 0.07146362960338593
Loss at iteration 1060 : 0.06022047996520996
Loss at iteration 1070 : 0.08316496014595032
Loss at iteration 1080 : 0.0610930398106575
Loss at iteration 1090 : 0.10378142446279526
Loss at iteration 1100 : 0.0856068953871727
Loss at iteration 1110 : 0.08626441657543182
Loss at iteration 1120 : 0.06965357065200806
Loss at iteration 1130 : 0.05709974095225334
Loss at iteration 1140 : 0.08215322345495224
Loss at iteration 1150 : 0.04905986785888672
Loss at iteration 1160 : 0.07774774730205536
Loss at iteration 1170 : 0.10522448271512985
Loss at iteration 1180 : 0.09795662760734558
Loss at iteration 1190 : 0.07383882254362106
Loss at iteration 1200 : 0.1272878497838974
Loss at iteration 1210 : 0.06693582981824875
The SSIM Value is: 0.7249901334444682
The PSNR Value is: 20.978129959106447
the epoch is: 198
Loss at iteration 10 : 0.08246034383773804
Loss at iteration 20 : 0.12057998031377792
Loss at iteration 30 : 0.06820400804281235
Loss at iteration 40 : 0.09900297969579697
Loss at iteration 50 : 0.11046218872070312
Loss at iteration 60 : 0.12945419549942017
Loss at iteration 70 : 0.09626265615224838
Loss at iteration 80 : 0.06081449240446091
Loss at iteration 90 : 0.059894733130931854
Loss at iteration 100 : 0.09328320622444153
Loss at iteration 110 : 0.11237609386444092
Loss at iteration 120 : 0.11527208983898163
Loss at iteration 130 : 0.06377831101417542
Loss at iteration 140 : 0.08280782401561737
Loss at iteration 150 : 0.06857286393642426
Loss at iteration 160 : 0.07024271786212921
Loss at iteration 170 : 0.04676259681582451
Loss at iteration 180 : 0.0622502863407135
Loss at iteration 190 : 0.06702947616577148
Loss at iteration 200 : 0.0611211359500885
Loss at iteration 210 : 0.04415444657206535
Loss at iteration 220 : 0.07526248693466187
Loss at iteration 230 : 0.12261171638965607
Loss at iteration 240 : 0.09424237906932831
Loss at iteration 250 : 0.05608110874891281
Loss at iteration 260 : 0.09590031951665878
Loss at iteration 270 : 0.10031630843877792
Loss at iteration 280 : 0.060762207955121994
Loss at iteration 290 : 0.05939812585711479
Loss at iteration 300 : 0.08193209022283554
Loss at iteration 310 : 0.0783599242568016
Loss at iteration 320 : 0.0936380922794342
Loss at iteration 330 : 0.10370120406150818
Loss at iteration 340 : 0.08948277682065964
Loss at iteration 350 : 0.11816897243261337
Loss at iteration 360 : 0.08859527111053467
Loss at iteration 370 : 0.08489420264959335
Loss at iteration 380 : 0.0800207331776619
Loss at iteration 390 : 0.1303771585226059
Loss at iteration 400 : 0.06747999042272568
Loss at iteration 410 : 0.05491819977760315
Loss at iteration 420 : 0.08453943580389023
Loss at iteration 430 : 0.1315191090106964
Loss at iteration 440 : 0.10918541252613068
Loss at iteration 450 : 0.07923653721809387
Loss at iteration 460 : 0.0767144113779068
Loss at iteration 470 : 0.11292120814323425
Loss at iteration 480 : 0.10649287700653076
Loss at iteration 490 : 0.07522948086261749
Loss at iteration 500 : 0.12952356040477753
Loss at iteration 510 : 0.07192504405975342
Loss at iteration 520 : 0.05667304992675781
Loss at iteration 530 : 0.07304375618696213
Loss at iteration 540 : 0.10564305633306503
Loss at iteration 550 : 0.07725691795349121
Loss at iteration 560 : 0.10293066501617432
Loss at iteration 570 : 0.07952549308538437
Loss at iteration 580 : 0.08085666596889496
Loss at iteration 590 : 0.10726787149906158
Loss at iteration 600 : 0.11094444990158081
Loss at iteration 610 : 0.09941156953573227
Loss at iteration 620 : 0.07879777252674103
Loss at iteration 630 : 0.08362090587615967
Loss at iteration 640 : 0.05609259754419327
Loss at iteration 650 : 0.08873331546783447
Loss at iteration 660 : 0.07084260135889053
Loss at iteration 670 : 0.09350105375051498
Loss at iteration 680 : 0.06586375087499619
Loss at iteration 690 : 0.09341421723365784
Loss at iteration 700 : 0.12752020359039307
Loss at iteration 710 : 0.04971305653452873
Loss at iteration 720 : 0.08992581814527512
Loss at iteration 730 : 0.08207736164331436
Loss at iteration 740 : 0.05820610374212265
Loss at iteration 750 : 0.046543508768081665
Loss at iteration 760 : 0.08467327058315277
Loss at iteration 770 : 0.08076688647270203
Loss at iteration 780 : 0.0418611578643322
Loss at iteration 790 : 0.1076873168349266
Loss at iteration 800 : 0.06293424218893051
Loss at iteration 810 : 0.05899805948138237
Loss at iteration 820 : 0.077365443110466
Loss at iteration 830 : 0.07504453510046005
Loss at iteration 840 : 0.06315427273511887
Loss at iteration 850 : 0.08283187448978424
Loss at iteration 860 : 0.11128183454275131
Loss at iteration 870 : 0.09899994730949402
Loss at iteration 880 : 0.05649104341864586
Loss at iteration 890 : 0.061490654945373535
Loss at iteration 900 : 0.12578003108501434
Loss at iteration 910 : 0.11880673468112946
Loss at iteration 920 : 0.12753701210021973
Loss at iteration 930 : 0.0697806179523468
Loss at iteration 940 : 0.10737186670303345
Loss at iteration 950 : 0.06852605938911438
Loss at iteration 960 : 0.08380156010389328
Loss at iteration 970 : 0.0789213478565216
Loss at iteration 980 : 0.1109323725104332
Loss at iteration 990 : 0.07020968943834305
Loss at iteration 1000 : 0.10933566093444824
Loss at iteration 1010 : 0.06673771142959595
Loss at iteration 1020 : 0.08517318964004517
Loss at iteration 1030 : 0.08051234483718872
Loss at iteration 1040 : 0.07645168900489807
Loss at iteration 1050 : 0.06527334451675415
Loss at iteration 1060 : 0.09706389904022217
Loss at iteration 1070 : 0.056074656546115875
Loss at iteration 1080 : 0.11070015281438828
Loss at iteration 1090 : 0.0691642016172409
Loss at iteration 1100 : 0.03917068988084793
Loss at iteration 1110 : 0.06295141577720642
Loss at iteration 1120 : 0.08481114357709885
Loss at iteration 1130 : 0.097457155585289
Loss at iteration 1140 : 0.11583461612462997
Loss at iteration 1150 : 0.07497388124465942
Loss at iteration 1160 : 0.07452322542667389
Loss at iteration 1170 : 0.0855303630232811
Loss at iteration 1180 : 0.08274568617343903
Loss at iteration 1190 : 0.07209163904190063
Loss at iteration 1200 : 0.06863069534301758
Loss at iteration 1210 : 0.06740808486938477
The SSIM Value is: 0.7191490868727366
The PSNR Value is: 20.78293628692627
the epoch is: 199
Loss at iteration 10 : 0.11445735394954681
Loss at iteration 20 : 0.09423211216926575
Loss at iteration 30 : 0.1057768389582634
Loss at iteration 40 : 0.07416535913944244
Loss at iteration 50 : 0.08874034136533737
Loss at iteration 60 : 0.108728788793087
Loss at iteration 70 : 0.07645563781261444
Loss at iteration 80 : 0.1025020107626915
Loss at iteration 90 : 0.07400110363960266
Loss at iteration 100 : 0.07923658937215805
Loss at iteration 110 : 0.06345738470554352
Loss at iteration 120 : 0.11190176010131836
Loss at iteration 130 : 0.08889758586883545
Loss at iteration 140 : 0.087083600461483
Loss at iteration 150 : 0.05879410356283188
Loss at iteration 160 : 0.07943092286586761
Loss at iteration 170 : 0.054795827716588974
Loss at iteration 180 : 0.03923812136054039
Loss at iteration 190 : 0.10579704493284225
Loss at iteration 200 : 0.07250283658504486
Loss at iteration 210 : 0.08114127069711685
Loss at iteration 220 : 0.05228334665298462
Loss at iteration 230 : 0.07807803153991699
Loss at iteration 240 : 0.08146502822637558
Loss at iteration 250 : 0.07417047768831253
Loss at iteration 260 : 0.07516898214817047
Loss at iteration 270 : 0.09393426030874252
Loss at iteration 280 : 0.10080757737159729
Loss at iteration 290 : 0.0978403091430664
Loss at iteration 300 : 0.06736159324645996
Loss at iteration 310 : 0.07252964377403259
Loss at iteration 320 : 0.097443126142025
Loss at iteration 330 : 0.13617345690727234
Loss at iteration 340 : 0.04951516538858414
Loss at iteration 350 : 0.13573522865772247
Loss at iteration 360 : 0.0698409229516983
Loss at iteration 370 : 0.08012203127145767
Loss at iteration 380 : 0.08838997781276703
Loss at iteration 390 : 0.05608642101287842
Loss at iteration 400 : 0.07096897065639496
Loss at iteration 410 : 0.07805736362934113
Loss at iteration 420 : 0.0651833713054657
Loss at iteration 430 : 0.068233922123909
Loss at iteration 440 : 0.07373470067977905
Loss at iteration 450 : 0.043947890400886536
Loss at iteration 460 : 0.11657708883285522
Loss at iteration 470 : 0.12176185846328735
Loss at iteration 480 : 0.06821035593748093
Loss at iteration 490 : 0.07055075466632843
Loss at iteration 500 : 0.06693053245544434
Loss at iteration 510 : 0.07091666758060455
Loss at iteration 520 : 0.1020447313785553
Loss at iteration 530 : 0.06996088474988937
Loss at iteration 540 : 0.06239995360374451
Loss at iteration 550 : 0.10498188436031342
Loss at iteration 560 : 0.07467418164014816
Loss at iteration 570 : 0.07898680865764618
Loss at iteration 580 : 0.08198080956935883
Loss at iteration 590 : 0.0699564591050148
Loss at iteration 600 : 0.09215521812438965
Loss at iteration 610 : 0.0705607682466507
Loss at iteration 620 : 0.07436438649892807
Loss at iteration 630 : 0.06726835668087006
Loss at iteration 640 : 0.0765879675745964
Loss at iteration 650 : 0.04769179970026016
Loss at iteration 660 : 0.09721158444881439
Loss at iteration 670 : 0.08608143776655197
Loss at iteration 680 : 0.10923847556114197
Loss at iteration 690 : 0.07575380802154541
Loss at iteration 700 : 0.08773799240589142
Loss at iteration 710 : 0.09215688705444336
Loss at iteration 720 : 0.05790714547038078
Loss at iteration 730 : 0.05635104700922966
Loss at iteration 740 : 0.07163500785827637
Loss at iteration 750 : 0.05453544482588768
Loss at iteration 760 : 0.12833838164806366
Loss at iteration 770 : 0.07236939668655396
Loss at iteration 780 : 0.10169368982315063
Loss at iteration 790 : 0.05256574973464012
Loss at iteration 800 : 0.059166111052036285
Loss at iteration 810 : 0.10905653238296509
Loss at iteration 820 : 0.04973075911402702
Loss at iteration 830 : 0.13769398629665375
Loss at iteration 840 : 0.07620255649089813
Loss at iteration 850 : 0.04863684996962547
Loss at iteration 860 : 0.07183018326759338
Loss at iteration 870 : 0.07022644579410553
Loss at iteration 880 : 0.08073841035366058
Loss at iteration 890 : 0.08897076547145844
Loss at iteration 900 : 0.0511058047413826
Loss at iteration 910 : 0.0885213166475296
Loss at iteration 920 : 0.08122695982456207
Loss at iteration 930 : 0.07109330594539642
Loss at iteration 940 : 0.10869848728179932
Loss at iteration 950 : 0.09232072532176971
Loss at iteration 960 : 0.09428904950618744
Loss at iteration 970 : 0.06491009891033173
Loss at iteration 980 : 0.06765148043632507
Loss at iteration 990 : 0.08261607587337494
Loss at iteration 1000 : 0.0680779218673706
Loss at iteration 1010 : 0.07448703050613403
Loss at iteration 1020 : 0.07267032563686371
Loss at iteration 1030 : 0.07218453288078308
Loss at iteration 1040 : 0.05870094150304794
Loss at iteration 1050 : 0.08586692810058594
Loss at iteration 1060 : 0.05719601362943649
Loss at iteration 1070 : 0.06202363222837448
Loss at iteration 1080 : 0.08499278128147125
Loss at iteration 1090 : 0.09320619702339172
Loss at iteration 1100 : 0.1180037260055542
Loss at iteration 1110 : 0.08983908593654633
Loss at iteration 1120 : 0.12104358524084091
Loss at iteration 1130 : 0.10095643997192383
Loss at iteration 1140 : 0.08412632346153259
Loss at iteration 1150 : 0.07894354313611984
Loss at iteration 1160 : 0.08929590880870819
Loss at iteration 1170 : 0.08897184580564499
Loss at iteration 1180 : 0.10042794048786163
Loss at iteration 1190 : 0.09799109399318695
Loss at iteration 1200 : 0.114630326628685
Loss at iteration 1210 : 0.08575329184532166
The SSIM Value is: 0.7205751180648804
The PSNR Value is: 20.807918802897134
